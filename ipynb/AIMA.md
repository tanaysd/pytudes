Planning and Acting in the Real World {#advanced-planning-chapter}
=====================================

[plan-execution-chapter]

The previous chapter introduced the most basic concepts,
representations, and algorithms for planning. Planners that are are used
in the real world for planning and scheduling the operations of
spacecraft, factories, and military campaigns are more complex; they
extend both the representation language and the way the planner
interacts with the environment. This chapter shows how. extends the
classical language for planning to talk about actions with durations and
resource constraints. describes methods for constructing plans that are
organized hierarchically. This allows human experts to communicate to
the planner what they know about how to solve the problem. Hierarchy
also lends itself to efficient plan construction because the planner can
solve a problem at an abstract level before delving into details.
presents agent architectures that can handle uncertain environments and
interleave deliberation with execution, and gives some examples of
real-world systems. shows how to plan when the environment contains
other agents.

Time, Schedules, and Resources {#resource-section}
------------------------------

[scheduling-section]

The classical planning representation talks about *what to
do*, and in *what order*, but the representation
cannot talk about time: *how long* an action takes and
*when* it occurs. For example, the planners of could
produce a schedule for an airline that says which planes are assigned to
which flights, but we really need to know departure and arrival times as
well. This is the subject matter of . The real world also imposes many ;
for example, an airline has a limited number of staff—and staff who are
on one flight cannot be on another at the same time. This section covers
methods for representing and solving planning problems that include
temporal and resource constraints.

The approach we take in this section is “plan first, schedule later”:
that is, we divide the overall problem into a *planning*
phase in which actions are selected, with some ordering constraints, to
meet the goals of the problem, and a later *scheduling*
phase, in which temporal information is added to the plan to ensure that
it meets resource and deadline constraints. This approach is common in
real-world manufacturing and logistical settings, where the planning
phase is often performed by human experts. The automated methods of can
also be used for the planning phase, provided that they produce plans
with just the minimal ordering constraints required for correctness. (),
(), and partial-order planners () can do this; search-based methods ()
produce totally ordered plans, but these can easily be converted to
plans with minimal ordering constraints.

### Representing temporal and resource constraints

A typical , as first introduced in , consists of a set of , each of
which consists a collection of with ordering constraints among them.
Each action has a and a set of resource constraints required by the
action. Each constraint specifies a *type* of resource
(e.g., bolts, wrenches, or pilots), the number of that resource
required, and whether that resource is (e.g., the bolts are no longer
available for use) or (e.g., a pilot is occupied during a flight but is
available again when the flight is over). Resources can also be
*produced* by actions with negative consumption, including
manufacturing, growing, and resupply actions. A solution to a job-shop
scheduling problem must specify the start times for each action and must
satisfy all the temporal ordering constraints and resource constraints.
As with search and planning problems, solutions can be evaluated
according to a cost function; this can be quite complicated, with
nonlinear resource costs, time-dependent delay costs, and so on. For
simplicity, we assume that the cost function is just the total duration
of the plan, which is called the .

[jobshop3-figure]

shows a simple example: a problem involving the assembly of two cars.
The problem consists of two jobs, each of the form
$[{AddEngine}, {AddWheels}, {Inspect}]$. Then the ${Resources}$
statement declares that there are four types of resources, and gives the
number of each type available at the start: 1 engine hoist, 1 wheel
station, 2 inspectors, and 500 lug nuts. The action schemas give the
duration and resource needs of each action. The lug nuts are
*consumed* as wheels are added to the car, whereas the
other resources are “borrowed” at the start of an action and released at
the action’s end.

The representation of resources as numerical quantities, such as
${Inspectors}(2)$, rather than as named entities, such as
${Inspector}(I_1)$ and ${Inspector}(I_2)$, is an example of a very
general technique called . The central idea of aggregation is to group
individual objects into quantities when the objects are all
indistinguishable with respect to the purpose at hand. In our assembly
problem, it does not matter *which* inspector inspects the
car, so there is no need to make the distinction. (The same idea works
in the missionaries-and-cannibals problem in .) Aggregation is essential
for reducing complexity. Consider what happens when a proposed schedule
has 10 concurrent ${Inspect}$ actions but only 9 inspectors are
available. With inspectors represented as quantities, a failure is
detected immediately and the algorithm backtracks to try another
schedule. With inspectors represented as individuals, the algorithm
backtracks to try all ${10}!$ ways of assigning inspectors to actions.

### Solving scheduling problems

We begin by considering just the temporal scheduling problem, ignoring
resource constraints. To minimize makespan (plan duration), we must find
the earliest start times for all the actions consistent with the
ordering constraints supplied with the problem. It is helpful to view
these ordering constraints as a directed graph relating the actions, as
shown in . We can apply the (CPM) to this graph to determine the
possible start and end times of each action. A through a graph
representing a partial-order plan is a linearly ordered sequence of
actions beginning with ${Start}$ and ending with ${Finish}$. (For
example, there are two paths in the partial-order plan in .)

[jobshop-cpm-figure]

The is that path whose total duration is longest; the path is “critical”
because it determines the duration of the entire plan—shortening other
paths doesn’t shorten the plan as a whole, but delaying the start of any
action on the critical path slows down the whole plan. Actions that are
off the critical path have a window of time in which they can be
executed. The window is specified in terms of an earliest possible start
time, ${ES}$, and a latest possible start time, ${LS}$. The quantity
${LS}$ – ${ES}$ is known as the of an action. We can see in that the
whole plan will take 85 minutes, that each action in the top job has 15
minutes of slack, and that each action on the critical path has no slack
(by definition). Together the ${ES}$ and ${LS}$ times for all the
actions constitute a for the problem.

The following formulas serve as a definition for ${ES}$ and ${LS}$
and also as the outline of a dynamic-programming algorithm to compute
them. $A$ and $B$ are actions, and ${A}\before{B}$ means that $A$ comes
before $B$:

() = 0\
(B) = ~A~(A) + (A)\
() = ()\
(A) = ~B~(B) - (A) .

The idea is that we start by assigning ${ES}({Start})$ to be $0$.
Then, as soon as we get an action $B$ such that all the actions that
come immediately before $B$ have ${ES}$ values assigned, we set
${ES}(B)$ to be the maximum of the earliest finish times of those
immediately preceding actions, where the earliest finish time of an
action is defined as the earliest start time plus the duration. This
process repeats until every action has been assigned an ${ES}$ value.
The ${LS}$ values are computed in a similar manner, working backward
from the ${Finish}$ action.

The complexity of the critical path algorithm is just $O(Nb)$, where $N$
is the number of actions and $b$ is the maximum branching factor into or
out of an action. (To see this, note that the ${LS}$ and ${ES}$
computations are done once for each action, and each computation
iterates over at most $b$ other actions.) Therefore, finding a
minimum-duration schedule, given a partial ordering on the actions and
no resource constraints, is quite easy.

Mathematically speaking, critical-path problems are easy to solve
because they are defined as a *conjunction* of
*linear* inequalities on the start and end times. When we
introduce resource constraints, the resulting constraints on start and
end times become more complicated. For example, the ${AddEngine}$
actions, which begin at the same time in , require the same
${EngineHoist}$ and so cannot overlap. The “cannot overlap” constraint
is a *disjunction* of two linear inequalities, one for each
possible ordering. The introduction of disjunctions turns out to make
scheduling with resource constraints NP-hard.

[jobshop-resources-figure]

shows the solution with the fastest completion time, 115 minutes. This
is 30 minutes longer than the 85 minutes required for a schedule without
resource constraints. Notice that there is no time at which both
inspectors are required, so we can immediately move one of our two
inspectors to a more productive position.

The complexity of scheduling with resource constraints is often seen in
practice as well as in theory. A challenge problem posed in 1963—to find
the optimal schedule for a problem involving just 10 machines and 10
jobs of 100 actions each—went unsolved for 23 years @Lawler+al:1993.
Many approaches have been tried, including branch-and-bound, simulated
annealing, tabu search, constraint satisfaction, and other techniques
from Chapters [search-chapter] and [advanced-search-chapter]. One simple
but popular heuristic is the algorithm: on each iteration, schedule for
the earliest possible start whichever unscheduled action has all its
predecessors scheduled and has the least slack; then update the ${ES}$
and ${LS}$ times for each affected action and repeat. The heuristic
resembles the minimum-remaining-values (MRV) heuristic in constraint
satisfaction. It often works well in practice, but for our assembly
problem it yields a 130–minute solution, not the 115–minute solution of
.

Up to this point, we have assumed that the set of actions and ordering
constraints is fixed. Under these assumptions, every scheduling problem
can be solved by a nonoverlapping sequence that avoids all resource
conflicts, provided that each action is feasible by itself. If a
scheduling problem is proving very difficult, however, it may not be a
good idea to solve it this way—it may be better to reconsider the
actions and constraints, in case that leads to a much easier scheduling
problem. Thus, it makes sense to *integrate* planning and
scheduling by taking into account durations and overlaps during the
construction of a partial-order plan. Several of the planning algorithms
in can be augmented to handle this information. For example,
partial-order planners can detect resource constraint violations in much
the same way they detect conflicts with causal links. Heuristics can be
devised to estimate the total completion time of a plan. This is
currently an active area of research.

Hierarchical Planning {#htn-section}
---------------------

The problem-solving and planning methods of the preceding chapters all
operate with a fixed set of atomic actions. Actions can be strung
together into sequences or branching networks; state-of-the-art
algorithms can generate solutions containing thousands of actions.

For plans executed by the human brain, atomic actions are muscle
activations. In very round numbers, we have about $10^3$ muscles to
activate (639, by some counts, but many of them have multiple subunits);
we can modulate their activation perhaps 10 times per second; and we are
alive and awake for about $10^9$ seconds in all. Thus, a human life
contains about $10^{13}$ actions, give or take one or two orders of
magnitude. Even if we restrict ourselves to planning over much shorter
time horizons—for example, a two-week vacation in Hawaii—a detailed
motor plan would contain around $10^{10}$ actions. This is a lot more
than 1000.

To bridge this gap, AI systems will probably have to do what humans
appear to do: plan at higher levels of abstraction. A reasonable plan
for the Hawaii vacation might be “Go to San Francisco airport; take
Hawaiian Airlines flight 11 to Honolulu; do vacation stuff for two
weeks; take Hawaiian Airlines flight 12 back to San Francisco; go home.”
Given such a plan, the action “Go to San Francisco airport” can be
viewed as a planning task in itself, with a solution such as “Drive to
the long-term parking lot; park; take the shuttle to the terminal.” Each
of these actions, in turn, can be decomposed further, until we reach the
level of actions that can be executed without deliberation to generate
the required motor control sequences.

In this example, we see that planning can occur both before and during
the execution of the plan; for example, one would probably defer the
problem of planning a route from a parking spot in long-term parking to
the shuttle bus stop until a particular parking spot has been found
during execution. Thus, that particular action will remain at an
abstract level prior to the execution phase. We defer discussion of this
topic until . Here, we concentrate on the aspect of , an idea that
pervades almost all attempts to manage complexity. For example, complex
software is created from a hierarchy of subroutines or object classes;
armies operate as a hierarchy of units; governments and corporations
have hierarchies of departments, subsidiaries, and branch offices. The
key benefit of hierarchical structure is that, at each level of the
hierarchy, a computational task, military mission, or administrative
function is reduced to a *small* number of activities at
the next lower level, so the computational cost of finding the correct
way to arrange those activities for the current problem is small.
Nonhierarchical methods, on the other hand, reduce a task to a
*large* number of individual actions; for large-scale
problems, this is completely impractical.

### High-level actions

The basic formalism we adopt to understand hierarchical decomposition
comes from the area of or HTN planning. As in classical planning (), we
assume full observability and determinism and the availability of a set
of actions, now called , with standard precondition–effect schemas. The
key additional concept is the or HLA—for example, the action “Go to San
Francisco airport” in the example given earlier. Each HLA has one or
more possible , into a sequence[^1] of actions, each of which may be an
HLA or a primitive action (which has no refinements by definition). For
example, the action “Go to San Francisco airport,” represented formally
as ${Go}({Home},{SFO})$, might have two possible refinements, as
shown in . The same figure shows a refinement for navigation in the
vacuum world: to get to a destination, take a step, and then go to the
destination.

These examples show that high-level actions and their refinements embody
knowledge about *how to do things*. For instance, the
refinements for ${Go}({Home},{SFO})$ say that to get to the
airport you can drive or take a taxi; buying milk, sitting down, and
moving the knight to e4 are not to be considered.

[HLA-example-figure]

An HLA refinement that contains only primitive actions is called an of
the HLA. For example, in the vacuum world, the sequences
$[{Right},{Right},{Down}]$ and $[{Down},{Right},{Right}]$
both implement the HLA ${Navigate}([1,3],[3,2])$. An implementation of
a high-level plan (a sequence of HLAs) is the concatenation of
implementations of each HLA in the sequence. Given the
precondition–effect definitions of each primitive action, it is
straightforward to determine whether any given implementation of a
high-level plan achieves the goal. We can say, then, that

a high-level plan achieves the goal from a given state if at least one
of its implementations achieves the goal from that state.

The “at least one” in this definition is crucial—not *all*
implementations need to achieve the goal, because the agent gets to
decide which implementation it will execute. Thus, the set of possible
implementations in HTN planning—each of which may have a different
outcome—is not the same as the set of possible outcomes in
nondeterministic planning. There, we required that a plan work for
*all* outcomes because the agent doesn’t get to choose the
outcome; nature does.

The simplest case is an HLA that has exactly one implementation. In that
case, we can compute the preconditions and effects of the HLA from those
of the implementation (see ) and then treat the HLA exactly as if it
were a primitive action itself. It can be shown that the right
collection of HLAs can result in the time complexity of blind search
dropping from exponential in the solution depth to linear in the
solution depth, although devising such a collection of HLAs may be a
nontrivial task in itself. When HLAs have multiple possible
implementations, there are two options: one is to search among the
implementations for one that works, as in ; the other is to reason
directly about the HLAs—despite the multiplicity of implementations—as
explained in . The latter method enables the derivation of provably
correct abstract plans, without the need to consider their
implementations.

### Searching for primitive solutions {#HTN-search-section}

HTN planning is often formulated with a single “top level” action called
, where the aim is to find an implementation of that achieves the goal.
This approach is entirely general. For example, classical planning
problems can be defined as follows: for each primitive action $a_i$,
provide one refinement of with steps $[a_i,\act{Act}]$. That creates a
recursive definition of that lets us add actions. But we need some way
to stop the recursion; we do that by providing one more refinement for ,
one with an empty list of steps and with a precondition equal to the
goal of the problem. This says that if the goal is already achieved,
then the right implementation is to do nothing.

The approach leads to a simple algorithm: repeatedly choose an HLA in
the current plan and replace it with one of its refinements, until the
plan achieves the goal. One possible implementation based on
breadth-first tree search is shown in . Plans are considered in order of
depth of nesting of the refinements, rather than number of primitive
steps. It is straightforward to design a graph-search version of the
algorithm as well as depth-first and iterative deepening versions.

[hierarchical-search-algorithm]

In essence, this form of hierarchical search explores the space of
sequences that conform to the knowledge contained in the HLA library
about how things are to be done. A great deal of knowledge can be
encoded, not just in the action sequences specified in each refinement
but also in the preconditions for the refinements. For some domains, HTN
planners have been able to generate huge plans with very little search.
For example, @Bell+Tate:1985, which combines HTN planning with
scheduling, has been used to develop production plans for . A typical
problem involves a product line of 350 different products, 35 assembly
machines, and over 2000 different operations. The planner generates a
30-day schedule with three 8-hour shifts a day, involving tens of
millions of steps. Another important aspect of HTN plans is that they
are, by definition, hierarchically structured; usually this makes them
easy for humans to understand.

The computational benefits of hierarchical search can be seen by
examining an idealized case. Suppose that a planning problem has a
solution with $d$ primitive actions. For a nonhierarchical, forward
state-space planner with $b$ allowable actions at each state, the cost
is $O(b^d)$, as explained in . For an HTN planner, let us suppose a very
regular refinement structure: each nonprimitive action has $r$ possible
refinements, each into $k$ actions at the next lower level. We want to
know how many different refinement trees there are with this structure.
Now, if there are $d$ actions at the primitive level, then the number of
levels below the root is $\log_k d$, so the number of internal
refinement nodes is $1+k+k^2+\cdots+k^{\log_k d -1} = (d-1)/(k-1)$. Each
internal node has $r$ possible refinements, so $r^{(d-1)/(k-1)}$
possible regular decomposition trees could be constructed. Examining
this formula, we see that keeping $r$ small and $k$ large can result in
huge savings: essentially we are taking the $k$th root of the
nonhierarchical cost, if $b$ and $r$ are comparable. Small $r$ and large
$k$ means a library of HLAs with a small number of refinements each
yielding a long action sequence (that nonetheless allows us to solve any
problem). This is not always possible: long action sequences that are
usable across a wide range of problems are extremely precious.

The key to HTN planning, then, is the construction of a plan library
containing known methods for implementing complex, high-level actions.
One method of constructing the library is to *learn* the
methods from problem-solving experience. After the excruciating
experience of constructing a plan from scratch, the agent can save the
plan in the library as a method for implementing the high-level action
defined by the task. In this way, the agent can become more and more
competent over time as new methods are built on top of old methods. One
important aspect of this learning process is the ability to
*generalize* the methods that are constructed, eliminating
detail that is specific to the problem instance (e.g., the name of the
builder or the address of the plot of land) and keeping just the key
elements of the plan. Methods for achieving this kind of generalization
are described in . It seems to us inconceivable that humans could be as
competent as they are without some such mechanism.

### Searching for abstract solutions {#angelic-semantics-section}

The hierarchical search algorithm in the preceding section refines HLAs
all the way to primitive action sequences to determine if a plan is
workable. This contradicts common sense: one should be able to determine
that the two-HLA high-level plan
$$[{Drive}({Home},{SFOLongTermParking}), {Shuttle}({SFOLongTermParking},{SFO})]$$
gets one to the airport without having to determine a precise route,
choice of parking spot, and so on. The solution seems obvious: write
precondition–effect descriptions of the HLAs, just as we write down what
the primitive actions do. From the descriptions, it ought to be easy to
prove that the high-level plan achieves the goal. This is the holy
grail, so to speak, of hierarchical planning because if we derive a
high-level plan that provably achieves the goal, working in a small
search space of high-level actions, then we can commit to that plan and
work on the problem of refining each step of the plan. This gives us the
exponential reduction we seek. For this to work, it has to be the case
that every high-level plan that “claims” to achieve the goal (by virtue
of the descriptions of its steps) does in fact achieve the goal in the
sense defined earlier: it must have at least one implementation that
does achieve the goal. This property has been called the for HLA
descriptions.

Writing HLA descriptions that satisfy the downward refinement property
is, in principle, easy: as long as the descriptions are
*true*, then any high-level plan that claims to achieve the
goal must in fact do so—otherwise, the descriptions are making some
false claim about what the HLAs do. We have already seen how to write
true descriptions for HLAs that have exactly one implementation (); a
problem arises when the HLA has *multiple* implementations.
How can we describe the effects of an action that can be implemented in
many different ways?

One safe answer (at least for problems where all preconditions and goals
are positive) is to include only the positive effects that are achieved
by *every* implementation of the HLA and the negative
effects of *any* implementation. Then the downward
refinement property would be satisfied. Unfortunately, this semantics
for HLAs is much too conservative. Consider again the HLA
${Go}({Home},{SFO})$, which has two refinements, and suppose, for
the sake of argument, a simple world in which one can always drive to
the airport and park, but taking a taxi requires ${Cash}$ as a
precondition. In that case, ${Go}({Home},{SFO})$ doesn’t always
get you to the airport. In particular, it fails if ${Cash}$ is false,
and so we cannot assert ${At}({Agent},{SFO})$ as an effect of the
HLA. This makes no sense, however; if the agent didn’t have ${Cash}$,
it would drive itself. Requiring that an effect hold for
*every* implementation is equivalent to assuming that
*someone else*—an adversary—will choose the implementation.
It treats the HLA’s multiple outcomes exactly as if the HLA were a
action, as in . For our case, the agent itself will choose the
implementation.

The programming languages community has coined the term for the case
where an adversary makes the choices, contrasting this with , where the
agent itself makes the choices. We borrow this term to define for HLA
descriptions. The basic concept required for understanding angelic
semantics is the of an HLA: given a state $s$, the reachable set for an
HLA $h$, written as $\reach{s}{h}$, is the set of states reachable by
any of the HLA’s implementations. The key idea is that the agent can
choose *which* element of the reachable set it ends up in
when it executes the HLA; thus, an HLA with multiple refinements is more
“powerful” than the same HLA with fewer refinements. We can also define
the reachable set of a sequences of HLAs. For example, the reachable set
of a sequence $[h_1,h_2]$ is the union of all the reachable sets
obtained by applying $h_2$ in each state in the reachable set of $h_1$:
$$\reach{s}{[h_1,h_2]} = \bigcup_{s'\in \reach{s}{h_1}} \reach{s'}{h_2}\ .$$
Given these definitions, a high-level plan—a sequence of HLAs—achieves
the goal if its reachable set *intersects* the set of goal
states. (Compare this to the much stronger condition for demonic
semantics, where every member of the reachable set has to be a goal
state.) Conversely, if the reachable set doesn’t intersect the goal,
then the plan definitely doesn’t work. illustrates these ideas.

[reachable-sets-figure]

The notion of reachable sets yields a straightforward algorithm: search
among high-level plans, looking for one whose reachable set intersects
the goal; once that happens, the algorithm can *commit* to
that abstract plan, knowing that it works, and focus on refining the
plan further. We will come back to the algorithmic issues later; first,
we consider the question of how the effects of an HLA—the reachable set
for each possible initial state—are represented. As with the classical
action schemas of , we represent the *changes* made to each
fluent. Think of a fluent as a state variable. A primitive action can
*add* or *delete* a variable or leave it
*unchanged*. (With conditional effects (see ) there is a
fourth possibility: flipping a variable to its opposite.)

An HLA under angelic semantics can do more: it can
*control* the value of a variable, setting it to true or
false depending on which implementation is chosen. In fact, an HLA can
have nine different effects on a variable: if the variable starts out
true, it can always keep it true, always make it false, or have a
choice; if the variable starts out false, it can always keep it false,
always make it true, or have a choice; and the three choices for each
case can be combined arbitrarily, making nine. Notationally, this is a
bit challenging. We’ll use the $\,\widetilde{\ }\,$ symbol to mean
“possibly, if the agent so chooses.” Thus, an effect $\widetilde{+}A$
means “possibly add $A$,” that is, either leave $A$ unchanged or make it
true. Similarly, $\widetilde{-}A$ means “possibly delete $A$” and
$\widetilde{\pm}A$ means “possibly add or delete $A$.” For example, the
HLA ${Go}({Home},{SFO})$, with the two refinements shown in ,
possibly deletes ${Cash}$ (if the agent decides to take a taxi), so it
should have the effect $\widetilde{-}{Cash}$. Thus, we see that the
descriptions of HLAs are *derivable*, in principle, from
the descriptions of their refinements—in fact, this is required if we
want true HLA descriptions, such that the downward refinement property
holds. Now, suppose we have the following schemas for the HLAs $h_1$ and
$h_2$:

(h~1~, , ) ,\
(h~2~, , ) .

That is, $h_1$ adds $A$ and possible deletes $B$, while $h_2$ possibly
adds $A$ and has full control over $C$. Now, if only $B$ is true in the
initial state and the goal is $A \land C$ then the sequence $[h_1,h_2]$
achieves the goal: we choose an implementation of $h_1$ that makes $B$
false, then choose an implementation of $h_2$ that leaves $A$ true and
makes $C$ true.

The preceding discussion assumes that the effects of an HLA—the
reachable set for any given initial state—can be described exactly by
describing the effect on each variable. It would be nice if this were
always true, but in many cases we can only approximate the effects
because an HLA may have infinitely many implementations and may produce
arbitrarily wiggly reachable sets—rather like the wiggly-belief-state
problem illustrated in on . For example, we said that
${Go}({Home},{SFO})$ possibly deletes ${Cash}$; it also possibly
adds ${At}({Car},{SFOLongTermParking})$; but it cannot do both—in
fact, it must do exactly one. As with belief states, we may need to
write *approximate* descriptions. We will use two kinds of
approximation: an $\oreach{s}{h}$ of an HLA $h$ may overstate the
reachable set, while a $\preach{s}{h}$ may understate the reachable set.
Thus, we have
$$\preach{s}{h} \subseteq \reach{s}{h} \subseteq \oreach{s}{h} \ .$$ For
example, an optimistic description of ${Go}({Home},{SFO})$ says
that it possible deletes ${Cash}$ *and* possibly adds
${At}({Car},{SFOLongTermParking})$. Another good example arises in
the 8-puzzle, half of whose states are unreachable from any given state
(see on ): the optimistic description of might well include the whole
state space, since the exact reachable set is quite wiggly.

[approximate-HLA-figure]

With approximate descriptions, the test for whether a plan achieves the
goal needs to be modified slightly. If the optimistic reachable set for
the plan doesn’t intersect the goal, then the plan doesn’t work; if the
pessimistic reachable set intersects the goal, then the plan does work
((a)). With exact descriptions, a plan either works or it doesn’t, but
with approximate descriptions, there is a middle ground: if the
optimistic set intersects the goal but the pessimistic set doesn’t, then
we cannot tell if the plan works ((b)). When this circumstance arises,
the uncertainty can be resolved by refining the plan. This is a very
common situation in human reasoning. For example, in planning the
aforementioned two-week Hawaii vacation, one might propose to spend two
days on each of seven islands. Prudence would indicate that this
ambitious plan needs to be refined by adding details of inter-island
transportation.

[angelic-search-algorithm]

An algorithm for hierarchical planning with approximate angelic
descriptions is shown in . For simplicity, we have kept to the same
overall scheme used previously in , that is, a breadth-first search in
the space of refinements. As just explained, the algorithm can detect
plans that will and won’t work by checking the intersections of the
optimistic and pessimistic reachable sets with the goal. (The details of
how to compute the reachable sets of a plan, given approximate
descriptions of each step, are covered in .) When a workable abstract
plan is found, the algorithm *decomposes* the original
problem into subproblems, one for each step of the plan. The initial
state and goal for each subproblem are obtained by regressing a
guaranteed-reachable goal state through the action schemas for each step
of the plan. (See for a discussion of how regression works.) (b)
illustrates the basic idea: the right-hand circled state is the
guaranteed-reachable goal state, and the left-hand circled state is the
intermediate goal obtained by regressing the goal through the final
action.

The ability to commit to or reject high-level plans can give a
significant computational advantage over , which in turn may have a
large advantage over plain old . Consider, for example, cleaning up a
large vacuum world consisting of rectangular rooms connected by narrow
corridors. It makes sense to have an HLA for (as shown in ) and one for
. (Cleaning the room could be implemented with the repeated application
of another HLA to clean each row.) Since there are five actions in this
domain, the cost for grows as $5^d$, where $d$ is the length of the
shortest solution (roughly twice the total number of squares); the
algorithm cannot manage even two $2\stimes 2$ rooms. is more efficient,
but still suffers from exponential growth because it tries all ways of
cleaning that are consistent with the hierarchy. scales approximately
linearly in the number of squares—it commits to a good high-level
sequence and prunes away the other options. Notice that cleaning a set
of rooms by cleaning each room in turn is hardly rocket science: it is
easy for humans precisely because of the hierarchical structure of the
task. When we consider how difficult humans find it to solve small
puzzles such as the 8-puzzle, it seems likely that the human capacity
for solving complex problems derives to a great extent from their skill
in abstracting and decomposing the problem to eliminate combinatorics.

The angelic approach can be extended to find least-cost solutions by
generalizing the notion of reachable set. Instead of a state being
reachable or not, it has a cost for the most efficient way to get there.
(The cost is $\infty$ for unreachable states.) The optimistic and
pessimistic descriptions bound these costs. In this way, angelic search
can find provably optimal abstract plans without considering their
implementations. The same approach can be used to obtain effective
algorithms for online search, in the style of LRTA (). In some ways,
such algorithms mirror aspects of human deliberation in tasks such as
planning a vacation to Hawaii—consideration of alternatives is done
initially at an abstract level over long time scales; some parts of the
plan are left quite abstract until execution time, such as how to spend
two lazy days on Molokai, while others parts are planned in detail, such
as the flights to be taken and lodging to be reserved—without these
refinements, there is no guarantee that the plan would be feasible.

Planning and Acting in Nondeterministic Domains {#nondeterministic-planning-section}
-----------------------------------------------

[real-world-planning-section]

In this section we extend planning to handle partially observable,
nondeterministic, and unknown environments. extended search in similar
ways, and the methods here are also similar: (also known as ) for
environments with no observations; for partially observable and
nondeterministic environments; and and for unknown environments.

While the basic concepts are the same as in , there are also significant
differences. These arise because planners deal with factored
representations rather than atomic representations. This affects the way
we represent the agent’s capability for action and observation and the
way we represent —the sets of possible physical states the agent might
be in—for unobservable and partially observable environments. We can
also take advantage of many of the domain-independent methods given in
for calculating search heuristics.

Consider this problem: given a chair and a table, the goal is to have
them match—have the same color. In the initial state we have two cans of
paint, but the colors of the paint and the furniture are unknown. Only
the table is initially in the agent’s field of view:

(() () (C~1~) (C~2~) ())\
((, c) (, c))\

There are two actions: removing the lid from a paint can and painting an
object using the paint from an open can. The action schemas are
straightforward, with one exception: we now allow preconditions and
effects to contain variables that are not part of the action’s variable
list. That is, ${Paint}(x,{can})$ does not mention the variable $c$,
representing the color of the paint in the can. In the fully observable
case, this is not allowed—we would have to name the action
${Paint}(x,{can}, c)$. But in the partially observable case, we
might or might not know what color is in the can. (The variable $c$ is
universally quantified, just like all the other variables in an action
schema.)

((),\
\
)\
((, ),\
\
)\

To solve a partially observable problem, the agent will have to reason
about the percepts it will obtain when it is executing the plan. The
percept will be supplied by the agent’s sensors when it is actually
acting, but when it is planning it will need a model of its sensors. In
, this model was given by a function, $\prog{Percept}(s)$. For planning,
we augment PDDL with a new type of schema, the :

((x,c),\
\
((can,c),\
\

The first schema says that whenever an object is in view, the agent will
perceive the color of the object (that is, for the object $x$, the agent
will learn the truth value of ${Color}(x,c)$ for all $c$). The second
schema says that if an open can is in view, then the agent perceives the
color of the paint in the can. Because there are no exogenous events in
this world, the color of an object will remain the same, even if it is
not being perceived, until the agent performs an action to change the
object’s color. Of course, the agent will need an action that causes
objects (one at a time) to come into view:

((x),\
\
)\

For a fully observable environment, we would have a ${Percept}$ axiom
with no preconditions for each fluent. A sensorless agent, on the other
hand, has no ${Percept}$ axioms at all. Note that even a sensorless
agent can solve the painting problem. One solution is to open any can of
paint and apply it to both chair and table, thus them to be the same
color (even though the agent doesn’t know what the color is).

A contingent planning agent with sensors can generate a better plan.
First, look at the table and chair to obtain their colors; if they are
already the same then the plan is done. If not, look at the paint cans;
if the paint in a can is the same color as one piece of furniture, then
apply that paint to the other piece. Otherwise, paint both pieces with
any color.

Finally, an online planning agent might generate a contingent plan with
fewer branches at first—perhaps ignoring the possibility that no cans
match any of the furniture—and deal with problems when they arise by
replanning. It could also deal with incorrectness of its action schemas.
Whereas a contingent planner simply assumes that the effects of an
action always succeed—that painting the chair does the job—a replanning
agent would check the result and make an additional plan to fix any
unexpected failure, such as an unpainted area or the original color
showing through.

In the real world, agents use a combination of approaches. Car
manufacturers sell spare tires and air bags, which are physical
embodiments of contingent plan branches designed to handle punctures or
crashes. On the other hand, most car drivers never consider these
possibilities; when a problem arises they respond as replanning agents.
In general, agents plan only for contingencies that have important
consequences and a nonnegligible chance of happening. Thus, a car driver
contemplating a trip across the Sahara desert should make explicit
contingency plans for breakdowns, whereas a trip to the supermarket
requires less advance planning. We next look at each of the three
approaches in more detail.

### Sensorless planning {#conformant-planning-section}

() introduced the basic idea of searching in belief-state space to find
a solution for sensorless problems. Conversion of a sensorless planning
problem to a belief-state planning problem works much the same way as it
did in ; the main differences are that the underlying physical
transition model is represented by a collection of action schemas and
the belief state can be represented by a logical formula instead of an
explicitly enumerated set of states. For simplicity, we assume that the
underlying planning problem is deterministic.

The initial belief state for the sensorless painting problem can ignore
${InView}$ fluents because the agent has no sensors. Furthermore, we
take as given the unchanging facts
${Object}({Table}) \land {Object}({Chair}) \land {Can}(C_1)
\land {Can}(C_2)$ because these hold in every belief state. The agent
doesn’t know the colors of the cans or the objects, or whether the cans
are open or closed, but it does know that objects and cans have colors:
$\All{x} \Exi{c} {Color}(x,c)$. After Skolemizing, (see ), we obtain
the initial belief state: $$b_0 = {Color}(x,C(x))\ .$$ In classical
planning, where the is made, we would assume that any fluent not
mentioned in a state is false, but in sensorless (and partially
observable) planning we have to switch to an in which states contain
both positive and negative fluents, and if a fluent does not appear, its
value is unknown. Thus, the belief state corresponds exactly to the set
of possible worlds that satisfy the formula. Given this initial belief
state, the following action sequence is a solution:
$$[{RemoveLid}({Can}_1), {Paint}({Chair}, {Can}_1), {Paint}({Table}, {Can}_1)] \ .$$
We now show how to progress the belief state through the action sequence
to show that the final belief state satisfies the goal.

First, note that in a given belief state $b$, the agent can consider any
action whose preconditions are satisfied by $b$. (The other actions
cannot be used because the transition model doesn’t define the effects
of actions whose preconditions might be unsatisfied.) According to (),
the general formula for updating the belief state $b$ given an
applicable action $a$ in a deterministic world is as follows:
$$b' = \result{b}{a} = \{s' : s'\eq \Result_P(s,a) \mbox{ and } s\in b\}$$
where $\Result_P$ defines the physical transition model. For the time
being, we assume that the initial belief state is always a conjunction
of literals, that is, a 1-CNF formula. To construct the new belief state
$b'$, we must consider what happens to each literal $\ell$ in each
physical state $s$ in $b$ when action $a$ is applied. For literals whose
truth value is already known in $b$, the truth value in $b'$ is computed
from the current value and the add list and delete list of the action.
(For example, if $\ell$ is in the delete list of the action, then
$\lnot \ell$ is added to $b'$.) What about a literal whose truth value
is unknown in $b$? There are three cases:

1.  If the action adds $\ell$, then $\ell$ will be true in $b'$
    regardless of its initial value.

2.  If the action deletes $\ell$, then $\ell$ will be false in $b'$
    regardless of its initial value.

3.  If the action does not affect $\ell$, then $\ell$ will retain its
    initial value (which is unknown) and will not appear in $b'$.

Hence, we see that the calculation of $b'$ is almost identical to the
observable case, which was specified by on :
$$b' = \result{b}{a} = (b - \noprog{Del}(a)) \union \noprog{Add}(a) \ .$$
We cannot quite use the set semantics because (1) we must make sure that
$b'$ does not contain both $\ell$ and $\lnot \ell$, and (2) atoms may
contain unbound variables. But it is still the case that $\result{b}{a}$
is computed by starting with $b$, setting any atom that appears in
$\noprog{Del}(a)$ to false, and setting any atom that appears in
$\noprog{Add}(a)$ to true. For example, if we apply
$\act{RemoveLid}({Can}_1)$ to the initial belief state $b_0$, we get
$$b_1 = {Color}(x,C(x)) \land {Open}({Can}_1)\ .$$ When we apply
the action ${Paint}({Chair}, {Can}_1)$, the precondition
${Color}({Can}_1, c)$ is satisfied by the known literal
${Color}(x,C(x))$ with binding $\{x/{Can}_1,c/C({Can}_1)\}$ and
the new belief state is
$$b_2 = {Color}(x,C(x)) \land {Open}({Can}_1) \land {Color}({Chair}, C({Can}_1))\ .$$
Finally, we apply the action ${Paint}({Table}, {Can}_1)$ to obtain

b~3~ = (x,C(x)) (~1~) (, C(~1~))\
 (, C(~1~)) .

The final belief state satisfies the goal,
${Color}({Table}, c) \land {Color}({Chair}, c)$, with the
variable $c$ bound to $C({Can}_1)$.

The preceding analysis of the update rule has shown a very important
fact:

the family of belief states defined as conjunctions of literals is
closed under updates defined by PDDL action schemas.

That is, if the belief state starts as a conjunction of literals, then
any update will yield a conjunction of literals. That means that in a
world with $n$ fluents, any belief state can be represented by a
conjunction of size $O(n)$. This is a very comforting result,
considering that there are $2^n$ states in the world. It says we can
compactly represent all the subsets of those $2^n$ states that we will
ever need. Moreover, the process of checking for belief states that are
subsets or supersets of previously visited belief states is also easy,
at least in the propositional case.

The fly in the ointment of this pleasant picture is that it only works
for action schemas that have the *same effects* for all
states in which their preconditions are satisfied. It is this property
that enables the preservation of the 1-CNF belief-state representation.
As soon as the effect can depend on the state, dependencies are
introduced between fluents and the 1-CNF property is lost. Consider, for
example, the simple vacuum world defined in . Let the fluents be
${AtL}$ and ${AtR}$ for the location of the robot and ${CleanL}$
and ${CleanR}$ for the state of the squares. According to the
definition of the problem, the $\act{Suck}$ action has no
precondition—it can always be done. The difficulty is that its effect
depends on the robot’s location: when the robot is ${AtL}$, the result
is ${CleanL}$, but when it is ${AtR}$, the result is ${CleanR}$.
For such actions, our action schemas will need something new: a . These
have the syntax “$\condeff{\v{condition}}{\v{effect}}$,” where
*condition* is a logical formula to be compared against the
current state, and *effect* is a formula describing the
resulting state. For the vacuum world, we have

(,\
 )  .

When applied to the initial belief state ${True}$, the resulting
belief state is
$({AtL} \land {CleanL}) \lor ({AtR} \land {CleanR})$, which is
no longer in 1-CNF. (This transition can be seen in on .) In general,
conditional effects can induce arbitrary dependencies among the fluents
in a belief state, leading to belief states of exponential size in the
worst case.

It is important to understand the difference between preconditions and
conditional effects. *All* conditional effects whose
conditions are satisfied have their effects applied to generate the
resulting state; if none are satisfied, then the resulting state is
unchanged. On the other hand, if a *precondition* is
unsatisfied, then the action is inapplicable and the resulting state is
undefined. From the point of view of sensorless planning, it is better
to have conditional effects than an inapplicable action. For example, we
could split ${Suck}$ into two actions with unconditional effects as
follows:

(,\
 ; )\
(,\
 ; ) .

Now we have only unconditional schemas, so the belief states all remain
in 1-CNF; unfortunately, we cannot determine the applicability of
${SuckL}$ and ${SuckR}$ in the initial belief state.

It seems inevitable, then, that nontrivial problems will involve wiggly
belief states, just like those encountered when we considered the
problem of state estimation for the wumpus world (see on ). The solution
suggested then was to use a to the exact belief state; for example, the
belief state can remain in 1-CNF if it contains all literals whose truth
values can be determined and treats all other literals as unknown. While
this approach is *sound*, in that it never generates an
incorrect plan, it is *incomplete* because it may be unable
to find solutions to problems that necessarily involve interactions
among literals. To give a trivial example, if the goal is for the robot
to be on a clean square, then $[{Suck}]$ is a solution but a
sensorless agent that insists on 1-CNF belief states will not find it.

Perhaps a better solution is to look for action sequences that keep the
belief state as simple as possible. For example, in the sensorless
vacuum world, the action sequence
$[{Right},{Suck},{Left},{Suck}]$ generates the following
sequence of belief states:

$$\begin{aligned}
b_0 &=& {True}\\
b_1 &=& {AtR}\\
b_2 &=& {AtR} \land {CleanR}\\
b_3 &=& {AtL} \land {CleanR}\\
b_4 &=& {AtL} \land {CleanR} \land {CleanL}\end{aligned}$$

That is, the agent *can* solve the problem while retaining
a 1-CNF belief state, even though some sequences (e.g., those beginning
with ) go outside 1-CNF. The general lesson is not lost on humans: we
are always performing little actions (checking the time, patting our
pockets to make sure we have the car keys, reading street signs as we
navigate through a city) to eliminate uncertainty and keep our belief
state manageable.

There is another, quite different approach to the problem of
unmanageably wiggly belief states: don’t bother computing them at all.
Suppose the initial belief state is $b_0$ and we would like to know the
belief state resulting from the action sequence $[a_1,\ldots,a_m]$.
Instead of computing it explicitly, just represent it as “$b_0$ then
$[a_1,\ldots,a_m]$.” This is a lazy but unambiguous representation of
the belief state, and it’s quite concise—$O(n+m)$ where $n$ is the size
of the initial belief state (assumed to be in 1-CNF) and $m$ is the
maximum length of an action sequence. As a belief-state representation,
it suffers from one drawback, however: determining whether the goal is
satisfied, or an action is applicable, may require a lot of computation.

The computation can be implemented as an entailment test: if $A_m$
represents the collection of successor-state axioms required to define
occurrences of the actions $a_1,\ldots,a_m$—as explained for in —and
$G_m$ asserts that the goal is true after $m$ steps, then the plan
achieves the goal if $b_0 \land
A_m \models G_m$, that is, if $b_0 \land A_m \land \lnot G_m$ is
unsatisfiable. Given a modern SAT solver, it may be possible to do this
much more quickly than computing the full belief state. For example, if
none of the actions in the sequence has a particular goal fluent in its
add list, the solver will detect this immediately. It also helps if
partial results about the belief state—for example, fluents known to be
true or false—are cached to simplify subsequent computations.

The final piece of the sensorless planning puzzle is a heuristic
function to guide the search. The meaning of the heuristic function is
the same as for classical planning: an estimate (perhaps admissible) of
the cost of achieving the goal from the given belief state. With belief
states, we have one additional fact: solving any subset of a belief
state is necessarily easier than solving the belief state:
$$\mbox{if } b_1 \subseteq b_2 \mbox{ then } h^*(b_1) \leq h^*(b_2)\ .$$
Hence, any admissible heuristic computed for a subset is admissible for
the belief state itself. The most obvious candidates are the singleton
subsets, that is, individual physical states. We can take any random
collection of states $s_1,\ldots,s_N$ that are in the belief state $b$,
apply any admissible heuristic $h$ from , and return
$$H(b) = \max\{h(s_1),\ldots,h(s_N)\}$$ as the heuristic estimate for
solving $b$. We could also use a planning graph directly on $b$ itself:
if it is a conjunction of literals (1-CNF), simply set those literals to
be the initial state layer of the graph. If $b$ is not in 1-CNF, it may
be possible to find sets of literals that together entail $b$. For
example, if $b$ is in disjunctive normal form (DNF), each term of the
DNF formula is a conjunction of literals that entails $b$ and can form
the initial layer of a planning graph. As before, we can take the
maximum of the heuristics obtained from each set of literals. We can
also use inadmissible heuristics such as the ignore-delete-lists
heuristic (), which seems to work quite well in practice.

### Contingent planning {#conditional-planning-section}

We saw in that contingent planning—the generation of plans with
conditional branching based on percepts—is appropriate for environments
with partial observability, nondeterminism, or both. For the partially
observable painting problem with the percept axioms given earlier, one
possible contingent solution is as follows:

[(), (),\
]

Variables in this plan should be considered existentially quantified;
the second line says that if there exists some color $c$ that is the
color of the table and the chair, then the agent need not do anything to
achieve the goal. When executing this plan, a contingent-planning agent
can maintain its belief state as a logical formula and evaluate each
branch condition by determining if the belief state entails the
condition formula or its negation. (It is up to the contingent-planning
algorithm to make sure that the agent will never end up in a belief
state where the condition formula’s truth value is unknown.) Note that
with first-order conditions, the formula may be satisfied in more than
one way; for example, the condition ${Color}({Table}, c) \land
{Color}({can}, c)$ might be satisfied by $\{{can}/{Can}_1\}$ and
by $\{{can}/{Can}_2\}$ if both cans are the same color as the table.
In that case, the agent can choose any satisfying substitution to apply
to the rest of the plan.

As shown in , calculating the new belief state after an action and
subsequent percept is done in two stages. The first stage calculates the
belief state after the action, just as for the sensorless agent:
$$\hat b = (b - \noprog{Del}(a)) \union \noprog{Add}(a)$$ where, as
before, we have assumed a belief state represented as a conjunction of
literals. The second stage is a little trickier. Suppose that percept
literals $p_1,\ldots,p_k$ are received. One might think that we simply
need to add these into the belief state; in fact, we can also infer that
the preconditions for sensing are satisfied. Now, if a percept $p$ has
exactly one percept axiom, ${Percept}(p,\Pre{c})$, where $c$ is a
conjunction of literals, then those literals can be thrown into the
belief state along with $p$. On the other hand, if $p$ has more than one
percept axiom whose preconditions might hold according to the predicted
belief state $\hat b$, then we have to add in the
*disjunction* of the preconditions. Obviously, this takes
the belief state outside and brings up the same complications as
conditional effects, with much the same classes of solutions.

Given a mechanism for computing exact or approximate belief states, we
can generate contingent plans with an extension of the
and–or forward search over belief states used in . Actions
with nondeterministic effects—which are defined simply by using a
disjunction in the of the action schema—can be accommodated with minor
changes to the belief-state update calculation and no change to the
search algorithm.[^2] For the heuristic function, many of the methods
suggested for sensorless planning are also applicable in the partially
observable, nondeterministic case.

### Online replanning {#execution-monitoring-section}

Imagine watching a spot-welding robot in a car plant. The robot’s fast,
accurate motions are repeated over and over again as each car passes
down the line. Although technically impressive, the robot probably does
not seem at all *intelligent* because the motion is a
fixed, preprogrammed sequence; the robot obviously doesn’t “know what
it’s doing” in any meaningful sense. Now suppose that a poorly attached
door falls off the car just as the robot is about to apply a spot-weld.
The robot quickly replaces its welding actuator with a gripper, picks up
the door, checks it for scratches, reattaches it to the car, sends an
email to the floor supervisor, switches back to the welding actuator,
and resumes its work. All of a sudden, the robot’s behavior seems
*purposive* rather than rote; we assume it results not from
a vast, precomputed contingent plan but from an online replanning
process—which means that the robot *does* need to know what
it’s trying to do.

Replanning presupposes some form of to determine the need for a new
plan. One such need arises when a contingent planning agent gets tired
of planning for every little contingency, such as whether the sky might
fall on its head.[^3] Some branches of a partially constructed
contingent plan can simply say ${Replan}$; if such a branch is reached
during execution, the agent reverts to planning mode. As we mentioned
earlier, the decision as to how much of the problem to solve in advance
and how much to leave to replanning is one that involves tradeoffs among
possible events with different costs and probabilities of occurring.
Nobody wants to have their car break down in the middle of the Sahara
desert and only then think about having enough water.

Replanning may also be needed if the agent’s model of the world is
incorrect. The model for an action may have a —for example, the agent
may not know that removing the lid of a paint can often requires a
screwdriver; the model may have a —for example, painting an object may
get paint on the floor as well; or the model may have a —for example,
the model given earlier has no notion of the amount of paint in a can,
of how its actions affect this amount, or of the need for the amount to
be nonzero. The model may also lack provision for such as someone
knocking over the paint can. Exogenous events can also include changes
in the goal, such as the addition of the requirement that the table and
chair not be painted black. Without the ability to monitor and replan,
an agent’s behavior is likely to be extremely fragile if it relies on
absolute correctness of its model.

The online agent has a choice of how carefully to monitor the
environment. We distinguish three levels:

-   : before executing an action, the agent verifies that all the
    preconditions still hold.

-   : before executing an action, the agent verifies that the remaining
    plan will still succeed.

-   : before executing an action, the agent checks to see if there is a
    better set of goals it could be trying to achieve.

In we see a schematic of action monitoring. The agent keeps track of
both its original plan, , and the part of the plan that has not been
executed yet, which is denoted by . After executing the first few steps
of the plan, the agent expects to be in state $E$. But the agent
observes it is actually in state $O$. It then needs to repair the plan
by finding some point $P$ on the original plan that it can get back to.
(It may be that $P$ is the goal state, $G$.) The agent tries to minimize
the total cost of the plan: the repair part (from $O$ to $P$) plus the
continuation (from $P$ to $G$).

[plan-repair-figure]

[chair-table-painting] Now let’s return to the example problem of
achieving a chair and table of matching color. Suppose the agent comes
up with this plan:

[(), (),\
] .

Now the agent is ready to execute the plan. Suppose the agent observes
that the table and can of paint are white and the chair is black. It
then executes ${Paint}({Chair}, {Can}_1)$. At this point a
classical planner would declare victory; the plan has been executed. But
an online execution monitoring agent needs to check the preconditions of
the remaining empty plan—that the table and chair are the same color.
Suppose the agent perceives that they do not have the same color—in
fact, the chair is now a mottled gray because the black paint is showing
through. The agent then needs to figure out a position in to aim for and
a repair action sequence to get there. The agent notices that the
current state is identical to the precondition before the
${Paint}({Chair}, {Can}_1)$ action, so the agent chooses the empty
sequence for and makes its be the same $[{Paint}]$ sequence that it
just attempted. With this new plan in place, execution monitoring
resumes, and the ${Paint}$ action is retried. This behavior will loop
until the chair is perceived to be completely painted. But notice that
the loop is created by a process of plan–execute–replan, rather than by
an explicit loop in a plan. Note also that the original plan need not
cover every contingency. If the agent reaches the step marked , it can
then generate a new plan (perhaps involving ${Can}_2$).

Action monitoring is a simple method of execution monitoring, but it can
sometimes lead to less than intelligent behavior. For example, suppose
there is no black or white paint, and the agent constructs a plan to
solve the painting problem by painting both the chair and table red.
Suppose that there is only enough red paint for the chair. With action
monitoring, the agent would go ahead and paint the chair red, then
notice that it is out of paint and cannot paint the table, at which
point it would replan a repair—perhaps painting both chair and table
green. A plan-monitoring agent can detect failure whenever the current
state is such that the remaining plan no longer works. Thus, it would
not waste time painting the chair red. Plan monitoring achieves this by
checking the preconditions for success of the entire remaining plan—that
is, the preconditions of each step in the plan, except those
preconditions that are achieved by another step in the remaining plan.
Plan monitoring cuts off execution of a doomed plan as soon as possible,
rather than continuing until the failure actually
occurs.[right-here-right-now][^4] Plan monitoring also allows for
—accidental success. If someone comes along and paints the table red at
the same time that the agent is painting the chair red, then the final
plan preconditions are satisfied (the goal has been achieved), and the
agent can go home early.

It is straightforward to modify a planning algorithm so that each action
in the plan is annotated with the action’s preconditions, thus enabling
action monitoring. It is slightly more complex to enable plan
monitoring. Partial-order and planning-graph planners have the advantage
that they have already built up structures that contain the relations
necessary for plan monitoring. Augmenting state-space planners with the
necessary annotations can be done by careful bookkeeping as the goal
fluents are regressed through the plan.

Now that we have described a method for monitoring and replanning, we
need to ask, “Does it work?” This is a surprisingly tricky question. If
we mean, “Can we guarantee that the agent will always achieve the goal?”
then the answer is no, because the agent could inadvertently arrive at a
dead end from which there is no repair. For example, the vacuum agent
might have a faulty model of itself and not know that its batteries can
run out. Once they do, it cannot repair any plans. If we rule out dead
ends—assume that there exists a plan to reach the goal from
*any* state in the environment—and assume that the
environment is really nondeterministic, in the sense that such a plan
always has *some* chance of success on any given execution
attempt, then the agent will eventually reach the goal.

Trouble occurs when an action is actually not nondeterministic, but
rather depends on some precondition that the agent does not know about.
For example, sometimes a paint can may be empty, so painting from that
can has no effect. No amount of retrying is going to change this.[^5]
One solution is to choose randomly from among the set of possible repair
plans, rather than to try the same one each time. In this case, the
repair plan of opening another can might work. A better approach is to a
better model. Every prediction failure is an opportunity for learning;
an agent should be able to modify its model of the world to accord with
its percepts. From then on, the replanner will be able to come up with a
repair that gets at the root problem, rather than relying on luck to
choose a good repair. This kind of learning is described in
Chapters [concept-learning-chapter] and [ilp-chapter].

Multiagent Planning {#multi-agent-planning-section}
-------------------

So far, we have assumed that only one agent is doing the sensing,
planning, and acting. When there are multiple agents in the environment,
each agent faces a in which it tries to achieve its own goals with the
help or hindrance of others.

Between the purely single-agent and truly multiagent cases is a wide
spectrum of problems that exhibit various degrees of decomposition of
the monolithic agent. An agent with multiple effectors that can operate
concurrently—for example, a human who can type and speak at the same
time—needs to do to manage each effector while handling positive and
negative interactions among the effectors. When the effectors are
physically decoupled into detached units—as in a fleet of delivery
robots in a factory—multieffector planning becomes . A multibody problem
is still a “standard” single-agent problem as long as the relevant
sensor information collected by each body can be pooled—either centrally
or within each body—to form a common estimate of the world state that
then informs the execution of the overall plan; in this case, the
multiple bodies act as a single body. When communication constraints
make this impossible, we have what is sometimes called a problem; this
is perhaps a misnomer, because the planning phase is centralized but the
execution phase is at least partially decoupled. In this case, the
subplan constructed for each body may need to include explicit
communicative actions with other bodies. For example, multiple
reconnaissance robots covering a wide area may often be out of radio
contact with each other and should share their findings during times
when communication is feasible.

When a single entity is doing the planning, there is really only one
goal, which all the bodies necessarily share. When the bodies are
distinct agents that do their own planning, they may still share
identical goals; for example, two human tennis players who form a
doubles team share the goal of winning the match. Even with shared
goals, however, the multibody and multiagent cases are quite different.
In a multibody robotic doubles team, a single plan dictates which body
will go where on the court and which body will hit the ball. In a
multiagent doubles team, on the other hand, each agent decides what to
do; without some method for , both agents may decide to cover the same
part of the court and each may leave the ball for the other to hit.

The clearest case of a multiagent problem, of course, is when the agents
have different goals. In tennis, the goals of two opposing teams are in
direct conflict, leading to the zero-sum situation of . Spectators could
be viewed as agents if their support or disdain is a significant factor
and can be influenced by the players’ conduct; otherwise, they can be
treated as an aspect of nature—just like the weather—that is assumed to
be indifferent to the players’ intentions.[^6]

Finally, some systems are a mixture of centralized and multiagent
planning. For example, a delivery company may do centralized, offline
planning for the routes of its trucks and planes each day, but leave
some aspects open for autonomous decisions by drivers and pilots who can
respond individually to traffic and weather situations. Also, the goals
of the company and its employees are brought into alignment, to some
extent, by the payment of (salaries and bonuses)—a sure sign that this
is a true multiagent system.

The issues involved in multiagent planning can be divided roughly into
two sets. The first, covered in , involves issues of representing and
planning for multiple simultaneous actions; these issues occur in all
settings from multieffector to multiagent planning. The second, covered
in , involves issues of cooperation, coordination, and competition
arising in true multiagent settings.

### Planning with multiple simultaneous actions {#multibody-section}

For the time being, we will treat the multieffector, multibody, and
multiagent settings in the same way, labeling them generically as
settings, using the generic term to cover effectors, bodies, and agents.
The goal of this section is to work out how to define transition models,
correct plans, and efficient planning algorithms for the multiactor
setting. A correct plan is one that, if executed by the actors, achieves
the goal. (In the true multiagent setting, of course, the agents may not
agree to execute any particular plan, but at least they will know what
plans *would* work if they *did* agree to
execute them.) For simplicity, we assume perfect : each action takes the
same amount of time and actions at each point in the joint plan are
simultaneous.

We begin with the transition model; for the deterministic case, this is
the function $\result{s}{a}$. In the single-agent setting, there might
be $b$ different choices for the action; $b$ can be quite large,
especially for first-order representations with many objects to act on,
but action schemas provide a concise representation nonetheless. In the
multiactor setting with $n$ actors, the single action $a$ is replaced by
a $\<a_1,\ldots,a_n\>$, where $a_i$ is the action taken by the $i$th
actor. Immediately, we see two problems: first, we have to describe the
transition model for $b^n$ different joint actions; second, we have a
joint planning problem with a branching factor of $b^n$.

Having put the actors together into a multiactor system with a huge
branching factor, the principal focus of research on multiactor planning
has been to *decouple* the actors to the extent possible,
so that the complexity of the problem grows linearly with $n$ rather
than exponentially. If the actors have no interaction with one
another—for example, $n$ actors each playing a game of solitaire—then we
can simply solve $n$ separate problems. If the actors are , can we
attain something close to this exponential improvement? This is, of
course, a central question in many areas of AI. We have seen it
explicitly in the context of CSPs, where “tree like” constraint graphs
yielded efficient solution methods (see ), as well as in the context of
disjoint pattern databases () and additive heuristics for planning ().

[tennis-pddl-algorithm]

The standard approach to loosely coupled problems is to pretend the
problems are completely decoupled and then fix up the interactions. For
the transition model, this means writing action schemas as if the actors
acted independently. Let’s see how this works for the doubles tennis
problem. Let’s suppose that at one point in the game, the team has the
goal of returning the ball that has been hit to them and ensuring that
at least one of them is covering the net. A first pass at a multiactor
definition might look like . With this definition, it is easy to see
that the following plan works:

& &\
& A: & [(A, ), (A, )]\
&B: & .

Problems arise, however, when a plan has both agents hitting the ball at
the same time. In the real world, this won’t work, but the action schema
for says that the ball will be returned successfully. Technically, the
difficulty is that preconditions constrain the *state* in
which an action can be executed successfully, but do not constrain other
actions that might mess it up. We solve this by augmenting action
schemas with one new feature: a stating which actions must or must not
be executed concurrently. For example, the ${Hit}$ action could be
described as follows:

((a, ),\
 b a (b, )\
\
 ) .

In other words, the ${Hit}$ action has its stated effect only if no
other ${Hit}$ action by another agent occurs at the same time. (In the
approach, this would be handled by a partial .) For some actions, the
desired effect is achieved *only* when another action
occurs concurrently. For example, two agents are needed to carry a
cooler full of beverages to the tennis court:

((a, , , ),\
 b a (b, , , )\
\
).

With these kinds of action schemas, any of the planning algorithms
described in can be adapted with only minor modifications to generate
multiactor plans. To the extent that the coupling among subplans is
loose—meaning that concurrency constraints come into play only rarely
during plan search—one would expect the various heuristics derived for
single-agent planning to also be effective in the multiactor context. We
could extend this approach with the refinements of the last two
chapters—HTNs, partial observability, conditionals, execution
monitoring, and replanning—but that is beyond the scope of this book.

### Planning with multiple agents: Cooperation and coordination {#real-multiagent-section}

Now let us consider the true multiagent setting in which each agent
makes its own plan. To start with, let us assume that the goals and
knowledge base are shared. One might think that this reduces to the
multibody case—each agent simply computes the joint solution and
executes its own part of that solution. Alas, the
\`\`*the”* in “*the joint solution*” is
misleading. For our doubles team, more than one joint solution exists:

& &\
& A: & [(A, ), (A)]\
&B: & .

If both agents can agree on either plan 1 or plan 2, the goal will be
achieved. But if $A$ chooses plan 2 and $B$ chooses plan 1, then nobody
will return the ball. Conversely, if $A$ chooses 1 and $B$ chooses 2,
then they will both try to hit the ball. The agents may realize this,
but how can they coordinate to make sure they agree on the plan?

One option is to adopt a before engaging in joint activity. A convention
is any constraint on the selection of joint plans. For example, the
convention “stick to your side of the court” would rule out plan 1,
causing the doubles partners to select plan 2. Drivers on a road face
the problem of not colliding with each other; this is (partially) solved
by adopting the convention “stay on the right side of the road” in most
countries; the alternative, “stay on the left side,” works equally well
as long as all agents in an environment agree. Similar considerations
apply to the development of human language, where the important thing is
not which language each individual should speak, but the fact that a
community all speaks the same language. When conventions are widespread,
they are called .

In the absence of a convention, agents can use to achieve common
knowledge of a feasible joint plan. For example, a tennis player could
shout “Mine!” or “Yours!” to indicate a preferred joint plan. We cover
mechanisms for communication in more depth in , where we observe that
communication does not necessarily involve a verbal exchange. For
example, one player can communicate a preferred joint plan to the other
simply by executing the first part of it. If agent $A$ heads for the
net, then agent $B$ is obliged to go back to the baseline to hit the
ball, because plan 2 is the only joint plan that begins with $A$’s
heading for the net. This approach to coordination, sometimes called ,
works when a single action (or short sequence of actions) is enough to
determine a joint plan unambiguously. Note that communication can work
as well with competitive agents as with cooperative ones.

Conventions can also arise through evolutionary processes. For example,
seed-eating harvester ants are social creatures that evolved from the
less social wasps. Colonies of ants execute very elaborate joint plans
without any centralized control—the queen’s job is to reproduce, not to
do centralized planning—and with very limited computation,
communication, and memory capabilities in each ant @Gordon:2000
[@Gordon:2007]. The colony has many roles, including interior workers,
patrollers, and foragers. Each ant chooses to perform a role according
to the local conditions it observes. For example, foragers travel away
from the nest, search for a seed, and when they find one, bring it back
immediately. Thus, the rate at which foragers return to the nest is an
approximation of the availability of food today. When the rate is high,
other ants abandon their current role and take on the role of scavenger.
The ants appear to have a convention on the importance of roles—foraging
is the most important—and ants will easily switch into the more
important roles, but not into the less important. There is some learning
mechanism: a colony learns to make more successful and prudent actions
over the course of its decades-long life, even though individual ants
live only about a year.

[boid-figure]

One final example of cooperative multiagent behavior appears in the
flocking behavior of birds. We can obtain a reasonable simulation of a
flock if each bird agent (sometimes called a [boid-page]) observes the
positions of its nearest neighbors and then chooses the heading and
acceleration that maximizes the weighted sum of these three components:

1.  Cohesion: a positive score for getting closer to the average
    position of the neighbors

2.  Separation: a negative score for getting too close to any one
    neighbor

3.  Alignment: a positive score for getting closer to the average
    heading of the neighbors

If all the boids execute this policy, the flock exhibits the of flying
as a pseudorigid body with roughly constant density that does not
disperse over time, and that occasionally makes sudden swooping motions.
You can see a still images in (a) and compare it to an actual flock in
(b). As with ants, there is no need for each agent to possess a joint
plan that models the actions of other agents.

The most difficult multiagent problems involve both cooperation with
members of one’s own team and competition against members of opposing
teams, all without centralized control. We see this in games such as
robotic soccer or the game shown in (c), in which two teams of software
agents compete to capture the control towers. As yet, methods for
efficient planning in these kinds of environments—for example, taking
advantage of loose coupling—are in their infancy.

This chapter has addressed some of the complications of planning and
acting in the real world. The main points:

-   Many actions consume , such as money, gas, or raw materials. It is
    convenient to treat these resources as numeric measures in a pool
    rather than try to reason about, say, each individual coin and bill
    in the world. Actions can generate and consume resources, and it is
    usually cheap and effective to check partial plans for satisfaction
    of resource constraints before attempting further refinements.

-   Time is one of the most important resources. It can be handled by
    specialized scheduling algorithms, or scheduling can be integrated
    with planning.

-   (HTN) planning allows the agent to take advice from the domain
    designer in the form of (HLAs) that can be implemented in various
    ways by lower-level action sequences. The effects of HLAs can be
    defined with , allowing provably correct high-level plans to be
    derived without consideration of lower-level implementations. HTN
    methods can create the very large plans required by many real-world
    applications.

-   Standard planning algorithms assume complete and correct information
    and deterministic, fully observable environments. Many domains
    violate this assumption.

-   allow the agent to sense the world during execution to decide what
    branch of the plan to follow. In some cases, or can be used to
    construct a plan that works without the need for perception. Both
    conformant and contingent plans can be constructed by search in the
    space of . Efficient representation or computation of belief states
    is a key problem.

-   An uses execution monitoring and splices in repairs as needed to
    recover from unexpected situations, which can be due to
    nondeterministic actions, exogenous events, or incorrect models of
    the environment.

-   planning is necessary when there are other agents in the environment
    with which to cooperate or compete. Joint plans can be constructed,
    but must be augmented with some form of coordination if two agents
    are to agree on which joint plan to execute.

-   This chapter extends classic planning to cover nondeterministic
    environments (where outcomes of actions are uncertain), but it is
    not the last word on planning. describes techniques for stochastic
    environments (in which outcomes of actions have probabilities
    associated with them): Markov decision processes, partially
    observable Markov decision processes, and game theory. In we show
    that reinforcement learning allows an agent to learn how to behave
    from past successes and failures.

Planning with time constraints was first dealt with by @Vere:1983. The
representation of time in plans was addressed by and by in the system.
@Tate+Whiter:1984 and @Wilkins:1988 [@Wilkins:1990] could reason about
the allocation of limited resources to various plan steps.
@Bell+Tate:1985, an HTN planner, had a uniform, general representation
for constraints on time and resources. In addition to the Hitachi
application mentioned in the text, has been applied to software
procurement planning at Price Waterhouse and back-axle assembly planning
at Jaguar Cars.

The two planners @Do+Rao:2001 and @Haslum+Geffner:2001 both used forward
state-space search with sophisticated heuristics to handle actions with
durations and resources. An alternative is to use very expressive action
languages, but guide them by human-written domain-specific heuristics,
as is done by ASPEN @Fukunaga+al:1997, HSTS @Jonsson+al:2000, and IxTeT
@Ghallab+Laruelle:1994.

A number of hybrid planning-and-scheduling systems have been deployed:
@Fox+al:1981 [@Fox:1990] has been used for job shop scheduling at
Westinghouse,  @Descotte+Latombe:1985 planned the machining and
construction of mechanical parts, was used for factory control, and was
used for naval logistics planning. We chose to present planning and
scheduling as two separate problems; @Cushing+al:2007 show that this can
lead to incompleteness on certain problems. There is a long history of
scheduling in aerospace. @Drabble:1990 was used to schedule
mission-command sequences for the satellite. @Aarup+al:1994 and
@Fuchs+al:1990, both based on , were used for spacecraft assembly and
observation planning, respectively, at the . @Johnston+Adorf:1992 was
used for observation planning at NASA for the Hubble Space Telescope,
while the Space Shuttle Ground Processing Scheduling System
@Deale+al:1994 does job-shop scheduling of up to 16,000 worker-shifts.
@Muscettola+al:1998 became the first autonomous planner–scheduler to
control a spacecraft when it flew onboard the probe in 1999. Space
applications have driven the development of algorithms for resource
allocations; see and . The literature on scheduling is presented in a
classic survey article @Lawler+al:1993, a recent book @Pinedo:2008, and
an edited handbook @Blazewicz+al:2007.

The facility in the program for learning —“macro-operators” consisting
of a sequence of primitive steps—could be considered the first mechanism
for hierarchical planning @Fikes+al:1972. Hierarchy was also used in the
system @Siklossy+Dreussi:1973. The system @Sacerdoti:1974 introduced the
idea of an , whereby planning at higher levels was permitted to ignore
lower-level preconditions of actions in order to derive the general
structure of a working plan. Austin Tate’s Ph.D. thesis [-@Tate:1975]
and work by Earl developed the basic ideas of HTN planning in its modern
form. Many practical planners, including and , are HTN planners. Yang
[-@Yang:1990] discusses properties of actions that make HTN planning
efficient. Erol, Hendler, and Nau [-@Erol+al:1994; -@Erol+al:1996]
present a complete hierarchical decomposition planner as well as a range
of complexity results for pure HTN planners. Our presentation of HLAs
and angelic semantics is due to Marthi *et
al.* [-@Marthi+al:2007; -@Marthi+al:2008]. have proposed an
approach in which decompositions are just another form of plan
refinement, similar to the refinements for non-hierarchical
partial-order planning.

Beginning with the work on macro-operators in , one of the goals of
hierarchical planning has been the reuse of previous planning experience
in the form of generalized plans. The technique of , described in depth
in , has been applied in several systems as a means of generalizing
previously computed plans, including  @Laird+al:1986 and
 @Carbonell+al:1989. An alternative approach is to store previously
computed plans in their original form and then reuse them to solve new,
similar problems by analogy to the original problem. This is the
approach taken by the field called  @Carbonell:1983
[@Alterman:1988; @Hammond:1989]. argues that case-based planning should
be analyzed as a form of refinement planning and provides a formal
foundation for case-based partial-order planning.

Early planners lacked conditionals and loops, but some could use
coercion to form conformant plans. Sacerdoti’s solved the “keys and
boxes” problem, a planning challenge problem in which the planner knows
little about the initial state, using coercion. Mason [-@Mason:1993]
argued that sensing often can and should be dispensed with in robotic
planning, and described a sensorless plan that can move a tool into a
specific position on a table by a sequence of tilting actions,
*regardless* of the initial position.

introduced the term , noting that sensorless plans are often effective
even if the agent has sensors. The first moderately efficient conformant
planner was Smith and Weld’s [-@Smith+Weld:1998] Conformant Graphplan or
. and independently developed -based conformant planners. Bonet and
Geffner [-@Bonet+Geffner:2000] describe a conformant planner based on
heuristic search in the space of belief states, drawing on ideas first
developed in the 1960s for partially observable Markov decision
processes, or POMDPs (see ).

Currently, there are three main approaches to conformant planning. The
first two use heuristic search in belief-state space:  @Bertoli+al:2001a
uses binary decision diagrams (BDDs) to represent belief states, whereas
adopt the lazy approach of computing precondition and goal tests on
demand using a SAT solver. The third approach, championed primarily by
Jussi , formulates the entire sensorless planning problem as a
quantified Boolean formula (QBF) and solves it using a general-purpose
QBF solver. Current conformant planners are five orders of magnitude
faster than . The winner of the 2006 conformant-planning track at the
International Planning Competition was $T_0$ @Palacios+Geffner:2007,
which uses heuristic search in belief-state space while keeping the
belief-state representation simple by defining derived literals that
cover conditional effects. discuss how a planning graph can be
generalized to generate good heuristics for conformant and contingent
planning.

There has been some confusion in the literature between the terms
“conditional” and “contingent” planning. Following , we use
“conditional” to mean a plan (or action) that has different effects
depending on the actual state of the world, and “contingent” to mean a
plan in which the agent can choose different actions depending on the
results of sensing. The problem of contingent planning received more
attention after the publication of Drew McDermott’s [-@McDermott:1978a]
influential article, *Planning and Acting*.

The contingent-planning approach described in the chapter is based on ,
and was influenced by the efficient search algorithms for cyclic
and–or graphs developed by and . describe (Model-Based
Planner), which uses binary decision diagrams to do conformant and
contingent planning.

In retrospect, it is now possible to see how the major classical
planning algorithms led to extended versions for uncertain domains.
Fast-forward heuristic search through state space led to forward search
in belief space @Bonet+Geffner:2000 [@Hoffmann+Brafman:2005]; led to
stochastic @Majercik+Littman:2003 and to planning with quantified
Boolean logic @Rintanen:2007; partial order planning led to
@Etzioni+al:1992 and  @Peot+Smith:1992; led to Sensory Graphplan or
@Weld+al:1998.

The first online planner with execution monitoring was @Fikes+al:1972,
which worked with the planner to control the robot Shakey. The planner
@McDermott:1978a treated a planning problem simply as a specification
for carrying out a complex action, so that execution and planning were
completely unified. (System for Interactive Planning and Execution
monitoring) @Wilkins:1988 [@Wilkins:1990] was the first planner to deal
systematically with the problem of replanning. It has been used in
demonstration projects in several domains, including planning operations
on the flight deck of an aircraft carrier, job-shop scheduling for an
Australian beer factory, and planning the construction of multistory
buildings @Kartam:1990.

In the mid-1980s, pessimism about the slow run times of planning systems
led to the proposal of reflex agents called systems @Brooks:1986
[@Agre+Chapman:1987]. @Agre+Chapman:1987 could play a (fully observable)
video game by using Boolean circuits combined with a “visual”
representation of current goals and the agent’s internal state.
“Universal plans” @Schoppers:1987 [@Schoppers:1989] were developed as a
lookup-table method for reactive planning, but turned out to be a
rediscovery of the idea of that had long been used in Markov decision
processes (see ). A universal plan (or a policy) contains a mapping from
any state to the action that should be taken in that state. surveys
online planning techniques, under the name *Agent-Centered
Search*.

Multiagent planning has leaped in popularity in recent years, although
it does have a long history. formalizes multiagent planning in
first-order logic, while gives a -style description. The notion of joint
intention, which is essential if agents are to execute a joint plan,
comes from work on communicative acts @Cohen+Levesque:1990
[@Cohen+al:1990]. show how to adapt partial-order planning to a
multiactor setting. devise a multiactor planning algorithm whose
complexity grows only linearly with the number of actors, provided that
the degree of coupling (measured partly by the of the graph of
interactions among agents) is bounded. show that an approach based on
bilinear programming outperforms the cover-set approach we outlined in
the chapter.

We have barely skimmed the surface of work on negotiation in multiagent
planning. Durfee and Lesser [-@Durfee+Lesser:1989] discuss how tasks can
be shared out among agents by negotiation. describe a system for playing
Diplomacy, a board game requiring negotiation, coalition formation, and
dishonesty. shows how agents can cooperate as teammates in the
competitive, dynamic, partially observable environment of robotic
soccer. In a later article, analyzes two competitive multiagent
environments—RoboCup, a robotic soccer competition, and TAC, the
auction-based Trading Agents Competition—and finds that the
computational intractability of our current theoretically well-founded
approaches has led to many multiagent systems being designed by
*ad hoc* methods.

In his highly influential *Society of Mind* theory, Marvin
Minsky [-@Minsky:1986; -@Minsky:2007] proposes that human minds are
constructed from an ensemble of agents. prove that, for the problem of
optimal path-finding, and given a limitation on the total amount of
computing resources, the best architecture for an agent is an ensemble
of subagents, each of which tries to optimize its own objective, and all
of which are in conflict with one another.

The model on is due to , who won an Academy Award for its application to
swarms of penguins in *Batman Returns*. The game and the
methods for learning strategies are described by .

Recent book on multiagent systems include those by , , , and . There is
an annual conference on autonomous agents and multiagent systems
(AAMAS).

The goals we have considered so far all ask the planner to make the
world satisfy the goal at just one time step. Not all goals can be
expressed this way: you do not achieve the goal of suspending a
chandelier above the ground by throwing it in the air. More seriously,
you wouldn’t want your spacecraft life-support system to supply oxygen
one day but not the next. A *maintenance goal* is achieved
when the agent’s plan causes a condition to hold continuously from a
given state onward. Describe how to extend the formalism of this chapter
to support maintenance goals.

You have a number of trucks with which to deliver a set of packages.
Each package starts at some location on a grid map, and has a
destination somewhere else. Each truck is directly controlled by moving
forward and turning. Construct a hierarchy of high-level actions for
this problem. What knowledge about the solution does your hierarchy
encode?

[HLA-unique-exercise] Suppose that a high-level action has exactly one
implementation as a sequence of primitive actions. Give an algorithm for
computing its preconditions and effects, given the complete refinement
hierarchy and schemas for the primitive actions.

Suppose that the optimistic reachable set of a high-level plan is a
superset of the goal set; can anything be concluded about whether the
plan achieves the goal? What if the pessimistic reachable set doesn’t
intersect the goal set? Explain.

[HLA-progression-exercise] Write an algorithm that takes an initial
state (specified by a set of propositional literals) and a sequence of
HLAs (each defined by preconditions and angelic specifications of
optimistic and pessimistic reachable sets) and computes optimistic and
pessimistic descriptions of the reachable set of the sequence.

In we showed how to describe actions in a scheduling problem by using
separate fields for , , and . Now suppose we wanted to combine
scheduling with nondeterministic planning, which requires
nondeterministic and conditional effects. Consider each of the three
fields and explain if they should remain separate fields, or if they
should become effects of the action. Give an example for each of the
three.

Some of the operations in standard programming languages can be modeled
as actions that change the state of the world. For example, the
assignment operation changes the contents of a memory location, and the
print operation changes the state of the output stream. A program
consisting of these operations can also be considered as a plan, whose
goal is given by the specification of the program. Therefore, planning
algorithms can be used to construct programs that achieve a given
specification.

1.  Write an action schema for the assignment operator (assigning the
    value of one variable to another). Remember that the original value
    will be overwritten!

2.  Show how object creation can be used by a planner to produce a plan
    for exchanging the values of two variables by using a temporary
    variable.

Consider the following argument: In a framework that allows uncertain
initial states, are just a notational convenience, not a source of
additional representational power. For any action schema $a$ with
nondeterministic effect $P \lor Q$, we could always replace it with the
conditional effects $\condeff{R}{P} \land
\condeff{\lnot R}{Q}$, which in turn can be reduced to two regular
actions. The proposition $R$ stands for a random proposition that is
unknown in the initial state and for which there are no sensing actions.
Is this argument correct? Consider separately two cases, one in which
only one instance of action schema $a$ is in the plan, the other in
which more than one instance is.

[conformant-flip-literal-exercise] Suppose the ${Flip}$ action always
changes the truth value of variable $L$. Show how to define its effects
by using an action schema with conditional effects. Show that, despite
the use of conditional effects, a 1-CNF belief state representation
remains in 1-CNF after a ${Flip}$.

In the blocks world we were forced to introduce two action schemas,
${Move}$ and ${MoveToTable}$, in order to maintain the ${Clear}$
predicate properly. Show how conditional effects can be used to
represent both of these cases with a single action.

[alt-vacuum-exercise] Conditional effects were illustrated for the
${Suck}$ action in the vacuum world—which square becomes clean depends
on which square the robot is in. Can you think of a new set of
propositional variables to define states of the vacuum world, such that
${Suck}$ has an *unconditional* description? Write out
the descriptions of ${Suck}$, ${Left}$, and ${Right}$, using your
propositions, and demonstrate that they suffice to describe all possible
states of the world.

Find a suitably dirty carpet, free of obstacles, and vacuum it. Draw the
path taken by the vacuum cleaner as accurately as you can. Explain it,
with reference to the forms of planning discussed in this chapter.

The following quotes are from the backs of shampoo bottles. Identify
each as an unconditional, conditional, or execution-monitoring plan. (a)
“Lather. Rinse. Repeat.” (b) “Apply shampoo to scalp and let it remain
for several minutes. Rinse and repeat if necessary.” (c) “See a doctor
if problems persist.”

Consider the following problem: A patient arrives at the doctor’s office
with symptoms that could have been caused either by dehydration or by
disease $D$ (but not both). There are two possible actions: ${Drink}$,
which unconditionally cures dehydration, and ${Medicate}$, which cures
disease $D$ but has an undesirable side effect if taken when the patient
is dehydrated. Write the problem description, and diagram a sensorless
plan that solves the problem, enumerating all relevant possible worlds.

To the medication problem in the previous exercise, add a ${Test}$
action that has the conditional effect ${CultureGrowth}$ when
${Disease}$ is true and in any case has the perceptual effect
${Known}({CultureGrowth})$. Diagram a conditional plan that solves
the problem and minimizes the use of the ${Medicate}$ action.

[^1]: HTN planners often allow refinement into partially ordered plans,
    and they allow the refinements of two different HLAs in a plan to
    *share* actions. We omit these important complications
    in the interest of understanding the basic concepts of hierarchical
    planning.

[^2]: If cyclic solutions are required for a nondeterministic problem,
    and–or search must be generalized to a loopy version
    such as LAO @Hansen+Zilberstein:2001.

[^3]: In 1954, a Mrs. Hodges of Alabama was hit by meteorite that
    crashed through her roof. In 1992, a piece of the Mbale meteorite
    hit a small boy on the head; fortunately, its descent was slowed by
    banana leaves @Jenniskens+al:1994. And in 2009, a German boy claimed
    to have been hit in the hand by a pea-sized meteorite. No serious
    injuries resulted from any of these incidents, suggesting that the
    need for preplanning against such contingencies is sometimes
    overstated.

[^4]: Plan monitoring means that finally, after pages, we have an agent
    that is smarter than a dung beetle (see ). A plan-monitoring agent
    would notice that the dung ball was missing from its grasp and would
    replan to get another ball and plug its hole.

[^5]: Futile repetition of a plan repair is exactly the behavior
    exhibited by the ().

[^6]: We apologize to residents of the United Kingdom, where the mere
    act of contemplating a game of tennis guarantees rain.
Beyond Classical Search {#advanced-search-chapter}
=======================

addressed a single category of problems: observable, deterministic,
known environments where the solution is a sequence of actions. In this
chapter, we look at what happens when these assumptions are relaxed. We
begin with a fairly simple case: Sections [local-search-section]
and [continuous-search-section] cover algorithms that perform purely in
the state space, evaluating and modifying one or more current states
rather than systematically exploring paths from an initial state. These
algorithms are suitable for problems in which all that matters is the
solution state, not the path cost to reach it. The family of local
search algorithms includes methods inspired by statistical physics ()
and evolutionary biology ().

Then, in
Sections [contingent-search-section]–[partially-observable-search-section],
we examine what happens when we relax the assumptions of determinism and
observability. The key idea is that if an agent cannot predict exactly
what percept it will receive, then it will need to consider what to do
under each that its percepts may reveal. With partial observability, the
agent will also need to keep track of the states it might be in.

Finally, investigates , in which the agent is faced with a state space
that is initially unknown and must be explored.

Local Search Algorithms and Optimization Problems {#local-search-section}
-------------------------------------------------

The search algorithms that we have seen so far are designed to explore
search spaces systematically. This systematicity is achieved by keeping
one or more paths in memory and by recording which alternatives have
been explored at each point along the path. When a goal is found, the
*path* to that goal also constitutes a
*solution* to the problem. In many problems, however, the
path to the goal is irrelevant. For example, in the 8-queens problem
(see ), what matters is the final configuration of queens, not the order
in which they are added. The same general property holds for many
important applications such as integrated-circuit design, factory-floor
layout, job-shop scheduling, automatic programming, telecommunications
network optimization, vehicle routing, and portfolio management.

If the path to the goal does not matter, we might consider a different
class of algorithms, ones that do not worry about paths at all.
algorithms operate using a single (rather than multiple paths) and
generally move only to neighbors of that node. Typically, the paths
followed by the search are not retained. Although local search
algorithms are not systematic, they have two key advantages: (1) they
use very little memory—usually a constant amount; and (2) they can often
find reasonable solutions in large or infinite (continuous) state spaces
for which systematic algorithms are unsuitable.

In addition to finding goals, local search algorithms are useful for
solving pure , in which the aim is to find the best state according to
an . Many optimization problems do not fit the “standard” search model
introduced in . For example, nature provides an objective
function—reproductive fitness—that Darwinian evolution could be seen as
attempting to optimize, but there is no “goal test” and no “path cost”
for this problem.

To understand local search, we find it useful to consider the (as in ).
A landscape has both “location” (defined by the state) and “elevation”
(defined by the value of the heuristic cost function or objective
function). If elevation corresponds to cost, then the aim is to find the
lowest valley—a ; if elevation corresponds to an objective function,
then the aim is to find the highest peak—a . (You can convert from one
to the other just by inserting a minus sign.) Local search algorithms
explore this landscape. A local search algorithm always finds a goal if
one exists; an algorithm always finds a global minimum/maximum.

[hill-climbing-figure]

### Hill-climbing search

The search algorithm ( version) is shown in . It is simply a loop that
continually moves in the direction of increasing value—that is, uphill.
It terminates when it reaches a “peak” where no neighbor has a higher
value. The algorithm does not maintain a search tree, so the data
structure for the current node need only record the state and the value
of the objective function. Hill climbing does not look ahead beyond the
immediate neighbors of the current state. This resembles trying to find
the top of Mount Everest in a thick fog while suffering from amnesia.

[hill-climbing-algorithm]

To illustrate hill climbing, we will use the introduced on . Local
search algorithms typically use a , where each state has 8 queens on the
board, one per column. The successors of a state are all possible states
generated by moving a single queen to another square in the same column
(so each state has $8\stimes 7\eq {56}$ successors). The heuristic cost
function $h$ is the number of pairs of queens that are attacking each
other, either directly or indirectly. The global minimum of this
function is zero, which occurs only at perfect solutions. (a) shows a
state with $h\eq
{17}$. The figure also shows the values of all its successors, with the
best successors having $h\eq {12}$. Hill-climbing algorithms typically
choose randomly among the set of best successors if there is more than
one.

[8queens-hc-figure]

Hill climbing is sometimes called because it grabs a good neighbor state
without thinking ahead about where to go next. Although greed is
considered one of the seven deadly sins, it turns out that greedy
algorithms often perform quite well. Hill climbing often makes rapid
progress toward a solution because it is usually quite easy to improve a
bad state. For example, from the state in (a), it takes just five steps
to reach the state in (b), which has $h\eq 1$ and is very nearly a
solution. Unfortunately, hill climbing often gets stuck for the
following reasons:

-   : a local maximum is a peak that is higher than each of its
    neighboring states but lower than the global maximum. Hill-climbing
    algorithms that reach the vicinity of a local maximum will be drawn
    upward toward the peak but will then be stuck with nowhere else to
    go. illustrates the problem schematically. More concretely, the
    state in (b) is a local maximum (i.e., a local minimum for the cost
    $h$); every move of a single queen makes the situation worse.

-   : a ridge is shown in . Ridges result in a sequence of local maxima
    that is very difficult for greedy algorithms to navigate.

-   : a plateau is a flat area of the state-space landscape. It can be a
    flat local maximum, from which no uphill exit exists, or a , from
    which progress is possible. (See .) A hill-climbing search might get
    lost on the plateau.

In each case, the algorithm reaches a point at which no progress is
being made. Starting from a randomly generated 8-queens state,
steepest-ascent hill climbing gets stuck 86% of the time, solving only
14% of problem instances. It works quickly, taking just 4 steps on
average when it succeeds and 3 when it gets stuck—not bad for a state
space with $8^8 \approx {17}$ million states.

The algorithm in halts if it reaches a plateau where the best successor
has the same value as the current state. Might it not be a good idea to
keep going—to allow a in the hope that the plateau is really a shoulder,
as shown in ? The answer is usually yes, but we must take care. If we
always allow sideways moves when there are no uphill moves, an infinite
loop will occur whenever the algorithm reaches a flat local maximum that
is not a shoulder. One common solution is to put a limit on the number
of consecutive sideways moves allowed. For example, we could allow up
to, say, 100 consecutive sideways moves in the 8-queens problem. This
raises the percentage of problem instances solved by hill climbing from
14% to 94%. Success comes at a cost: the algorithm averages roughly 21
steps for each successful instance and 64 for each failure.

Many variants of hill climbing have been invented. chooses at random
from among the uphill moves; the probability of selection can vary with
the steepness of the uphill move. This usually converges more slowly
than steepest ascent, but in some state landscapes, it finds better
solutions. implements stochastic hill climbing by generating successors
randomly until one is generated that is better than the current state.
This is a good strategy when a state has many (e.g., thousands) of
successors.

[ridge-figure]

The hill-climbing algorithms described so far are incomplete—they often
fail to find a goal when one exists because they can get stuck on local
maxima. [random-restart-page] adopts the well-known adage, “If at first
you don’t succeed, try, try again.” It conducts a series of
hill-climbing searches from randomly generated initial states,[^1] until
a goal is found. It is trivially complete with probability approaching
1, because it will eventually generate a goal state as the initial
state. If each hill-climbing search has a probability $p$ of success,
then the expected number of restarts required is $1/p$. For 8-queens
instances with no sideways moves allowed, $p\approx {0.14}$, so we need
roughly 7 iterations to find a goal (6 failures and 1 success). The
expected number of steps is the cost of one successful iteration plus
$(1-p)/p$ times the cost of failure, or roughly 22 steps in all. When we
allow sideways moves, $1/{0.94}
\approx {1.06}$ iterations are needed on average and $(1\stimes {21}) +
({0.06}/{0.94})\stimes {64} \approx {25}$ steps. For 8-queens, then,
random-restart hill climbing is very effective indeed. Even for three
million queens, the approach can find solutions in under a minute.[^2]

The success of hill climbing depends very much on the shape of the
state-space landscape: if there are few local maxima and plateaux,
random-restart hill climbing will find a good solution very quickly. On
the other hand, many real problems have a landscape that looks more like
a widely scattered family of balding porcupines on a flat floor, with
miniature porcupines living on the tip of each porcupine needle,
*ad infinitum*. NP-hard problems typically have an
exponential number of local maxima to get stuck on. Despite this, a
reasonably good local maximum can often be found after a small number of
restarts.

### Simulated annealing

A hill-climbing algorithm that *never* makes “downhill”
moves toward states with lower value (or higher cost) is guaranteed to
be incomplete, because it can get stuck on a local maximum. In contrast,
a purely random walk—that is, moving to a successor chosen uniformly at
random from the set of successors—is complete but extremely inefficient.
Therefore, it seems reasonable to try to combine hill climbing with a
random walk in some way that yields both efficiency and completeness. is
such an algorithm. In metallurgy, is the process used to temper or
harden metals and glass by heating them to a high temperature and then
gradually cooling them, thus allowing the material to reach a low-energy
crystalline state. To explain simulated annealing, we switch our point
of view from hill climbing to (i.e., minimizing cost) and imagine the
task of getting a ping-pong ball into the deepest crevice in a bumpy
surface. If we just let the ball roll, it will come to rest at a local
minimum. If we shake the surface, we can bounce the ball out of the
local minimum. The trick is to shake just hard enough to bounce the ball
out of local minima but not hard enough to dislodge it from the global
minimum. The simulated-annealing solution is to start by shaking hard
(i.e., at a high temperature) and then gradually reduce the intensity of
the shaking (i.e., lower the temperature).

The innermost loop of the simulated-annealing algorithm () is quite
similar to hill climbing. Instead of picking the *best*
move, however, it picks a *random* move. If the move
improves the situation, it is always accepted. Otherwise, the algorithm
accepts the move with some probability less than 1. The probability
decreases exponentially with the “badness” of the move—the amount
$\Delta E$ by which the evaluation is worsened. The probability also
decreases as the “temperature” goes down: “bad” moves are more likely to
be allowed at the start when is high, and they become more unlikely as
decreases. If the lowers slowly enough, the algorithm will find a global
optimum with probability approaching 1.

[simulated-annealing-algorithm]

Simulated annealing was first used extensively to solve VLSI layout
problems in the early 1980s. It has been applied widely to factory
scheduling and other large-scale optimization tasks. In , you are asked
to compare its performance to that of random-restart hill climbing on
the 8-queens puzzle.

### Local beam search

[beam-search-section]

Keeping just one node in memory might seem to be an extreme reaction to
the problem of memory limitations. The algorithm[^3] keeps track of $k$
states rather than just one. It begins with $k$ randomly generated
states. At each step, all the successors of all $k$ states are
generated. If any one is a goal, the algorithm halts. Otherwise, it
selects the $k$ best successors from the complete list and repeats.

At first sight, a local beam search with $k$ states might seem to be
nothing more than running $k$ random restarts in parallel instead of in
sequence. In fact, the two algorithms are quite different. In a
random-restart search, each search process runs independently of the
others.

In a local beam search, useful information is passed among the parallel
search threads.

In effect, the states that generate the best successors say to the
others, “Come over here, the grass is greener!” The algorithm quickly
abandons unfruitful searches and moves its resources to where the most
progress is being made.

In its simplest form, local beam search can suffer from a lack of
diversity among the $k$ states—they can quickly become concentrated in a
small region of the state space, making the search little more than an
expensive version of hill climbing. A variant called , analogous to
stochastic hill climbing, helps alleviate this problem. Instead of
choosing the best $k$ from the the pool of candidate successors,
stochastic beam search chooses $k$ successors at random, with the
probability of choosing a given successor being an increasing function
of its value. Stochastic beam search bears some resemblance to the
process of natural selection, whereby the “successors” (offspring) of a
“state” (organism) populate the next generation according to its “value”
(fitness).

### Genetic algorithms {#ga-section}

A (or ) is a variant of stochastic beam search in which successor states
are generated by combining *two* parent states rather than
by modifying a single state. The analogy to natural selection is the
same as in stochastic beam search, except that now we are dealing with
sexual rather than asexual reproduction.

Like beam searches, GAs begin with a set of $k$ randomly generated
states, called the . Each state, or , is represented as a string over a
finite alphabet—most commonly, a string of 0s and 1s. For example, an
8-queens state must specify the positions of 8 queens, each in a column
of 8 squares, and so requires $8\stimes \log_2 8 \eq {24}$ bits.
Alternatively, the state could be represented as 8 digits, each in the
range from 1 to 8. (We demonstrate later that the two encodings behave
differently.) (a) shows a population of four 8-digit strings
representing 8-queens states.

The production of the next generation of states is shown in (b)–(e). In
(b), each state is rated by the objective function, or (in GA
terminology) the . A fitness function should return higher values for
better states, so, for the 8-queens problem we use the number of
*nonattacking* pairs of queens, which has a value of 28 for
a solution. The values of the four states are 24, 23, 20, and 11. In
this particular variant of the genetic algorithm, the probability of
being chosen for reproducing is directly proportional to the fitness
score, and the percentages are shown next to the raw scores.

[genetic-figure]

[8queens-crossover-figure]

In (c), two pairs are selected at random for reproduction, in accordance
with the probabilities in (b). Notice that one individual is selected
twice and one not at all.[^4] For each pair to be mated, a point is
chosen randomly from the positions in the string. In , the crossover
points are after the third digit in the first pair and after the fifth
digit in the second pair.[^5]

In (d), the offspring themselves are created by crossing over the parent
strings at the crossover point. For example, the first child of the
first pair gets the first three digits from the first parent and the
remaining digits from the second parent, whereas the second child gets
the first three digits from the second parent and the rest from the
first parent. The 8-queens states involved in this reproduction step are
shown in . The example shows that when two parent states are quite
different, the crossover operation can produce a state that is a long
way from either parent state. It is often the case that the population
is quite diverse early on in the process, so crossover (like simulated
annealing) frequently takes large steps in the state space early in the
search process and smaller steps later on when most individuals are
quite similar.

Finally, in (e), each location is subject to random with a small
independent probability. One digit was mutated in the first, third, and
fourth offspring. In the 8-queens problem, this corresponds to choosing
a queen at random and moving it to a random square in its column.
describes an algorithm that implements all these steps.

[genetic-algorithm]

Like stochastic beam search, genetic algorithms combine an uphill
tendency with random exploration and exchange of information among
parallel search threads. The primary advantage, if any, of genetic
algorithms comes from the crossover operation. Yet it can be shown
mathematically that, if the positions of the genetic code are permuted
initially in a random order, crossover conveys no advantage.
Intuitively, the advantage comes from the ability of crossover to
combine large blocks of letters that have evolved independently to
perform useful functions, thus raising the level of granularity at which
the search operates. For example, it could be that putting the first
three queens in positions 2, 4, and 6 (where they do not attack each
other) constitutes a useful block that can be combined with other blocks
to construct a solution.

The theory of genetic algorithms explains how this works using the idea
of a , which is a substring in which some of the positions can be left
unspecified. For example, the schema 246\*\*\*\*\* describes all
8-queens states in which the first three queens are in positions 2, 4,
and 6, respectively. Strings that match the schema (such as 24613578)
are called of the schema. It can be shown that if the average fitness of
the instances of a schema is above the mean, then the number of
instances of the schema within the population will grow over time.
Clearly, this effect is unlikely to be significant if adjacent bits are
totally unrelated to each other, because then there will be few
contiguous blocks that provide a consistent benefit. Genetic algorithms
work best when schemata correspond to meaningful components of a
solution. For example, if the string is a representation of an antenna,
then the schemata may represent components of the antenna, such as
reflectors and deflectors. A good component is likely to be good in a
variety of different designs. This suggests that successful use of
genetic algorithms requires careful engineering of the representation.

In practice, genetic algorithms have had a widespread impact on
optimization problems, such as circuit layout and job-shop scheduling.
At present, it is not clear whether the appeal of genetic algorithms
arises from their performance or from their æsthetically
pleasing origins in the theory of evolution. Much work remains to be
done to identify the conditions under which genetic algorithms perform
well.

Local Search in Continuous Spaces {#continuous-search-section}
---------------------------------

In , we explained the distinction between discrete and continuous
environments, pointing out that most real-world environments are
continuous. Yet none of the algorithms we have described (except for
first-choice hill climbing and simulated annealing) can handle
continuous state and action spaces, because they have infinite branching
factors. This section provides a *very brief* introduction
to some local search techniques for finding optimal solutions in
continuous spaces. The literature on this topic is vast; many of the
basic techniques originated in the 17th century, after the development
of calculus by Newton and Leibniz.[^6] We find uses for these techniques
at several places in the book, including the chapters on learning,
vision, and robotics.

We begin with an example. Suppose we want to place three new airports
anywhere in Romania, such that the sum of squared distances from each
city on the map () to its nearest airport is minimized. The state space
is then defined by the coordinates of the airports: $(x_1,y_1)$,
$(x_2,y_2)$, and $(x_3,y_3)$. This is a *six-dimensional*
space; we also say that states are defined by six . (In general, states
are defined by an $n$-dimensional vector of variables, $\x$.) Moving
around in this space corresponds to moving one or more of the airports
on the map. The objective function $f(x_1,y_1,x_2,y_2,x_3,y_3)$ is
relatively easy to compute for any particular state once we compute the
closest cities. Let $C_i$ be the set of cities whose closest airport (in
the current state) is airport $i$. Then, *in the neighborhood of
the current state*, where the $C_i$s remain constant, we have

$$f(x_1,y_1,x_2,y_2,x_3,y_3) = \sum_{i\eq 1}^3 \sum_{c\in C_i} (x_i - x_c)^2 + (y_i - y_c)^2\ .
\label{airport-objective-equation}$$

This expression is correct *locally*, but not globally
because the sets $C_i$ are (discontinuous) functions of the state.

One way to avoid continuous problems is simply to the neighborhood of
each state. For example, we can move only one airport at a time in
either the $x$ or $y$ direction by a fixed amount $\pm
\delta$. With 6 variables, this gives 12 possible successors for each
state. We can then apply any of the local search algorithms described
previously. We could also apply stochastic hill climbing and simulated
annealing directly, without discretizing the space. These algorithms
choose successors randomly, which can be done by generating random
vectors of length $\delta$.

Many methods attempt to use the of the landscape to find a maximum. The
gradient of the objective function is a vector $\nabla f$ that gives the
magnitude and direction of the steepest slope. For our problem, we have
$$\nabla f=\left(\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial y_1},
              \frac{\partial f}{\partial x_2},\frac{\partial f}{\partial y_2},
              \frac{\partial f}{\partial x_3},\frac{\partial f}{\partial y_3}
        \right)\ .$$ In some cases, we can find a maximum by solving the
equation $\nabla
f\eq 0$. (This could be done, for example, if we were placing just one
airport; the solution is the arithmetic mean of all the cities’
coordinates.) In many cases, however, this equation cannot be solved in
closed form. For example, with three airports, the expression for the
gradient depends on what cities are closest to each airport in the
current state. This means we can compute the gradient
*locally* (but not *globally*); for example,

$$\frac{\partial f}{\partial x_1} = 2 \sum_{c\in C_1} (x_i - x_c)\ .
\label{airport-gradient-equation}$$

Given a locally correct expression for the gradient, we can perform
steepest-ascent hill climbing by updating the current state according to
the formula $$\x \leftarrow \x + \alpha \nabla f(\x)\ ,$$ where $\alpha$
is a small constant often called the . In other cases, the objective
function might not be available in a differentiable form at all—for
example, the value of a particular set of airport locations might be
determined by running some large-scale economic simulation package. In
those cases, we can calculate a so-called by evaluating the response to
small increments and decrements in each coordinate. Empirical gradient
search is the same as steepest-ascent hill climbing in a discretized
version of the state space.

Hidden beneath the phrase “$\alpha$ is a small constant” lies a huge
variety of methods for adjusting $\alpha$. The basic problem is that, if
$\alpha$ is too small, too many steps are needed; if $\alpha$ is too
large, the search could overshoot the maximum. The technique of tries to
overcome this dilemma by extending the current gradient
direction—usually by repeatedly doubling $\alpha$—until $f$ starts to
decrease again. The point at which this occurs becomes the new current
state. There are several schools of thought about how the new direction
should be chosen at this point.

For many problems, the most effective algorithm is the venerable method.
This is a general technique for finding roots of functions—that is,
solving equations of the form $g(x)\eq 0$. It works by computing a new
estimate for the root $x$ according to Newton’s formula
$$x \leftarrow x - g(x)/g'(x)\ .$$ To find a maximum or minimum of $f$,
we need to find $\x$ such that the *gradient* is zero
(i.e., $\nabla f(\x)\eq \zeroes$). Thus, $g(x)$ in Newton’s formula
becomes $\nabla f(\x)$, and the update equation can be written in
matrix–vector form as
$$\x \leftarrow \x - \H^{-1}_f(\x) \nabla f(\x)\ ,$$ where $\H_f(\x)$ is
the matrix of second derivatives, whose elements $H_{ij}$ are given by
$\partial^2 f/\partial x_i \partial x_j$. For our airport example, we
can see from that $\H_f(\x)$ is particularly simple: the off-diagonal
elements are zero and the diagonal elements for airport $i$ are just
twice the number of cities in $C_i$. A moment’s calculation shows that
one step of the update moves airport $i$ directly to the centroid of
$C_i$, which is the minimum of the local expression for $f$ from .[^7]
For high-dimensional problems, however, computing the $n^2$ entries of
the Hessian and inverting it may be expensive, so many approximate
versions of the Newton–Raphson method have been developed.

Local search methods suffer from local maxima, ridges, and plateaux in
continuous state spaces just as much as in discrete spaces. Random
restarts and simulated annealing can be used and are often helpful.
High-dimensional continuous spaces are, however, big places in which it
is easy to get lost.

A final topic with which a passing acquaintance is useful is . An
optimization problem is constrained if solutions must satisfy some hard
constraints on the values of the variables. For example, in our
airport-siting problem, we might constrain sites to be inside Romania
and on dry land (rather than in the middle of lakes). The difficulty of
constrained optimization problems depends on the nature of the
constraints and the objective function. The best-known category is that
of problems, in which constraints must be linear inequalities forming a
[convexity-page][^8] and the objective function is also linear. The time
complexity of linear programming is polynomial in the number of
variables.

Linear programming is probably the most widely studied and broadly
useful class of optimization problems. It is a special case of the more
general problem of , which allows the constraint region to be any convex
region and the objective to be any function that is convex within the
constraint region. Under certain conditions, convex optimization
problems are also polynomially solvable and may be feasible in practice
with thousands of variables. Several important problems in machine
learning and control theory can be formulated as convex optimization
problems (see ).

Searching with Nondeterministic Actions {#contingent-search-section}
---------------------------------------

In , we assumed that the environment is fully observable and
deterministic and that the agent knows what the effects of each action
are. Therefore, the agent can calculate exactly which state results from
any sequence of actions and always knows which state it is in. Its
percepts provide no new information after each action, although of
course they tell the agent the initial state.

When the environment is either partially observable or nondeterministic
(or both), percepts become useful. In a partially observable
environment, every percept helps narrow down the set of possible states
the agent might be in, thus making it easier for the agent to achieve
its goals. When the environment is nondeterministic, percepts tell the
agent which of the possible outcomes of its actions has actually
occurred. In both cases, the future percepts cannot be determined in
advance and the agent’s future actions will depend on those future
percepts. So the solution to a problem is not a sequence but a (also
known as a ) that specifies what to do depending on what percepts are
received. In this section, we examine the case of nondeterminism,
deferring partial observability to .

### The erratic vacuum world

As an example, we use the vacuum world, first introduced in and defined
as a search problem in . Recall that the state space has eight states,
as shown in . There are three actions—, , and —and the goal is to clean
up all the dirt (states 7 and 8). If the environment is observable,
deterministic, and completely known, then the problem is trivially
solvable by any of the algorithms in and the solution is an action
sequence. For example, if the initial state is 1, then the action
sequence [,,] will reach a goal state, 8.

[vacuum2-states-figure]

Now suppose that we introduce nondeterminism in the form of a powerful
but erratic vacuum cleaner. In the , the action works as follows:

-   When applied to a dirty square the action cleans the square and
    sometimes cleans up dirt in an adjacent square, too.

-   When applied to a clean square the action sometimes deposits dirt on
    the carpet.[^9]

To provide a precise formulation of this problem, we need to generalize
the notion of a from . Instead of defining the transition model by a
function that returns a single state, we use a function that returns a
*set* of possible outcome states. For example, in the
erratic vacuum world, the action in state 1 leads to a state in the set
$\{5,7\}$—the dirt in the right-hand square may or may not be vacuumed
up.

We also need to generalize the notion of a to the problem. For example,
if we start in state 1, there is no single *sequence* of
actions that solves the problem. Instead, we need a contingency plan
such as the following:

$$[\act{Suck},\condstep{{State} \eq 5}{[\act{Right},\act{Suck}]}{\noplan}]\ .
\label{erratic-vacuum-solution-equation}$$

Thus, solutions for nondeterministic problems can contain nested ––
statements; this means that they are *trees* rather than
sequences. This allows the selection of actions based on contingencies
arising during execution. Many problems in the real, physical world are
contingency problems because exact prediction is impossible. For this
reason, many people keep their eyes open while walking around or
driving.

### and–or search trees

The next question is how to find contingent solutions to
nondeterministic problems. As in , we begin by constructing search
trees, but here the trees have a different character. In a deterministic
environment, the only branching is introduced by the agent’s own choices
in each state. We call these nodes . In the vacuum world, for example,
at an or node the agent chooses *or*
*or* . In a nondeterministic environment, branching is also
introduced by the *environment’s* choice of outcome for
each action. We call these nodes . For example, the action in state 1
leads to a state in the set $\{5,7\}$, so the agent would need to find a
plan for state 5 *and* for state 7. These two kinds of
nodes alternate, leading to an as illustrated in .

[erratic-vacuum-and-or-plan-figure]

A solution for an and–or search problem is a subtree that
(1) has a goal node at every leaf, (2) specifies one action at each of
its or nodes, and (3) includes every outcome branch at each
of its and nodes. The solution is shown in bold lines in
the figure; it corresponds to the plan given in . (The plan uses
if–then–else notation to handle the and branches, but when
there are more than two branches at a node, it might be better to use a
construct.) Modifying the basic problem-solving agent shown in to
execute contingent solutions of this kind is straightforward. One may
also consider a somewhat different agent design, in which the agent can
act *before* it has found a guaranteed plan and deals with
some contingencies only as they arise during execution. This type of of
search and execution is also useful for exploration problems (see ) and
for game playing (see ).

[and-or-graph-search-algorithm]

gives a recursive, depth-first algorithm for and–or graph
search. One key aspect of the algorithm is the way in which it deals
with cycles, which often arise in nondeterministic problems (e.g., if an
action sometimes has no effect or if an unintended effect can be
corrected). If the current state is identical to a state on the path
from the root, then it returns with failure. This doesn’t mean that
there is *no* solution from the current state; it simply
means that if there *is* a noncyclic solution, it must be
reachable from the earlier incarnation of the current state, so the new
incarnation can be discarded. With this check, we ensure that the
algorithm terminates in every finite state space, because every path
must reach a goal, a dead end, or a repeated state. Notice that the
algorithm does not check whether the current state is a repetition of a
state on some *other* path from the root, which is
important for efficiency. investigates this issue. and–or
graphs can also be explored by breadth-first or best-first methods. The
concept of a heuristic function must be modified to estimate the cost of
a contingent solution rather than a sequence, but the notion of
admissibility carries over and there is an analog of the A algorithm for
finding optimal solutions. Pointers are given in the bibliographical
notes at the end of the chapter.

### Try, try again {#cyclic-plan-section}

Consider the slippery vacuum world, which is identical to the ordinary
(non-erratic) vacuum world except that movement actions sometimes fail,
leaving the agent in the same location. For example, moving in state 1
leads to the state set $\{1,2\}$. shows part of the search graph;
clearly, there are no longer any acyclic solutions from state 1, and
would return with failure. There is, however, a , which is to keep
trying ${Right}$ until it works. We can express this solution by
adding a to denote some portion of the plan and using that label later
instead of repeating the plan itself. Thus, our cyclic solution is
$$[\act{Suck}, L_1:\ {Right}, \condstep{{State}\eq 5}{L_1}{{Suck}}]\ .$$
(A better syntax for the looping part of this plan would be
“$\whilestep{{State}\eq 5}{{Right}}$.”) In general a cyclic plan may
be considered a solution provided that every leaf is a goal state and
that a leaf is reachable from every point in the plan. The modifications
needed to are covered in . The key realization is that a loop in the
state space back to a state $L$ translates to a loop in the plan back to
the point where the subplan for state $L$ is executed.

[slippery-vacuum-loop-plan-figure]

Given the definition of a cyclic solution, an agent executing such a
solution will eventually reach the goal *provided that each
outcome of a nondeterministic action eventually occurs*. Is this
condition reasonable? It depends on the reason for the nondeterminism.
If the action rolls a die, then it’s reasonable to suppose that
eventually a six will be rolled. If the action is to insert a hotel card
key into the door lock, but it doesn’t work the first time, then perhaps
it will eventually work, or perhaps one has the wrong key (or the wrong
room!). After seven or eight tries, most people will assume the problem
is with the key and will go back to the front desk to get a new one. One
way to understand this decision is to say that the initial problem
formulation (observable, nondeterministic) is abandoned in favor of a
different formulation (partially observable, deterministic) where the
failure is attributed to an unobservable property of the key. We have
more to say on this issue in .

Searching with Partial Observations {#partially-observable-search-section}
-----------------------------------

We now turn to the problem of partial observability, where the agent’s
percepts do not suffice to pin down the exact state. As noted at the
beginning of the previous section, if the agent is in one of several
possible states, then an action may lead to one of several possible
outcomes—*even if the environment is deterministic*. The
key concept required for solving partially observable problems is the ,
representing the agent’s current belief about the possible physical
states it might be in, given the sequence of actions and percepts up to
that point.[belief-state-page] We begin with the simplest scenario for
studying belief states, which is when the agent has no sensors at all;
then we add in partial sensing as well as nondeterministic actions.

### Searching with no observation {#conformant-section}

When the agent’s percepts provide *no information at all*,
we have what is called a problem or sometimes a problem. At first, one
might think the sensorless agent has no hope of solving a problem if it
has no idea what state it’s in; in fact, sensorless problems are quite
often solvable. Moreover, sensorless agents can be surprisingly useful,
primarily because they *don’t* rely on sensors working
properly. In manufacturing systems, for example, many ingenious methods
have been developed for orienting parts correctly from an unknown
initial position by using a sequence of actions with no sensing at all.
The high cost of sensing is another reason to avoid it: for example,
doctors often prescribe a broad-spectrum antibiotic rather than using
the contingent plan of doing an expensive blood test, then waiting for
the results to come back, and then prescribing a more specific
antibiotic and perhaps hospitalization because the infection has
progressed too far.

We can make a sensorless version of the vacuum world. Assume that the
agent knows the geography of its world, but doesn’t know its location or
the distribution of dirt. In that case, its initial state could be any
element of the set $\{1,2,3,4,5,6,7,8\}$. Now, consider what happens if
it tries the action . This will cause it to be in one of the states
$\{2,4,6,8\}$—the agent now has more information! Furthermore, the
action sequence [,] will always end up in one of the states $\{4,8\}$.
Finally, the sequence [,,,] is guaranteed to reach the goal state 7 no
matter what the start state. We say that the agent can the world into
state 7.

To solve sensorless problems, we search in the space of belief states
rather than physical states.[^10] Notice that in belief-state space, the
problem is *fully observable* because the agent always
knows its own belief state. Furthermore, the solution (if any) is always
a sequence of actions. This is because, as in the ordinary problems of ,
the percepts received after each action are completely
predictable—they’re always empty! So there are no contingencies to plan
for. This is true *even if the environment is
nondeterminstic*.

It is instructive to see how the belief-state search problem is
constructed. Suppose the underlying physical problem $P$ is defined by
$_P$, $_P$, $_P$, and $_P$. Then we can define the corresponding
sensorless problem as follows:

The entire belief-state space contains every possible set of physical
states. If $P$ has $N$ states, then the sensorless problem has up to
$2^N$ states, although many may be unreachable from the initial state.

Typically the set of all states in $P$, although in some cases the agent
will have more knowledge than this.

This is slightly tricky. Suppose the agent is in belief state
$b \eq \{s_1,s_2\}$, but $\noprog{Actions}_P(s_1)
\neq \noprog{Actions}_P(s_2)$; then the agent is unsure of which actions
are legal. If we assume that illegal actions have no effect on the
environment, then it is safe to take the *union* of all the
actions in any of the physical states in the current belief state $b$:
$$\noprog{Actions}(b) = \bigcup_{s \in b} \noprog{Actions}_P(s) \ .$$ On
the other hand, if an illegal action might be the end of the world, it
is safer to allow only the *intersection*, that is, the set
of actions legal in *all* the states. For the vacuum world,
every state has the same legal actions, so both methods give the same
result.

The agent doesn’t know which state in the belief state is the right one;
so as far as it knows, it might get to any of the states resulting from
applying the action to one of the physical states in the belief state.
For deterministic actions, the set of states that might be reached is

$$b' = \result{b}{a} = \{s' : s'\eq \Result_P(s,a) \mbox{ and } s\in b\}\ .
\label{deterministic-belief-update-equation}$$

With deterministic actions, $b'$ is never larger than $b$. With
nondeterminism, we have

$$\begin{aligned}
   b' = \result{b}{a} &=& \{s' : s'\in \Results_P(s,a) \mbox{ and } s\in b\} \\
                      &=& \bigcup_{s\in b} \Results_P(s,a)\ ,\end{aligned}$$

which may be larger than $b$, as shown in . The process of generating
the new belief state after the action is called the step; the notation
$b' =
\prog{Predict}_P(b,a)$ will come in handy.

The agent wants a plan that is sure to work, which means that a belief
state satisfies the goal only if *all* the physical states
in it satisfy $_P$. The agent may *accidentally* achieve
the goal earlier, but it won’t *know* that it has done so.

This is also tricky. If the same action can have different costs in
different states, then the cost of taking an action in a given belief
state could be one of several values. (This gives rise to a new class of
problems, which we explore in .)[multivalued-sensorless-page] For now we
assume that the cost of an action is the same in all states and so can
be transferred directly from the underlying physical problem.

shows the reachable belief-state space for the deterministic, sensorless
vacuum world. There are only 12 reachable belief states out of
$2^8 \eq 256$ possible belief states.

[vacuum-prediction-figure]

[vacuum2-sets-figure]

The preceding definitions enable the automatic construction of the
belief-state problem formulation from the definition of the underlying
physical problem. Once this is done, we can apply any of the search
algorithms of . In fact, we can do a little bit more than that. In
“ordinary” graph search, newly generated states are tested to see if
they are identical to existing states. This works for belief states,
too; for example, in , the action sequence [,,] starting at the initial
state reaches the same belief state as [,,], namely, $\{5,7\}$. Now,
consider the belief state reached by [], namely, $\{1,3,5,7\}$.
Obviously, this is not identical to $\{5,7\}$, but it is a
*superset*. It is easy to prove () that if an action
sequence is a solution for a belief state $b$, it is also a solution for
any subset of $b$. Hence, we can discard a path reaching $\{1,3,5,7\}$
if $\{5,7\}$ has already been generated. Conversely, if $\{1,3,5,7\}$
has already been generated and found to be solvable, then any
*subset*, such as $\{5,7\}$, is guaranteed to be solvable.
This extra level of pruning may dramatically improve the efficiency of
sensorless problem solving.

Even with this improvement, however, sensorless problem-solving as we
have described it is seldom feasible in practice. The difficulty is not
so much the vastness of the belief-state space—even though it is
exponentially larger than the underlying physical state space; in most
cases the branching factor and solution length in the belief-state space
and physical state space are not so different. The real difficulty lies
with the size of each belief state. For example, the initial belief
state for the $10 \stimes
10$ vacuum world contains $100 \stimes 2^{100}$ or around $10^{32}$
physical states—far too many if we use the atomic representation, which
is an explicit list of states.

One solution is to represent the belief state by some more compact
description. In English, we could say the agent knows “Nothing” in the
initial state; after moving , we could say, “Not in the rightmost
column,” and so on. explains how to do this in a formal representation
scheme. Another approach is to avoid the standard search algorithms,
which treat belief states as black boxes just like any other problem
state. Instead, we can look *inside* the belief states and
develop algorithms that build up the solution one physical state at a
time. For example, in the sensorless vacuum world, the initial belief
state is $\{1,2,3,4,5,6,7,8\}$, and we have to find an action sequence
that works in all 8 states. We can do this by first finding a solution
that works for state 1; then we check if it works for state 2; if not,
go back and find a different solution for state 1, and so on. Just as an
and–or search has to find a solution for every branch at an
and node, this algorithm has to find a solution for every
state in the belief state; the difference is that and–or
search can find a different solution for each branch, whereas an
incremental belief-state search has to find *one* solution
that works for *all* the states.

The main advantage of the incremental approach is that it is typically
able to detect failure quickly—when a belief state is unsolvable, it is
usually the case that a small subset of the belief state, consisting of
the first few states examined, is also unsolvable. In some cases, this
leads to a speedup proportional to the size of the belief states, which
may themselves be as large as the physical state space itself.

Even the most efficient solution algorithm is not of much use when no
solutions exist. Many things just cannot be done without sensing. For
example, the sensorless is impossible. On the other hand, a little bit
of sensing can go a long way. For example, every 8-puzzle instance is
solvable if just one square is visible—the solution involves moving each
tile in turn into the visible square and then keeping track of its
location.

### Searching with observations {#percept-update-section}

For a general partially observable problem, we have to specify how the
environment generates percepts for the agent. For example, we might
define the local-sensing vacuum world to be one in which the agent has a
position sensor and a local dirt sensor but has no sensor capable of
detecting dirt in other squares. The formal problem specification
includes a $\prog{Percept}(s)$ function that returns the percept
received in a given state. (If sensing is nondeterministic, then we use
a function that returns a set of possible percepts.) For example, in the
local-sensing vacuum world, the in state 1 is $[A,{Dirty}]$. Fully
observable problems are a special case in which
$\noprog{Percept}(s)\eq s$ for every state $s$, while sensorless
problems are a special case in which $\noprog{Percept}(s)\eq {null}$.

[vacuum-prediction-update-figure]

When observations are partial, it will usually be the case that several
states could have produced any given percept. For example, the percept
$[A,{Dirty}]$ is produced by state 3 as well as by state 1. Hence,
given this as the initial percept, the initial belief state for the
local-sensing vacuum world will be $\{1,3\}$. The , , and are
constructed from the underlying physical problem just as for sensorless
problems, but the transition model is a bit more complicated. We can
think of transitions from one belief state to the next for a particular
action as occurring in three stages, as shown in :

-   The stage is the same as for sensorless problems: given the action
    $a$ in belief state $b$, the predicted belief state is
    ${\hat b} \eq \noprog{Predict}(b,a)$.[^11]

-   The stage determines the set of percepts $o$ that could be observed
    in the predicted belief state:
    $$\noprog{Possible-Percepts}({\hat b}) = \{o : o\eq \noprog{Percept}(s)
      \mbox{ and } s\in {\hat b}\}\ .$$

-   The stage determines, for each possible percept, the belief state
    that would result from the percept. The new belief state $b_o$ is
    just the set of states in ${\hat b}$ that could have produced the
    percept:
    $$b_o = \noprog{Update}({\hat b},o) = \{s : o\eq \noprog{Percept}(s)
      \mbox{ and } s\in {\hat b}\}\ .$$ Notice that each updated belief
    state $b_o$ can be no larger than the predicted belief state
    ${\hat b}$; observations can only help reduce uncertainty compared
    to the sensorless case. Moreover, for deterministic sensing, the
    belief states for the different possible percepts will be disjoint,
    forming a *partition* of the original predicted belief
    state.

Putting these three stages together, we obtain the possible belief
states resulting from a given action and the subsequent possible
percepts:

$$\begin{aligned}
  \Results(b,a) = \{b_o : b_o &=& \noprog{Update}(\noprog{Predict}(b,a),o) \mbox{ and } \nonumber\\
                            o &\in& \noprog{Possible-Percepts}(\noprog{Predict}(b,a))\}\ .
\label{poproblem-results-equation}\end{aligned}$$

Again, the nondeterminism in the partially observable problem comes from
the inability to predict exactly which percept will be received after
acting; underlying nondeterminism in the physical environment may
*contribute* to this inability by enlarging the belief
state at the prediction stage, leading to more percepts at the
observation stage.

### Solving partially observable problems

The preceding section showed how to derive the function for a
nondeterministic belief-state problem from an underlying physical
problem and the function. Given such a formulation, the
and–or search algorithm of can be applied directly to
derive a solution. shows part of the search tree for the local-sensing
vacuum world, assuming an initial percept $[A,{Dirty}]$. The solution
is the conditional plan
$$[\act{Suck},\act{Right},\condstep{{Bstate}\eq
  \{6\}}{\act{Suck}}{\noplan}]\ .$$ Notice that, because we supplied a
belief-state problem to the and–or search algorithm, it
returned a conditional plan that tests the belief state rather than the
actual state. This is as it should be: in a partially observable
environment the agent won’t be able to execute a solution that requires
testing the actual state.

[local-sensing-vacuum-and-or-figure]

As in the case of standard search algorithms applied to sensorless
problems, the and–or search algorithm treats belief states
as black boxes, just like any other states. One can improve on this by
checking for previously generated belief states that are subsets or
supersets of the current state, just as for sensorless problems. One can
also derive incremental search algorithms, analogous to those described
for sensorless problems, that provide substantial speedups over the
black-box approach.

### An agent for partially observable environments

[kindergarten-vacuum-filtering-figure]

The design of a problem-solving agent for partially observable
environments is quite similar to the simple problem-solving agent in :
the agent formulates a problem, calls a search algorithm (such as ) to
solve it, and executes the solution. There are two main differences.
First, the solution to a problem will be a conditional plan rather than
a sequence; if the first step is an if–then–else expression, the agent
will need to test the condition in the if-part and execute the then-part
or the else-part accordingly. Second, the agent will need to maintain
its belief state as it performs actions and receives percepts. This
process resembles the prediction–observation–update process in but is
actually simpler because the percept is given by the environment rather
than calculated by the agent. Given an initial belief state $b$, an
action $a$, and a percept $o$, the new belief state is:

$$b' = \noprog{Update}(\noprog{Predict}(b,a),o)\ .
\label{poproblem-filtering-equation}$$

shows the belief state being maintained in the
*kindergarten* vacuum world with local sensing, wherein any
square may become dirty at any time unless the agent is actively
cleaning it at that moment.[^12]

In partially observable environments—which include the vast majority of
real-world environments—maintaining one’s belief state is a core
function of any intelligent system. This function goes under various
names, including , and . is called a state estimator because it computes
the new belief state from the previous one rather than by examining the
entire percept sequence. If the agent is not to “fall behind,” the
computation has to happen as fast as percepts are coming in. As the
environment becomes more complex, the exact update computation becomes
infeasible and the agent will have to compute an approximate belief
state, perhaps focusing on the implications of the percept for the
aspects of the environment that are of current interest. Most work on
this problem has been done for stochastic, continuous-state environments
with the tools of probability theory, as explained in . Here we will
show an example in a discrete environment with detrministic sensors and
nondeterministic actions.

[vacuum-localization-nondet-page] The example concerns a robot with the
task of : working out where it is, given a map of the world and a
sequence of percepts and actions. Our robot is placed in the maze-like
environment of . The robot is equipped with four sonar sensors that tell
whether there is an obstacle—the outer wall or a black square in the
figure—in each of the four compass directions. We assume that the
sensors give perfectly correct data, and that the robot has a correct
map of the enviornment. But unfortunately the robot’s navigational
system is broken, so when it executes a ${Move}$ action, it moves
randomly to one of the adjacent squares. The robot’s task is to
determine its current location.

Suppose the robot has just been switched on, so it does not know where
it is. Thus its initial belief state $b$ consists of the set of all
locations. The the robot receives the percept *NSW*,
meaning there are obstacles to the north, west, and south, and does an
update using the equation $b_o \eq \noprog{Update}(b)$, yielding the 4
locations shown in (a). You can inspect the maze to see that those are
the only four locations that yield the percept ${NWS}$.

[vacuum-maze-ch4-figure]

Next the robot executes a ${Move}$ action, but the result is
nondeterministic. The new belief state, $b_a \eq
\noprog{Predict}(b_o, {Move})$, contains all the locations that are
one step away from the locations in $b_o$. When the second percept,
${NS}$, arrives, the robot does $\noprog{Update}(b_a, {NS})$ and
finds that the belief state has collapsed down to the single location
shown in (b). That’s the only location that could be the result of
$$\noprog{Update}(\noprog{Predict}(\noprog{Update}(b, {NSW}), {Move}), {NS}) \ .$$
With nondetermnistic actions the step grows the belief state, but the
step shrinks it back down—as long as the percepts provide some useful
identifying information. Sometimes the percepts don’t help much for
localization: If there were one or more long east-west corridors, then a
robot could receive a long sequence of $NS$ percepts, but never know
where in the corridor(s) it was.

Online Search Agents and Unknown Environments {#online-search-section}
---------------------------------------------

So far we have concentrated on agents that use algorithms. They compute
a complete solution before setting foot in the real world and then
execute the solution. In contrast, an [^13] agent computation and
action: first it takes an action, then it observes the environment and
computes the next action. Online search is a good idea in dynamic or
semidynamic domains—domains where there is a penalty for sitting around
and computing too long. Online search is also helpful in
nondeterministic domains because it allows the agent to focus its
computational efforts on the contingencies that actually arise rather
than those that *might* happen but probably won’t. Of
course, there is a tradeoff: the more an agent plans ahead, the less
often it will find itself up the creek without a paddle.

Online search is a *necessary* idea for unknown
environments, where the agent does not know what states exist or what
its actions do. In this state of ignorance, the agent faces an and must
use its actions as experiments in order to learn enough to make
deliberation worthwhile.

The canonical example of online search is a robot that is placed in a
new building and must explore it to build a map that it can use for
getting from $A$ to $B$. Methods for escaping from labyrinths—required
knowledge for aspiring heroes of antiquity—are also examples of online
search algorithms. Spatial exploration is not the only form of
exploration, however. Consider a newborn baby: it has many possible
actions but knows the outcomes of none of them, and it has experienced
only a few of the possible states that it can reach. The baby’s gradual
discovery of how the world works is, in part, an online search process.

### Online search problems

An online search problem must be solved by an agent executing actions,
rather than by pure computation. We assume a deterministic and fully
observable environment ( relaxes these assumptions), but we stipulate
that the agent knows only the following:

-   $\noprog{Actions}(s)$, which returns a list of actions allowed in
    state $s$;

-   The step-cost function $c(s,a,s')$—note that this cannot be used
    until the agent knows that $s'$ is the outcome; and

-   $\noprog{Goal-Test}(s)$.

Note in particular that the agent *cannot* determine
$\result{s}{a}$ except by actually being in $s$ and doing $a$. For
example, in the maze problem shown in , the agent does not know that
going ${Up}$ from (1,1) leads to (1,2); nor, having done that, does it
know that going ${Down}$ will take it back to (1,1). This degree of
ignorance can be reduced in some applications—for example, a robot
explorer might know how its movement actions work and be ignorant only
of the locations of obstacles.

Finally, the agent might have access to an admissible heuristic function
$h(s)$ that estimates the distance from the current state to a goal
state. For example, in , the agent might know the location of the goal
and be able to use the Manhattan-distance heuristic.

[maze-3x3-figure]

Typically, the agent’s objective is to reach a goal state while
minimizing cost. (Another possible objective is simply to explore the
entire environment.) The cost is the total path cost of the path that
the agent actually travels. It is common to compare this cost with the
path cost of the path the agent would follow *if it knew the
search space in advance*—that is, the actual shortest path (or
shortest complete exploration). In the language of online algorithms,
this is called the ; we would like it to be as small as possible.

[dead-end-figure]

Although this sounds like a reasonable request, it is easy to see that
the best achievable competitive ratio is infinite in some cases. For
example, if some actions are —i.e., they lead to a state from which no
action leads back to the previous state—the online search might
accidentally reach a state from which no goal state is reachable.
Perhaps the term “accidentally” is unconvincing—after all, there might
be an algorithm that happens not to take the dead-end path as it
explores. Our claim, to be more precise, is that

no algorithm can avoid dead ends in all state spaces.

Consider the two dead-end state spaces in (a). To an online search
algorithm that has visited states $S$ and $A$, the two state spaces look
*identical*, so it must make the same decision in both.
Therefore, it will fail in one of them. This is an example of an —we can
imagine an adversary constructing the state space while the agent
explores it and putting the goals and dead ends wherever it chooses.

Dead ends are a real difficulty for robot exploration—staircases, ramps,
cliffs, one-way streets, and all kinds of natural terrain present
opportunities for irreversible actions. To make progress, we simply
assume that the state space is —that is, some goal state is reachable
from every reachable state. State spaces with reversible actions, such
as mazes and 8-puzzles, can be viewed as undirected graphs and are
clearly safely explorable.

Even in safely explorable environments, no bounded competitive ratio can
be guaranteed if there are paths of unbounded cost. This is easy to show
in environments with irreversible actions, but in fact it remains true
for the reversible case as well, as (b) shows. For this reason, it is
common to describe the performance of online search algorithms in terms
of the size of the entire state space rather than just the depth of the
shallowest goal.

### Online search agents

After each action, an online agent receives a percept telling it what
state it has reached; from this information, it can augment its map of
the environment. The current map is used to decide where to go next.
This interleaving of planning and action means that online search
algorithms are quite different from the offline search algorithms we
have seen previously. For example, offline algorithms such as A can
expand a node in one part of the space and then immediately expand a
node in another part of the space, because node expansion involves
simulated rather than real actions. An online algorithm, on the other
hand, can discover successors only for a node that it physically
occupies. To avoid traveling all the way across the tree to expand the
next node, it seems better to expand nodes in a *local*
order. Depth-first search has exactly this property because (except when
backtracking) the next node expanded is a child of the previous node
expanded.

[online-dfs-agent-algorithm]

An online depth-first search agent is shown in . This agent stores its
map in a table, ${\Result}[s,a]$, that records the state resulting from
executing action $a$ in state $s$. Whenever an action from the current
state has not been explored, the agent tries that action. The difficulty
comes when the agent has tried all the actions in a state. In offline
depth-first search, the state is simply dropped from the queue; in an
online search, the agent has to backtrack physically. In depth-first
search, this means going back to the state from which the agent most
recently entered the current state. To achieve that, the algorithm keeps
a table that lists, for each state, the predecessor states to which the
agent has not yet backtracked. If the agent has run out of states to
which it can backtrack, then its search is complete.

We recommend that the reader trace through the progress of when applied
to the maze given in . It is fairly easy to see that the agent will, in
the worst case, end up traversing every link in the state space exactly
twice. For exploration, this is optimal; for finding a goal, on the
other hand, the agent’s competitive ratio could be arbitrarily bad if it
goes off on a long excursion when there is a goal right next to the
initial state. An online variant of iterative deepening solves this
problem; for an environment that is a uniform tree, the competitive
ratio of such an agent is a small constant.

Because of its method of backtracking, works only in state spaces where
the actions are reversible. There are slightly more complex algorithms
that work in general state spaces, but no such algorithm has a bounded
competitive ratio.

### Online local search

Like depth-first search, has the property of locality in its node
expansions. In fact, because it keeps just one current state in memory,
hill-climbing search is *already* an online search
algorithm! Unfortunately, it is not very useful in its simplest form
because it leaves the agent sitting at local maxima with nowhere to go.
Moreover, random restarts cannot be used, because the agent cannot
transport itself to a new state.

Instead of random restarts, one might consider using a to explore the
environment. A random walk simply selects at random one of the available
actions from the current state; preference can be given to actions that
have not yet been tried. It is easy to prove that a random walk will
*eventually* find a goal or complete its exploration,
provided that the space is finite.[^14] On the other hand, the process
can be very slow. shows an environment in which a random walk will take
exponentially many steps to find the goal because, at each step,
backward progress is twice as likely as forward progress. The example is
contrived, of course, but there are many real-world state spaces whose
topology causes these kinds of “traps” for random walks.

[quicksand-figure]

Augmenting hill climbing with *memory* rather than
randomness turns out to be a more effective approach. The basic idea is
to store a “current best estimate” $H(s)$ of the cost to reach the goal
from each state that has been visited. $H(s)$ starts out being just the
heuristic estimate $h(s)$ and is updated as the agent gains experience
in the state space. shows a simple example in a one-dimensional state
space. In (a), the agent seems to be stuck in a flat local minimum at
the shaded state. Rather than staying where it is, the agent should
follow what seems to be the best path to the goal given the current cost
estimates for its neighbors. The estimated cost to reach the goal
through a neighbor $s'$ is the cost to get to $s'$ plus the estimated
cost to get to a goal from there—that is, $c(s,a,s') + H(s')$. In the
example, there are two actions, with estimated costs $1+9$ and $1+2$, so
it seems best to move right. Now, it is clear that the cost estimate of
2 for the shaded state was overly optimistic. Since the best move cost 1
and led to a state that is at least 2 steps from a goal, the shaded
state must be at least 3 steps from a goal, so its $H$ should be updated
accordingly, as shown in (b). Continuing this process, the agent will
move back and forth twice more, updating $H$ each time and “flattening
out” the local minimum until it escapes to the right.

An agent implementing this scheme, which is called learning real-time A
(), is shown in . Like , it builds a map of the environment in the
řesult table. It updates the cost estimate for the state it has just
left and then chooses the “apparently best” move according to its
current cost estimates. One important detail is that actions that have
not yet been tried in a state $s$ are always assumed to lead immediately
to the goal with the least possible cost, namely $h(s)$. This encourages
the agent to explore new, possibly promising paths.

[lrta-progress-figure]

[lrta-agent-algorithm]

An LRTA agent is guaranteed to find a goal in any finite, safely
explorable environment. Unlike A, however, it is not complete for
infinite state spaces—there are cases where it can be led infinitely
astray. It can explore an environment of $n$ states in $O(n^2)$ steps in
the worst case, but often does much better. The LRTA agent is just one
of a large family of online agents that one can define by specifying the
action selection rule and the update rule in different ways. We discuss
this family, developed originally for stochastic environments, in .

### Learning in online search

The initial ignorance of online search agents provides several
opportunities for learning. First, the agents learn a “map” of the
environment—more precisely, the outcome of each action in each
state—simply by recording each of their experiences. (Notice that the
assumption of deterministic environments means that one experience is
enough for each action.) Second, the local search agents acquire more
accurate estimates of the cost of each state by using local updating
rules, as in LRTA. In , we show that these updates eventually converge
to *exact* values for every state, provided that the agent
explores the state space in the right way. Once exact values are known,
optimal decisions can be taken simply by moving to the lowest-cost
successor—that is, pure hill climbing is then an optimal strategy.

If you followed our suggestion to trace the behavior of in the
environment of , you will have noticed that the agent is not very
bright. For example, after it has seen that the ${Up}$ action goes
from (1,1) to (1,2), the agent still has no idea that the ${Down}$
action goes back to (1,1) or that the ${Up}$ action also goes from
(2,1) to (2,2), from (2,2) to (2,3), and so on. In general, we would
like the agent to learn that ${Up}$ increases the $y$-coordinate
unless there is a wall in the way, that ${Down}$ reduces it, and so
on. For this to happen, we need two things. First, we need a formal and
explicitly manipulable representation for these kinds of general rules;
so far, we have hidden the information inside the black box called the
function. is devoted to this issue. Second, we need algorithms that can
construct suitable general rules from the specific observations made by
the agent. These are covered in .

This chapter has examined search algorithms for problems beyond the
“classical” case of finding the shortest path to a goal in an
observable, deterministic, discrete environment.

-   *Local search* methods such as operate on
    complete-state formulations, keeping only a small number of nodes in
    memory. Several stochastic algorithms have been developed, including
    , which returns optimal solutions when given an appropriate cooling
    schedule.

-   Many local search methods apply also to problems in continuous
    spaces. and problems obey certain restrictions on the shape of the
    state space and the nature of the objective function, and admit
    polynomial-time algorithms that are often extremely efficient in
    practice.

-   A is a stochastic hill-climbing search in which a large population
    of states is maintained. New states are generated by and by , which
    combines pairs of states from the population.

-   In environments, agents can apply and–or search to
    generate plans that reach the goal regardless of which outcomes
    occur during execution.

-   When the environment is partially observable, the represents the set
    of possible states that the agent might be in.

-   Standard search algorithms can be applied directly to belief-state
    space to solve , and belief-state and–or search can
    solve general partially observable problems. Incremental algorithms
    that construct solutions state-by-state within a belief state are
    often more efficient.

-   arise when the agent has no idea about the states and actions of its
    environment. For safely explorable environments, agents can build a
    map and find a goal if one exists. Updating heuristic estimates from
    experience provides an effective method to escape from local minima.

Local search techniques have a long history in mathematics and computer
science. Indeed, the Newton–Raphson method @Newton:1664 [@Raphson:1690]
can be seen as a very efficient local search method for continuous
spaces in which gradient information is available. is a classic
reference for optimization algorithms that do not require such
information. Beam search, which we have presented as a local search
algorithm, originated as a bounded-width variant of dynamic programming
for speech recognition in the system @Lowerre:1976. A related algorithm
is analyzed in depth by .

The topic of local search was reinvigorated in the early 1990s by
surprisingly good results for large constraint-satisfaction problems
such as $n$-queens @Minton+al:1992 and logical reasoning @Selman+al:1992
and by the incorporation of randomness, multiple simultaneous searches,
and other improvements. This renaissance of what Christos Papadimitriou
has called “New Age” algorithms also sparked increased interest among
theoretical computer scientists @Koutsoupias+Papadimitriou:1992
[@Aldous+Vazirani:1994]. In the field of operations research, a variant
of hill climbing called has gained popularity @Glover+Laguna:1997. This
algorithm maintains a tabu list of $k$ previously visited states that
cannot be revisited; as well as improving efficiency when searching
graphs, this list can allow the algorithm to escape from some local
minima. Another useful improvement on hill climbing is the
algorithm @Boyan+Moore:1998. The idea is to use the local maxima found
by random-restart hill climbing to get an idea of the overall shape of
the landscape. The algorithm fits a smooth surface to the set of local
maxima and then calculates the global maximum of that surface
analytically. This becomes the new restart point. The algorithm has been
shown to work in practice on hard problems. showed that the run times of
systematic backtracking algorithms often have a , which means that the
probability of a very long run time is more than would be predicted if
the run times were exponentially distributed. When the run time
distribution is heavy-tailed, random restarts find a solution faster, on
average, than a single run to completion.

Simulated annealing was first described by , who borrowed directly from
the (which is used to simulate complex systems in
physics @Metropolis+al:1953 and was supposedly invented at a Los Alamos
dinner party). Simulated annealing is now a field in itself, with
hundreds of papers published every year.

Finding optimal solutions in continuous spaces is the subject matter of
several fields, including , , and the . The basic techniques are
explained well by ; cover a wide range of algorithms and provide working
software.

As Andrew Moore points out, researchers have taken inspiration for
search and optimization algorithms from a wide variety of fields of
study: metallurgy (simulated annealing), biology (genetic algorithms),
economics (market-based algorithms), entomology (ant colony
optimization), neurology (neural networks), animal behavior
(reinforcement learning), mountaineering (hill climbing), and others.

(LP) was first studied systematically by the Russian mathematician
Leonid Kantorovich [-@Kantorovich:1939]. It was one of the first
applications of computers; the  @Dantzig:1949 is still used despite
worst-case exponential complexity. developed the far more efficient
family of methods, which was shown to have polynomial complexity for the
more general class of convex optimization problems by . Excellent
introductions to convex optimization are provided by and .

Work by Sewall Wright [-@Wright:1931] on the concept of a was an
important precursor to the development of genetic algorithms. In the
1950s, several statisticians, including Box [-@Box:1957] and
Friedman [-@Friedman:1959], used evolutionary techniques for
optimization problems, but it wasn’t until
Rechenberg [-@Rechenberg:1965] introduced to solve optimization problems
for airfoils that the approach gained popularity. In the 1960s and
1970s, John Holland [-@Holland:1975] championed genetic algorithms, both
as a useful tool and as a method to expand our understanding of
adaptation, biological or otherwise @Holland:1995. The
movement @Langton:1995 takes this idea one step further, viewing the
products of genetic algorithms as *organisms* rather than
solutions to problems. Work in this field by and has done much to
clarify the implications of the Baldwin effect. For general background
on evolution, we recommend , , and .

Most comparisons of genetic algorithms to other approaches (especially
stochastic hill climbing) have found that the genetic algorithms are
slower to converge @O'Reilly+Oppacher:1994
[@Mitchell+al:1994; @Juels+Wattenberg:1996; @Baluja:1997]. Such findings
are not universally popular within the GA community, but recent attempts
within that community to understand population-based search as an
approximate form of Bayesian learning (see ) might help close the gap
between the field and its critics @Pelikan+al:1999. The theory of may
also explain the performance of GAs @Rabani+al:1998. See for an example
of GAs applied to antenna design, and for an application to
computer-aided design.

The field of is closely related to genetic algorithms. The principal
difference is that the representations that are mutated and combined are
programs rather than bit strings. The programs are represented in the
form of expression trees; the expressions can be in a standard language
such as Lisp or can be specially designed to represent circuits, robot
controllers, and so on. Crossover involves splicing together subtrees
rather than substrings. This form of mutation guarantees that the
offspring are well-formed expressions, which would not be the case if
programs were manipulated as strings.

Interest in genetic programming was spurred by John Koza’s
work @Koza:1992 [@Koza:1994], but it goes back at least to early
experiments with machine code by and with finite-state automata by . As
with genetic algorithms, there is debate about the effectiveness of the
technique. describe experiments in the use of genetic programming to
design circuit devices.

The journals *Evolutionary Computation* and *IEEE
Transactions on Evolutionary Computation* cover genetic
algorithms and genetic programming; articles are also found in
*Complex Systems*, *Adaptive Behavior*, and
*Artificial Life*. The main conference is the
*Genetic and Evolutionary Computation Conference* (GECCO).
Good overview texts on genetic algorithms are given by , , and , and by
the free online book by .

The unpredictability and partial observability of real environments were
recognized early on in robotics projects that used planning techniques,
including Shakey @Fikes+al:1972 and  @Michie:1974. The problems received
more attention after the publication of McDermott’s [-@McDermott:1978a]
influential article, *Planning and Acting*.

The first work to make explicit use of and–or trees seems
to have been Slagle’s program for symbolic integration, mentioned in .
applied the idea to propositional theorem proving, a topic discussed in
, and introduced a search algorithm similar to . The algorithm was
further developed and formalized by , who also described AO$^*$—which,
as its name suggests, finds optimal solutions given an admissible
heuristic. AO$^*$ was analyzed and improved by . AO$^*$ is a top-down
algorithm; a bottom-up generalization of A is ALD, for A Lightest
Derivation @Felzenszwalb+McAllester:2007. Interest in
and–or search has undergone a revival in recent years, with
new algorithms for finding cyclic solutions @Jimenez+Torras:2000
[@Hansen+Zilberstein:2001] and new techniques inspired by dynamic
programming @Bonet+Geffner:2005.

The idea of transforming partially observable problems into belief-state
problems originated with for the much more complex case of probabilistic
uncertainty (see ). studied the problem of robotic manipulation without
sensors, using a continuous form of belief-state search. They showed
that it was possible to orient a part on a table from an arbitrary
initial position by a well-designed sequence of tilting actions. More
practical methods, based on a series of precisely oriented diagonal
barriers across a conveyor belt, use the same algorithmic
insights @Wiegley+al:1996.

The belief-state approach was reinvented in the context of sensorless
and partially observable search problems by . Additional work was done
on sensorless problems in the logic-based planning
community @Goldman+Boddy:1996 [@Smith+Weld:1998]. This work has
emphasized concise representations for belief states, as explained in .
introduced the first effective heuristics for belief-state search; these
were refined by . The incremental approach to belief-state search, in
which solutions are constructed incrementally for subsets of states
within each belief state, was studied in the planning literature by ;
several new incremental algorithms were introduced for nondeterministic,
partially observable problems by . Additional references for planning in
stochastic, partially observable environments appear in .

Algorithms for exploring unknown state spaces have been of interest for
many centuries. Depth-first search in a maze can be implemented by
keeping one’s left hand on the wall; loops can be avoided by marking
each junction. Depth-first search fails with irreversible actions; the
more general problem of exploring (i.e., graphs in which each node has
equal numbers of incoming and outgoing edges) was solved by an algorithm
due to . The first thorough algorithmic study of the exploration problem
for arbitrary graphs was carried out by , who developed a completely
general algorithm but showed that no bounded competitive ratio is
possible for exploring a general graph. examined the question of finding
paths to a goal in geometric path-planning environments (where all
actions are reversible). They showed that a small competitive ratio is
achievable with square obstacles, but with general rectangular obstacles
no bounded ratio can be achieved. (See .)

The LRTA algorithm was developed by as part of an investigation into for
environments in which the agent must act after searching for only a
fixed amount of time (a common situation in two-player games). LRTA is
in fact a special case of reinforcement learning algorithms for
stochastic environments @Barto+al:1995. Its policy of optimism under
uncertainty—always head for the closest unvisited state—can result in an
exploration pattern that is less efficient in the uninformed case than
simple depth-first search @Koenig:2000. show that online iterative
deepening search is optimally efficient for finding a goal in a uniform
tree with no heuristic information. Several informed variants on the
LRTA theme have been developed with different methods for searching and
updating within the known portion of the graph @Pemberton+Korf:1992. As
yet, there is no good understanding of how to find goals with optimal
efficiency when using heuristic information.

Give the name of the algorithm that results from each of the following
special cases:

1.  Local beam search with $k = 1$.

2.  Local beam search with one initial state and no limit on the number
    of states retained.

3.  Simulated annealing with $T = 0$ at all times (and omitting the
    termination test).

4.  Simulated annealing with $T=\infty$ at all times.

5.  Genetic algorithm with population size $N = 1$.

considers the problem of building railway tracks under the assumption
that pieces fit exactly with no slack. Now consider the real problem, in
which pieces don’t fit exactly but allow for up to 10 degrees of
rotation to either side of the “proper” alignment. Explain how to
formulate the problem so it could be solved by simulated annealing.

In this exercise, we explore the use of local search methods to solve
TSPs of the type defined in .

1.  Implement and test a hill-climbing method to solve TSPs. Compare the
    results with optimal solutions obtained from the A algorithm with
    the MST heuristic ().

2.  Repeat part (a) using a genetic algorithm instead of hill climbing.
    You may want to consult for some suggestions for representations.

[hill-climbing-exercise]Generate a large number of 8-puzzle and 8-queens
instances and solve them (where possible) by hill climbing
(steepest-ascent and first-choice variants), hill climbing with random
restart, and simulated annealing. Measure the search cost and percentage
of solved problems and graph these against the optimal solution cost.
Comment on your results.

[cond-plan-repeated-exercise] The algorithm in checks for repeated
states only on the path from the root to the current state. Suppose
that, in addition, the algorithm were to store *every*
visited state and check against that list. (See in for an example.)
Determine the information that should be stored and how the algorithm
should use that information when a repeated state is found.
(*Hint*: You will need to distinguish at least between
states for which a successful subplan was constructed previously and
states for which no subplan could be found.) Explain how to use labels,
as defined in , to avoid having multiple copies of subplans.

[cond-loop-exercise]Explain precisely how to modify the algorithm to
generate a cyclic plan if no acyclic plan exists. You will need to deal
with three issues: labeling the plan steps so that a cyclic plan can
point back to an earlier part of the plan, modifying so that it
continues to look for acyclic plans after finding a cyclic plan, and
augmenting the plan representation to indicate whether a plan is cyclic.
Show how your algorithm works on (a) the slippery vacuum world, and (b)
the slippery, erratic vacuum world. You might wish to use a computer
implementation to check your results.

In we introduced belief states to solve sensorless search problems. A
sequence of actions solves a sensorless problem if it maps every
physical state in the initial belief state $b$ to a goal state. Suppose
the agent knows $h^*(s)$, the true optimal cost of solving the physical
state $s$ in the fully observable problem, for every state $s$ in $b$.
Find an admissible heuristic $h(b)$ for the sensorless problem in terms
of these costs, and prove its admissibilty. Comment on the accuracy of
this heuristic on the sensorless vacuum problem of . How well does A
perform?

[belief-state-superset-exercise] This exercise explores subset–superset
relations between belief states in sensorless or partially observable
environments.

1.  Prove that if an action sequence is a solution for a belief state
    $b$, it is also a solution for any subset of $b$. Can anything be
    said about supersets of $b$?

2.  Explain in detail how to modify graph search for sensorless problems
    to take advantage of your answers in (a).

3.  Explain in detail how to modify and–or search for
    partially observable problems, beyond the modifications you describe
    in (b).

[multivalued-sensorless-exercise] On it was assumed that a given action
would have the same cost when executed in any physical state within a
given belief state. (This leads to a belief-state search problem with
well-defined step costs.) Now consider what happens when the assumption
does not hold. Does the notion of optimality still make sense in this
context, or does it require modification? Consider also various possible
definitions of the “cost” of executing an action in a belief state; for
example, we could use the *minimum* of the physical costs;
or the *maximum*; or a cost *interval* with
the lower bound being the minimum cost and the upper bound being the
maximum; or just keep the set of all possible costs for that action. For
each of these, explore whether A (with modifications if necessary) can
return optimal solutions.

[vacuum-solvable-exercise]Consider the sensorless version of the erratic
vacuum world. Draw the belief-state space reachable from the initial
belief state $\{\N{1,2,3,4,5,6,7,8}\}$, and explain why the problem is
unsolvable.

[vacuum-solvable-exercise]Consider the sensorless version of the erratic
vacuum world. Draw the belief-state space reachable from the initial
belief state $\{\N{1,3,5,7}\}$, and explain why the problem is
unsolvable.

[path-planning-agent-exercise]We can turn the navigation problem in into
an environment as follows:

-   The percept will be a list of the positions, *relative to the
    agent*, of the visible vertices. The percept does
    *not* include the position of the robot! The robot must
    learn its own position from the map; for now, you can assume that
    each location has a different “view.”

-   Each action will be a vector describing a straight-line path to
    follow. If the path is unobstructed, the action succeeds; otherwise,
    the robot stops at the point where its path first intersects an
    obstacle. If the agent returns a zero motion vector and is at the
    goal (which is fixed and known), then the environment teleports the
    agent to a *random location* (not inside an obstacle).

-   The performance measure charges the agent 1 point for each unit of
    distance traversed and awards 1000 points each time the goal is
    reached.

1.  Implement this environment and a problem-solving agent for it. After
    each teleportation, the agent will need to formulate a new problem,
    which will involve discovering its current location.

2.  Document your agent’s performance (by having the agent generate
    suitable commentary as it moves around) and report its performance
    over 100 episodes.

3.  Modify the environment so that 30% of the time the agent ends up at
    an unintended destination (chosen randomly from the other visible
    vertices if any; otherwise, no move at all). This is a crude model
    of the motion errors of a real robot. Modify the agent so that when
    such an error is detected, it finds out where it is and then
    constructs a plan to get back to where it was and resume the old
    plan. Remember that sometimes getting back to where it was might
    also fail! Show an example of the agent successfully overcoming two
    successive motion errors and still reaching the goal.

4.  Now try two different recovery schemes after an error: (1) head for
    the closest vertex on the original route; and (2) replan a route to
    the goal from the new location. Compare the performance of the three
    recovery schemes. Would the inclusion of search costs affect the
    comparison?

5.  Now suppose that there are locations from which the view is
    identical. (For example, suppose the world is a grid with square
    obstacles.) What kind of problem does the agent now face? What do
    solutions look like?

[online-offline-exercise]Suppose that an agent is in a $3\stimes 3$ maze
environment like the one shown in . The agent knows that its initial
location is (1,1), that the goal is at (3,3), and that the actions
${Up}$, ${Down}$, ${Left}$, ${Right}$ have their usual effects
unless blocked by a wall. The agent does *not* know where
the internal walls are. In any given state, the agent perceives the set
of legal actions; it can also tell whether the state is one it has
visited before.

1.  Explain how this online search problem can be viewed as an offline
    search in belief-state space, where the initial belief state
    includes all possible environment configurations. How large is the
    initial belief state? How large is the space of belief states?

2.  How many distinct percepts are possible in the initial state?

3.  Describe the first few branches of a contingency plan for this
    problem. How large (roughly) is the complete plan?

Notice that this contingency plan is a solution for *every
possible environment* fitting the given description. Therefore,
interleaving of search and execution is not strictly necessary even in
unknown environments.

[online-offline-exercise]Suppose that an agent is in a $3\stimes 3$ maze
environment like the one shown in . The agent knows that its initial
location is (3,3), that the goal is at (1,1), and that the four actions
${Up}$, ${Down}$, ${Left}$, ${Right}$ have their usual effects
unless blocked by a wall. The agent does *not* know where
the internal walls are. In any given state, the agent perceives the set
of legal actions; it can also tell whether the state is one it has
visited before or is a new state.

1.  Explain how this online search problem can be viewed as an offline
    search in belief-state space, where the initial belief state
    includes all possible environment configurations. How large is the
    initial belief state? How large is the space of belief states?

2.  How many distinct percepts are possible in the initial state?

3.  Describe the first few branches of a contingency plan for this
    problem. How large (roughly) is the complete plan?

Notice that this contingency plan is a solution for *every
possible environment* fitting the given description. Therefore,
interleaving of search and execution is not strictly necessary even in
unknown environments.

[path-planning-hc-exercise]In this exercise, we examine hill climbing in
the context of robot navigation, using the environment in as an example.

1.  Repeat using hill climbing. Does your agent ever get stuck in a
    local minimum? Is it *possible* for it to get stuck
    with convex obstacles?

2.  Construct a nonconvex polygonal environment in which the agent gets
    stuck.

3.  Modify the hill-climbing algorithm so that, instead of doing a
    depth-1 search to decide where to go next, it does a depth-$k$
    search. It should find the best $k$-step path and do one step along
    it, and then repeat the process.

4.  Is there some $k$ for which the new algorithm is guaranteed to
    escape from local minima?

5.  Explain how LRTA enables the agent to escape from local minima in
    this case.

Like DFS, online DFS is incomplete for reversible state spaces with
infinite paths. For example, suppose that states are points on the
infinite two-dimensional grid and actions are unit vectors $(1,0)$,
$(0,1)$, $(-1,0)$, $(0,-1)$, tried in that order. Show that online DFS
starting at $(0,0)$ will not reach $(1,-1)$. Suppose the agent can
observe, in addition to its current state, all successor states and the
actions that would lead to them. Write an algorithm that is complete
even for bidirected state spaces with infinite paths. What states does
it visit in reaching $(1,-1)$?

Relate the time complexity of LRTA to its space complexity.

[^1]: Generating a *random* state from an implicitly
    specified state space can be a hard problem in itself.

[^2]: prove that it is best, in some cases, to restart a randomized
    search algorithm after a particular, fixed amount of time and that
    this can be *much* more efficient than letting each
    search continue indefinitely. Disallowing or limiting the number of
    sideways moves is an example of this idea.

[^3]: Local beam search is an adaptation of , which is a path-based
    algorithm.

[^4]: There are many variants of this selection rule. The method of , in
    which all individuals below a given threshold are discarded, can be
    shown to converge faster than the random version @Baum+al:1995.

[^5]: It is here that the encoding matters. If a 24-bit encoding is used
    instead of 8 digits, then the crossover point has a 2/3 chance of
    being in the middle of a digit, which results in an essentially
    arbitrary mutation of that digit.

[^6]: A basic knowledge of multivariate calculus and vector arithmetic
    is useful for reading this section.

[^7]: In general, the Newton–Raphson update can be seen as fitting a
    quadratic surface to $f$ at $\x$ and then moving directly to the
    minimum of that surface—which is also the minimum of $f$ if $f$ is
    quadratic.

[^8]: A set of points ${\cal S}$ is convex if the line joining any two
    points in ${\cal S}$ is also contained in ${\cal S}$. A is one for
    which the space “above” it forms a convex set; by definition, convex
    functions have no local (as opposed to global) minima.

[^9]: We assume that most readers face similar problems and can
    sympathize with our agent. We apologize to owners of modern,
    efficient home appliances who cannot take advantage of this
    pedagogical device.

[^10]: In a fully observable environment, each belief state contains one
    physical state. Thus, we can view the algorithms in as searching in
    a belief-state space of singleton belief states.

[^11]: Here, and throughout the book, the “hat” in $\hat{b}$ means an
    estimated or predicted value for $b$.

[^12]: The usual apologies to those who are unfamiliar with the effect
    of small children on the environment.

[^13]: The term “online” is commonly used in computer science to refer
    to algorithms that must process input data as they are received
    rather than waiting for the entire input data set to become
    available.

[^14]: Random walks are complete on infinite one-dimensional and
    two-dimensional grids. On a three-dimensional grid, the probability
    that the walk ever returns to the starting point is only about
    0.3405 @Hughes:1995.
Intelligent Agents {#agents-chapter}
==================

identified the concept of as central to our approach to artificial
intelligence. In this chapter, we make this notion more concrete. We
will see that the concept of rationality can be applied to a wide
variety of agents operating in any imaginable environment. Our plan in
this book is to use this concept to develop a small set of design
principles for building successful agents—systems that can reasonably be
called .

We begin by examining agents, environments, and the coupling between
them. The observation that some agents behave better than others leads
naturally to the idea of a rational agent—one that behaves as well as
possible. How well an agent can behave depends on the nature of the
environment; some environments are more difficult than others. We give a
crude categorization of environments and show how properties of an
environment influence the design of suitable agents for that
environment. We describe a number of basic “skeleton” agent designs,
which we flesh out in the rest of the book.

Agents and Environments {#agent-environment-section}
-----------------------

[agent-environment-figure]

An is anything that can be viewed as perceiving its through and acting
upon that environment through . This simple idea is illustrated in . A
human agent has eyes, ears, and other organs for sensors and hands,
legs, vocal tract, and so on for actuators. A robotic agent might have
cameras and infrared range finders for sensors and various motors for
actuators. A software agent receives keystrokes, file contents, and
network packets as sensory inputs and acts on the environment by
displaying on the screen, writing files, and sending network packets.

We use the term to refer to the agent’s perceptual inputs at any given
instant. An agent’s is the complete history of everything the agent has
ever perceived. In general,

an agent’s choice of action at any given instant can depend on the
entire percept sequence observed to date, but not on anything it hasn’t
perceived.

By specifying the agent’s choice of action for every possible percept
sequence, we have said more or less everything there is to say about the
agent. Mathematically speaking, we say that an agent’s behavior is
described by the that maps any given percept sequence to an action.

We can imagine *tabulating* the agent function that
describes any given agent; for most agents, this would be a very large
table—infinite, in fact, unless we place a bound on the length of
percept sequences we want to consider. Given an agent to experiment
with, we can, in principle, construct this table by trying out all
possible percept sequences and recording which actions the agent does in
response.[^1] The table is, of course, an *external*
characterization of the agent. *Internally*, the agent
function for an artificial agent will be implemented by an . It is
important to keep these two ideas distinct. The agent function is an
abstract mathematical description; the agent program is a concrete
implementation, running within some physical system.

To illustrate these ideas, we use a very simple example—the
vacuum-cleaner world shown in . This world is so simple that we can
describe everything that happens; it’s also a made-up world, so we can
invent many variations. This particular world has just two locations:
squares $A$ and $B$. The vacuum agent perceives which square it is in
and whether there is dirt in the square. It can choose to move left,
move right, suck up the dirt, or do nothing. One very simple agent
function is the following: if the current square is dirty, then suck;
otherwise, move to the other square. A partial tabulation of this agent
function is shown in and an agent program that implements it appears in
on .[simple-vacuum-world-page]

[vacuum-world-figure]

[tbp] [vacuum-agent-function-table]

Looking at , we see that various vacuum-world agents can be defined
simply by filling in the right-hand column in various ways. The obvious
question, then, is this:

What is the right way to fill out the table?

In other words, what makes an agent good or bad, intelligent or stupid?
We answer these questions in the next section.

Before closing this section, we should emphasize that the notion of an
agent is meant to be a tool for analyzing systems, not an absolute
characterization that divides the world into agents and non-agents. One
could view a hand-held calculator as an agent that chooses the action of
displaying “4” when given the percept sequence “2 + 2 =,” but such an
analysis would hardly aid our understanding of the calculator. In a
sense, all areas of engineering can be seen as designing artifacts that
interact with the world; AI operates at (what the authors consider to
be) the most interesting end of the spectrum, where the artifacts have
significant computational resources and the task environment requires
nontrivial decision making.

Good Behavior: The Concept of Rationality {#rationality-section}
-----------------------------------------

A is one that does the right thing—conceptually speaking, every entry in
the table for the agent function is filled out correctly. Obviously,
doing the right thing is better than doing the wrong thing, but what
does it mean to do the right thing?

We answer this age-old question in an age-old way: by considering the
*consequences* of the agent’s behavior. When an agent is
plunked down in an environment, it generates a sequence of actions
according to the percepts it receives. This sequence of actions causes
the environment to go through a sequence of states. If the sequence is
desirable, then the agent has performed well. This notion of
desirability is captured by a that evaluates any given sequence of
environment states.

Notice that we said *environment* states, not
*agent* states. If we define success in terms of agent’s
opinion of its own performance, an agent could achieve perfect
rationality simply by deluding itself that its performance was perfect.
Human agents in particular are notorious for “sour grapes”—believing
they did not really want something (e.g., a Nobel Prize) after not
getting it.

Obviously, there is not one fixed performance measure for all tasks and
agents; typically, a designer will devise one appropriate to the
circumstances. This is not as easy as it sounds. Consider, for example,
the vacuum-cleaner agent from the preceding section. We might propose to
measure performance by the amount of dirt cleaned up in a single
eight-hour shift. With a rational agent, of course, what you ask for is
what you get. A rational agent can maximize this performance measure by
cleaning up the dirt, then dumping it all on the floor, then cleaning it
up again, and so on. A more suitable performance measure would reward
the agent for having a clean floor. For example, one point could be
awarded for each clean square at each time step (perhaps with a penalty
for electricity consumed and noise generated).

As a general rule, it is better to design performance measures according
to what one actually wants in the environment, rather than according to
how one thinks the agent should behave.

Even when the obvious pitfalls are avoided, there remain some knotty
issues to untangle. For example, the notion of “clean floor” in the
preceding paragraph is based on average cleanliness over time. Yet the
same average cleanliness can be achieved by two different agents, one of
which does a mediocre job all the time while the other cleans
energetically but takes long breaks. Which is preferable might seem to
be a fine point of , but in fact it is a deep philosophical question
with far-reaching implications. Which is better—a reckless life of highs
and lows, or a safe but humdrum existence? Which is better—an economy
where everyone lives in moderate poverty, or one in which some live in
plenty while others are very poor? We leave these questions as an
exercise for the diligent reader.

### Rationality

What is rational at any given time depends on four things:

-   The performance measure that defines the criterion of success.

-   The agent’s prior knowledge of the environment.

-   The actions that the agent can perform.

-   The agent’s percept sequence to date.

This leads to a :

For each possible percept sequence, a rational agent should select an
action that is expected to maximize its performance measure, given the
evidence provided by the percept sequence and whatever built-in
knowledge the agent has.[MEU-page]

Consider the simple vacuum-cleaner agent that cleans a square if it is
dirty and moves to the other square if not; this is the agent function
tabulated in . Is this a rational agent? That depends! First, we need to
say what the performance measure is, what is known about the
environment, and what sensors and actuators the agent has. Let us assume
the following:[vacuum-rationality-page]

-   The performance measure awards one point for each clean square at
    each time step, over a “lifetime” of 1000 time steps.

-   The “geography” of the environment is known *a priori*
    () but the dirt distribution and the initial location of the agent
    are not. Clean squares stay clean and sucking cleans the current
    square. The ${Left}$ and ${Right}$ actions move the agent left
    and right except when this would take the agent outside the
    environment, in which case the agent remains where it is.

-   The only available actions are ${Left}$, ${Right}$, and
    ${Suck}$.

-   The agent correctly perceives its location and whether that location
    contains dirt.

We claim that *under these circumstances* the agent is
indeed rational; its expected performance is at least as high as any
other agent’s. asks you to prove this.

One can see easily that the same agent would be irrational under
different circumstances. For example, once all the dirt is cleaned up,
the agent will oscillate needlessly back and forth; if the performance
measure includes a penalty of one point for each movement left or right,
the agent will fare poorly. A better agent for this case would do
nothing once it is sure that all the squares are clean. If clean squares
can become dirty again, the agent should occasionally check and re-clean
them if needed. If the geography of the environment is unknown, the
agent will need to explore it rather than stick to squares $A$ and $B$.
asks you to design agents for these cases.

### Omniscience, learning, and autonomy

We need to be careful to distinguish between rationality and . An
omniscient agent knows the *actual* outcome of its actions
and can act accordingly; but omniscience is impossible in reality.
Consider the following example: I am walking along the Champs Elysées
one day and I see an old friend across the street. There is no traffic
nearby and I’m not otherwise engaged, so, being rational, I start to
cross the street. Meanwhile, at 33,000 feet, a cargo door falls off a
passing airliner,[^2] and before I make it to the other side of the
street I am flattened. Was I irrational to cross the street? It is
unlikely that my obituary would read “Idiot attempts to cross street.”

This example shows that rationality is not the same as perfection.
Rationality maximizes *expected* performance, while
perfection maximizes *actual* performance. Retreating from
a requirement of perfection is not just a question of being fair to
agents. The point is that if we expect an agent to do what turns out to
be the best action after the fact, it will be impossible to design an
agent to fulfill this specification—unless we improve the performance of
crystal balls or time machines.

Our definition of rationality does not require omniscience, then,
because the rational choice depends only on the percept sequence
*to date*. We must also ensure that we haven’t
inadvertently allowed the agent to engage in decidedly underintelligent
activities. For example, if an agent does not look both ways before
crossing a busy road, then its percept sequence will not tell it that
there is a large truck approaching at high speed. Does our definition of
rationality say that it’s now OK to cross the road? Far from it! First,
it would not be rational to cross the road given this uninformative
percept sequence: the risk of accident from crossing without looking is
too great. Second, a rational agent should choose the “looking” action
before stepping into the street, because looking helps maximize the
expected performance. Doing actions *in order to modify future
percepts*—sometimes called —is an important part of rationality
and is covered in depth in . A second example of information gathering
is provided by the that must be undertaken by a vacuum-cleaning agent in
an initially unknown environment.

Our definition requires a rational agent not only to gather information
but also to as much as possible from what it perceives. The agent’s
initial configuration could reflect some prior knowledge of the
environment, but as the agent gains experience this may be modified and
augmented. There are extreme cases in which the environment is
completely known *a priori*. In such cases, the agent need
not perceive or learn; it simply acts correctly. Of course, such agents
are fragile. Consider the lowly dung[dung-beetle-page] beetle. After
digging its nest and laying its eggs, it fetches a ball of dung from a
nearby heap to plug the entrance. If the ball of dung is removed from
its grasp *en route*, the beetle continues its task and
pantomimes plugging the nest with the nonexistent dung ball, never
noticing that it is missing. Evolution has built an assumption into the
beetle’s behavior, and when it is violated, unsuccessful behavior
results. Slightly more intelligent is the sphex wasp. The female sphex
will dig a burrow, go out and sting a caterpillar and drag it to the
burrow, enter the burrow again to check all is well, drag the
caterpillar inside, and lay its eggs. The caterpillar serves as a food
source when the eggs hatch. So far so good, but if an entomologist moves
the caterpillar a few inches away while the sphex is doing the check, it
will revert to the “drag” step of its plan and will continue the plan
without modification, even after dozens of caterpillar-moving
interventions. The sphex is unable to learn that its innate plan is
failing, and thus will not change it.

To the extent that an agent relies on the prior knowledge of its
designer rather than on its own percepts, we say that the agent lacks
.[autonomy-page] A rational agent should be autonomous—it should learn
what it can to compensate for partial or incorrect prior knowledge. For
example, a vacuum-cleaning agent that learns to foresee where and when
additional dirt will appear will do better than one that does not. As a
practical matter, one seldom requires complete autonomy from the start:
when the agent has had little or no experience, it would have to act
randomly unless the designer gave some assistance. So, just as evolution
provides animals with enough built-in reflexes to survive long enough to
learn for themselves, it would be reasonable to provide an artificial
intelligent agent with some initial knowledge as well as an ability to
learn. After sufficient experience of its environment, the behavior of a
rational agent can become effectively *independent* of its
prior knowledge. Hence, the incorporation of learning allows one to
design a single rational agent that will succeed in a vast variety of
environments.

The Nature of Environments {#environments-section}
--------------------------

[PEAS-section]

Now that we have a definition of rationality, we are almost ready to
think about building rational agents. First, however, we must think
about , which are essentially the “problems” to which rational agents
are the “solutions.” We begin by showing how to specify a task
environment, illustrating the process with a number of examples. We then
show that task environments come in a variety of flavors. The flavor of
the task environment directly affects the appropriate design for the
agent program.

### Specifying the task environment {#PEAS-subsection}

In our discussion of the rationality of the simple vacuum-cleaner agent,
we had to specify the performance measure, the environment, and the
agent’s actuators and sensors. We group all these under the heading of
the . For the acronymically minded, we call this the
(**P**erformance, **E**nvironment,
**A**ctuators, **S**ensors) description. In
designing an agent, the first step must always be to specify the task
environment as fully as possible.

The vacuum world was a simple example; let us consider a more complex
problem: an automated taxi driver. We should point out, before the
reader becomes alarmed, that a fully automated taxi is currently
somewhat beyond the capabilities of existing technology. ( describes an
existing driving robot.) The full driving task is extremely
*open-ended*. There is no limit to the novel combinations
of circumstances that can arise—another reason we chose it as a focus
for discussion. summarizes the PEAS description for the taxi’s task
environment. We discuss each element in more detail in the following
paragraphs.

[htbp] [taxi-table]

First, what is the to which we would like our automated driver to
aspire? Desirable qualities include getting to the correct destination;
minimizing fuel consumption and wear and tear; minimizing the trip time
or cost; minimizing violations of traffic laws and disturbances to other
drivers; maximizing safety and passenger comfort; maximizing profits.
Obviously, some of these goals conflict, so tradeoffs will be required.

Next, what is the driving that the taxi will face? Any taxi driver must
deal with a variety of roads, ranging from rural lanes and urban alleys
to 12-lane freeways. The roads contain other traffic, pedestrians, stray
animals, road works, police cars, puddles, and potholes. The taxi must
also interact with potential and actual passengers. There are also some
optional choices. The taxi might need to operate in Southern California,
where snow is seldom a problem, or in Alaska, where it seldom is not. It
could always be driving on the right, or we might want it to be flexible
enough to drive on the left when in Britain or Japan. Obviously, the
more restricted the environment, the easier the design problem.

The for an automated taxi include those available to a human driver:
control over the engine through the accelerator and control over
steering and braking. In addition, it will need output to a display
screen or voice synthesizer to talk back to the passengers, and perhaps
some way to communicate with other vehicles, politely or otherwise.

The basic for the taxi will include one or more controllable video
cameras so that it can see the road; it might augment these with
infrared or sonar sensors to detect distances to other cars and
obstacles. To avoid speeding tickets, the taxi should have a
speedometer, and to control the vehicle properly, especially on curves,
it should have an accelerometer. To determine the mechanical state of
the vehicle, it will need the usual array of engine, fuel, and
electrical system sensors. Like many human drivers, it might want a
global positioning system (GPS) so that it doesn’t get lost. Finally, it
will need a keyboard or microphone for the passenger to request a
destination.

In , we have sketched the basic PEAS elements for a number of additional
agent types. Further examples appear in . It may come as a surprise to
some readers that our list of agent types includes some programs that
operate in the entirely artificial environment defined by keyboard input
and character output on a screen. “Surely,” one might say, “this is not
a real environment, is it?” In fact, what matters is not the distinction
between “real” and “artificial” environments, but the complexity of the
relationship among the behavior of the agent, the percept sequence
generated by the environment, and the performance measure. Some “real”
environments are actually quite simple. For example, a robot designed to
inspect parts as they come by on a conveyor belt can make use of a
number of simplifying assumptions: that the lighting is always just so,
that the only thing on the conveyor belt will be parts of a kind that it
knows about, and that only two actions (accept or reject) are possible.

[tbp] [agent-type-table]

In contrast, some (or software robots or ) exist in rich, unlimited
domains. Imagine a softbot Web site operator designed to scan Internet
news sources and show the interesting items to its users, while selling
advertising space to generate revenue. To do well, that operator will
need some natural language processing abilities, it will need to learn
what each user and advertiser is interested in, and it will need to
change its plans dynamically—for example, when the connection for one
news source goes down or when a new one comes online. The Internet is an
environment whose complexity rivals that of the physical world and whose
inhabitants include many artificial and human agents.

### Properties of task environments {#env-properties-subsection}

The range of task environments that might arise in AI is obviously vast.
We can, however, identify a fairly small number of dimensions along
which task environments can be categorized. These dimensions determine,
to a large extent, the appropriate agent design and the applicability of
each of the principal families of techniques for agent implementation.
First, we list the dimensions, then we analyze several task environments
to illustrate the ideas. The definitions here are informal; later
chapters provide more precise statements and examples of each kind of
environment.

[env-properties-list] vs. : If an agent’s sensors give it access to the
complete state of the environment at each point in time, then we say
that the task environment is fully observable. A task environment is
effectively fully observable if the sensors detect all aspects that are
*relevant* to the choice of action; relevance, in turn,
depends on the performance measure. Fully observable environments are
convenient because the agent need not maintain any internal state to
keep track of the world. An environment might be partially observable
because of noisy and inaccurate sensors or because parts of the state
are simply missing from the sensor data—for example, a vacuum agent with
only a local dirt sensor cannot tell whether there is dirt in other
squares, and an automated taxi cannot see what other drivers are
thinking. If the agent has no sensors at all then the environment is .
One might think that in such cases the agent’s plight is hopeless, but,
as we discuss in , the agent’s goals may still be achievable, sometimes
with certainty.

vs. : The distinction between single-agent and multiagent environments
may seem simple enough. For example, an agent solving a crossword puzzle
by itself is clearly in a single-agent environment, whereas an agent
playing chess is in a two-agent environment. There are, however, some
subtle issues. First, we have described how an entity *may*
be viewed as an agent, but we have not explained which entities
*must* be viewed as agents. Does an agent $A$ (the taxi
driver for example) have to treat an object $B$ (another vehicle) as an
agent, or can it be treated merely as an object behaving according to
the laws of physics, analogous to waves at the beach or leaves blowing
in the wind? The key distinction is whether $B$’s behavior is best
described as maximizing a performance measure whose value depends on
agent $A$’s behavior. For example, in chess, the opponent entity $B$ is
trying to maximize its performance measure, which, by the rules of
chess, minimizes agent $A$’s performance measure. Thus, chess is a
multiagent environment. In the taxi-driving environment, on the other
hand, avoiding collisions maximizes the performance measure of all
agents, so it is a partially multiagent environment. It is also
partially competitive because, for example, only one car can occupy a
parking space. The agent-design problems in multiagent environments are
often quite different from those in single-agent environments; for
example, often emerges as a rational behavior in multiagent
environments; in some competitive environments, is rational because it
avoids the pitfalls of predictability.

vs. . If the next state of the environment is completely determined by
the current state and the action executed by the agent, then we say the
environment is deterministic; otherwise, it is stochastic. In principle,
an agent need not worry about uncertainty in a fully observable,
deterministic environment. (In our definition, we ignore uncertainty
that arises purely from the actions of other agents in a multiagent
environment; thus, a game can be deterministic even though each agent
may be unable to predict the actions of the others.) If the environment
is partially observable, however, then it could *appear* to
be stochastic. Most real situations are so complex that it is impossible
to keep track of all the unobserved aspects; for practical purposes,
they must be treated as stochastic. Taxi driving is clearly stochastic
in this sense, because one can never predict the behavior of traffic
exactly; moreover, one’s tires blow out and one’s engine seizes up
without warning. The vacuum world as we described it is deterministic,
but variations can include stochastic elements such as randomly
appearing dirt and an unreliable suction mechanism (). We say an
environment is if it is not fully observable or not deterministic. One
final note: our use of the word “stochastic” generally implies that
uncertainty about outcomes is quantified in terms of probabilities; a
environment is one in which actions are characterized by their
*possible* outcomes, but no probabilities are attached to
them. Nondeterministic environment descriptions are usually associated
with performance measures that require the agent to succeed for
*all possible* outcomes of its actions.

[episodic-page] vs. : In an episodic task environment, the agent’s
experience is divided into atomic episodes. In each episode the agent
receives a percept and then performs a single action. Crucially, the
next episode does not depend on the actions taken in previous episodes.
Many classification tasks are episodic. For example, an agent that has
to spot defective parts on an assembly line bases each decision on the
current part, regardless of previous decisions; moreover, the current
decision doesn’t affect whether the next part is defective. In
sequential environments, on the other hand, the current decision could
affect all future decisions.[^3] Chess and taxi driving are sequential:
in both cases, short-term actions can have long-term consequences.
Episodic environments are much simpler than sequential environments
because the agent does not need to think ahead.

vs. : If the environment can change while an agent is deliberating, then
we say the environment is dynamic for that agent; otherwise, it is
static. Static environments are easy to deal with because the agent need
not keep looking at the world while it is deciding on an action, nor
need it worry about the passage of time. Dynamic environments, on the
other hand, are continuously asking the agent what it wants to do; if it
hasn’t decided yet, that counts as deciding to do nothing. If the
environment itself does not change with the passage of time but the
agent’s performance score does, then we say the environment is . Taxi
driving is clearly dynamic: the other cars and the taxi itself keep
moving while the driving algorithm dithers about what to do next. Chess,
when played with a clock, is semidynamic. Crossword puzzles are static.

vs. : The discrete/continuous distinction applies to the
*state* of the environment, to the way *time*
is handled, and to the *percepts* and
*actions* of the agent. For example, the chess environment
has a finite number of distinct states (excluding the clock). Chess also
has a discrete set of percepts and actions. Taxi driving is a
continuous-state and continuous-time problem: the speed and location of
the taxi and of the other vehicles sweep through a range of continuous
values and do so smoothly over time. Taxi-driving actions are also
continuous (steering angles, etc.). Input from digital cameras is
discrete, strictly speaking, but is typically treated as representing
continuously varying intensities and locations.

vs. : Strictly speaking, this distinction refers not to the environment
itself but to the agent’s (or designer’s) state of knowledge about the
“laws of physics” of the environment. In a known environment, the
outcomes (or outcome probabilities if the environment is stochastic) for
all actions are given. Obviously, if the environment is unknown, the
agent will have to learn how it works in order to make good decisions.
Note that the distinction between known and unknown environments is not
the same as the one between fully and partially observable environments.
It is quite possible for a *known* environment to be
*partially* observable—for example, in solitaire card
games, I know the rules but am still unable to see the cards that have
not yet been turned over. Conversely, an *unknown*
environment can be *fully* observable—in a new video game,
the screen may show the entire game state but I still don’t know what
the buttons do until I try them.

As one might expect, the hardest case is *partially
observable*, *multiagent*,
*stochastic*, *sequential*,
*dynamic*, *continuous*, and
*unknown*. Taxi driving is hard in all these senses, except
that for the most part the driver’s environment is known. Driving a
rented car in a new country with unfamiliar geography and traffic laws
is a lot more exciting.

lists the properties of a number of familiar environments. Note that the
answers are not always cut and dried. For example, we describe the
part-picking robot as episodic, because it normally considers each part
in isolation. But if one day there is a large batch of defective parts,
the robot should learn from several observations that the distribution
of defects has changed, and should modify its behavior for subsequent
parts. We have not included a “known/unknown” column because, as
explained earlier, this is not strictly a property of the environment.
For some environments, such as chess and poker, it is quite easy to
supply the agent with full knowledge of the rules, but it is nonetheless
interesting to consider how an agent might learn to play these games
without such knowledge.

Several of the answers in the table depend on how the task environment
is defined. We have listed the medical-diagnosis task as single-agent
because the disease process in a patient is not profitably modeled as an
agent; but a medical-diagnosis system might also have to deal with
recalcitrant patients and skeptical staff, so the environment could have
a multiagent aspect. Furthermore, medical diagnosis is episodic if one
conceives of the task as selecting a diagnosis given a list of symptoms;
the problem is sequential if the task can include proposing a series of
tests, evaluating progress over the course of treatment, and so on.
Also, many environments are episodic at higher levels than the agent’s
individual actions. For example, a chess tournament consists of a
sequence of games; each game is an episode because (by and large) the
contribution of the moves in one game to the agent’s overall performance
is not affected by the moves in its previous game. On the other hand,
decision making within a single game is certainly sequential.

[bt]

[environment-type-table]

The code repository associated with this book (aima.cs.berkeley.edu)
includes implementations of a number of environments, together with a
general-purpose environment simulator that places one or more agents in
a simulated environment, observes their behavior over time, and
evaluates them according to a given performance measure. Such
experiments are often carried out not for a single environment but for
many environments drawn from an . For example, to evaluate a taxi driver
in simulated traffic, we would want to run many simulations with
different traffic, lighting, and weather conditions. If we designed the
agent for a single scenario, we might be able to take advantage of
specific properties of the particular case but might not identify a good
design for driving in general. For this reason, the code repository also
includes an for each environment class that selects particular
environments (with certain likelihoods) in which to run the agent. For
example, the vacuum environment generator initializes the dirt pattern
and agent location randomly. We are then interested in the agent’s
average performance over the environment class. A rational agent for a
given environment class maximizes this average performance.
Exercises [vacuum-start-exercise] to [vacuum-finish-exercise] take you
through the process of developing an environment class and evaluating
various agents therein.

The Structure of Agents {#agent-program-section}
-----------------------

So far we have talked about agents by describing
*behavior*—the action that is performed after any given
sequence of percepts. Now we must bite the bullet and talk about how the
insides work. The job of AI is to design an that implements the agent
function—the mapping from percepts to actions. We assume this program
will run on some sort of computing device with physical sensors and
actuators—we call this the :
$$\mbox{\em agent} = \mbox{\em architecture} + \mbox{\em program}\ .$$
Obviously, the program we choose has to be one that is appropriate for
the architecture. If the program is going to recommend actions like
*Walk*, the architecture had better have legs. The
architecture might be just an ordinary PC, or it might be a robotic car
with several onboard computers, cameras, and other sensors. In general,
the architecture makes the percepts from the sensors available to the
program, runs the program, and feeds the program’s action choices to the
actuators as they are generated. Most of this book is about designing
agent programs, although Chapters [perception-chapter] and
[robotics-chapter] deal directly with the sensors and actuators.

### Agent programs

The agent programs that we design in this book all have the same
skeleton: they take the current percept as input from the sensors and
return an action to the actuators.[^4] Notice the difference between the
agent program, which takes the current percept as input, and the agent
function, which takes the entire percept history. The agent program
takes just the current percept as input because nothing more is
available from the environment; if the agent’s actions need to depend on
the entire percept sequence, the agent will have to remember the
percepts.

We describe the agent programs in the simple pseudocode language that is
defined in . (The online code repository contains implementations in
real programming languages.) For example, shows a rather trivial agent
program that keeps track of the percept sequence and then uses it to
index into a table of actions to decide what to do. The table—an example
of which is given for the vacuum world in —represents explicitly the
agent function that the agent program embodies. To build a rational
agent in this way, we as designers must construct a table that contains
the appropriate action for every possible percept sequence.

[table-agent-algorithm]

It is instructive to consider why the table-driven approach to agent
construction is doomed to failure. Let ${\cal P}$ be the set of possible
percepts and let $T$ be the lifetime of the agent (the total number of
percepts it will receive). The lookup table will contain
$\sum_{t\eq 1}^T |{\cal P}|^t$ entries. Consider the automated taxi: the
visual input from a single camera comes in at the rate of roughly 27
megabytes per second (30 frames per second, ${640} \stimes {480}$ pixels
with 24 bits of color information). This gives a lookup table with over
${10}^{{250},{000},{000},{000}}$ entries for an hour’s driving. Even the
lookup table for chess—a tiny, well-behaved fragment of the real
world—would have at least ${10}^{{150}}$ entries. The daunting size of
these tables (the number of atoms in the observable universe is less
than ${10}^{{80}}$) means that (a) no physical agent in this universe
will have the space to store the table, (b) the designer would not have
time to create the table, (c) no agent could ever learn all the right
table entries from its experience, and (d) even if the environment is
simple enough to yield a feasible table size, the designer still has no
guidance about how to fill in the table entries.

Despite all this, *does* do what we want: it implements the
desired agent function. The key challenge for AI is to find out how to
write programs that, to the extent possible, produce rational behavior
from a smallish program rather than from a vast table. We have many
examples showing that this can be done successfully in other areas: for
example, the huge tables of square roots used by engineers and
schoolchildren prior to the 1970s have now been replaced by a five-line
program for Newton’s method running on electronic calculators. The
question is, can AI do for general intelligent behavior what Newton did
for ? We believe the answer is yes.

In the remainder of this section, we outline four basic kinds of agent
programs that embody the principles underlying almost all intelligent
systems:

-   Simple reflex agents;

-   Model-based reflex agents;

-   Goal-based agents; and

-   Utility-based agents.

Each kind of agent program combines particular components in particular
ways to generate actions. explains in general terms how to convert all
these agents into *learning agents* that can improve the
performance of their components so as to generate better actions.
Finally, describes the variety of ways in which the components
themselves can be represented within the agent. This variety provides a
major organizing principle for the field and for the book itself.

### Simple reflex agents

The simplest kind of agent is the . These agents select actions on the
basis of the *current* percept, ignoring the rest of the
percept history. For example, the vacuum agent whose agent function is
tabulated in is a simple reflex agent, because its decision is based
only on the current location and on whether that location contains dirt.
An agent program for this agent is shown in .

[reflex-vacuum-agent-algorithm]

Notice that the vacuum agent program is very small indeed compared to
the corresponding table. The most obvious reduction comes from ignoring
the percept history, which cuts down the number of possibilities from
$4^T$ to just 4. A further, small reduction comes from the fact that
when the current square is dirty, the action does not depend on the
location.

Simple reflex behaviors occur even in more complex environments. Imagine
yourself as the driver of the automated taxi. If the car in front brakes
and its brake lights come on, then you should notice this and initiate
braking. In other words, some processing is done on the visual input to
establish the condition we call “The car in front is braking.” Then,
this triggers some established connection in the agent program to the
action “initiate braking.” We call such a connection a ,[^5] written as

Humans also have many such connections, some of which are learned
responses (as for driving) and some of which are innate reflexes (such
as blinking when something approaches the eye). In the course of the
book, we show several different ways in which such connections can be
learned and implemented.

The program in is specific to one particular vacuum environment. A more
general and flexible approach is first to build a general-purpose
interpreter for condition–action rules and then to create rule sets for
specific task environments. gives the structure of this general program
in schematic form, showing how the condition–action rules allow the
agent to make the connection from percept to action. (Do not worry if
this seems trivial; it gets more interesting shortly.) We use rectangles
to denote the current internal state of the agent’s decision process,
and ovals to represent the background information used in the process.
The agent program, which is also very simple, is shown in . The function
generates an abstracted description of the current state from the
percept, and the function returns the first rule in the set of rules
that matches the given state description. Note that the description in
terms of “rules” and “matching” is purely conceptual; actual
implementations can be as simple as a collection of logic gates
implementing a Boolean circuit.

[simple-reflex-agent-figure]

[simple-reflex-agent-algorithm]

Simple reflex agents have the admirable property of being simple, but
they turn out to be of limited intelligence. The agent in will work

only if the correct decision can be made on the basis of only the
current percept—that is, only if the environment is fully observable.

Even a little bit of unobservability can cause serious trouble. For
example, the braking rule given earlier assumes that the condition
*car-in-front-is-braking* can be determined from the
current percept—a single frame of video. This works if the car in front
has a centrally mounted brake light. Unfortunately, older models have
different configurations of taillights, brake lights, and turn-signal
lights, and it is not always possible to tell from a single image
whether the car is braking. A simple reflex agent driving behind such a
car would either brake continuously and unnecessarily, or, worse, never
brake at all.

We can see a similar problem arising in the vacuum world. Suppose that a
simple reflex vacuum agent is deprived of its location sensor and has
only a dirt sensor. Such an agent has just two possible percepts:
$[{Dirty}]$ and $[{Clean}]$. It can ${Suck}$ in response to
$[{Dirty}]$; what should it do in response to $[{Clean}]$? Moving
${Left}$ fails (forever) if it happens to start in square $A$, and
moving ${Right}$ fails (forever) if it happens to start in square $B$.
Infinite loops are often unavoidable for simple reflex agents operating
in partially observable environments.

Escape from infinite loops is possible if the agent can its actions. For
example, if the vacuum agent perceives $[{Clean}]$, it might flip a
coin to choose between ${Left}$ and ${Right}$. It is easy to show
that the agent will reach the other square in an average of two steps.
Then, if that square is dirty, the agent will clean it and the task will
be complete. Hence, a randomized simple reflex agent might outperform a
deterministic simple reflex agent.

We mentioned in that randomized behavior of the right kind can be
rational in some multiagent environments. In single-agent environments,
randomization is usually *not* rational. It is a useful
trick that helps a simple reflex agent in some situations, but in most
cases we can do much better with more sophisticated deterministic
agents.

### Model-based reflex agents

The most effective way to handle partial observability is for the agent
to *keep track of the part of the world it can’t see now*.
That is, the agent should maintain some sort of that depends on the
percept history and thereby reflects at least some of the unobserved
aspects of the current state. For the braking problem, the internal
state is not too extensive—just the previous frame from the camera,
allowing the agent to detect when two red lights at the edge of the
vehicle go on or off simultaneously. For other driving tasks such as
changing lanes, the agent needs to keep track of where the other cars
are if it can’t see them all at once. And for any driving to be possible
at all, the agent needs to keep track of where its keys are.

Updating this internal state information as time goes by requires two
kinds of knowledge to be encoded in the agent program. First, we need
some information about how the world evolves independently of the
agent—for example, that an overtaking car generally will be closer
behind than it was a moment ago. Second, we need some information about
how the agent’s own actions affect the world—for example, that when the
agent turns the steering wheel clockwise, the car turns to the right, or
that after driving for five minutes northbound on the freeway, one is
usually about five miles north of where one was five minutes ago. This
knowledge about “how the world works”—whether implemented in simple
Boolean circuits or in complete scientific theories—is called a of the
world. An agent that uses such a model is called a .

[model-based-reflex-agent-figure]

[model-based-reflex-agent-algorithm]

gives the structure of the model-based reflex agent with internal state,
showing how the current percept is combined with the old internal state
to generate the updated description of the current state, based on the
agent’s model of how the world works. The agent program is shown in .
The interesting part is the function , which is responsible for creating
the new internal state description. The details of how models and states
are represented vary widely depending on the type of environment and the
particular technology used in the agent design. Detailed examples of
models and updating algorithms appear in
Chapters [advanced-search-chapter], [kr-chapter],
[advanced-planning-chapter], [dbn-chapter], [complex-decisions-chapter],
and [robotics-chapter].

Regardless of the kind of representation used, it is seldom possible for
the agent to determine the current state of a partially observable
environment *exactly*. Instead, the box labeled “what the
world is like now” () represents the agent’s “best guess” (or sometimes
best guesses). For example, an automated taxi may not be able to see
around the large truck that has stopped in front of it and can only
guess about what may be causing the hold-up. Thus, uncertainty about the
current state may be unavoidable, but the agent still has to make a
decision.

A perhaps less obvious point about the internal “state” maintained by a
model-based agent is that it does not have to describe “what the world
is like now” in a literal sense. For example, the taxi may be driving
back home, and it may have a rule telling it to fill up with gas on the
way home unless it has at least half a tank. Although “driving back
home” may *seem* to an aspect of the world state, the fact
of the taxi’s *destination* is actually an aspect of the
agent’s internal state. If you find this puzzling, consider that the
taxi could be in exactly the same place at the same time, but intending
to reach a different destination.

### Goal-based agents {#goal-based-agents-section}

Knowing something about the current state of the environment is not
always enough to decide what to do. For example, at a road junction, the
taxi can turn left, turn right, or go straight on. The correct decision
depends on where the taxi is trying to get to. In other words, as well
as a current state description, the agent needs some sort of information
that describes situations that are desirable—for example, being at the
passenger’s destination. The agent program can combine this with the
model (the same information as was used in the model-based reflex agent)
to choose actions that achieve the goal. shows the goal-based agent’s
structure.

[goal-based-agent-figure]

Sometimes goal-based action selection is straightforward—for example,
when goal satisfaction results immediately from a single action.
Sometimes it will be more tricky—for example, when the agent has to
consider long sequences of twists and turns in order to find a way to
achieve the goal. (Chapters [search-chapter] to [game-playing-chapter])
and (Chapters [planning-chapter] and [advanced-planning-chapter]) are
the subfields of AI devoted to finding action sequences that achieve the
agent’s goals.

Notice that decision making of this kind is fundamentally different from
the condition–action rules described earlier, in that it involves
consideration of the future—both “What will happen if I do
such-and-such?” and “Will that make me happy?” In the reflex agent
designs, this information is not explicitly represented, because the
built-in rules map directly from percepts to actions. The reflex agent
brakes when it sees brake lights. A goal-based agent, in principle,
could reason that if the car in front has its brake lights on, it will
slow down. Given the way the world usually evolves, the only action that
will achieve the goal of not hitting other cars is to brake.

Although the goal-based agent appears less efficient, it is more
flexible because the knowledge that supports its decisions is
represented explicitly and can be modified. If it starts to rain, the
agent can update its knowledge of how effectively its brakes will
operate; this will automatically cause all of the relevant behaviors to
be altered to suit the new conditions. For the reflex agent, on the
other hand, we would have to rewrite many condition–action rules. The
goal-based agent’s behavior can easily be changed to go to a different
destination, simply by specifying that destination as the goal. The
reflex agent’s rules for when to turn and when to go straight will work
only for a single destination; they must all be replaced to go somewhere
new.

### Utility-based agents {#utility-based-agent-section}

Goals alone are not enough to generate high-quality behavior in most
environments. For example, many action sequences will get the taxi to
its destination (thereby achieving the goal) but some are quicker,
safer, more reliable, or cheaper than others. Goals just provide a crude
binary distinction between “happy” and “unhappy” states. A more general
performance measure should allow a comparison of different world states
according to exactly how happy they would make the agent. Because
“happy” does not sound very scientific, economists and computer
scientists use the term instead.[^6]

We have already seen that a performance measure assigns a score to any
given sequence of environment states, so it can easily distinguish
between more and less desirable ways of getting to the taxi’s
destination. An agent’s is essentially an internalization of the
performance measure. If the internal utility function and the external
performance measure are in agreement, then an agent that chooses actions
to maximize its utility will be rational according to the external
performance measure.

Let us emphasize again that this is not the *only* way to
be rational—we have already seen a rational agent program for the vacuum
world () that has no idea what its utility function is—but, like
goal-based agents, a utility-based agent has many advantages in terms of
flexibility and learning. Furthermore, in two kinds of cases, goals are
inadequate but a utility-based agent can still make rational decisions.
First, when there are conflicting goals, only some of which can be
achieved (for example, speed and safety), the utility function specifies
the appropriate tradeoff. Second, when there are several goals that the
agent can aim for, none of which can be achieved with certainty, utility
provides a way in which the likelihood of success can be weighed against
the importance of the goals.

Partial observability and stochasticity are ubiquitous in the real
world, and so, therefore, is decision making under uncertainty.
Technically speaking, a rational utility-based agent chooses the action
that maximizes the of the action outcomes—that is, the utility the agent
expects to derive, on average, given the probabilities and utilities of
each outcome. ( defines expectation more precisely.) In , we show that
any rational agent must behave *as if* it possesses a
utility function whose expected value it tries to maximize. An agent
that possesses an *explicit* utility function can make
rational decisions with a general-purpose algorithm that does not depend
on the specific utility function being maximized. In this way, the
“global” definition of rationality—designating as rational those agent
functions that have the highest performance—is turned into a “local”
constraint on rational-agent designs that can be expressed in a simple
program.

[utility-based-agent-figure]

The utility-based agent structure appears in . Utility-based agent
programs appear in , where we design decision-making agents that must
handle the uncertainty inherent in stochastic or partially observable
environments.

At this point, the reader may be wondering, “Is it that simple? We just
build agents that maximize expected utility, and we’re done?” It’s true
that such agents would be intelligent, but it’s not simple. A
utility-based agent has to model and keep track of its environment,
tasks that have involved a great deal of research on perception,
representation, reasoning, and learning. The results of this research
fill many of the chapters of this book. Choosing the utility-maximizing
course of action is also a difficult task, requiring ingenious
algorithms that fill several more chapters. Even with these algorithms,
perfect rationality is usually unachievable in practice because of
computational complexity, as we noted in .

### Learning agents {#learning-agents-section}

We have described agent programs with various methods for selecting
actions. We have not, so far, explained how the agent programs
*come into being*. In his famous early paper,
Turing [-@Turing:1950] considers the idea of actually programming his
intelligent machines by hand. He estimates how much work this might take
and concludes “Some more expeditious method seems desirable.” The method
he proposes is to build learning machines and then to teach them. In
many areas of AI, this is now the preferred method for creating
state-of-the-art systems. Learning has another advantage, as we noted
earlier: it allows the agent to operate in initially unknown
environments and to become more competent than its initial knowledge
alone might allow. In this section, we briefly introduce the main ideas
of learning agents. Throughout the book, we comment on opportunities and
methods for learning in particular kinds of agents. goes into much more
depth on the learning algorithms themselves.

A learning agent can be divided into four conceptual components, as
shown in . The most important distinction is between the , which is
responsible for making improvements, and the , which is responsible for
selecting external actions. The performance element is what we have
previously considered to be the entire agent: it takes in percepts and
decides on actions. The learning element uses feedback from the on how
the agent is doing and determines how the performance element should be
modified to do better in the future.

The design of the learning element depends very much on the design of
the performance element. When trying to design an agent that learns a
certain capability, the first question is not “How am I going to get it
to learn this?” but “What kind of performance element will my agent need
to do this once it has learned how?” Given an agent design, learning
mechanisms can be constructed to improve every part of the agent.

[learning-agent-figure]

The critic tells the learning element how well the agent is doing with
respect to a fixed performance standard. The critic is necessary because
the percepts themselves provide no indication of the agent’s success.
For example, a chess program could receive a percept indicating that it
has checkmated its opponent, but it needs a performance standard to know
that this is a good thing; the percept itself does not say so. It is
important that the performance standard be fixed. Conceptually, one
should think of it as being outside the agent altogether because the
agent must not modify it to fit its own behavior.

The last component of the learning agent is the . It is responsible for
suggesting actions that will lead to new and informative experiences.
The point is that if the performance element had its way, it would keep
doing the actions that are best, given what it knows. But if the agent
is willing to explore a little and do some perhaps suboptimal actions in
the short run, it might discover much better actions for the long run.
The problem generator’s job is to suggest these exploratory actions.
This is what scientists do when they carry out experiments. Galileo did
not think that dropping rocks from the top of a tower in Pisa was
valuable in itself. He was not trying to break the rocks or to modify
the brains of unfortunate passers-by. His aim was to modify his own
brain by identifying a better theory of the motion of objects.

To make the overall design more concrete, let us return to the automated
taxi example. The performance element consists of whatever collection of
knowledge and procedures the taxi has for selecting its driving actions.
The taxi goes out on the road and drives, using this performance
element. The critic observes the world and passes information along to
the learning element. For example, after the taxi makes a quick left
turn across three lanes of traffic, the critic observes the shocking
language used by other drivers. From this experience, the learning
element is able to formulate a rule saying this was a bad action, and
the performance element is modified by installation of the new rule. The
problem generator might identify certain areas of behavior in need of
improvement and suggest experiments, such as trying out the brakes on
different road surfaces under different conditions.

The learning element can make changes to any of the “knowledge”
components shown in the agent diagrams
(Figures [simple-reflex-agent-figure],
[model-based-reflex-agent-figure], [goal-based-agent-figure], and
[utility-based-agent-figure]). The simplest cases involve learning
directly from the percept sequence. Observation of pairs of successive
states of the environment can allow the agent to learn “How the world
evolves,” and observation of the results of its actions can allow the
agent to learn “What my actions do.” For example, if the taxi exerts a
certain braking pressure when driving on a wet road, then it will soon
find out how much deceleration is actually achieved. Clearly, these two
learning tasks are more difficult if the environment is only partially
observable.

The forms of learning in the preceding paragraph do not need to access
the external performance standard—in a sense, the standard is the
universal one of making predictions that agree with experiment. The
situation is slightly more complex for a utility-based agent that wishes
to learn utility information. For example, suppose the taxi-driving
agent receives no tips from passengers who have been thoroughly shaken
up during the trip. The external performance standard must inform the
agent that the loss of tips is a negative contribution to its overall
performance; then the agent might be able to learn that violent
maneuvers do not contribute to its own utility. In a sense, the
performance standard distinguishes part of the incoming percept as a (or
) that provides direct feedback on the quality of the agent’s behavior.
Hard-wired performance standards such as pain and hunger in animals can
be understood in this way. This issue is discussed further in .

In summary, agents have a variety of components, and those components
can be represented in many ways within the agent program, so there
appears to be great variety among learning methods. There is, however, a
single unifying theme. Learning in intelligent agents can be summarized
as a process of modification of each component of the agent to bring the
components into closer agreement with the available feedback
information, thereby improving the overall performance of the agent.

### How the components of agent programs work {#representational-axis-section}

We have described agent programs (in very high-level terms) as
consisting of various components, whose function it is to answer
questions such as: “What is the world like now?” “What action should I
do now?” “What do my actions do?” The next question for a student of AI
is, “How on earth do these components work?” It takes about a thousand
pages to begin to answer that question properly, but here we want to
draw the reader’s attention to some basic distinctions among the various
ways that the components can represent the environment that the agent
inhabits.

Roughly speaking, we can place the representations along an axis of
increasing complexity and expressive power—, , and . To illustrate these
ideas, it helps to consider a particular agent component, such as the
one that deals with “What my actions do.” This component describes the
changes that might occur in the environment as the result of taking an
action, and provides schematic depictions of how those transitions might
be represented.

[atomic-factored-structured-figure]

In an each state of the world is indivisible—it has no internal
structure. Consider the problem of finding a driving route from one end
of a country to the other via some sequence of cities (we address this
problem in on ). For the purposes of solving this problem, it may
suffice to reduce the state of world to just the name of the city we are
in—a single atom of knowledge; a “black box” whose only discernible
property is that of being identical to or different from another black
box. The algorithms underlying and
(Chapters [search-chapter]–[game-playing-chapter]), (), and () all work
with atomic representations—or, at least, they treat representations
*as if* they were atomic.

Now consider a higher-fidelity description for the same problem, where
we need to be concerned with more than just atomic location in one city
or another; we might need to pay attention to how much gas is in the
tank, our current GPS coordinates, whether or not the oil warning light
is working, how much spare change we have for toll crossings, what
station is on the radio, and so on. A splits up each state into a fixed
set of or , each of which can have a . While two different atomic states
have nothing in common—they are just different black boxes—two different
factored states can share some attributes (such as being at some
particular GPS location) and not others (such as having lots of gas or
having no gas); this makes it much easier to work out how to turn one
state into another. With factored representations, we can also represent
*uncertainty*—for example, ignorance about the amount of
gas in the tank can be represented by leaving that attribute blank. Many
important areas of AI are based on factored representations, including
algorithms (), (), (),
(Chapters [probability-chapter]–[decision-theory-chapter]), and the
algorithms in
Chapters [concept-learning-chapter], [bayesian-learning-chapter],
and [reinforcement-learning-chapter].

For many purposes, we need to understand the world as having
*things* in it that are *related* to each
other, not just variables with values. For example, we might notice that
a large truck ahead of us is reversing into the driveway of a dairy farm
but a cow has got loose and is blocking the truck’s path. A factored
representation is unlikely to be pre-equipped with the attribute
${TruckAheadBackingIntoDairyFarmDrivewayBlockedByLooseCow}$ with value
${true}$ or ${false}$. Instead, we would need a , in which objects
such as cows and trucks and their various and varying relationships can
be described explicitly. (See (c).) Structured representations underlie
and (Chapters [fol-chapter], [logical-inference-chapter],
and [kr-chapter]), (), () and much of (). In fact, almost everything
that humans express in natural language concerns objects and their
relationships.

As we mentioned earlier, the axis along which atomic, factored, and
structured representations lie is the axis of increasing . Roughly
speaking, a more expressive representation can capture, at least as
concisely, everything a less expressive one can capture, plus some more.
Often, the more expressive language is *much* more concise;
for example, the rules of chess can be written in a page or two of a
structured-representation language such as first-order logic but require
thousands of pages when written in a factored-representation language
such as propositional logic. On the other hand, reasoning and learning
become more complex as the expressive power of the representation
increases. To gain the benefits of expressive representations while
avoiding their drawbacks, intelligent systems for the real world may
need to operate at all points along the axis simultaneously.

This chapter has been something of a whirlwind tour of AI, which we have
conceived of as the science of agent design. The major points to recall
are as follows:

-   An is something that perceives and acts in an environment. The for
    an agent specifies the action taken by the agent in response to any
    percept sequence.

-   The evaluates the behavior of the agent in an environment. A acts so
    as to maximize the expected value of the performance measure, given
    the percept sequence it has seen so far.

-   A specification includes the performance measure, the external
    environment, the actuators, and the sensors. In designing an agent,
    the first step must always be to specify the task environment as
    fully as possible.

-   Task environments vary along several significant dimensions. They
    can be fully or partially observable, single-agent or multiagent,
    deterministic or stochastic, episodic or sequential, static or
    dynamic, discrete or continuous, and known or unknown.

-   The implements the agent function. There exists a variety of basic
    agent-program designs reflecting the kind of information made
    explicit and used in the decision process. The designs vary in
    efficiency, compactness, and flexibility. The appropriate design of
    the agent program depends on the nature of the environment.

-   respond directly to percepts, whereas maintain internal state to
    track aspects of the world that are not evident in the current
    percept. act to achieve their goals, and try to maximize their own
    expected “happiness.”

-   All agents can improve their performance through .

The central role of action in intelligence—the notion of practical
reasoning—goes back at least as far as Aristotle’s *Nicomachean
Ethics*. Practical reasoning was also the subject of
McCarthy’s [-@McCarthy:1958] influential paper “Programs with Common
Sense.” The fields of robotics and control theory are, by their very
nature, concerned principally with physical agents. The concept of a in
control theory is identical to that of an agent in AI. Perhaps
surprisingly, AI has concentrated for most of its history on isolated
components of agents—question-answering systems, theorem-provers, vision
systems, and so on—rather than on whole agents. The discussion of agents
in the text by Genesereth and Nilsson [-@Genesereth+Nilsson:1987] was an
influential exception. The whole-agent view is now widely accepted and
is a central theme in recent texts @Poole+al:1998
[@Nilsson:1998; @Padgham+Winikoff:2004; @Jones:2007].

traced the roots of the concept of rationality in philosophy and
economics. In AI, the concept was of peripheral interest until the
mid-1980s, when it began to suffuse many discussions about the proper
technical foundations of the field. A paper by Jon Doyle [-@Doyle:1983]
predicted that rational agent design would come to be seen as the core
mission of AI, while other popular topics would spin off to form new
disciplines.

Careful attention to the properties of the environment and their
consequences for rational agent design is most apparent in the control
theory tradition—for example, classical control systems
@Dorf+Bishop:2004 [@Kirk:2004] handle fully observable, deterministic
environments; stochastic optimal control @Kumar+Varaiya:1986
[@Bertsekas+Shreve:2007] handles partially observable, stochastic
environments; and hybrid control @Henzinger+Sastry:1998
[@Cassandras+Lygeros:2006] deals with environments containing both
discrete and continuous elements. The distinction between fully and
partially observable environments is also central in the literature
developed in the field of  @Puterman:1994, which we discuss in .

Reflex agents were the primary model for psychological behaviorists such
as Skinner [-@Skinner:1953], who attempted to reduce the psychology of
organisms strictly to input/output or stimulus/response mappings. The
advance from behaviorism to functionalism in psychology, which was at
least partly driven by the application of the computer metaphor to
agents @Putnam:1960 [@Lewis:1966], introduced the internal state of the
agent into the picture. Most work in AI views the idea of pure reflex
agents with state as too simple to provide much leverage, but work by
Rosenschein [-@Rosenschein:1985] and Brooks [-@Brooks:1986] questioned
this assumption (see ). In recent years, a great deal of work has gone
into finding efficient algorithms for keeping track of complex
environments @Hamscher+al:1992 [@Simon:2006]. The Remote Agent program
(described on ) that controlled the spacecraft is a particularly
impressive example @Muscettola+al:1998 [@Jonsson+al:2000].

Goal-based agents are presupposed in everything from Aristotle’s view of
practical reasoning to McCarthy’s early papers on logical AI. Shakey the
Robot @Fikes+Nilsson:1971 [@Nilsson:1984] was the first robotic
embodiment of a logical, goal-based agent. A full logical analysis of
goal-based agents appeared in Genesereth and
Nilsson [-@Genesereth+Nilsson:1987], and a goal-based programming
methodology called agent-oriented programming was developed by
Shoham [-@Shoham:1993]. The agent-based approach is now extremely
popular in software engineering @Ciancarini+Wooldridge:2001. It has also
infiltrated the area of operating systems, where refers to computer
systems and networks that monitor and control themselves with a
perceive–act loop and machine learning methods @Kephart+Chess:2003.
Noting that a collection of agent programs designed to work well
together in a true multiagent environment necessarily exhibits
modularity—the programs share no internal state and communicate with
each other only through the environment—it is common within the field of
to design the agent program of a single agent as a collection of
autonomous sub-agents. In some cases, one can even prove that the
resulting system gives the same optimal solutions as a monolithic
design.[MAS-page]

The goal-based view of agents also dominates the cognitive psychology
tradition in the area of problem solving, beginning with the enormously
influential *Human Problem Solving* @Newell+Simon:1972 and
running through all of Newell’s later work @Newell:1990. Goals, further
analyzed as *desires* (general) and
*intentions* (currently pursued), are central to the theory
of agents developed by Bratman [-@Bratman:1987]. This theory has been
influential both in natural language understanding and multiagent
systems.

Horvitz *et al.* [-@Horvitz+al:1988] specifically suggest
the use of rationality conceived as the maximization of expected utility
as a basis for AI. The text by Pearl [-@Pearl:1988] was the first in AI
to cover probability and utility theory in depth; its exposition of
practical methods for reasoning and decision making under uncertainty
was probably the single biggest factor in the rapid shift towards
utility-based agents in the 1990s (see ).

The general design for learning agents portrayed in is classic in the
machine learning literature @Buchanan+al:1978 [@Mitchell:1997]. Examples
of the design, as embodied in programs, go back at least as far as
Arthur Samuel’s [-@Samuel:1959; -@Samuel:1967] learning program for
playing checkers. Learning agents are discussed in depth in .

Interest in agents and in agent design has risen rapidly in recent
years, partly because of the growth of the Internet and the perceived
need for automated and mobile  @Etzioni+Weld:1994. Relevant papers are
collected in *Readings in Agents* @Huhns+Singh:1998 and
*Foundations of Rational Agency* @Wooldridge+Rao:1999.
Texts on multiagent systems usually provide a good introduction to many
aspects of agent design @Weiss:2000 [@Wooldridge:2002]. Several
conference series devoted to agents began in the 1990s, including the
International Workshop on Agent Theories, Architectures, and Languages
(ATAL), the International Conference on Autonomous Agents (AGENTS), and
the International Conference on Multi-Agent Systems (ICMAS). In 2002,
these three merged to form the International Joint Conference on
Autonomous Agents and Multi-Agent Systems (AAMAS). The journal
*Autonomous Agents and Multi-Agent Systems* was founded in
1998. Finally, *Dung Beetle Ecology* @Hanski+Cambefort:1991
provides a wealth of interesting information on the behavior of dung
beetles. YouTube features inspiring video recordings of their
activities.

Suppose that the performance measure is concerned with just the first
$T$ time steps of the environment and ignores everything thereafter.
Show that a rational agent’s action may depend not just on the state of
the environment but also on the time step it has reached.

[vacuum-rationality-exercise]Let us examine the rationality of various
vacuum-cleaner agent functions.

1.  Show that the simple vacuum-cleaner agent function described in is
    indeed rational under the assumptions listed on .

2.  Describe a rational agent function for the case in which each
    movement costs one point. Does the corresponding agent program
    require internal state?

3.  Discuss possible agent designs for the cases in which clean squares
    can become dirty and the geography of the environment is unknown.
    Does it make sense for the agent to learn from its experience in
    these cases? If so, what should it learn? If not, why not?

Write an essay on the relationship between evolution and one or more of
autonomy, intelligence, and learning.

For each of the following assertions, say whether it is true or false
and support your answer with examples or counterexamples where
appropriate.

1.  An agent that senses only partial information about the state cannot
    be perfectly rational.

2.  There exist task environments in which no pure reflex agent can
    behave rationally.

3.  There exists a task environment in which every agent is rational.

4.  The input to an agent program is the same as the input to the agent
    function.

5.  Every agent function is implementable by some program/machine
    combination.

6.  Suppose an agent selects its action uniformly at random from the set
    of possible actions. There exists a deterministic task environment
    in which this agent is rational.

7.  It is possible for a given agent to be perfectly rational in two
    distinct task environments.

8.  Every agent is rational in an unobservable environment.

9.  A perfectly rational poker-playing agent never loses.

[PEAS-exercise] For each of the following activities, give a PEAS
description of the task environment and characterize it in terms of the
properties listed in .

-   Playing soccer.

-   Exploring the subsurface oceans of Titan.

-   Shopping for used AI books on the Internet.

-   Playing a tennis match.

-   Practicing tennis against a wall.

-   Performing a high jump.

-   Knitting a sweater.

-   Bidding on an item at an auction.

[PEAS-exercise] For each of the following activities, give a PEAS
description of the task environment and characterize it in terms of the
properties listed in .

-   Performing a gymnastics floor routine.

-   Exploring the subsurface oceans of Titan.

-   Playing soccer.

-   Shopping for used AI books on the Internet.

-   Practicing tennis against a wall.

-   Performing a high jump.

-   Bidding on an item at an auction.

Define in your own words the following terms: agent, agent function,
agent program, rationality, autonomy, reflex agent, model-based agent,
goal-based agent, utility-based agent, learning agent.

[agent-fn-prog-exercise]This exercise explores the differences between
agent functions and agent programs.

1.  Can there be more than one agent program that implements a given
    agent function? Give an example, or show why one is not possible.

2.  Are there agent functions that cannot be implemented by any agent
    program?

3.  Given a fixed machine architecture, does each agent program
    implement exactly one agent function?

4.  Given an architecture with $n$ bits of storage, how many different
    possible agent programs are there?

5.  Suppose we keep the agent program fixed but speed up the machine by
    a factor of two. Does that change the agent function?

Write pseudocode agent programs for the goal-based and utility-based
agents.

Consider a simple thermostat that turns on a furnace when the
temperature is at least 3 degrees below the setting, and turns off a
furnace when the temperature is at least 3 degrees above the setting. Is
a thermostat an instance of a simple reflex agent, a model-based reflex
agent, or a goal-based agent?

The following exercises all concern the implementation of environments
and agents for the vacuum-cleaner world.

[vacuum-start-exercise]Implement a performance-measuring environment
simulator for the vacuum-cleaner world depicted in and specified on .
Your implementation should be modular so that the sensors, actuators,
and environment characteristics (size, shape, dirt placement, etc.) can
be changed easily. (*Note:* for some choices of programming
language and operating system there are already implementations in the
online code repository.)

Implement a simple reflex agent for the vacuum environment in . Run the
environment with this agent for all possible initial dirt configurations
and agent locations. Record the performance score for each configuration
and the overall average score.

[vacuum-motion-penalty-exercise]Consider a modified version of the
vacuum environment in , in which the agent is penalized one point for
each movement.

1.  Can a simple reflex agent be perfectly rational for this
    environment? Explain.

2.  What about a reflex agent with state? Design such an agent.

3.  How do your answers to **a** and **b**
    change if the agent’s percepts give it the clean/dirty status of
    every square in the environment?

[vacuum-unknown-geog-exercise]Consider a modified version of the vacuum
environment in , in which the geography of the environment—its extent,
boundaries, and obstacles—is unknown, as is the initial dirt
configuration. (The agent can go ${Up}$ and ${Down}$ as well as
${Left}$ and ${Right}$.)

1.  Can a simple reflex agent be perfectly rational for this
    environment? Explain.

2.  Can a simple reflex agent with a *randomized* agent
    function outperform a simple reflex agent? Design such an agent and
    measure its performance on several environments.

3.  Can you design an environment in which your randomized agent will
    perform poorly? Show your results.

4.  Can a reflex agent with state outperform a simple reflex agent?
    Design such an agent and measure its performance on several
    environments. Can you design a rational agent of this type?

[vacuum-bump-exercise] Repeat for the case in which the location sensor
is replaced with a “bump” sensor that detects the agent’s attempts to
move into an obstacle or to cross the boundaries of the environment.
Suppose the bump sensor stops working; how should the agent behave?

[vacuum-finish-exercise]The vacuum environments in the preceding
exercises have all been deterministic. Discuss possible agent programs
for each of the following stochastic versions:

1.  Murphy’s law: twenty-five percent of the time, the ${Suck}$ action
    fails to clean the floor if it is dirty and deposits dirt onto the
    floor if the floor is clean. How is your agent program affected if
    the dirt sensor gives the wrong answer 10% of the time?

2.  Small children: At each time step, each clean square has a 10%
    chance of becoming dirty. Can you come up with a rational agent
    design for this case?

[^1]: If the agent uses some randomization to choose its actions, then
    we would have to try each sequence many times to identify the
    probability of each action. One might imagine that acting randomly
    is rather silly, but we show later in this chapter that it can be
    very intelligent.

[^2]: See N. Henderson, “New door latches urged for Boeing 747 jumbo
    jets,” *Washington Post*, August 24, 1989.

[^3]: The word “sequential” is also used in computer science as the
    antonym of “parallel.” The two meanings are largely unrelated.

[^4]: There are other choices for the agent program skeleton; for
    example, we could have the agent programs be that run asynchronously
    with the environment. Each such coroutine has an input and output
    port and consists of a loop that reads the input port for percepts
    and writes actions to the output port.

[^5]: Also called , , or .

[^6]: The word “utility” here refers to “the quality of being useful,”
    not to the electric company or waterworks.
Probabilistic Reasoning {#bayes-nets-chapter}
=======================

introduced the basic elements of probability theory and noted the
importance of independence and conditional independence relationships in
simplifying probabilistic representations of the world. This chapter
introduces a systematic way to represent such relationships explicitly
in the form of . We define the syntax and semantics of these networks
and show how they can be used to capture uncertain knowledge in a
natural and efficient way. We then show how probabilistic inference,
although computationally intractable in the worst case, can be done
efficiently in many practical situations. We also describe a variety of
approximate inference algorithms that are often applicable when exact
inference is infeasible. We explore ways in which probability theory can
be applied to worlds with objects and relations—that is, to
*first-order*, as opposed to *propositional*,
representations. Finally, we survey alternative approaches to uncertain
reasoning.

Representing Knowledge in an Uncertain Domain
---------------------------------------------

In , we saw that the full joint probability distribution can answer any
question about the domain, but can become intractably large as the
number of variables grows. Furthermore, specifying probabilities for
possible worlds one by one is unnatural and tedious.

We also saw that independence and conditional independence relationships
among variables can greatly reduce the number of probabilities that need
to be specified in order to define the full joint distribution. This
section introduces a data structure called a [^1] to represent the
dependencies among variables. Bayesian networks can represent
essentially *any* full joint probability distribution and
in many cases can do so very concisely.

A Bayesian network is a directed graph in which each node is annotated
with quantitative probability information. The full specification is as
follows:

1.  Each node corresponds to a random variable, which may be discrete or
    continuous.

2.  A set of directed links or arrows connects pairs of nodes. If there
    is an arrow from node $X$ to node $Y$, $X$ is said to be a
    *parent* of $Y$$\!$. The graph has no
    directed cycles (and hence is a directed acyclic graph, or DAG.

3.  Each node $X_i$ has a conditional probability distribution
    $\pv(X_i\given \Parents(X_i))$ that quantifies the effect of the
    parents on the node.

The topology of the network—the set of nodes and links—specifies the
conditional independence relationships that hold in the domain, in a way
that will be made precise shortly. The *intuitive* meaning
of an arrow is typically that $X$ has a *direct influence*
on $Y$$\!$, which suggests that causes should be parents of
effects. It is usually easy for a domain expert to decide what direct
influences exist in the domain—much easier, in fact, than actually
specifying the probabilities themselves. Once the topology of the
Bayesian network is laid out, we need only specify a conditional
probability distribution for each variable, given its parents. We will
see that the combination of the topology and the conditional
distributions suffices to specify (implicitly) the full joint
distribution for all the variables.

[dentist-network-figure]

Recall the simple world described in , consisting of the variables
${Toothache}$, ${Cavity}$, ${Catch}$, and ${Weather}$. We argued
that ${Weather}$ is independent of the other variables; furthermore,
we argued that ${Toothache}$ and ${Catch}$ are conditionally
independent, given ${Cavity}$. These relationships are represented by
the Bayesian network structure shown in . Formally, the conditional
independence of ${Toothache}$ and ${Catch}$, given ${Cavity}$, is
indicated by the *absence* of a link between
${Toothache}$ and ${Catch}$. Intuitively, the network represents the
fact that ${Cavity}$ is a direct cause of ${Toothache}$ and
${Catch}$, whereas no direct causal relationship exists between
${Toothache}$ and ${Catch}$.

Now consider the following example, which is just a little more complex.
You have a new burglar alarm installed at home. It is fairly reliable at
detecting a burglary, but also responds on occasion to minor
earthquakes. (This example is due to Judea Pearl, a resident of Los
Angeles—hence the acute interest in earthquakes.) You also have two
neighbors, John and Mary, who have promised to call you at work when
they hear the alarm. John nearly always calls when he hears the alarm,
but sometimes confuses the telephone ringing with the alarm and calls
then, too. Mary, on the other hand, likes rather loud music and often
misses the alarm altogether. Given the evidence of who has or has not
called, we would like to estimate the probability of a burglary.

[burglary-figure]

A Bayesian network for this domain appears in . The network structure
shows that burglary and earthquakes directly affect the probability of
the alarm’s going off, but whether John and Mary call depends only on
the alarm. The network thus represents our assumptions that they do not
perceive burglaries directly, they do not notice minor earthquakes, and
they do not confer before calling.

The conditional distributions in are shown as a , or CPT. (This form of
table can be used for discrete variables; other representations,
including those suitable for continuous variables, are described in .)
Each row in a CPT contains the conditional probability of each node
value for a . A conditioning case is just a possible combination of
values for the parent nodes—a miniature possible world, if you like.
Each row must sum to 1, because the entries represent an exhaustive set
of cases for the variable. For Boolean variables, once you know that the
probability of a true value is $p$, the probability of false must be 1 –
$p$, so we often omit the second number, as in . In general, a table for
a Boolean variable with $k$ Boolean parents contains $2^k$ independently
specifiable probabilities. A node with no parents has only one row,
representing the prior probabilities of each possible value of the
variable.

Notice that the network does not have nodes corresponding to Mary’s
currently listening to loud music or to the telephone ringing and
confusing John. These factors are summarized in the uncertainty
associated with the links from ${Alarm}$ to ${JohnCalls}$ and
${MaryCalls}$. This shows both laziness and ignorance in operation: it
would be a lot of work to find out why those factors would be more or
less likely in any particular case, and we have no reasonable way to
obtain the relevant information anyway. The probabilities actually
summarize a *potentially infinite* set of circumstances in
which the alarm might fail to go off (high humidity, power failure, dead
battery, cut wires, a dead mouse stuck inside the bell, etc.) or John or
Mary might fail to call and report it (out to lunch, on vacation,
temporarily deaf, passing helicopter, etc.). In this way, a small agent
can cope with a very large world, at least approximately. The degree of
approximation can be improved if we introduce additional relevant
information.

The Semantics of Bayesian Networks {#bn-semantics-section}
----------------------------------

The previous section described what a network is, but not what it means.
There are two ways in which one can understand the semantics of Bayesian
networks. The first is to see the network as a representation of the
joint probability distribution. The second is to view it as an encoding
of a collection of conditional independence statements. The two views
are equivalent, but the first turns out to be helpful in understanding
how to *construct* networks, whereas the second is helpful
in designing inference procedures.

### Representing the full joint distribution

Viewed as a piece of “syntax,” a Bayesian network is a directed acyclic
graph with some numeric parameters attached to each node. One way to
define what the network means—its semantics—is to define the way in
which it represents a specific joint distribution over all the
variables. To do this, we first need to retract (temporarily) what we
said earlier about the parameters associated with each node. We said
that those parameters correspond to conditional probabilities
$\pv(X_i\given \Parents(X_i))$; this is a true statement, but until we
assign semantics to the network as a whole, we should think of them just
as numbers $\theta(X_i\given \Parents(X_i))$.

A generic entry in the joint distribution is the probability of a
conjunction of particular assignments to each variable, such as
$P(X_1 \eq x_1 \land \ldots
\land X_n \eq x_n)$. We use the notation $P(x_1,\ldots,x_n)$ as an
abbreviation for this. The value of this entry is given by the formula

$$P(x_1,\ldots,x_n) = \prod\limits_{i \eq 1}^n  \theta(x_i\given \parents(X_i))\ ,
\label{parameter-joint-repn-equation}$$

where $\parents(X_i)$ denotes the values of $\Parents(X_i)$ that appear
in $x_1,\ldots,x_n$. Thus, each entry in the joint distribution is
represented by the product of the appropriate elements of the
conditional probability tables (CPTs) in the Bayesian network.

From this definition, it is easy to prove that the parameters
$\theta(X_i\given \Parents(X_i))$ are exactly the conditional
probabilities $\pv(X_i\given \Parents(X_i))$ implied by the joint
distribution (see ). Hence, we can rewrite as

$$P(x_1,\ldots,x_n) = \prod\limits_{i \eq 1}^n  P(x_i\given \parents(X_i))\ .
\label{joint-repn-equation}$$

In other words, the tables we have been calling conditional probability
tables really *are* conditional probability tables
according to the semantics defined in .

To illustrate this, we can calculate the probability that the alarm has
sounded, but neither a burglary nor an earthquake has occurred, and both
John and Mary call. We multiply entries from the joint distribution
(using single-letter names for the variables):

$$\begin{aligned}
P(j, m, a, \lnot b, \lnot e) 
     &=& P(j\given a) P(m\given a) P(a\given \lnot b \land \lnot e) P(\lnot b)P(\lnot e) \\
     &=&  {0.90} \times {0.70} \times {0.001} \times {0.999} \times {0.998}  =  {0.000628}\ .\end{aligned}$$

explained that the full joint distribution can be used to answer any
query about the domain. If a Bayesian network is a representation of the
joint distribution, then it too can be used to answer any query, by
summing all the relevant joint entries. explains how to do this, but
also describes methods that are much more efficient.

#### A method for constructing Bayesian networks

defines what a given Bayesian network means. The next step is to explain
how to *construct* a Bayesian network in such a way that
the resulting joint distribution is a good representation of a given
domain. We will now show that implies certain conditional independence
relationships that can be used to guide the knowledge engineer in
constructing the topology of the network. First, we rewrite the entries
in the joint distribution in terms of conditional probability, using the
product rule (see ):
$$P(x_1,\ldots,x_n) = P(x_n\given x_{n-1},\ldots,x_1) 
                          P(x_{n-1},\ldots,x_1)\ .$$ Then we repeat the
process, reducing each conjunctive probability to a conditional
probability and a smaller conjunction. We end up with one big product:

$$\begin{aligned}
P(x_1,\ldots, x_n) &=& P(x_n\given x_{n-1},\ldots,x_1) 
                             P(x_{n-1}\given x_{n-2},\ldots,x_1)
\;\cdots\;P(x_2\given x_1) P(x_1) \\
&=& \prod\limits_{i\eq 1}^n P(x_i \given  x_{i-1},\ldots, x_1)\ .\end{aligned}$$

This identity is called the . It holds for any set of random variables.
Comparing it with , we see that the specification of the joint
distribution is equivalent to the general assertion that, for every
variable $X_i$ in the network,

$$\pv(X_i\given X_{i-1},\ldots,X_1) = \pv(X_i\given \Parents(X_i)) \ ,
\label{parent-insulation-equation}$$

provided that $\Parents(X_i) \subseteq \{ X_{i-1},\ldots,X_1 \}$. This
last condition is satisfied by numbering the nodes in a way that is
consistent with the partial order implicit in the graph structure.

What says is that the Bayesian network is a correct representation of
the domain only if each node is conditionally independent of its other
predecessors in the node ordering, given its parents. We can satisfy
this condition with this methodology:

1.  *Nodes:* First determine the set of variables that are
    required to model the domain. Now order them, $\{X_1,\ldots,X_n\}$.
    Any order will work, but the resulting network will be more compact
    if the variables are ordered such that causes precede effects.

2.  *Links: * For $i$ = 1 to $n$ do:

    -   Choose, from $X_1,\ldots,X_{i-1}$, a minimal set of parents for
        $X_i$, such that is satisfied.

    -   For each parent insert a link from the parent to $X_i$.

    -   CPTs: Write down the conditional probability table,
        $\pv(X_i|{Parents}(X_i))$.

Intuitively, the parents of node $X_i$ should contain all those nodes in
$X_1,\, \ldots,\,
X_{i-1}$ that

directly influence

$X_i$. For example, suppose we have completed the network in except for
the choice of parents for ${MaryCalls}$. ${MaryCalls}$ is certainly
influenced by whether there is a ${Burglary}$ or an ${Earthquake}$,
but not *directly* influenced. Intuitively, our knowledge
of the domain tells us that these events influence Mary’s calling
behavior only through their effect on the alarm. Also, given the state
of the alarm, whether John calls has no influence on Mary’s calling.
Formally speaking, we believe that the following conditional
independence statement holds:

$$\pv({MaryCalls} \given  {JohnCalls}, {Alarm}, {Earthquake}, {Burglary})
   = \pv({MaryCalls} \given  {Alarm})\ .$$

Thus, ${Alarm}$ will be the only parent node for ${MaryCalls}$.

Because each node is connected only to earlier nodes, this construction
method guarantees that the network is acyclic. Another important
property of Bayesian networks is that they contain no redundant
probability values. If there is no redundancy, then there is no chance
for inconsistency:

it is impossible for the knowledge engineer or domain expert to create a
Bayesian network that violates the axioms of probability.

#### Compactness and node ordering

As well as being a complete and nonredundant representation of the
domain, a Bayesian network can often be far more *compact*
than the full joint distribution. This property is what makes it
feasible to handle domains with many variables. The compactness of
Bayesian networks is an example of a general property of (also called )
systems. In a locally structured system, each subcomponent interacts
directly with only a bounded number of other components, regardless of
the total number of components. Local structure is usually associated
with linear rather than exponential growth in complexity. In the case of
Bayesian networks, it is reasonable to suppose that in most domains each
random variable is directly influenced by at most $k$ others, for some
constant $k$. If we assume $n$ Boolean variables for simplicity, then
the amount of information needed to specify each conditional probability
table will be at most $2^k$ numbers, and the complete network can be
specified by $n2^k$ numbers. In contrast, the joint distribution
contains $2^n$ numbers. To make this concrete, suppose we have
$n\eq {30}$ nodes, each with five parents ($k\eq 5$). Then the Bayesian
network requires 960 numbers, but the full joint distribution requires
over a billion.

There are domains in which each variable can be influenced directly by
all the others, so that the network is fully connected. Then specifying
the conditional probability tables requires the same amount of
information as specifying the joint distribution. In some domains, there
will be slight dependencies that should strictly be included by adding a
new link. But if these dependencies are tenuous, then it may not be
worth the additional complexity in the network for the small gain in
accuracy. For example, one might object to our burglary network on the
grounds that if there is an earthquake, then John and Mary would not
call even if they heard the alarm, because they assume that the
earthquake is the cause. Whether to add the link from ${Earthquake}$
to ${JohnCalls}$ and ${MaryCalls}$ (and thus enlarge the tables)
depends on comparing the importance of getting more accurate
probabilities with the cost of specifying the extra information.

[burglary-mess-figure]

Even in a locally structured domain, we will get a compact Bayesian
network only if we choose the node ordering well. What happens if we
happen to choose the wrong order? Consider the burglary example again.
Suppose we decide to add the nodes in the order ${MaryCalls}$,
${JohnCalls}$, ${Alarm}$, ${Burglary}$, ${Earthquake}$. We then
get the somewhat more complicated network shown in (a). The process goes
as follows:

-   Adding ${MaryCalls}$: No parents.

-   Adding ${JohnCalls}$: If Mary calls, that probably means the alarm
    has gone off, which of course would make it more likely that John
    calls. Therefore, ${JohnCalls}$ needs ${MaryCalls}$ as a parent.

-   Adding ${Alarm}$: Clearly, if both call, it is more likely that
    the alarm has gone off than if just one or neither calls, so we need
    both ${MaryCalls}$ and ${JohnCalls}$ as parents.

-   Adding ${Burglary}$: If we know the alarm state, then the call
    from John or Mary might give us information about our phone ringing
    or Mary’s music, but not about burglary:
    $$\pv({Burglary}\given {Alarm},{JohnCalls},{MaryCalls}) = \pv({Burglary}\given {Alarm}) \ .$$
    Hence we need just ${Alarm}$ as parent.

-   Adding ${Earthquake}$: If the alarm is on, it is more likely that
    there has been an earthquake. (The alarm is an earthquake detector
    of sorts.) But if we know that there has been a burglary, then that
    explains the alarm, and the probability of an earthquake would be
    only slightly above normal. Hence, we need both ${Alarm}$ and
    ${Burglary}$ as parents.

The resulting network has two more links than the original network in
and requires three more probabilities to be specified. What’s worse,
some of the links represent tenuous relationships that require difficult
and unnatural probability judgments, such as assessing the probability
of ${Earthquake}$, given ${Burglary}$ and ${Alarm}$. This
phenomenon is quite general and is related to the distinction between
and models introduced in (see also ). If we try to build a diagnostic
model with links from symptoms to causes (as from ${MaryCalls}$ to
${Alarm}$ or ${Alarm}$ to ${Burglary}$), we end up having to
specify additional dependencies between otherwise independent causes
(and often between separately occurring symptoms as well).

If we stick to a causal model, we end up having to specify fewer
numbers, and the numbers will often be easier to come up with.

In the domain of medicine, for example, it has been shown by Tversky and
Kahneman [-@Tversky+Kahneman:1982] that expert physicians prefer to give
probability judgments for causal rules rather than for diagnostic ones.

​(b) shows a very bad node ordering: ${MaryCalls}$, ${JohnCalls}$,
${Earthquake}$, ${Burglary}$, ${Alarm}$. This network requires 31
distinct probabilities to be specified—exactly the same number as the
full joint distribution. It is important to realize, however, that any
of the three networks can represent *exactly the same joint
distribution*. The last two versions simply fail to represent all
the conditional independence relationships and hence end up specifying a
lot of unnecessary numbers instead.

### Conditional independence relations in Bayesian networks

We have provided a “numerical” semantics for Bayesian networks in terms
of the representation of the full joint distribution, as in . Using this
semantics to derive a method for constructing Bayesian networks, we were
led to the consequence that a node is conditionally independent of its
other predecessors, given its parents. It turns out that we can also go
in the other direction. We can start from a “topological” semantics that
specifies the conditional independence relationships encoded by the
graph structure, and from this we can derive the “numerical” semantics.
The topological semantics[^2] specifies that each variable is
conditionally independent of its non-, given its parents. For example,
in , ${JohnCalls}$ is independent of ${Burglary}$, ${Earthquake}$,
and ${MaryCalls}$ given the value of ${Alarm}$. The definition is
illustrated in (a). From these conditional independence assertions and
the interpretation of the network parameters
$\theta(X_i\given \Parents(X_i))$ as specifications of conditional
probabilities $\pv(X_i\given \Parents(X_i))$, the full joint
distribution given in can be reconstructed. In this sense, the
“numerical” semantics and the “topological” semantics are equivalent.

[conditional-independence-figure]

Another important independence property is implied by the topological
semantics: a node is conditionally independent of all other nodes in the
network, given its parents, children, and children’s parents—that is,
given its . ( asks you to prove this.) For example, ${Burglary}$ is
independent of ${JohnCalls}$ and ${MaryCalls}$, given ${Alarm}$
and ${Earthquake}$. This property is illustrated in (b).
[markov-blanket-page]

Efficient Representation of Conditional Distributions {#canonical-distribution-section}
-----------------------------------------------------

Even if the maximum number of parents $k$ is smallish, filling in the
CPT for a node requires up to $O(2^k)$ numbers and perhaps a great deal
of experience with all the possible conditioning cases. In fact, this is
a worst-case scenario in which the relationship between the parents and
the child is completely arbitrary. Usually, such relationships are
describable by a that fits some standard pattern. In such cases, the
complete table can be specified by naming the pattern and perhaps
supplying a few parameters—much easier than supplying an exponential
number of parameters.

The simplest example is provided by . A deterministic node has its value
specified exactly by the values of its parents, with no uncertainty. The
relationship can be a logical one: for example, the relationship between
the parent nodes ${Canadian}$, ${US}$, ${Mexican}$ and the child
node ${NorthAmerican}$ is simply that the child is the disjunction of
the parents. The relationship can also be numerical: for example, if the
parent nodes are the prices of a particular model of car at several
dealers and the child node is the price that a bargain hunter ends up
paying, then the child node is the minimum of the parent values; or if
the parent nodes are a lake’s inflows (rivers, runoff, precipitation)
and outflows (rivers, evaporation, seepage) and the child is the change
in the water level of the lake, then the value of the child is the sum
of the inflow parents minus the sum of the outflow parents.

Uncertain relationships can often be characterized by so-called logical
relationships. The standard example is the relation, which is a
generalization of the logical OR. In propositional logic, we might say
that ${Fever}$ is true if and only if ${Cold}$, ${Flu}$, or
${Malaria}$ is true. The noisy-OR model allows for uncertainty about
the ability of each parent to cause the child to be true—the causal
relationship between parent and child may be *inhibited*,
and so a patient could have a cold, but not exhibit a fever. The model
makes two assumptions. First, it assumes that all the possible causes
are listed. (If some are missing, we can always add a so-called that
covers “miscellaneous causes.”) Second, it assumes that inhibition of
each parent is independent of inhibition of any other parents: for
example, whatever inhibits ${Malaria}$ from causing a fever is
independent of whatever inhibits ${Flu}$ from causing a fever. Given
these assumptions, ${Fever}$ is *false* if and only if
all its *true* parents are inhibited, and the probability
of this is the product of the inhibition probabilities $q$ for each
parent. Let us suppose these individual inhibition probabilities are as
follows:

$$\begin{aligned}
\lefteqn{q_{{\rm cold}} = P(\lnot {fever}\given {cold},\lnot {flu},\lnot {malaria}) = {0.6}\ ,}\\
\lefteqn{q_{{\rm flu}} = P(\lnot {fever}\given \lnot {cold},{flu},\lnot {malaria}) = {0.2}\ ,}\\
\lefteqn{q_{{\rm malaria}} = P(\lnot {fever}\given \lnot {cold},\lnot {flu},{malaria}) = {0.1}\ .}\end{aligned}$$

Then, from this information and the noisy-OR assumptions, the entire CPT
can be built. The general rule is that
$$P(x_i\given \parents(X_i)) = 1- \prod_{\{j: X_j\eq {true}\}} q_j \ ,$$
where the product is taken over the parents that are set to true for
that row of the CPT. The following table illustrates this calculation:

|ccc|@  l|@  l| & & &
$P({Fever})$ & $P(\lnot {Fever})$\
F & F & F & ${0.0}$ & ${1.0}$\
F & F & T & ${0.9}$ & ${\mbf{0.1}}$\
F & T & F & ${0.8}$ & ${\mbf{0.2}}$\
F & T & T & ${0.98}$ & ${0.02} = {0.2} \times {0.1}$\
T & F & F & ${0.4}$ & ${\mbf{0.6}}$\
T & F & T & ${0.94}$ & ${0.06} = {0.6} \times {0.1}$\
T & T & F & ${0.88}$ & ${0.12} = {0.6} \times {0.2}$\
T & T & T & ${0.988}$ & ${0.012} = {0.6} \times {0.2} \times {0.1}$\

In general, noisy logical relationships in which a variable depends on
$k$ parents can be described using $O(k)$ parameters instead of $O(2^k)$
for the full conditional probability table. This makes assessment and
learning much easier. For example, the network @Pradhan+al:1994 uses
noisy-OR and noisy-MAX distributions to model relationships among
diseases and symptoms in internal medicine. With 448 nodes and 906
links, it requires only 8,254 values instead of 133,931,430 for a
network with full CPTs.[cpcs-page]

#### Bayesian nets with continuous variables

Many real-world problems involve continuous quantities, such as height,
mass, temperature, and money; in fact, much of statistics deals with
random variables whose domains are continuous. By definition, continuous
variables have an infinite number of possible values, so it is
impossible to specify conditional probabilities explicitly for each
value. One possible way to handle continuous variables is to avoid them
by using —that is, dividing up the possible values into a fixed set of
intervals. For example, temperatures could be divided into
(${<}0\deg {\rm C}$), ($0\deg
{\rm C}{-}{100}\deg {\rm C}$), and (${>}{100}\deg {\rm C}$).
Discretization is sometimes an adequate solution, but often results in a
considerable loss of accuracy and very large CPTs. The most common
solution is to define standard families of probability density functions
(see ) that are specified by a finite number of . For example, a
Gaussian (or normal) distribution $N(\mu,\sigma^2)(x)$ has the mean
$\mu$ and the variance $\sigma^2$ as parameters. Yet another
solution—sometimes called a representation—is to define the conditional
distribution implicitly with a collection of instances, each containing
specific values of the parent and child variables. We explore this
approach further in .

A network with both discrete and continuous variables is called a . To
specify a hybrid network, we have to specify two new kinds of
distributions: the conditional distribution for a continuous variable
given discrete or continuous parents; and the conditional distribution
for a discrete variable given continuous parents. Consider the simple
example in , in which a customer buys some fruit depending on its cost,
which depends in turn on the size of the harvest and whether the
government’s subsidy scheme is operating. The variable ${Cost}$ is
continuous and has continuous and discrete parents; the variable
${Buys}$ is discrete and has a continuous parent.

[continuous-net-figure]

For the ${Cost}$ variable, we need to specify $\pv({Cost} \given 
{Harvest},{Subsidy})$. The discrete parent is handled by
enumeration—that is, by specifying both
$P({Cost} \given  {Harvest},{subsidy})$ and
$P({Cost} \given  {Harvest},\lnot {subsidy})$. To handle
${Harvest}$, we specify how the distribution over the cost $c$ depends
on the continuous value $h$ of ${Harvest}$. In other words, we specify
the *parameters* of the cost distribution as a function of
$h$. The most common choice is the distribution, in which the child has
a Gaussian distribution whose mean $\mu$ varies linearly with the value
of the parent and whose standard deviation $\sigma$ is fixed. We need
two distributions, one for ${subsidy}$ and one for
$\lnot {subsidy}$, with different parameters:

$$\begin{aligned}
P(c\given h,{subsidy}) & = & N(a_t h + b_t, \sigma_t^2)(c)
 = \frac{1}{\sigma_t \sqrt{2\pi}}\ 
 e^{-\frac{1}{2} 
          \left(\frac{c-(a_t h + b_t)}{\sigma_t}\right)^2}\\
P(c\given h,\lnot {subsidy}) & = & N(a_f h + b_f, \sigma_f^2)(c)
 = \frac{1}{\sigma_f \sqrt{2\pi}}\ 
 e^{-\frac{1}{2} 
          \left(\frac{c-(a_f h + b_f)}{\sigma_f}\right)^2} \ .\end{aligned}$$

[LG-network-page]For this example, then, the conditional distribution
for ${Cost}$ is specified by naming the linear Gaussian distribution
and providing the parameters $a_t$, $b_t$, $\sigma_t$, $a_f$, $b_f$, and
$\sigma_f$. Figures [linear-gaussian-figure](a) and (b) show these two
relationships. Notice that in each case the slope is negative, because
cost decreases as supply increases. (Of course, the assumption of
linearity implies that the cost becomes negative at some point; the
linear model is reasonable only if the harvest size is limited to a
narrow range.) (c) shows the distribution $P(c\given h)$, averaging over
the two possible values of ${Subsidy}$ and assuming that each has
prior probability 0.5. This shows that even with very simple models,
quite interesting distributions can be represented.

[linear-gaussian-figure]

The linear Gaussian conditional distribution has some special
properties. A network containing only continuous variables with linear
Gaussian distributions has a joint distribution that is a multivariate
Gaussian distribution (see ) over all the variables (). Furthermore, the
posterior distribution given any evidence also has this
property[linear-gaussian-page].[^3] When discrete variables are added as
parents (not as children) of continuous variables, the network defines a
, or CG, distribution: given any assignment to the discrete variables,
the distribution over the continuous variables is a multivariate
Gaussian.

Now we turn to the distributions for discrete variables with continuous
parents. Consider, for example, the ${Buys}$ node in . It seems
reasonable to assume that the customer will buy if the cost is low and
will not buy if it is high and that the probability of buying varies
smoothly in some intermediate region. In other words, the conditional
distribution is like a “soft” threshold function. One way to make soft
thresholds is to use the *integral* of the standard normal
distribution: $$\Phi(x) = \int_{-\infty}^{x} N(0,1)(x) dx\ .$$ Then the
probability of ${Buys}$ given ${Cost}$ might be
$$P({buys} \given {Cost} \eq c) = \Phi((-c + \mu)/\sigma) \ ,$$
which means that the cost threshold occurs around $\mu$, the width of
the threshold region is proportional to $\sigma$, and the probability of
buying decreases as cost increases. This (pronounced “pro-bit” and short
for “probability unit”) is illustrated in (a)[probit-page]. The form can
be justified by proposing that the underlying decision process has a
hard threshold, but that the precise location of the threshold is
subject to random Gaussian noise.

[probit-logit-figure]

An alternative to the probit model is the (pronounced “low-jit”). It
uses the $1/(1+e^{{-}x})$ to produce a soft threshold:
$$P({buys} \given {Cost} \eq c) = \frac{1}{1+{exp}(-2\frac{-c+\mu}{\sigma})} \ .$$
This is illustrated in (b). The two distributions look similar, but the
logit actually has much longer “tails.” The probit is often a better fit
to real situations, but the logit is sometimes easier to deal with
mathematically. It is used widely in neural networks (). Both probit and
logit can be generalized to handle multiple continuous parents by taking
a linear combination of the parent values.

Exact Inference in Bayesian Networks {#exact-inference-section}
------------------------------------

The basic task for any probabilistic inference system is to compute the
posterior probability distribution for a set of , given some observed
—that is, some assignment of values to a set of . To simplify the
presentation, we will consider only one query variable at a time; the
algorithms can easily be extended to queries with multiple variables. We
will use the notation from : $X$ denotes the query variable; $\E$
denotes the set of evidence variables $E_1,\ldots,
E_m$, and $\e$ is a particular observed event; $\mbf{Y}$ will denotes
the nonevidence, nonquery variables $Y_1,\ldots, Y_l$ (called the ).
Thus, the complete set of variables is $\X\eq \{X\}\union \E \union \Y$.
A typical query asks for the posterior probability distribution
$\pv(X\given \e)$.

In the burglary network, we might observe the event in which
${JohnCalls}\eq {true}$ and ${MaryCalls}\eq {true}$. We could
then ask for, say, the probability that a burglary has occurred:
$$\pv({Burglary}\given {JohnCalls}\eq {true},{MaryCalls}\eq {true}) = \<{0.284},{0.716}\>\ .$$
In this section we discuss exact algorithms for computing posterior
probabilities and will consider the complexity of this task. It turns
out that the general case is intractable, so covers methods for
approximate inference.

### Inference by enumeration

explained that any conditional probability can be computed by summing
terms from the full joint distribution. More specifically, a query
$\pv(X\given \e)$ can be answered using , which we repeat here for
convenience:
$$\pv(X\given \e) = \alpha\, \pv(X,\e) = \alpha\, \sum_{\sy} \pv(X,\e,\y)\ .$$
Now, a Bayesian network gives a complete representation of the full
joint distribution. More specifically, on shows that the terms
$P(x,\e,\y)$ in the joint distribution can be written as products of
conditional probabilities from the network. Therefore,

a query can be answered using a Bayesian network by computing sums of
products of conditional probabilities from the network.

Consider the query
$\pv({Burglary}\given {JohnCalls}\eq {true},{MaryCalls}\eq
{true})$. The hidden variables for this query are ${Earthquake}$ and
${Alarm}$. From , using initial letters for the variables to shorten
the expressions, we have[^4]
$$\pv(B\given j,m) = \alpha\, \pv(B,j,m) = \alpha\, \sum_{e}\sum_{a} \pv(B,j,m,e,a,)\ .$$
The semantics of Bayesian networks () then gives us an expression in
terms of CPT entries. For simplicity, we do this just for
${Burglary}\eq {true}$:
$$P(b\given j,m) = \alpha\, \sum_e \sum_a P(b)P(e)P(a\given b,e)P(j\given a)P(m\given a) \ .$$
To compute this expression, we have to add four terms, each computed by
multiplying five numbers. In the worst case, where we have to sum out
almost all the variables, the complexity of the algorithm for a network
with $n$ Boolean variables is $O(n2^n)$.

An improvement can be obtained from the following simple observations:
the $P(b)$ term is a constant and can be moved outside the summations
over $a$ and $e$, and the $P(e)$ term can be moved outside the summation
over $a$. Hence, we have

$$P(b\given j,m) = \alpha\, P(b)\sum_e P(e)\sum_a P(a\given b,e)P(j\given a)P(m\given a) \ .
\label{enumeration-example-equation}$$

This expression can be evaluated by looping through the variables in
order, multiplying CPT entries as we go. For each summation, we also
need to loop over the variable’s possible values. The structure of this
computation is shown in . Using the numbers from , we obtain
$P(b\given j,m) = \alpha \stimes {0.00059224}$. The corresponding
computation for $\lnot b$ yields $\alpha\stimes {0.0014919}$; hence,
$$\pv(B\given j,m) = 
        \alpha\, \<{0.00059224},{0.0014919}\> \approx \<{0.284},{0.716}\>\ .$$
That is, the chance of a burglary, given calls from both neighbors, is
about 28%.

The evaluation process for the expression in is shown as an expression
tree in . The algorithm in evaluates such trees using depth-first
recursion. The algorithm is very similar in structure to the
backtracking algorithm for solving CSPs () and the DPLL algorithm for
satisfiability ().

The space complexity of is only linear in the number of variables: the
algorithm sums over the full joint distribution without ever
constructing it explicitly. Unfortunately, its time complexity for a
network with $n$ Boolean variables is always $O(2^n)$—better than the
$O(n\,2^n)$ for the simple approach described earlier, but still rather
grim.

Note that the tree in makes explicit the *repeated
subexpressions* evaluated by the algorithm. The products
$P(j\given a)P(m\given a)$ and $P(j\given \lnot a)P(m\given \lnot a)$
are computed twice, once for each value of $e$. The next section
describes a general method that avoids such wasted computations.

[enumeration-tree-figure]

[enumeration-algorithm]

### The variable elimination algorithm

The enumeration algorithm can be improved substantially by eliminating
repeated calculations of the kind illustrated in . The idea is simple:
do the calculation once and save the results for later use. This is a
form of dynamic programming. There are several versions of this
approach; we present the algorithm, which is the simplest. Variable
elimination works by evaluating expressions such as in
*right-to-left* order (that is, *bottom up* in
). Intermediate results are stored, and summations over each variable
are done only for those portions of the expression that depend on the
variable.

Let us illustrate this process for the burglary network. We evaluate the
expression $$\pv(B\given j,m) 
    = \alpha\, \underbrace{\pv(B)}_{\subf_1(B)} 
             \sum_e \underbrace{P(e)}_{\subf_2(E)}
             \sum_a \underbrace{\pv(a\given B,e)}_{\subf_3(A,B,E)}
             \underbrace{P(j\given a)}_{\subf_4(A)}
             \underbrace{P(m\given a)}_{\subf_5(A)}\ .$$ Notice that we
have annotated each part of the expression with the name of the
corresponding ; each factor is a matrix indexed by the values of its
argument variables. For example, the factors $\f_4(A)$ and $\f_5(A)$
corresponding to $P(j\given a)$ and $P(m\given a)$ depend just on $A$
because $J$ and $M$ are fixed by the query. They are therefore
two-element vectors: $$\f_4(A) = \left(\begin{array}{c} P(j\given a)\\
                                P(j\given \lnot a)
               \end{array}\right)
        = \left(\begin{array}{c} 0.90\\
                                 0.05
               \end{array}\right)  
               \qquad\qquad
\f_5(A) = \left(\begin{array}{c} P(m\given a)\\
                                P(m\given \lnot a)
                \end{array}\right)
        = \left(\begin{array}{c} 0.70\\
                                 0.01
               \end{array}\right) \ .$$ $\f_3(A,B,E)$ will be a
$2\stimes 2\stimes 2$ matrix, which is hard to show on the printed page.
(The “first” element is given by $P(a\given b,e)\eq 0.95$ and the “last”
by $P(\lnot a\given \lnot b,\lnot e)\eq 0.999$.) In terms of factors,
the query expression is written as $$\pv(B\given j,m) 
    = \alpha\, \f_1(B) \stimes \sum_e \f_2(E) \stimes \sum_a \f_3(A,B,E) \stimes \f_4(A) \stimes \f_5(A)$$
where the “${\times}$” operator is not ordinary matrix multiplication
but instead the operation, to be described shortly.

The process of evaluation is a process of summing out variables (right
to left) from pointwise products of factors to produce new factors,
eventually yielding a factor that is the solution, i.e., the posterior
distribution over the query variable. The steps are as follows:

-   First, we sum out $A$ from the product of $\f_3$, $\f_4$, and
    $\f_5$. This gives us a new $2\times 2$ factor $\f_6(B,E)$ whose
    indices range over just $B$ and $E$:

    $$\begin{aligned}
    \f_6(B,E) &=& \sum_a \f_3(A,B,E)\stimes \f_4(A) \stimes \f_5(A)\\
                       &=& (\f_3(a,B,E)\stimes \f_4(a) \stimes \f_5(a)) + (\f_3(\lnot a,B,E)\stimes \f_4(\lnot a) \stimes \f_5(\lnot a))\,.\end{aligned}$$

    Now we are left with the expression $$\pv(B\given j,m) 
        = \alpha\, \f_1(B) \stimes \sum_e \f_2(E) \stimes \f_6(B,E)\ .$$

-   Next, we sum out $E$ from the product of $\f_2$ and $\f_6$:

    $$\begin{aligned}
    \f_7(B) &=& \sum_e \f_2(E) \stimes \f_6(B,E) \\
            &=& \f_2(e)\stimes \f_6(B,e) + \f_2(\lnot e)\stimes \f_6(B,\lnot e) \ .\end{aligned}$$

    This leaves the expression
    $$\pv(B\given j,m) = \alpha\, \f_1(B) \stimes \f_7(B)$$ which can be
    evaluated by taking the pointwise product and normalizing the
    result.

Examining this sequence, we see that two basic computational operations
are required: pointwise product of a pair of factors, and summing out a
variable from a product of factors. The next section describes each of
these operations.

#### Operations on factors

The pointwise product of two factors $\f_1$ and $\f_2$ yields a new
factor $\f$ whose variables are the *union* of the
variables in $\f_1$ and $\f_2$ and whose elements are given by the
product of the corresponding elements in the two factors. Suppose the
two factors have variables $Y_1,\ldots,Y_k$ in common. Then we have
$$\f(X_1\ldots X_j,Y_1\ldots Y_k,Z_1\ldots Z_l) =
\f_1(X_1\ldots X_j,Y_1\ldots Y_k)\;
\f_2(Y_1\ldots Y_k,Z_,\ldots Z_l).$$ If all the variables are binary,
then $\f_1$ and $\f_2$ have $2^{j+k}$ and $2^{k+l}$ entries,
respectively, and the pointwise product has $2^{j+k+l}$ entries. For
example, given two factors $\f_1(A, B)$ and $\f_2(B, C)$, the pointwise
product $\f_1 \stimes \f_2 \eq \f_3(A,B,C)$ has $2^{1+1+1}\eq 8$
entries, as illustrated in . Notice that the factor resulting from a
pointwise product can contain more variables than any of the factors
being multiplied and that the size of a factor is exponential in the
number of variables. This is where both space and time complexity arise
in the variable elimination algorithm.

[tbp] [pointwise-table]

Summing out a variable from a product of factors is done by adding up
the submatrices formed by fixing the variable to each of its values in
turn. For example, to sum out $A$ from $\f_3(A,B,C)$, we write

$$\begin{aligned}
\f(B,C) &=& \sum_a \f_3(A,B,C) = \f_3(a,B,C) + \f_3(\lnot a,B,C) \\
          &=& 
              \left(\begin{array}{cc} .06 & .24\\
                                     .42 & .28
               \end{array}\right)
   +          \left(\begin{array}{cc} .18 & .72\\
                                     .06 & .04
               \end{array}\right)
   =          \left(\begin{array}{cc} .24 & .96\\
                                     .48 & .32
               \end{array}\right)\ .\end{aligned}$$

The only trick is to notice that any factor that does *not*
depend on the variable to be summed out can be moved outside the
summation. For example, if we were to sum out $E$ first in the burglary
network, the relevant part of the expression would be
$$\sum_e \f_2(E)\stimes \f_3(A,B,E) \stimes \f_4(A) \stimes \f_5(A) = \f_4(A) \stimes \f_5(A) \stimes \sum_e \f_2(E)\stimes \f_3(A,B,E)\,.$$
Now the pointwise product inside the summation is computed, and the
variable is summed out of the resulting matrix.

Notice that matrices are *not* multiplied until we need to
sum out a variable from the accumulated product. At that point, we
multiply just those matrices that include the variable to be summed out.
Given functions for pointwise product and summing out, the variable
elimination algorithm itself can be written quite simply, as shown in .

[elimination-ask-algorithm]

#### Variable ordering and variable relevance

The algorithm in includes an unspecified function to choose an ordering
for the variables. Every choice of ordering yields a valid algorithm,
but different orderings cause different intermediate factors to be
generated during the calculation. For example, in the calculation shown
previously, we eliminated $A$ before $E$; if we do it the other way, the
calculation becomes $$\pv(B\given j,m) 
    = \alpha\, \f_1(B) \stimes \sum_a  \f_4(A) \stimes \f_5(A)
    \stimes \sum_e \f_2(E) \stimes \f_3(A,B,E)\ ,$$ during which a new
factor $\f_6(A,B)$ will be generated.

In general, the time and space requirements of variable elimination are
dominated by the size of the largest factor constructed during the
operation of the algorithm. This in turn is determined by the order of
elimination of variables and by the structure of the network. It turns
out to be intractable to determine the optimal ordering, but several
good heuristics are available. One fairly effective method is a greedy
one: eliminate whichever variable minimizes the size of the next factor
to be constructed.

Let us consider one more query:
$\pv({JohnCalls}\given {Burglary}\eq {true})$. As usual, the first
step is to write out the nested summation:
$$\pv(J\given b) = \alpha\, P(b) \sum_e P(e) \sum_a P(a\given b,e) \pv(J\given a) \sum_m P(m\given a)\ .$$
Evaluating this expression from right to left, we notice something
interesting: $\sum_m P(m\given a)$ is equal to 1 by definition! Hence,
there was no need to include it in the first place; the variable $M$ is
*irrelevant* to this query. Another way of saying this is
that the result of the query
$P({JohnCalls}\given {Burglary}\eq {true})$ is unchanged if we
remove ${MaryCalls}$ from the network altogether. In general, we can
remove any leaf node that is not a query variable or an evidence
variable. After its removal, there may be some more leaf nodes, and
these too may be irrelevant. Continuing this process, we eventually find
that

every variable that is not an ancestor of a query variable or evidence
variable is irrelevant to the query.

A variable elimination algorithm can therefore remove all these
variables before evaluating the query.

### The complexity of exact inference

The complexity of exact inference in Bayesian networks depends strongly
on the structure of the network. The burglary network of belongs to the
family of networks in which there is at most one undirected path between
any two nodes in the network. These are called networks or
[polytree-page], and they have a particularly nice property:

The time and space complexity of exact inference in polytrees is linear
in the size of the network.

Here, the size is defined as the number of CPT entries; if the number of
parents of each node is bounded by a constant, then the complexity will
also be linear in the number of nodes.

For networks, such as that of (a), variable elimination can have
exponential time and space complexity in the worst case, even when the
number of parents per node is bounded. This is not surprising when one
considers that

because it includes inference in propositional logic as a special case,
inference in Bayesian networks is NP-hard.

In fact, it can be shown () that the problem is as hard as that of
computing the *number* of satisfying assignments for a
propositional logic formula. This means that it is \#P-hard (“number-P
hard”)—that is, strictly harder than problems.

There is a close connection between the complexity of Bayesian network
inference and the complexity of constraint satisfaction problems (CSPs).
As we discussed in , the difficulty of solving a discrete CSP is related
to how “treelike” its constraint graph is. Measures such as , which
bound the complexity of solving a CSP, can also be applied directly to
Bayesian networks. Moreover, the variable elimination algorithm can be
generalized to solve CSPs as well as Bayesian networks.

### Clustering algorithms

The variable elimination algorithm is simple and efficient for answering
individual queries. If we want to compute posterior probabilities for
all the variables in a network, however, it can be less efficient. For
example, in a polytree network, one would need to issue $O(n)$ queries
costing $O(n)$ each, for a total of $O(n^2)$ time. Using algorithms
(also known as algorithms), the time can be reduced to $O(n)$. For this
reason, these algorithms are widely used in commercial Bayesian network
tools.

[rain-clustering-figure]

The basic idea of clustering is to join individual nodes of the network
to form cluster nodes in such a way that the resulting network is a
polytree. For example, the multiply connected network shown in (a) can
be converted into a polytree by combining the ${Sprinkler}$ and
${Rain}$ node into a cluster node called ${Sprinkler}{+}{Rain}$,
as shown in (b). The two Boolean nodes are replaced by a “meganode” that
takes on four possible values: $tt$, $tf$, $ft$, and $ff$. The meganode
has only one parent, the Boolean variable ${Cloudy}$, so there are two
conditioning cases. Although this example doesn’t show it, the process
of clustering often produces meganodes that share some variables.

Once the network is in polytree form, a special-purpose inference
algorithm is required, because ordinary inference methods cannot handle
meganodes that share variables with each other. Essentially, the
algorithm is a form of constraint propagation (see ) where the
constraints ensure that neighboring meganodes agree on the posterior
probability of any variables that they have in common. With careful
bookkeeping, this algorithm is able to compute posterior probabilities
for all the nonevidence nodes in the network in time
*linear* in the size of the clustered network. However, the
NP-hardness of the problem has not disappeared: if a network requires
exponential time and space with variable elimination, then the CPTs in
the clustered network will necessarily be exponentially large.

Approximate Inference in Bayesian Networks {#approx-inference-section}
------------------------------------------

Given the intractability of exact inference in large, multiply connected
networks, it is essential to consider approximate inference methods.
This section describes randomized sampling algorithms, also called
algorithms, that provide approximate answers whose accuracy depends on
the number of samples generated. Monte Carlo algorithms, of which
simulated annealing () is an example, are used in many branches of
science to estimate quantities that are difficult to calculate exactly.
In this section, we are interested in sampling applied to the
computation of posterior probabilities. We describe two families of
algorithms: direct sampling and Markov chain sampling. Two other
approaches—variational methods and loopy propagation—are mentioned in
the notes at the end of the chapter.

### Direct sampling methods

The primitive element in any sampling algorithm is the generation of
samples from a known probability distribution. For example, an unbiased
coin can be thought of as a random variable ${Coin}$ with values
$\<{heads},{tails}\>$ and a prior distribution
$\pv({Coin})=\< {0.5},{0.5}\>$. Sampling from this distribution is
exactly like flipping the coin: with probability 0.5 it will return
${heads}$, and with probability 0.5 it will return ${tails}$. Given
a source of random numbers uniformly distributed in the range $[0,1]$,
it is a simple matter to sample any distribution on a single variable,
whether discrete or continuous. (See .)

The simplest kind of random sampling process for Bayesian networks
generates events from a network that has no evidence associated with it.
The idea is to sample each variable in turn, in topological order. The
probability distribution from which the value is sampled is conditioned
on the values already assigned to the variable’s parents. This algorithm
is shown in . We can illustrate its operation on the network in (a),
assuming an ordering $[{Cloudy},{Sprinkler},{Rain},{WetGrass}]$:

1.  Sample from $\pv({Cloudy}) = \<{0.5},{0.5}\>$, value is
    ${true}$.

2.  Sample from
    $\pv({Sprinkler}\given {Cloudy}\eq {true}) = \<{0.1},{0.9}\>$,
    value is ${false}$.

3.  Sample from
    $\pv({Rain}\given {Cloudy}\eq {true}) = \<{0.8},{0.2}\>$,
    value is ${true}$.

4.  Sample from
    $\pv({WetGrass}\given {Sprinkler}\eq {false},{Rain}\eq {true}) = \<{0.9},{0.1}\>$,
    value is ${true}$.

In this case, returns the event
$[{true},{false},{true},{true}]$.

[prior-sample-algorithm]

It is easy to see that generates samples from the prior joint
distribution specified by the network. First, let
$S_{{PS}}(x_1,\ldots,x_n)$ be the probability that a specific event is
generated by the algorithm. *Just looking at the sampling
process*, we have
$$S_{{PS}}(x_1\ldots x_n) = \prod_{i\eq 1}^n P(x_i \given  \parents(X_i))$$
because each sampling step depends only on the parent values. This
expression should look familiar, because it is also the probability of
the event according to the Bayesian net’s representation of the joint
distribution, as stated in . That is, we have
$$S_{{PS}}(x_1\ldots x_n) = P(x_1\ldots x_n)\ .$$ This simple fact
makes it easy to answer questions by using samples.

In any sampling algorithm, the answers are computed by counting the
actual samples generated. Suppose there are $N$ total samples, and let
$N_{{PS}}(x_1,
\ldots,x_n)$ be the number of times the specific event $x_1,\ldots, x_n$
occurs in the set of samples. We expect this number, as a fraction of
the total, to converge in the limit to its expected value according to
the sampling probability:

$$\lim_{N\to\infty} \frac{N_{{PS}}(x_1,\ldots, x_n)}{N} =
S_{{PS}}(x_1,\ldots,x_n) = P(x_1,\ldots, x_n)\ .
\label{limit-equation}$$

For example, consider the event produced earlier:
$[{true},{false},{true},{true}]$. The sampling probability for
this event is
$$S_{{PS}}({true},{false},{true},{true}) = {0.5} \stimes {0.9} \stimes {0.8} \stimes {0.9} = {0.324}\ .$$
Hence, in the limit of large $N$, we expect 32.4% of the samples to be
of this event.

Whenever we use an approximate equality (“$\approx$”) in what follows,
we mean it in exactly this sense—that the estimated probability becomes
exact in the large-sample limit. Such an estimate is called . For
example, one can produce a consistent estimate of the probability of any
partially specified event $x_1,\ldots,x_m$, where $m\leq n$, as follows:

$$P(x_1,\ldots,x_m) \approx N_{{PS}}(x_1,\ldots,x_m)/N\ .
\label{partial-limit-equation}$$

That is, the probability of the event can be estimated as the fraction
of all complete events generated by the sampling process that match the
partially specified event. For example, if we generate 1000 samples from
the sprinkler network, and 511 of them have ${Rain}\eq {true}$, then
the estimated probability of rain, written as
$\hat P({Rain}\eq {true})$, is 0.511.

#### Rejection sampling in Bayesian networks

is a general method for producing samples from a hard-to-sample
distribution given an easy-to-sample distribution. In its simplest form,
it can be used to compute conditional probabilities—that is, to
determine $P(X\given \e)$. The algorithm is shown in . First, it
generates samples from the prior distribution specified by the network.
Then, it rejects all those that do not match the evidence. Finally, the
estimate $\hat P(X\eq x \given  \e)$ is obtained by counting how often
$X\eq x$ occurs in the remaining samples.

Let $\hat{\pv}(X\given \e)$ be the estimated distribution that the
algorithm returns. From the definition of the algorithm, we have
$$\hat{\pv}(X\given \e) = \alpha\, \mbf{N}_{{PS}}(X,\e) = \frac{\mbf{N}_{{PS}}(X,\e)}{N_{{PS}}(\e)}\ .$$
From , this becomes
$$\hat{\pv}(X\given \e) \approx \frac{\pv(X,\e)}{P(\e)} = \pv(X\given \e)\ .$$
That is, rejection sampling produces a consistent estimate of the true
probability.

Continuing with our example from (a), let us assume that we wish to
estimate $\pv({Rain}\given {Sprinkler}\eq {true})$, using 100
samples. Of the 100 that we generate, suppose that 73 have
${Sprinkler}\eq {false}$ and are rejected, while 27 have
${Sprinkler}\eq {true}$; of the 27, 8 have ${Rain}\eq {true}$
and 19 have ${Rain}\eq {false}$. Hence,
$$\pv({Rain}\given {Sprinkler}\eq {true}) \approx \noprog{Normalize}(\<8,{19}\>) =
\<{0.296},{0.704}\>\ .$$ The true answer is $\<{0.3},{0.7}\>$. As more
samples are collected, the estimate will converge to the true answer.
The standard deviation of the error in each probability will be
proportional to $1/\sqrt{n}$, where $n$ is the number of samples used in
the estimate.

[rejection-sampling-algorithm]

The biggest problem with rejection sampling is that it rejects so many
samples! The fraction of samples consistent with the evidence $\e$ drops
exponentially as the number of evidence variables grows, so the
procedure is simply unusable for complex problems.

Notice that rejection sampling is very similar to the estimation of
conditional probabilities directly from the real world. For example, to
estimate $\pv({Rain}\given {RedSkyAtNight}\eq {true})$, one can
simply count how often it rains after a red sky is observed the previous
evening—ignoring those evenings when the sky is not red. (Here, the
world itself plays the role of the sample-generation algorithm.)
Obviously, this could take a long time if the sky is very seldom red,
and that is the weakness of rejection sampling.

#### Likelihood weighting

avoids the inefficiency of rejection sampling by generating only events
that are consistent with the evidence $\e$. It is a particular instance
of the general statistical technique of , tailored for inference in
Bayesian networks. We begin by describing how the algorithm works; then
we show that it works correctly—that is, generates consistent
probability estimates.

(see ) fixes the values for the evidence variables $\E$ and samples only
the nonevidence variables. This guarantees that each event generated is
consistent with the evidence. Not all events are equal, however. Before
tallying the counts in the distribution for the query variable, each
event is weighted by the *likelihood* that the event
accords to the evidence, as measured by the product of the conditional
probabilities for each evidence variable, given its parents.
Intuitively, events in which the actual evidence appears unlikely should
be given less weight.

Let us apply the algorithm to the network shown in (a), with the query
$\pv({Rain}\given {Cloudy}\eq {true},{WetGrass}\eq {true})$
and the ordering , , , . (Any topological ordering will do.) The process
goes as follows: First, the weight $w$ is set to 1.0. Then an event is
generated:

1.  ${Cloudy}$ is an evidence variable with value ${true}$.
    Therefore, we set
    $$w \leftarrow w \stimes P({Cloudy}\eq {true}) = {0.5}\ .$$

2.  ${Sprinkler}$ is not an evidence variable, so sample from
    $\pv({Sprinkler}\given  {Cloudy}\eq {true}) = \<{0.1},{0.9}\>$;
    suppose this returns ${false}$.

3.  Similarly, sample from
    $\pv({Rain}\given {Cloudy}\eq {true}) = \<{0.8},{0.2}\>$;
    suppose this returns ${true}$.

4.  ${WetGrass}$ is an evidence variable with value ${true}$.
    Therefore, we set
    $$w \leftarrow w \stimes P({WetGrass}\eq {true}\given {Sprinkler}\eq {false},{Rain}\eq {true}) = {0.45}\ .$$

Here returns the event $[{true},{false},{true},{true}]$ with
weight 0.45, and this is tallied under ${Rain}\eq {true}$.

[likelihood-weighting-algorithm]

To understand why likelihood weighting works, we start by examining the
sampling probability $S_{{WS}}$ for . Remember that the evidence
variables $\E$ are fixed with values $\e$. We call the nonevidence
variables $\Z$ (including the query variable $X$). The algorithm samples
each variable in $\Z$ given its parent values:

$$S_{{WS}}(\z,\e) = \prod_{i\eq 1}^l P(z_i\given \parents(Z_i))\ .
\label{lw-sampling-equation}$$

Notice that $\Parents(Z_i)$ can include both nonevidence variables and
evidence variables. Unlike the prior distribution $P(\z)$, the
distribution $S_{{WS}}$ pays some attention to the evidence: the
sampled values for each $Z_i$ will be influenced by evidence among
$Z_i$’s ancestors. For example, when sampling ${Sprinkler}$ the
algorithm pays attention to the evidence ${Cloudy}\eq {true}$ in its
parent variable. On the other hand, $S_{{WS}}$ pays less attention to
the evidence than does the true posterior distribution $P(\z\given \e)$,
because the sampled values for each $Z_i$ *ignore* evidence
among $Z_i$’s non-ancestors.[^5] For example, when sampling
${Sprinkler}$ and ${Rain}$ the algorithm ignores the evidence in the
child variable ${WetGrass}\eq {true}$; this means it will generate
many samples with ${Sprinkler}\eq{false}$ and ${Rain}\eq{false}$
despite the fact that the evidence actually rules out this case.

The likelihood weight $w$ makes up for the difference between the actual
and desired sampling distributions. The weight for a given sample $\x$,
composed from $\z$ and $\e$, is the product of the likelihoods for each
evidence variable given its parents (some or all of which may be among
the $Z_i$s):

$$w(\z,\e) = \prod_{i\eq 1}^m P(e_i \given  \parents(E_i))\ .
\label{lw-weight-equation}$$

Multiplying Equations ([lw-sampling-equation])
and ([lw-weight-equation]), we see that the *weighted*
probability of a sample has the particularly convenient form

$$\begin{aligned}
S_{{WS}}(\z,\e) w(\z,\e)
   & = & \prod_{i\eq 1}^l P(z_i\given \parents(Z_i))\prod_{i\eq 1}^m P(e_i \given 
             \parents(E_i)) \nonumber \\
   & = & P(\mbf{z},\e)
\label{lw-weighted-sampling-equation}\end{aligned}$$

because the two products cover all the variables in the network,
allowing us to use for the joint probability.

Now it is easy to show that likelihood weighting estimates are
consistent. For any particular value $x$ of $X$, the estimated posterior
probability can be calculated as follows:

$$\begin{aligned}
\hat{P}(x\given \e) 
  &=& \alpha\, \sum_{\sy} N_{{WS}}(x,\mbf{y},\e) w(x,\mbf{y},\e)
 \qquad\mbox{ from \noprog{Likelihood-Weighting}}\\
  &\approx& \alpha' \sum_{\sy} S_{{WS}}(x,\mbf{y},\e) w(x,\mbf{y},\e)
 \qquad\mbox{ for large }N\\
  &=& \alpha' \sum_{\sy} P(x,\mbf{y},\e) 
                       \qquad\qquad\qquad\mbox{ by \eqref{lw-weighted-sampling-equation}}\\
  &=& \alpha' P(x,\e) = P(x\given \e)\ .\end{aligned}$$

Hence, likelihood weighting returns consistent estimates.

Because likelihood weighting uses all the samples generated, it can be
much more efficient than rejection sampling. It will, however, suffer a
degradation in performance as the number of evidence variables
increases. This is because most samples will have very low weights and
hence the weighted estimate will be dominated by the tiny fraction of
samples that accord more than an infinitesimal likelihood to the
evidence. The problem is exacerbated if the evidence variables occur
late in the variable ordering, because then the nonevidence variables
will have no evidence in their parents and ancestors to guide the
generation of samples. This means the samples will be simulations that
bear little resemblance to the reality suggested by the evidence.

### Inference by Markov chain simulation {#mcmc-section}

​(MCMC) algorithms work quite differently from rejection sampling and
likelihood weighting. Instead of generating each sample from scratch,
MCMC algorithms generate each sample by making a random change to the
preceding sample. It is therefore helpful to think of an MCMC algorithm
as being in a particular *current state* specifying a value
for every variable and generating a *next state* by making
random changes to the current state. (If this reminds you of simulated
annealing from or from , that is because both are members of the MCMC
family.) Here we describe a particular form of MCMC called , which is
especially well suited for Bayesian networks. (Other forms, some of them
significantly more powerful, are discussed in the notes at the end of
the chapter.) We will first describe what the algorithm does, then we
will explain why it works.

#### Gibbs sampling in Bayesian networks

The Gibbs sampling algorithm for Bayesian networks starts with an
arbitrary state (with the evidence variables fixed at their observed
values) and generates a next state by randomly sampling a value for one
of the nonevidence variables $X_i$. The sampling for $X_i$ is done
*conditioned on the current values of the variables in the Markov
blanket of* $X_i$. (Recall from that the Markov blanket of a
variable consists of its parents, children, and children’s parents.) The
algorithm therefore wanders randomly around the state space—the space of
possible complete assignments—flipping one variable at a time, but
keeping the evidence variables fixed.

Consider the query
$\pv({Rain}\given {Sprinkler}\eq {true},{WetGrass}\eq {true})$
applied to the network in (a). The evidence variables ${Sprinkler}$
and ${WetGrass}$ are fixed to their observed values and the
nonevidence variables ${Cloudy}$ and ${Rain}$ are initialized
randomly—let us say to ${true}$ and ${false}$ respectively. Thus,
the initial state is $[{true},{true},{false},{true}]$. Now the
nonevidence variables are sampled repeatedly in an arbitrary order. For
example:

1.  ${Cloudy}$ is sampled, given the current values of its Markov
    blanket variables: in this case, we sample from
    $\pv({Cloudy}\given {Sprinkler}\eq {true},{Rain}\eq {false})$.
    (Shortly, we will show how to calculate this distribution.) Suppose
    the result is ${Cloudy}\eq {false}$. Then the new current state
    is $[{false},{true},{false},{true}]$.

2.  ${Rain}$ is sampled, given the current values of its Markov
    blanket variables: in this case, we sample from
    $\pv({Rain}\given {Cloudy}\eq {false}, {Sprinkler}\eq {true}, {WetGrass}\eq {true})$.
    Suppose this yields ${Rain}\eq {true}$. The new current state is
    $[{false},{true},{true},{true}]$.

Each state visited during this process is a sample that contributes to
the estimate for the query variable ${Rain}$. If the process visits 20
states where ${Rain}$ is true and 60 states where ${Rain}$ is false,
then the answer to the query is
$\noprog{Normalize}(\<{20},{60}\>) = \<{0.25},{0.75}\>$. The complete
algorithm is shown in .

[gibbs-algorithm]

#### Why Gibbs sampling works

We will now show that Gibbs sampling returns consistent estimates for
posterior probabilities. The material in this section is quite
technical, but the basic claim is straightforward:

the sampling process settles into a “dynamic equilibrium” in which the
long-run fraction of time spent in each state is exactly proportional to
its posterior probability.

This remarkable property follows from the specific with which the
process moves from one state to another, as defined by the conditional
distribution given the Markov blanket of the variable being sampled.

Let $\transition{\x}{\x'}$ be the probability that the process makes a
transition from state $\x$ to state $\x'$. This transition probability
defines what is called a on the state space. (Markov chains also figure
prominently in Chapters [dbn-chapter] and [complex-decisions-chapter].)
Now suppose that we run the Markov chain for $t$ steps, and let
$\pi_t(\x)$ be the probability that the system is in state $\x$ at time
$t$. Similarly, let $\pi_{t+1}(\x')$ be the probability of being in
state $\x'$ at time $t+1$. Given $\pi_t(\x)$, we can calculate
$\pi_{t+1}(\x')$ by summing, for all states the system could be in at
time $t$, the probability of being in that state times the probability
of making the transition to $\x'$:
$$\pi_{t+1}(\x') = \sum_{\sx} \pi_t(\x) \transition{\x}{\x'}\ .$$ We say
that the chain has reached its [mcmc-stationary-page] if
$\pi_t\eq \pi_{t+1}$. Let us call this stationary distribution $\pi$;
its defining equation is therefore

$$\pi(\x') = \sum_{\sx} \pi(\x) \transition{\x}{\x'}
      \qquad\mbox{for all }\x'\ . \label{stationarity-equation}$$

Provided the transition probability distribution $q$ is —that is, every
state is reachable from every other and there are no strictly periodic
cycles—there is exactly one distribution $\pi$ satisfying this equation
for any given $q$.

can be read as saying that the expected “outflow” from each state (i.e.,
its current “population”) is equal to the expected “inflow” from all the
states. One obvious way to satisfy this relationship is if the expected
flow between any pair of states is the same in both directions; that is,

$$\pi(\x) \transition{\x}{\x'} 
   = \pi(\x') \transition{\x'}{\x}
      \qquad\mbox{for all }\x,\ \x'\ .
\label{detailed-balance-equation}$$

When these equations hold, we say that $\transition{\x}{\x'}$ is in with
$\pi(\x)$.

We can show that detailed balance implies stationarity simply by summing
over $\x$ in . We have $$\sum_{\sx} \pi(\x) \transition{\x}{\x'} = 
\sum_{\sx} \pi(\x') \transition{\x'}{\x} =
\pi(\x') \sum_{\sx} \transition{\x'}{\x} =
\pi(\x')$$ where the last step follows because a transition from $\x'$
is guaranteed to occur.

The transition probability $\transition{\x}{\x'}$ defined by the
sampling step in is actually a special case of the more general
definition of Gibbs sampling, according to which each variable is
sampled conditionally on the current values of *all* the
other variables. We start by showing that this general definition of
Gibbs sampling satisfies the detailed balance equation with a stationary
distribution equal to $P(\x\given \e)$, (the true posterior distribution
on the nonevidence variables). Then, we simply observe that, for
Bayesian networks, sampling conditionally on all variables is equivalent
to sampling conditionally on the variable’s Markov blanket (see ).

To analyze the general Gibbs sampler, which samples each $X_i$ in turn
with a transition probability $q_i$ that conditions on all the other
variables, we define $\otherthan{\X_i}$ to be these other variables
(except the evidence variables); their values in the current state are
$\otherthan{\x_i}$. If we sample a new value $x_i'$ for $X_i$
conditionally on all the other variables, including the evidence, we
have $$\transitioni{\x}{\x'} =
  \transitioni{(x_i,\otherthan{\x_i})}{(x_i',\otherthan{\x_i})} =
    P(x_i'\given \otherthan{\x_i},\e)\ .$$ Now we show that the
transition probability for each step of the Gibbs sampler is in detailed
balance with the true posterior:

$$\begin{aligned}
\lefteqn{\pi(\x) \transitioni{\x}{\x'} 
   = P(\x\given \e) P(x_i'\given \otherthan{\x_i},\e) 
     =  P(x_i,\otherthan{\x_i}\given \e)P(x_i'\given \otherthan{\x_i},\e)}  \\
   &=& P(x_i\given \otherthan{\x_i},\e)P(\otherthan{\x_i}\given \e)
       P(x_i'\given \otherthan{\x_i},\e) \qquad\mbox{(using the chain
                            rule on the first term)} \\
   &=& P(x_i\given \otherthan{\x_i},\e)P(x_i',\otherthan{\x_i}\given \e)
      \qquad\quad\qquad\ \,\mbox{(using the chain rule backward)} \\
   &=& \pi(\x') \transitioni{\x'}{\x} \ .\end{aligned}$$

We can think of the loop “ $Z_i$ in ” in as defining one large
transition probability $q$ that is the sequential composition
$q_1 \circ q_2 \circ \cdots
\circ q_n$ of the transition probabilities for the individual variables.
It is easy to show () that if each of $q_i$ and $q_j$ has $\pi$ as its
stationary distribution, then the sequential composition $q_i\circ q_j$
does too; hence the transition probability $q$ for the whole loop has
$P(\x\given \e)$ as its stationary distribution. Finally, unless the
CPTs contain probabilities of 0 or 1—which can cause the state space to
become disconnected—it is easy to see that $q$ is ergodic. Hence, the
samples generated by Gibbs sampling will eventually be drawn from the
true posterior distribution.

The final step is to show how to perform the general Gibbs sampling
step—sampling $X_i$ from $\pv(X_i\given \otherthan{\x_i},\e)$—in a
Bayesian network. Recall from that a variable is independent of all
other variables given its Markov blanket; hence,
$$P(x_i'\given \otherthan{\x_i},\e) = P(x_i'\given \markovBlanket(X_i)) \ ,$$
where $\markovBlanket(X_i)$ denotes the values of the variables in
$X_i$’s Markov blanket, $\MarkovBlanket(X_i)$. As shown in , the
probability of a variable given its Markov blanket is proportional to
the probability of the variable given its parents times the probability
of each child given its respective parents:

$$P(x_i'\given \markovBlanket(X_i)) = \alpha\, P(x_i'\given \parents(X_i)) \times \!\!\!\!\! \prod_{Y_j\in
\Children(X_i)} \!\!\!\!\! P(y_j\given \parents(Y_j))\ .
\label{markov-blanket-equation}$$

Hence, to flip each variable $X_i$ conditioned on its Markov blanket,
the number of multiplications required is equal to the number of $X_i$’s
children.

Relational and First-Order Probability Models {#fopl-section}
---------------------------------------------

In , we explained the representational advantages possessed by
first-order logic in comparison to propositional logic. First-order
logic commits to the existence of objects and relations among them and
can express facts about *some* or *all* of the
objects in a domain. This often results in representations that are
vastly more concise than the equivalent propositional descriptions. Now,
Bayesian networks are essentially propositional: the set of random
variables is fixed and finite, and each has a fixed domain of possible
values. This fact limits the applicability of Bayesian networks.

If we can find a way to combine probability theory with the expressive
power of first-order representations, we expect to be able to increase
dramatically the range of problems that can be handled.

[recommendation-bn-figure]

For example, suppose that an online book retailer would like to provide
overall evaluations of products based on recommendations received from
its customers. The evaluation will take the form of a posterior
distribution over the quality of the book, given the available evidence.
The simplest solution to base the evaluation on the average
recommendation, perhaps with a variance determined by the number of
recommendations, but this fails to take into account the fact that some
customers are kinder than others and some are less honest than others.
Kind customers tend to give high recommendations even to fairly mediocre
books, while dishonest customers give very high or very low
recommendations for reasons other than quality—for example, they might
work for a publisher.[^6]

For a single customer $C_1$, recommending a single book $B_1$, the Bayes
net might look like the one shown in (a). (Just as in , expressions with
parentheses such as ${Honest}(C_1)$ are just fancy symbols—in this
case, fancy names for random variables.) With two customers and two
books, the Bayes net looks like the one in (b). For larger numbers of
books and customers, it becomes completely impractical to specify the
network by hand.

Fortunately, the network has a lot of repeated structure. Each
${Recommendation}(c,b)$ variable has as its parents the variables
${Honest}(c)$, ${Kindness}(c)$, and ${Quality}(b)$. Moreover, the
CPTs for all the ${Recommendation}(c,b)$ variables are identical, as
are those for all the ${Honest}(c)$ variables, and so on. The
situation seems tailor-made for a first-order language. We would like to
say something like
$${Recommendation}(c,b) \sim {RecCPT}({Honest}(c),{Kindness}(c),{Quality}(b))$$
with the intended meaning that a customer’s recommendation for a book
depends on the customer’s honesty and kindness and the book’s quality
according to some fixed CPT. This section develops a language that lets
us say exactly this, and a lot more besides.

### Possible worlds

Recall from that a probability model defines a set $\Omega$ of possible
worlds with a probability $P(\omega)$ for each world $\omega$. For
Bayesian networks, the possible worlds are assignments of values to
variables; for the Boolean case in particular, the possible worlds are
identical to those of propositional logic. For a first-order probability
model, then, it seems we need the possible worlds to be those of
first-order logic—that is, a set of objects with relations among them
and an interpretation that maps constant symbols to objects, predicate
symbols to relations, and function symbols to functions on those
objects. (See .) The model also needs to define a probability for each
such possible world, just as a Bayesian network defines a probability
for each assignment of values to variables.

Let us suppose, for a moment, that we have figured out how to do this.
Then, as usual (see ), we can obtain the probability of any first-order
logical sentence $\phi$ as a sum over the possible worlds where it is
true:

$$P(\phi) = \sum_{\omega:\phi\ \mbox{{\scriptsize is true in }} \omega} P(\omega)\ .
\label{fopl-summation-equation}$$

Conditional probabilities $P(\phi \given \e)$ can be obtained similarly,
so we can, in principle, ask any question we want of our model—e.g.,
“Which books are most likely to be recommended highly by dishonest
customers?”—and get an answer. So far, so good.

[all-models-both-figure]

There is, however, a problem: the set of first-order models is infinite.
We saw this explicitly in on , which we show again in (top). This means
that (1) the summation in could be infeasible, and (2) specifying a
complete, consistent distribution over an infinite set of worlds could
be very difficult.

explores one approach to dealing with this problem. The idea is to
borrow not from the standard semantics of first-order logic but from the
defined in (). The database semantics makes the —here, we adopt it for
the constant symbols. It also assumes —there are no more objects than
those that are named. We can then guarantee a finite set of possible
worlds by making the set of objects in each world be exactly the set of
constant symbols that are used; as shown in (bottom), there is no
uncertainty about the mapping from symbols to objects or about the
objects that exist. We will call models defined in this way , or
RPMs.[^7] The most significant difference between the semantics of RPMs
and the database semantics introduced in is that RPMs do not make the
—obviously, assuming that every unknown fact is false doesn’t make sense
in a probabilistic reasoning system!

When the underlying assumptions of database semantics fail to hold, RPMs
won’t work well. For example, a book retailer might use an ISBN
(International Standard Book Number) as a constant symbol to name each
book, even though a given “logical” book (e.g., “Gone With the Wind”)
may have several ISBNs. It would make sense to aggregate recommendations
across multiple ISBNs, but the retailer may not know for sure which
ISBNs are really the same book. (Note that we are not reifying the
*individual copies* of the book, which might be necessary
for used-book sales, car sales, and so on.) Worse still, each customer
is identified by a login ID, but a dishonest customer may have thousands
of IDs! In the computer security field, these multiple IDs are called
and their use to confound a reputation system is called a . Thus, even a
simple application in a relatively well-defined, online domain involves
both (what are the real books and customers underlying the observed
data) and (which symbol really refer to the same object). We need to
bite the bullet and define probability models based on the standard
semantics of first-order logic, for which the possible worlds vary in
the objects they contain and in the mappings from symbols to objects.
shows how to do this.

### Relational probability models {#rpm-section}

Like first-order logic, RPMs have constant, function, and predicate
symbols. (It turns out to be easier to view predicates as functions that
return ${true}$ or ${false}$.) We will also assume a for each
function, that is, a specification of the type of each argument and the
function’s value. If the type of each object is known, many spurious
possible worlds are eliminated by this mechanism. For the
book-recommendation domain, the types are ${Customer}$ and ${Book}$,
and the type signatures for the functions and predicates are as follows:

: {,} : {1,2,3,4,5}\
: {1,2,3,4,5}\
: {1,2,3,4,5}\

The constant symbols will be whatever customer and book names appear in
the retailer’s data set. In the example given earlier ((b)), these were
$C_1,\ C_2$ and $B_1,\ B_2$.

Given the constants and their types, together with the functions and
their type signatures, the random variables of the RPM are obtained by
instantiating each function with each possible combination of objects:
${Honest}(C_1)$, ${Quality}(B_2)$, ${Recommendation}(C_1,B_2)$,
and so on. These are exactly the variables appearing in (b). Because
each type has only finitely many instances, the number of basic random
variables is also finite.

To complete the RPM, we have to write the dependencies that govern these
random variables. There is one dependency statement for each function,
where each argument of the function is a logical variable (i.e., a
variable that ranges over objects, as in first-order logic):

​(c) \~0.99,0.01\
(c) \~0.1,0.1,0.2,0.3,0.3\
(b) \~0.05,0.2,0.4,0.2,0.15\
(c,b) \~((c),(c),(b))\

where ${RecCPT}$ is a separately defined conditional distribution with
$2\stimes 5 \stimes 5\eq 50$ rows, each with 5 entries. The semantics of
the RPM can be obtained by instantiating these dependencies for all
known constants, giving a Bayesian network (as in (b)) that defines a
joint distribution over the RPM’s random variables.[^8]

We can refine the model by introducing a [CSI-page] to reflect the fact
that dishonest customers ignore quality when giving a recommendation;
moreover, kindness plays no role in their decisions. A context-specific
independence allows a variable to be independent of some of its parents
given certain values of others; thus, ${Recommendation}(c,b)$ is
independent of ${Kindness}(c)$ and ${Quality}(b)$ when
${Honest}(c)\eq {false}$:

$$\begin{aligned}
  {Recommendation}(c,b) \sim&& \k{if } {Honest}(c)\ \k{ then } \\
                              &&\;\;\;\;\;\;{HonestRecCPT}({Kindness}(c),{Quality}(b))\\
                              && \k{else } \langle 0.4, 0.1, 0.0, 0.1, 0.4 \rangle\ .\end{aligned}$$

This kind of dependency may look like an ordinary if–then–else statement
on a programming language, but there is a key difference: the inference
engine *doesn’t necessarily know the value of the conditional
test*!

We can elaborate this model in endless ways to make it more realistic.
For example, suppose that an honest customer who is a fan of a book’s
author always gives the book a 5, regardless of quality:

$$\begin{aligned}
  {Recommendation}(c,b) \sim&& \k{if } {Honest}(c) \k{ then} \\
                             &&  \;\;\;\;\;\;\k{if } {Fan}(c,{Author}(b)) \k{ then } {Exactly}(5) \\
                             &&  \;\;\;\;\;\;\k{else }{HonestRecCPT}({Kindness}(c),{Quality}(b))\\
                              && \k{else } \langle 0.4, 0.1, 0.0, 0.1, 0.4 \rangle\end{aligned}$$

Again, the conditional test ${Fan}(c,{Author}(b))$ is unknown, but
if a customer gives only 5s to a particular author’s books and is not
otherwise especially kind, then the posterior probability that the
customer is a fan of that author will be high. Furthermore, the
posterior distribution will tend to discount the customer’s 5s in
evaluating the quality of that author’s books.

[author-multiplexer-figure]

In the preceding example, we implicitly assumed that the value of
${Author}(b)$ is known for every $b$, but this may not be the case.
How can the system reason about whether, say, $C_1$ is a fan of
${Author}(B_2)$ when ${Author}(B_2)$ is unknown? The answer is that
the system may have to reason about *all possible authors*.
Suppose (to keep things simple) that there are just two authors, $A_1$
and $A_2$. Then ${Author}(B_2)$ is a random variable with two possible
values, $A_1$ and $A_2$, and it is a parent of
${Recommendation}(C_1,B_2)$. The variables ${Fan}(C_1,A_1)$ and
${Fan}(C_1,A_2)$ are parents too. The conditional distribution for
${Recommendation}(C_1,B_2)$ is then essentially a in which the
${Author}(B_2)$ parent acts as a selector to choose which of
${Fan}(C_1,A_1)$ and ${Fan}(C_1,A_2)$ actually gets to influence the
recommendation. A fragment of the equivalent Bayes net is shown in .
Uncertainty in the value of ${Author}(B_2)$, which affects the
dependency structure of the network, is an instance of .

In case you are wondering how the system can possibly work out who the
author of $B_2$ is: consider the possibility that three other customers
are fans of $A_1$ (and have no other favorite authors in common) and all
three have given $B_2$ a 5, even though most other customers find it
quite dismal. In that case, it is extremely likely that $A_1$ is the
author of $B_2$. The emergence of sophisticated reasoning like this from
an RPM model of just a few lines is an intriguing example of how
probabilistic influences spread through the web of interconnections
among objects in the model. As more dependencies and more objects are
added, the picture conveyed by the posterior distribution often becomes
clearer and clearer.

The next question is how to do inference in RPMs. One approach is to
collect the evidence and query and the constant symbols therein,
construct the equivalent Bayes net, and apply any of the inference
methods discussed in this chapter. This technique is called . The
obvious drawback is that the resulting Bayes net may be very large.
Furthermore, if there are many candidate objects for an unknown relation
or function—for example, the unknown author of $B_2$—then some variables
in the network may have many parents.

Fortunately, much can be done to improve on generic inference
algorithms. First, the presence of repeated substructure in the unrolled
Bayes net means that many of the factors constructed during variable
elimination (and similar kinds of tables constructed by clustering
algorithms) will be identical; effective caching schemes have yielded
speedups of three orders of magnitude for large networks. Second,
inference methods developed to take advantage of context-specific
independence in Bayes nets find many applications in RPMs. Third, MCMC
inference algorithms have some interesting properties when applied to
RPMs with relational uncertainty. MCMC works by sampling complete
possible worlds, so in each state the relational structure is completely
known. In the example given earlier, each MCMC state would specify the
value of ${Author}(B_2)$, and so the other potential authors are no
longer parents of the recommendation nodes for $B_2$. For MCMC, then,
relational uncertainty causes no increase in network complexity;
instead, the MCMC process includes transitions that change the
relational structure, and hence the dependency structure, of the
unrolled network.

All of the methods just described assume that the RPM has to be
partially or completely unrolled into a Bayesian network. This is
exactly analogous to the method of for first-order logical inference.
(See .) Resolution theorem-provers and logic programming systems avoid
propositionalizing by instantiating the logical variables only as needed
to make the inference go through; that is, they *lift* the
inference process above the level of ground propositional sentences and
make each lifted step do the work of many ground steps. The same idea
applied in probabilistic inference. For example, in the variable
elimination algorithm, a lifted factor can represent an entire set of
ground factors that assign probabilities to random variables in the RPM,
where those random variables differ only in the constant symbols used to
construct them. The details of this method are beyond the scope of this
book, but references are given at the end of the chapter.

### Open-universe probability models {#blog-section}

We argued earlier that database semantics was appropriate for situations
in which we know exactly the set of relevant objects that exist and can
identify them unambiguously. (In particular, all observations about an
object are correctly associated with the constant symbol that names it.)
In many real-world settings, however, these assumptions are simply
untenable. We gave the examples of multiple ISBNs and sibyl attacks in
the book-recommendation domain (to which we will return in a moment),
but the phenomenon is far more pervasive:

-   A vision system doesn’t know what exists, if anything, around the
    next corner, and may not know if the object it sees now is the same
    one it saw a few minutes ago.

-   A text-understanding system does not know in advance the entities
    that will be featured in a text, and must reason about whether
    phrases such as “Mary,” “Dr. Smith,” “she,” “his cardiologist,” “his
    mother,” and so on refer to the same object.

-   An intelligence analyst hunting for spies never knows how many spies
    there really are and can only guess whether various pseudonyms,
    phone numbers, and sightings belong to the same individual.

In fact, a major part of human cognition seems to require learning what
objects exist and being able to connect observations—which almost never
come with unique IDs attached—to hypothesized objects in the world.

For these reasons, we need to be able to write so-called probability
models or OUPMs based on the standard semantics of first-order logic, as
illustrated at the top of . A language for OUPMs provides a way of
writing such models easily while guaranteeing a unique, consistent
probability distribution over the infinite space of possible worlds.

The basic idea is to understand how ordinary Bayesian networks and RPMs
manage to define a unique probability model and to transfer that insight
to the first-order setting. In essence, a Bayes net
*generates* each possible world, event by event, in the
topological order defined by the network structure, where each event is
an assignment of a value to a variable. An RPM extends this to entire
sets of events, defined by the possible instantiations of the logical
variables in a given predicate or function. OUPMs go further by allowing
generative steps that *add objects* to the possible world
under construction, where the number and type of objects may depend on
the objects that are already in that world. That is, the event being
generated is not the assignment of a value to a variable, but the very
*existence* of objects.

One way to do this in OUPMs is to add statements that define conditional
distributions over the numbers of objects of various kinds. For example,
in the book-recommendation domain, we might want to distinguish between
*customers* (real people) and their *login
IDs*. Suppose we expect somewhere between 100 and 10,000 distinct
customers (whom we cannot observe directly). We can express this as a
prior log-normal distribution[^9] as follows:
$$\#\,{Customer} \sim {LogNormal}[6.9, 2.3^2]()\ .$$ We expect
honest customers to have just one ID, whereas dishonest customers might
have anywhere between 10 and 1000 IDs:

$$\begin{aligned}
  \#\,{LoginID}({Owner}\eq c) \sim && \k{if } {Honest}(c) \k{ then } {Exactly}(1) \\
                                       && \k{else } {LogNormal}[6.9, 2.3^2]()\ .\end{aligned}$$

This statement defines the number of login IDs for a given owner, who is
a customer. The ${Owner}$ function is called an because it says where
each generated object came from. In the formal semantics of (as distinct
from first-order logic), the domain elements in each possible world are
actually generation histories (e.g., “the fourth login ID of the seventh
customer”) rather than simple tokens.

Subject to technical conditions of acyclicity and well-foundedness
similar to those for RPMs, open-universe models of this kind define a
unique distribution over possible worlds. Furthermore, there exist
inference algorithms such that, for every such well-defined model and
every first-order query, the answer returned approaches the true
posterior arbitrarily closely in the limit. There are some tricky issues
involved in designing these algorithms. For example, an MCMC algorithm
cannot sample directly in the space of possible worlds when the size of
those worlds is unbounded; instead, it samples finite, partial worlds,
relying on the fact that only finitely many objects can be relevant to
the query in distinct ways. Moreover, transitions must allow for merging
two objects into one or splitting one into two. (Details are given in
the references at the end of the chapter.) Despite these complications,
the basic principle established in still holds: the probability of any
sentence is well defined and can be calculated.

Research in this area is still at an early stage, but already it is
becoming clear that first-order probabilistic reasoning yields a
tremendous increase in the effectiveness of AI systems at handling
uncertain information. Potential applications include those mentioned
above—computer vision, text understanding, and intelligence analysis—as
well as many other kinds of sensor interpretation.

Other Approaches to Uncertain Reasoning {#alternative-uncertainty-section}
---------------------------------------

Other sciences (e.g., physics, genetics, and economics) have long
favored probability as a model for uncertainty. In 1819, Pierre Laplace
said, “Probability theory is nothing but common sense reduced to
calculation.” In 1850, James Maxwell said, “The true logic for this
world is the calculus of Probabilities, which takes account of the
magnitude of the probability which is, or ought to be, in a reasonable
man’s mind.”

Given this long tradition, it is perhaps surprising that AI has
considered many alternatives to probability. The earliest expert systems
of the 1970s ignored uncertainty and used strict logical reasoning, but
it soon became clear that this was impractical for most real-world
domains. The next generation of expert systems (especially in medical
domains) used probabilistic techniques. Initial results were promising,
but they did not scale up because of the exponential number of
probabilities required in the full joint distribution. (Efficient
Bayesian network algorithms were unknown then.) As a result,
probabilistic approaches fell out of favor from roughly 1975 to 1988,
and a variety of alternatives to probability were tried for a variety of
reasons:

-   One common view is that probability theory is essentially numerical,
    whereas human judgmental reasoning is more “qualitative.” Certainly,
    we are not consciously aware of doing numerical calculations of
    degrees of belief. (Neither are we aware of doing unification, yet
    we seem to be capable of some kind of logical reasoning.) It might
    be that we have some kind of numerical degrees of belief encoded
    directly in strengths of connections and activations in our neurons.
    In that case, the difficulty of conscious access to those strengths
    is not surprising. One should also note that qualitative reasoning
    mechanisms can be built directly on top of probability theory, so
    the “no numbers” argument against probability has little force.
    Nonetheless, some qualitative schemes have a good deal of appeal in
    their own right. One of the best studied is , which treats
    conclusions not as “believed to a certain degree,” but as “believed
    until a better reason is found to believe something else.” Default
    reasoning is covered in .

-   approaches to uncertainty have also been tried. Such approaches hope
    to build on the success of logical rule-based systems, but add a
    sort of “fudge factor” to each rule to accommodate uncertainty.
    These methods were developed in the mid-1970s and formed the basis
    for a large number of expert systems in medicine and other areas.

-   One area that we have not addressed so far is the question of , as
    opposed to uncertainty. Consider the flipping of a coin. If we know
    that the coin is fair, then a probability of 0.5 for heads is
    reasonable. If we know that the coin is biased, but we do not know
    which way, then 0.5 for heads is again reasonable. Obviously, the
    two cases are different, yet the outcome probability seems not to
    distinguish them. The uses degrees of belief to represent an agent’s
    knowledge of the probability of a proposition.

-   Probability makes the same ontological commitment as logic: that
    propositions are true or false in the world, even if the agent is
    uncertain as to which is the case. Researchers in have proposed an
    ontology that allows : that a proposition can be “sort of” true.
    Vagueness and uncertainty are in fact orthogonal issues.

The next three subsections treat some of these approaches in slightly
more depth. We will not provide detailed technical material, but we cite
references for further study.

### Rule-based methods for uncertain reasoning

Rule-based systems emerged from early work on practical and intuitive
systems for logical inference. Logical systems in general, and logical
rule-based systems in particular, have three desirable properties:

In logical systems, whenever we have a rule of the form $A \implies B$,
we can conclude $B$, given evidence $A$, *without worrying about
any other rules.* In probabilistic systems, we need to consider
*all* the evidence.

Once a logical proof is found for a proposition $B$, the proposition can
be used regardless of how it was derived. That is, it can be from its
justification. In dealing with probabilities, on the other hand, the
source of the evidence for a belief is important for subsequent
reasoning.

In logic, the truth of complex sentences can be computed from the truth
of the components. Probability combination does not work this way,
except under strong global independence assumptions.

There have been several attempts to devise uncertain reasoning schemes
that retain these advantages. The idea is to attach degrees of belief to
propositions and rules and to devise purely local schemes for combining
and propagating those degrees of belief. The schemes are also
truth-functional; for example, the degree of belief in $A \lor B$ is a
function of the belief in $A$ and the belief in $B$.

The bad news for rule-based systems is that the properties of

locality, detachment, and truth-functionality are simply not appropriate
for uncertain reasoning.

Let us look at truth-functionality first. Let $H_1$ be the event that a
fair coin flip comes up heads, let $T_1$ be the event that the coin
comes up tails on that same flip, and let $H_2$ be the event that the
coin comes up heads on a second flip. Clearly, all three events have the
same probability, 0.5, and so a truth-functional system must assign the
same belief to the disjunction of any two of them. But we can see that
the probability of the disjunction depends on the events themselves and
not just on their probabilities:

|c|c||c| $P(A)$ & $P(B)$ & $P(A \lor B)$\
& $P(H_1) = {0.5}$ & $P(H_1\lor H_1) = {0.50}$\
$P(H_1) = {0.5}$ & $P(T_1) = {0.5}$ & $P(H_1\lor T_1) = {1.00}$\
& $P(H_2) = {0.5}$ & $P(H_1\lor H_2) = {0.75}$\

It gets worse when we chain evidence together. Truth-functional systems
have of the form $A
\mapsto B$ that allow us to compute the belief in $B$ as a function of
the belief in the rule and the belief in $A$. Both forward- and
backward-chaining systems can be devised. The belief in the rule is
assumed to be constant and is usually specified by the knowledge
engineer—for example, as $A \mapsto_{{0.9}} B$.

Consider the wet-grass situation from (a) (). If we wanted to be able to
do both causal and diagnostic reasoning, we would need the two rules

 .

These two rules form a feedback loop: evidence for ${Rain}$ increases
the belief in ${WetGrass}$, which in turn increases the belief in
${Rain}$ even more. Clearly, uncertain reasoning systems need to keep
track of the paths along which evidence is propagated.

Intercausal reasoning (or explaining away) is also tricky. Consider what
happens when we have the two rules

 .

Suppose we see that the sprinkler is on. Chaining forward through our
rules, this increases the belief that the grass will be wet, which in
turn increases the belief that it is raining. But this is ridiculous:
the fact that the sprinkler is on explains away the wet grass and should
*reduce* the belief in rain. A truth-functional system acts
as if it also believes ${Sprinkler} \mapsto {Rain}$.

Given these difficulties, how can truth-functional systems be made
useful in practice? The answer lies in restricting the task and in
carefully engineering the rule base so that undesirable interactions do
not occur. The most famous example of a truth-functional system for
uncertain reasoning is the model, which was developed for the medical
diagnosis program and was widely used in expert systems of the late
1970s and 1980s. Almost all uses of certainty factors involved rule sets
that were either purely diagnostic (as in ) or purely causal.
Furthermore, evidence was entered only at the “roots” of the rule set,
and most rule sets were singly connected. Heckerman [-@Heckerman:1986]
has shown that, under these circumstances, a minor variation on
certainty-factor inference was exactly equivalent to Bayesian inference
on polytrees. In other circumstances, certainty factors could yield
disastrously incorrect degrees of belief through overcounting of
evidence. As rule sets became larger, undesirable interactions between
rules became more common, and practitioners found that the certainty
factors of many other rules had to be “tweaked” when new rules were
added. For these reasons, Bayesian networks have largely supplanted
rule-based methods for uncertain reasoning.

### Representing ignorance: Dempster–Shafer theory

The is designed to deal with the distinction between and . Rather than
computing the probability of a proposition, it computes the probability
that the evidence supports the proposition. This measure of belief is
called a , written ${Bel}(X)$.

We return to coin flipping for an example of belief functions. Suppose
you pick a coin from a magician’s pocket. Given that the coin might or
might not be fair, what belief should you ascribe to the event that it
comes up heads? Dempster–Shafer theory says that because you have no
evidence either way, you have to say that the belief
${Bel}({Heads}) = 0$ and also that ${Bel}(\lnot {Heads}) = 0$.
This makes Dempster–Shafer reasoning systems skeptical in a way that has
some intuitive appeal. Now suppose you have an expert at your disposal
who testifies with 90% certainty that the coin is fair (i.e., he is 90%
sure that $P({Heads}) = {0.5}$). Then Dempster–Shafer theory gives
${Bel}({Heads}) = {0.9}
\times {0.5} = {0.45}$ and likewise ${Bel}(\lnot {Heads}) = {0.45}$.
There is still a 10 percentage point “gap” that is not accounted for by
the evidence.

The mathematical underpinnings of Dempster–Shafer theory have a similar
flavor to those of probability theory; the main difference is that,
instead of assigning probabilities to possible worlds, the theory
assigns to *sets* of possible world, that is, to events.
The masses still must add to 1 over all possible events. ${Bel}(A)$ is
defined to be the sum of masses for all events that are subsets of
(i.e., that entail) $A$, including $A$ itself. With this definition,
${Bel}(A)$ and ${Bel}(\lnot A)$ sum to *at most* 1, and
the gap—the interval between ${Bel}(A)$ and $1- {Bel}(\lnot A)$—is
often interpreted as bounding the probability of $A$.

As with default reasoning, there is a problem in connecting beliefs to
actions. Whenever there is a gap in the beliefs, then a decision problem
can be defined such that a Dempster–Shafer system is unable to make a
decision. In fact, the notion of utility in the Dempster–Shafer model is
not yet well understood because the meanings of masses and beliefs
themselves have yet to be understood. has argued that ${Bel}(A)$
should be interpreted not as a degree of belief in $A$ but as the
probability assigned to all the possible worlds (now interpreted as
logical theories) in which $A$ is *provable*. While there
are cases in which this quantity might be of interest, it is not the
same as the probability that $A$ is true.

A Bayesian analysis of the coin-flipping example would suggest that no
new formalism is necessary to handle such cases. The model would have
two variables: the ${Bias}$ of the coin (a number between 0 and 1,
where 0 is a coin that always shows tails and 1 a coin that always shows
heads) and the outcome of the next ${Flip}$. The prior probability
distribution for ${Bias}$ would reflect our beliefs based on the
source of the coin (the magician’s pocket): some small probability that
it is fair and some probability that it is heavily biased toward heads
or tails. The conditional distribution $\pv({Flip}\given {Bias})$
simply defines how the bias operates. If $\pv(Bias)$ is symmetric about
0.5, then our prior probability for the flip is
$$P({Flip}\eq {heads}) = \int_0^1 P({Bias}\eq x) P({Flip}\eq {heads}\given {Bias}\eq x) \,dx = 0.5\ .$$
This is the same prediction as if we believe strongly that the coin is
fair, but that does *not* mean that probability theory
treats the two situations identically. The difference arises
*after* the flips in computing the posterior distribution
for ${Bias}$. If the coin came from a bank, then seeing it come up
heads three times running would have almost no effect on our strong
prior belief in its fairness; but if the coin comes from the magician’s
pocket, the same evidence will lead to a stronger posterior belief that
the coin is biased toward heads. Thus, a Bayesian approach expresses our
“ignorance” in terms of how our beliefs would change in the face of
future information gathering.

### Representing vagueness: Fuzzy sets and fuzzy logic

is a means of specifying how well an object satisfies a vague
description. For example, consider the proposition “Nate is tall.” Is
this true if Nate is $5'\;{10}''$? Most people would hesitate to answer
“true” or “false,” preferring to say, “sort of.” Note that this is not a
question of uncertainty about the external world—we are sure of Nate’s
height. The issue is that the linguistic term “tall” does not refer to a
sharp demarcation of objects into two classes—there are
*degrees* of tallness. For this reason,

fuzzy set theory is not a method for uncertain reasoning at all.

Rather, fuzzy set theory treats ${Tall}$ as a fuzzy predicate and says
that the truth value of ${Tall}({Nate})$ is a number between 0 and
1, rather than being just ${true}$ or ${false}$. The name “fuzzy
set” derives from the interpretation of the predicate as implicitly
defining a set of its members—a set that does not have sharp boundaries.

is a method for reasoning with logical expressions describing membership
in fuzzy sets. For example, the complex sentence
${Tall}({Nate}) \land
{Heavy}({Nate})$ has a fuzzy truth value that is a function of the
truth values of its components. The standard rules for evaluating the
fuzzy truth, $T$, of a complex sentence are

T(A B) = (T(A),T(B))\
T(A B) = (T(A),T(B))\
T(A) = 1 - T(A) .

Fuzzy logic is therefore a truth-functional system—a fact that causes
serious difficulties. For example, suppose that
$T({Tall}({Nate}))\eq {0.6}$ and $T({Heavy}({Nate}))\eq {0.4}$.
Then we have $T({Tall}({Nate}) \land {Heavy}({Nate})) \eq 0.4$,
which seems reasonable, but we also get the result
$T({Tall}({Nate}) \land \lnot
  {Tall}({Nate})) \eq 0.4$, which does not. Clearly, the problem
arises from the inability of a truth-functional approach to take into
account the correlations or anticorrelations among the component
propositions.

is a methodology for constructing control systems in which the mapping
between real-valued input and output parameters is represented by fuzzy
rules. Fuzzy control has been very successful in commercial products
such as automatic transmissions, video cameras, and electric shavers.
Critics \<see, e.g.,\>Elkan:1993 argue that these
applications are successful because they have small rule bases, no
chaining of inferences, and tunable parameters that can be adjusted to
improve the system’s performance. The fact that they are implemented
with fuzzy operators might be incidental to their success; the key is
simply to provide a concise and intuitive way to specify a smoothly
interpolated, real-valued function.

There have been attempts to provide an explanation of fuzzy logic in
terms of probability theory. One idea is to view assertions such as
“Nate is Tall” as discrete observations made concerning a continuous
hidden variable, Nate’s actual ${Height}$. The probability model
specifies $P(\mbox{Observer says Nate is tall} \mid {Height})$,
perhaps using a as described on . A posterior distribution over Nate’s
height can then be calculated in the usual way, for example, if the
model is part of a hybrid Bayesian network. Such an approach is not
truth-functional, of course. For example, the conditional distribution
$$P(\mbox{Observer says Nate is tall and heavy} \mid {Height}, {Weight})$$
allows for interactions between height and weight in the causing of the
observation. Thus, someone who is eight feet tall and weighs 190 pounds
is very unlikely to be called “tall and heavy,” even though “eight feet”
counts as “tall” and “190 pounds” counts as “heavy.”

Fuzzy predicates can also be given a probabilistic interpretation in
terms of —that is, random variables whose possible values are sets of
objects. For example, ${Tall}$ is a random set whose possible values
are sets of people. The probability $P({Tall}\eq S_1)$, where $S_1$ is
some particular set of people, is the probability that exactly that set
would be identified as “tall” by an observer. Then the probability that
“Nate is tall” is the sum of the probabilities of all the sets of which
Nate is a member.

Both the hybrid Bayesian network approach and the random sets approach
appear to capture aspects of fuzziness without introducing degrees of
truth. Nonetheless, there remain many open issues concerning the proper
representation of linguistic observations and continuous
quantities—issues that have been neglected by most outside the fuzzy
community.

This chapter has described , a well-developed representation for
uncertain knowledge. Bayesian networks play a role roughly analogous to
that of propositional logic for definite knowledge.

-   A Bayesian network is a directed acyclic graph whose nodes
    correspond to random variables; each node has a conditional
    distribution for the node, given its parents.

-   Bayesian networks provide a concise way to represent relationships
    in the domain.

-   A Bayesian network specifies a full joint distribution; each joint
    entry is defined as the product of the corresponding entries in the
    local conditional distributions. A Bayesian network is often
    exponentially smaller than an explicitly enumerated joint
    distribution.

-   Many conditional distributions can be represented compactly by
    canonical families of distributions. , which include both discrete
    and continuous variables, use a variety of canonical distributions.

-   Inference in Bayesian networks means computing the probability
    distribution of a set of query variables, given a set of evidence
    variables. Exact inference algorithms, such as , evaluate sums of
    products of conditional probabilities as efficiently as possible.

-   In (singly connected networks), exact inference takes time linear in
    the size of the network. In the general case, the problem is
    intractable.

-   Stochastic approximation techniques such as and can give reasonable
    estimates of the true posterior probabilities in a network and can
    cope with much larger networks than can exact algorithms.

-   Probability theory can be combined with representational ideas from
    first-order logic to produce very powerful systems for reasoning
    under uncertainty. (RPMs) include representational restrictions that
    guarantee a well-defined probability distribution that can be
    expressed as an equivalent Bayesian network. handle and , defining
    probabilty distributions over the infinite space of first-order
    possible worlds.

-   Various alternative systems for reasoning under uncertainty have
    been suggested. Generally speaking, systems are not well suited for
    such reasoning.

The use of networks to represent probabilistic information began early
in the 20th century, with the work of Sewall Wright on the probabilistic
analysis of genetic inheritance and animal growth factors @Wright:1921
[@Wright:1934]. I. J. Good [-@Good:1961], in collaboration with Alan
Turing, developed probabilistic representations and Bayesian inference
methods that could be regarded as a forerunner of modern Bayesian
networks—although the paper is not often cited in this context.[^10] The
same paper is the original source for the noisy-OR model.

The representation for decision problems, which incorporated a DAG
representation for random variables, was used in decision analysis in
the late 1970s (see ), but only enumeration was used for evaluation.
Judea Pearl developed the message-passing method for carrying out
inference in tree networks @Pearl:1982 and polytree
networks @Kim+Pearl:1983 and explained the importance of causal rather
than diagnostic probability models, in contrast to the certainty-factor
systems then in vogue.

The first expert system using Bayesian networks was @Kim:1983. Early
applications in medicine included the system for diagnosing
neuromuscular disorders @Andersen+al:1989 and the system for pathology
@Heckerman:1991. The system @Pradhan+al:1994 is a Bayesian network for
internal medicine consisting of 448 nodes, 906 links and 8,254
conditional probability values. (The front cover shows a portion of the
network.)

Applications in engineering include the Electric Power Research
Institute’s work on monitoring power generators @Morjaria+al:1995, ’s
work on displaying time-critical information at Mission Control in
Houston @Horvitz+Barry:1995, and the general field of , which aims to
infer unobserved local properties of nodes and links in the Internet
from observations of end-to-end message performance @Castro+al:2004.
Perhaps the most widely used Bayesian network systems have been the
diagnosis-and-repair modules (e.g., the Printer Wizard) in
Windows @Breese+Heckerman:1996 and the Office Assistant in Microsoft
Office @Horvitz+al:1998. Another important application area is biology:
Bayesian networks have been used for identifying human genes by
reference to mouse genes @Zhang+al:2003, inferring cellular networks ,
and many other tasks in bioinformatics. We could go on, but instead
we’ll refer you to , a 400-page guide to applications of Bayesian
networks.

Ross , working in the influence diagram community, developed the first
complete algorithm for general Bayesian networks. His method was based
on goal-directed reduction of the network using posterior-preserving
transformations. developed a clustering algorithm for exact inference in
general Bayesian networks, utilizing a conversion to a directed polytree
of clusters in which message passing was used to achieve consistency
over variables shared between clusters. A similar approach, developed by
the statisticians David Spiegelhalter and Steffen
Lauritzen @Lauritzen+Spiegelhalter:1988, is based on conversion to an
undirected form of graphical model called a . This approach is
implemented in the system, an efficient and widely used tool for
uncertain reasoning @Andersen+al:1989. show how to exploit
context-specific independence in clustering algorithms.

The basic idea of variable elimination—that repeated computations within
the overall sum-of-products expression can be avoided by
caching—appeared in the symbolic probabilistic inference (SPI)
algorithm @Shachter+al:1990. The elimination algorithm we describe is
closest to that developed by . Criteria for pruning irrelevant variables
were developed by and by ; the criterion we give is a simple special
case of these. shows how the variable elimination idea is essentially
identical to  @Bertele+Brioschi:1972, an algorithmic approach that can
be applied to solve a range of inference problems in Bayesian
networks—for example, finding the for a set of observations. This
connects Bayesian network algorithms to related methods for solving CSPs
and gives a direct measure of the complexity of exact inference in terms
of the tree width of the network. describe a method of preventing
exponential growth in the size of factors computed in variable
elimination; their algorithm breaks down large factors into products of
smaller factors and simultaneously computes an error bound for the
resulting approximation.

The inclusion of continuous random variables in Bayesian networks was
considered by and ; these papers discussed networks containing only
continuous variables with linear Gaussian distributions. The inclusion
of discrete variables has been investigated by and implemented in the
cHUGIN system @Olesen:1993. Further analysis of linear Gaussian models,
with connections to many other models used in statistics, appears in The
probit distribution is usually attributed to and , although it had been
discovered several times in the 19th century. Bliss’s work was expanded
considerably by . The probit has been used widely for modeling discrete
choice phenomena and can be extended to handle more than two
choices @Daganzo:1979. The logit model was introduced by ; initially
much derided, it eventually became more popular than the probit model.
gives a simple justification for its use.

showed that the general problem of inference in unconstrained Bayesian
networks is NP-hard, and Paul Dagum and Mike Luby [-@Dagum+Luby:1993]
showed the corresponding approximation problem to be NP-hard. Space
complexity is also a serious problem in both clustering and variable
elimination methods. The method of , which was developed for CSPs in ,
avoids the construction of exponentially large tables. In a Bayesian
network, a cutset is a set of nodes that, when instantiated, reduces the
remaining nodes to a polytree that can be solved in linear time and
space. The query is answered by summing over all the instantiations of
the cutset, so the overall space requirement is still
linear @Pearl:1988. describes a recursive conditioning algorithm that
allows a complete range of space/time tradeoffs.

The development of fast approximation algorithms for Bayesian network
inference is a very active area, with contributions from statistics,
computer science, and physics. The rejection sampling method is a
general technique that is long known to statisticians; it was first
applied to Bayesian networks by Max , who called it . Likelihood
weighting, which was developed by Fung and Chang [-@Fung+Chang:1989] and
Shachter and Peot [-@Shachter+Peot:1989], is an example of the
well-known statistical method of . describe an adaptive version of
likelihood weighting that works well even when the evidence has very low
prior likelihood.

Markov chain Monte Carlo (MCMC) algorithms began with the , due to ,
which was also the source of the simulated annealing algorithm described
in . The Gibbs sampler was devised by for inference in
undirected Markov networks. The application of MCMC to Bayesian networks
is due to . The papers collected by cover a wide variety of applications
of MCMC, several of which were developed in the well-known
package @Gilks+al:1994.

There are two very important families of approximation methods that we
did not cover in the chapter. The first is the family of methods, which
can be used to simplify complex calculations of all kinds. The basic
idea is to propose a reduced version of the original problem that is
simple to work with, but that resembles the original problem as closely
as possible. The reduced problem is described by some $\blambda$ that
are adjusted to minimize a distance function $D$ between the original
and the reduced problem, often by solving the system of equations
$\partial
D/\partial \blambda \eq 0$. In many cases, strict upper and lower bounds
can be obtained. Variational methods have long been used in
statistics @Rustagi:1976. In statistical physics, the method is a
particular variational approximation in which the individual variables
making up the model are assumed to be completely independent. This idea
was applied to solve large undirected Markov networks
@Peterson+Anderson:1987 [@Parisi:1988]. developed the mathematical
foundations for applying variational methods to Bayesian networks and
obtained accurate lower-bound approximations for sigmoid networks with
the use of mean-field methods. extended the methodology to obtain both
lower and upper bounds. Since these early papers, variational methods
have been applied to many specific families of models. The remarkable
paper by provides a unifying theoretical analysis of the literature on
variational methods.

A second important family of approximation algorithms is based on
Pearl’s polytree message-passing algorithm [-@Pearl:1982]. This
algorithm can be applied to general networks, as suggested by . The
results might be incorrect, or the algorithm might fail to terminate,
but in many cases, the values obtained are close to the true values.
Little attention was paid to this so-called (or BP) approach until
observed that message passing in a multiply connected Bayesian network
was exactly the computation performed by the algorithm @Berrou+al:1993,
which provided a major breakthrough in the design of efficient
error-correcting codes. The implication is that BP is both fast and
accurate on the very large and very highly connected networks used for
decoding and might therefore be useful more generally. presented a
promising empirical study of BP’s performance, and established strong
convergence results for BP on linear Gaussian networks. shows how an
approximation called loopy belief propagation works, and when the
approximation is correct. made further connections between loopy
propagation and ideas from statistical physics.

The connection between probability and first-order languages was first
studied by . and defined a language in which probabilities could be
associated with first-order sentences and for which models were
probability measures on possible worlds. Within AI, this idea was
developed for propositional logic by and for first-order logic by . The
first extensive investigation of knowledge representation issues in such
languages was carried out by . The basic idea is that each sentence in
the knowledge base expressed a *constraint* on the
distribution over possible worlds; one sentence entails another if it
expresses a stronger constraint. For example, the sentence
$\All{x} P(Hungry(x)) > 0.2$ rules out distributions in which any object
is hungry with probability less than 0.2; thus, it entails the sentence
$\All{x} P(Hungry(x)) > 0.1$. It turns out that writing a
*consistent* set of sentences in these languages is quite
difficult and constructing a unique probability model nearly impossible
unless one adopts the representation approach of Bayesian networks by
writing suitable sentences about conditional probabilities.

Beginning in the early 1990s, researchers working on complex
applications noticed the expressive limitations of Bayesian networks and
developed various languages for writing “templates” with logical
variables, from which large networks could be constructed automatically
for each problem instance @Breese:1992 [@Wellman+al:1992]. The most
important such language was (Bayesian inference Using Gibbs
Sampling) @Gilks+al:1994, which combined Bayesian networks with the
notation common in statistics. (In , an indexed random variable looks
like $X[i]$, where $i$ has a defined integer range.) These languages
inherited the key property of Bayesian networks: every well-formed
knowledge base defines a unique, consistent probability model. Languages
with well-defined semantics based on unique names and domain closure
drew on the representational capabilities of logic
programming @Poole:1993 [@Sato+Kameya:1997; @Kersting+al:2000] and
semantic networks @Koller+Pfeffer:1998 [@Pfeffer:2000]. went on to
develop , which represents first-order probability models as
probabilistic programs in a programming language extended with a
randomization primitive. Another important thread was the combination of
relational and first-order notations with (undirected) Markov
networks @Taskar+al:2002 [@Domingos+Richardson:2004], where the emphasis
has been less on knowledge representation and more on learning from
large data sets.

Initially, inference in these models was performed by generating an
equivalent Bayesian network. introduced a variable elimination algorithm
that cached each computed factor for reuse by later computations
involving the same relations but different objects, thereby realizing
some of the computational gains of lifting. The first truly lifted
inference algorithm was a lifted form of variable elimination described
by and subsequently improved by . Further advances, including cases
where certain aggregate probabilities can be computed in closed form,
are described by and . studied the application of MCMC to avoid building
the complete equivalent Bayes net in cases of relational and identity
uncertainty. collect many important papers on first-order probability
models and their use in machine learning.

Probabilistic reasoning about identity uncertainty has two distinct
origins. In statistics, the problem of arises when data records do not
contain standard unique identifiers—for example, various citations of
this book might name its first author “Stuart Russell” or
“S. J. Russell” or even “Stewart Russle,” and other authors may use the
some of the same names. Literally hundreds of companies exist solely to
solve record linkage problems in financial, medical, census, and other
data. Probabilistic analysis goes back to work by ; the Fellegi–Sunter
model [-@Fellegi+Sunter:1969], which is essentially naive Bayes applied
to matching, still dominates current practice. The second origin for
work on identity uncertainty is multitarget tracking @Sittler:1964,
which we cover in . For most of its history, work in symbolic AI assumed
erroneously that sensors could supply sentences with unique identifiers
for objects. The issue was studied in the context of language
understanding by and in the context of surveillance by
@Huang+Russell:1998 and . developed a complex generative model for
authors, papers, and citation strings, involving both relational and
identity uncertainty, and demonstrated high accuracy for citation
information extraction. The first formally defined language for
open-universe probability models was  @Milch+al:2005, which came with a
complete (albeit slow) MCMC inference algorithm for all well-defined
mdoels. (The program code faintly visible on the front cover of this
book is part of a model for detecting nuclear explosions from seismic
signals as part of the UN Comprehensive Test Ban Treaty verification
regime.) describes another open-universe modeling language called .

As explained in , early probabilistic systems fell out of favor in the
early 1970s, leaving a partial vacuum to be filled by alternative
methods. Certainty factors were invented for use in the medical expert
system @Shortliffe:1976, which was intended both as an engineering
solution and as a model of human judgment under uncertainty. The
collection *Rule-Based Expert
Systems* @Buchanan+Shortliffe:1984 provides a complete overview
of and its descendants \<see also\>Stefik:1995. David
Heckerman [-@Heckerman:1986] showed that a slightly modified version of
certainty factor calculations gives correct probabilistic results in
some cases, but results in serious overcounting of evidence in other
cases. The expert system @Duda+al:1979 used a rule-based approach in
which the rules were justified by a (seldom tenable) global independence
assumption.

originates with a paper by Arthur Dempster [-@Dempster:1968] proposing a
generalization of probability to interval values and a combination rule
for using them. Later work by Glenn Shafer [-@Shafer:1976] led to the
Dempster-Shafer theory’s being viewed as a competing approach to
probability. and analyze the relationship between the Dempster–Shafer
theory and standard probability theory.

Fuzzy sets were developed by Lotfi Zadeh [-@Zadeh:1965] in response to
the perceived difficulty of providing exact inputs to intelligent
systems. The text by provides a thorough introduction to fuzzy set
theory; papers on fuzzy applications are collected in . As we mentioned
in the text, fuzzy logic has often been perceived incorrectly as a
direct competitor to probability theory, whereas in fact it addresses a
different set of issues.  @Zadeh:1978 was introduced to handle
uncertainty in fuzzy systems and has much in common with probability.
Dubois and Prade [-@Dubois+Prade:1994] survey the connections between
possibility theory and probability theory.

The resurgence of probability depended mainly on Pearl’s development of
Bayesian networks as a method for representing and using conditional
independence information. This resurgence did not come without a fight;
Peter Cheeseman’s [-@Cheeseman:1985] pugnacious “In Defense of
Probability” and his later article “An Inquiry into Computer
Understanding” @Cheeseman:1988 [with commentaries] give something of the
flavor of the debate. Eugene Charniak helped present the ideas to AI
researchers with a popular article, “Bayesian networks without
tears”[^11] [-@Charniak:1991], and book [-@Charniak:1993]. The book by
also helped introduce Bayesian networks to AI researchers. One of the
principal philosophical objections of the logicists was that the
numerical calculations that probability theory was thought to require
were not apparent to introspection and presumed an unrealistic level of
precision in our uncertain knowledge. The development of  @Wellman:1990
provided a purely qualitative abstraction of Bayesian networks, using
the notion of positive and negative influences between variables.
Wellman shows that in many cases such information is sufficient for
optimal decision making without the need for the precise specification
of probability values. take a similar approach. Work by Adnan Darwiche
and Matt Ginsberg [-@Darwiche+Ginsberg:1992] extracts the basic
properties of conditioning and evidence combination from probability
theory and shows that they can also be applied in logical and default
reasoning. Often, programs speak louder than words, and the ready
availability of high-quality software such as the @Murphy:2001
accelerated the adoption of the technology.

The most important single publication in the growth of Bayesian networks
was undoubtedly the text *Probabilistic Reasoning in Intelligent
Systems* @Pearl:1988. Several excellent texts @Lauritzen:1996
[@Jensen:2001; @Korb+Nicholson:2003; @Jensen:2007; @Darwiche:2009; @Koller+Friedman:2009]
provide thorough treatments of the topics we have covered in this
chapter. New research on probabilistic reasoning appears both in
mainstream AI journals, such as *Artificial Intelligence*
and the *Journal of AI Research*, and in more specialized
journals, such as the *International Journal of Approximate
Reasoning*. Many papers on graphical models, which
include Bayesian networks, appear in statistical journals. The
proceedings of the conferences on Uncertainty in Artificial Intelligence
(UAI), Neural Information Processing Systems (NIPS), and Artificial
Intelligence and Statistics (AISTATS) are excellent sources for current
research.

We have a bag of three biased coins $a$, $b$, and $c$ with probabilities
of coming up heads of 20%, 60%, and 80%, respectively. One coin is drawn
randomly from the bag (with equal likelihood of drawing each of the
three coins), and then the coin is flipped three times to generate the
outcomes $X_1$, $X_2$, and $X_3$.

1.  Draw the Bayesian network corresponding to this setup and define the
    necessary CPTs.

2.  Calculate which coin was most likely to have been drawn from the bag
    if the observed flips come out heads twice and tails once.

We have a bag of three biased coins $a$, $b$, and $c$ with probabilities
of coming up heads of 30%, 60%, and 75%, respectively. One coin is drawn
randomly from the bag (with equal likelihood of drawing each of the
three coins), and then the coin is flipped three times to generate the
outcomes $X_1$, $X_2$, and $X_3$.

1.  Draw the Bayesian network corresponding to this setup and define the
    necessary CPTs.

2.  Calculate which coin was most likely to have been drawn from the bag
    if the observed flips come out heads twice and tails once.

[cpt-equivalence-exercise] on defines the joint distribution represented
by a Bayesian network in terms of the parameters
$\theta(X_i\given \Parents(X_i))$. This exercise asks you to derive the
equivalence between the parameters and the conditional probabilities
$\pv(X_i\given \Parents(X_i))$ from this definition.

1.  Consider a simple network $X\rightarrow Y\rightarrow Z$ with three
    Boolean variables. Use () to express the conditional probability
    $P(z\given y)$ as the ratio of two sums, each over entries in the
    joint distribution $\pv(X,Y,Z)$.

2.  Now use to write this expression in terms of the network parameters
    $\theta(X)$, $\theta(Y\given X)$, and $\theta(Z\given Y)$.

3.  Next, expand out the summations in your expression from part (b),
    writing out explicitly the terms for the true and false values of
    each summed variable. Assuming that all network parameters satisfy
    the constraint $\sum_{x_i} \theta(x_i\given \parents(X_i))\eq 1$,
    show that the resulting expression reduces to $\theta(x\given y)$.

4.  Generalize this derivation to show that
    $\theta(X_i\given \Parents(X_i)) = \pv(X_i\given \Parents(X_i))$ for
    any Bayesian network.

The operation of in a Bayesian network allows us to change the direction
of an arc $X\rightarrow Y$ while preserving the joint probability
distribution that the network represents @Shachter:1986. Arc reversal
may require introducing new arcs: all the parents of $X$ also become
parents of $Y$, and all parents of $Y$ also become parents of $X$.

1.  Assume that $X$ and $Y$ start with $m$ and $n$ parents,
    respectively, and that all variables have $k$ values. By calculating
    the change in size for the CPTs of $X$ and $Y$, show that the total
    number of parameters in the network cannot decrease during arc
    reversal. (*Hint*: the parents of $X$ and $Y$ need not
    be disjoint.)

2.  Under what circumstances can the total number remain constant?

3.  Let the parents of $X$ be $\U \cup \V$ and the parents of $Y$ be
    $\V \cup \W$, where $\U$ and $\W$ are disjoint. The formulas for the
    new CPTs after arc reversal are as follows:

    $$\begin{aligned}
    \pv(Y\given \U,\V,\W) &=& \sum_x \pv(Y\given \V,\W, x) \pv(x\given \U, \V) \\
    \pv(X\given \U,\V,\W, Y) &=& \pv(Y\given X, \V, \W) \pv(X\given \U, \V) / \pv(Y\given \U,\V,\W)\ .\end{aligned}$$

    Prove that the new network expresses the same joint distribution
    over all variables as the original network.

Consider the Bayesian network in .

1.  If no evidence is observed, are ${Burglary}$ and ${Earthquake}$
    independent? Prove this from the numerical semantics and from the
    topological semantics.

2.  If we observe ${Alarm}\eq {true}$, are ${Burglary}$ and
    ${Earthquake}$ independent? Justify your answer by calculating
    whether the probabilities involved satisfy the definition of
    conditional independence.

Suppose that in a Bayesian network containing an unobserved variable
$Y$, all the variables in the Markov blanket ${MB}(Y)$ have been
observed.

1.  Prove that removing the node $Y$ from the network will not affect
    the posterior distribution for any other unobserved variable in the
    network.

2.  Discuss whether we can remove $Y$ if we are planning to use (i)
    rejection sampling and (ii) likelihood weighting.

[handedness-figure]

[handedness-exercise] Let $H_x$ be a random variable denoting the
handedness of an individual $x$, with possible values $l$ or $r$. A
common hypothesis is that left- or right-handedness is inherited by a
simple mechanism; that is, perhaps there is a gene $G_x$, also with
values $l$ or $r$, and perhaps actual handedness turns out mostly the
same (with some probability $s$) as the gene an individual possesses.
Furthermore, perhaps the gene itself is equally likely to be inherited
from either of an individual’s parents, with a small nonzero probability
$m$ of a random mutation flipping the handedness.

1.  Which of the three networks in claim that
    $ \pv(G_{{father}},G_{{mother}},G_{{child}}) = \pv(G_{{father}})\pv(G_{{mother}})\pv(G_{{child}})$?

2.  Which of the three networks make independence claims that are
    consistent with the hypothesis about the inheritance of handedness?

3.  Which of the three networks is the best description of the
    hypothesis?

4.  Write down the CPT for the $G_{{child}}$ node in network (a), in
    terms of $s$ and $m$.

5.  Suppose that $P(G_{{father}}\eq l)=P(G_{{mother}}\eq l)=q$. In
    network (a), derive an expression for $P(G_{{child}}\eq l)$ in
    terms of $m$ and $q$ only, by conditioning on its parent nodes.

6.  Under conditions of genetic equilibrium, we expect the distribution
    of genes to be the same across generations. Use this to calculate
    the value of $q$, and, given what you know about handedness in
    humans, explain why the hypothesis described at the beginning of
    this question must be wrong.

[markov-blanket-exercise] The of a variable is defined on . Prove that a
variable is independent of all other variables in the network, given its
Markov blanket and derive ().

[car-starts-figure]

Consider the network for car diagnosis shown in .

1.  Extend the network with the Boolean variables ${IcyWeather}$ and
    ${StarterMotor}$.

2.  Give reasonable conditional probability tables for all the nodes.

3.  How many independent values are contained in the joint probability
    distribution for eight Boolean nodes, assuming that no conditional
    independence relations are known to hold among them?

4.  How many independent probability values do your network tables
    contain?

5.  The conditional distribution for ${Starts}$ could be described as
    a distribution. Define this family in general and relate it to the
    noisy-OR distribution.

Consider a simple Bayesian network with root variables ${Cold}$,
${Flu}$, and ${Malaria}$ and child variable ${Fever}$, with a
noisy-OR conditional distribution for ${Fever}$ as described in . By
adding appropriate auxiliary variables for inhibition events and
fever-inducing events, construct an equivalent Bayesian network whose
CPTs (except for root variables) are deterministic. Define the CPTs and
prove equivalence.

[LG-exercise] Consider the family of linear Gaussian networks, as
defined on .

1.  In a two-variable network, let $X_1$ be the parent of $X_2$, let
    $X_1$ have a Gaussian prior, and let $\pv(X_2\given X_1)$ be a
    linear Gaussian distribution. Show that the joint distribution
    $P(X_1,X_2)$ is a multivariate Gaussian, and calculate its
    covariance matrix.

2.  Prove by induction that the joint distribution for a general linear
    Gaussian network on $X_1,\ldots,X_n$ is also a multivariate
    Gaussian.

[multivalued-probit-exercise] The probit distribution defined on
describes the probability distribution for a Boolean child, given a
single continuous parent.

1.  How might the definition be extended to cover multiple continuous
    parents?

2.  How might it be extended to handle a *multivalued*
    child variable? Consider both cases where the child’s values are
    ordered (as in selecting a gear while driving, depending on speed,
    slope, desired acceleration, etc.) and cases where they are
    unordered (as in selecting bus, train, or car to get to work).
    (*Hint*: Consider ways to divide the possible values
    into two sets, to mimic a Boolean variable.)

In your local nuclear power station, there is an alarm that senses when
a temperature gauge exceeds a given threshold. The gauge measures the
temperature of the core. Consider the Boolean variables $A$ (alarm
sounds), $F_A$ (alarm is faulty), and $F_G$ (gauge is faulty) and the
multivalued nodes $G$ (gauge reading) and $T$ (actual core temperature).

1.  Draw a Bayesian network for this domain, given that the gauge is
    more likely to fail when the core temperature gets too high.

2.  Is your network a polytree? Why or why not?

3.  Suppose there are just two possible actual and measured
    temperatures, normal and high; the probability that the gauge gives
    the correct temperature is $x$ when it is working, but $y$ when it
    is faulty. Give the conditional probability table associated with
    $G$.

4.  Suppose the alarm works correctly unless it is faulty, in which case
    it never sounds. Give the conditional probability table associated
    with $A$.

5.  Suppose the alarm and gauge are working and the alarm sounds.
    Calculate an expression for the probability that the temperature of
    the core is too high, in terms of the various conditional
    probabilities in the network.

[telescope-exercise] Two astronomers in different parts of the world
make measurements $M_1$ and $M_2$ of the number of stars $N$ in some
small region of the sky, using their telescopes. Normally, there is a
small possibility $e$ of error by up to one star in each direction. Each
telescope can also (with a much smaller probability $f$) be badly out of
focus (events $F_1$ and $F_2$), in which case the scientist will
undercount by three or more stars (or if $N$ is less than 3, fail to
detect any stars at all). Consider the three networks shown in .

1.  Which of these Bayesian networks are correct (but not necessarily
    efficient) representations of the preceding information?

2.  Which is the best network? Explain.

3.  Write out a conditional distribution for $\pv(M_1\given N)$, for the
    case where $N\elt\{1,2,3\}$ and $M_1\elt\{0,1,2,3,4\}$. Each entry
    in the conditional distribution should be expressed as a function of
    the parameters $e$ and/or $f$.

4.  Suppose $M_1\eq 1$ and $M_2\eq 3$. What are the
    *possible* numbers of stars if you assume no prior
    constraint on the values of $N$?

5.  What is the *most likely* number of stars, given these
    observations? Explain how to compute this, or if it is not possible
    to compute, explain what additional information is needed and how it
    would affect the result.

Consider the network shown in (ii), and assume that the two telescopes
work identically. $N\elt\{1,2,3\}$ and $M_1,M_2\elt\{0,1,2,3,4\}$, with
the symbolic CPTs as described in . Using the enumeration algorithm ( on
), calculate the probability distribution
$\pv(N\given M_1\eq 2,M_2\eq 2)$.

[telescope-nets-figure]

[politics-figure]

Consider the Bayes net shown in .

1.  Which of the following are asserted by the network
    *structure*?

    1.  $\pv(B,I,M) = \pv(B)\pv(I)\pv(M)$.

    2.  $\pv(J\given G) = \pv(J\given G,I)$.

    3.  $\pv(M\given G,B,I) = \pv(M\given G,B,I,J)$.

2.  Calculate the value of $P(b,i,\lnot m,g,j)$.

3.  Calculate the probability that someone goes to jail given that they
    broke the law, have been indicted, and face a politically motivated
    prosecutor.

4.  A (see ) allows a variable to be independent of some of its parents
    given certain values of others. In addition to the usual conditional
    independences given by the graph structure, what context-specific
    independences exist in the Bayes net in ?

5.  Suppose we want to add the variable $P\eq {PresidentialPardon}$ to
    the network; draw the new network and briefly explain any links you
    add.

Consider the Bayes net shown in .

1.  Which, if any, of the following are asserted by the network
    *structure* (ignoring the CPTs for now)?

    1.  $\pv(B,I,M) = \pv(B)\pv(I)\pv(M)$.

    2.  $\pv(J\given G) = \pv(J\given G,I)$.

    3.  $\pv(M\given G,B,I) = \pv(M\given G,B,I,J)$.

2.  Calculate the value of $P(b,i,m,\lnot g,j)$.

3.  Calculate the probability that someone goes to jail given that they
    broke the law, have been indicted, and face a politically motivated
    prosecutor.

4.  A (see ) allows a variable to be independent of some of its parents
    given certain values of others. In addition to the usual conditional
    independences given by the graph structure, what context-specific
    independences exist in the Bayes net in ?

5.  Suppose we want to add the variable $P\eq {PresidentialPardon}$ to
    the network; draw the new network and briefly explain any links you
    add.

[VE-exercise] Consider the variable elimination algorithm in ().

1.  applies variable elimination to the query
    $$\pv({Burglary}\given {JohnCalls}\eq {true},{MaryCalls}\eq {true})\ .$$
    Perform the calculations indicated and check that the answer is
    correct.

2.  Count the number of arithmetic operations performed, and compare it
    with the number performed by the enumeration algorithm.

3.  Suppose a network has the form of a *chain*: a sequence
    of Boolean variables $X_1,\ldots, X_n$ where
    $\Parents(X_i)\eq \{X_{i-1}\}$ for $i\eq 2,\ldots,n$. What is the
    complexity of computing $\pv(X_1\given X_n\eq
    {true})$ using enumeration? Using variable elimination?

4.  Prove that the complexity of running variable elimination on a
    polytree network is linear in the size of the tree for any variable
    ordering consistent with the network structure.

[bn-complexity-exercise] Investigate the complexity of exact inference
in general Bayesian networks:

1.  Prove that any 3-SAT problem can be reduced to exact inference in a
    Bayesian network constructed to represent the particular problem and
    hence that exact inference is NP-hard. (*Hint*:
    Consider a network with one variable for each proposition symbol,
    one for each clause, and one for the conjunction of clauses.)

2.  The problem of counting the number of satisfying assignments for a
    3-SAT problem is \#P-complete. Show that exact inference is at least
    as hard as this.

[primitive-sampling-exercise] Consider the problem of generating a
random sample from a specified distribution on a single variable. Assume
you have a random number generator that returns a random number
uniformly distributed between 0 and 1.

1.  Let $X$ be a discrete variable with $P(X\eq x_i)\eq p_i$ for
    $i\elt\{1,\ldots,k\}$. The of $X$ gives the probability that
    $X\elt\{x_1,\ldots,x_j\}$ for each possible $j$. (See also .)
    Explain how to calculate the cumulative distribution in $O(k)$ time
    and how to generate a single sample of $X$ from it. Can the latter
    be done in less than $O(k)$ time?

2.  Now suppose we want to generate $N$ samples of $X$, where $N\gg k$.
    Explain how to do this with an expected run time per sample that is
    *constant* (i.e., independent of $k$).

3.  Now consider a continuous-valued variable with a parameterized
    distribution (e.g., Gaussian). How can samples be generated from
    such a distribution?

4.  Suppose you want to query a continuous-valued variable and you are
    using a sampling algorithm such as to do the inference. How would
    you have to modify the query-answering process?

Consider the query $\pv({Rain}\given {Sprinkler}\eq
{true},{WetGrass}\eq {true})$ in (a) () and how Gibbs sampling can
answer it.

1.  How many states does the Markov chain have?

2.  Calculate the $\mbf{Q}$ containing $\transition{\mbf{y}}{\mbf{y}'}$
    for all $\mbf{y}$, $\mbf{y}'$.

3.  What does $\mbf{Q}^2$, the square of the transition matrix,
    represent?

4.  What about $\mbf{Q}^n$ as $n\to \infty$?

5.  Explain how to do probabilistic inference in Bayesian networks,
    assuming that $\mbf{Q}^n$ is available. Is this a practical way to
    do inference?

[gibbs-proof-exercise] This exercise explores the stationary
distribution for Gibbs sampling methods.

1.  The convex composition $[\alpha, q_1; 1-\alpha, q_2]$ of $q_1$ and
    $q_2$ is a transition probability distribution that first chooses
    one of $q_1$ and $q_2$ with probabilities $\alpha$ and $1-\alpha$,
    respectively, and then applies whichever is chosen. Prove that if
    $q_1$ and $q_2$ are in detailed balance with $\pi$, then their
    convex composition is also in detailed balance with $\pi$.
    (*Note*: this result justifies a variant of in which
    variables are chosen at random rather than sampled in a fixed
    sequence.)

2.  Prove that if each of $q_1$ and $q_2$ has $\pi$ as its stationary
    distribution, then the sequential composition $q \eq q_1 \circ q_2$
    also has $\pi$ as its stationary distribution.

[MH-exercise] The algorithm is a member of the MCMC family; as such, it
is designed to generate samples $\x$ (eventually) according to target
probabilities $\pi(\x)$. (Typically we are interested in sampling from
$\pi(\x)\eq P(\x\given \e)$.) Like simulated annealing,
Metropolis–Hastings operates in two stages. First, it samples a new
state $\x'$ from a $q(\x'\given \x)$, given the current state $\x$.
Then, it probabilistically accepts or rejects $\x'$ according to the
$$\alpha(\x'\given \x) = \min\ \left(1,\frac{\pi(\x')q(\x\given \x')}{\pi(\x)q(\x'\given \x)}  \right)\ .$$
If the proposal is rejected, the state remains at $\x$.

1.  Consider an ordinary Gibbs sampling step for a specific variable
    $X_i$. Show that this step, considered as a proposal, is guaranteed
    to be accepted by Metropolis–Hastings. (Hence, Gibbs sampling is a
    special case of Metropolis–Hastings.)

2.  Show that the two-step process above, viewed as a transition
    probability distribution, is in detailed balance with $\pi$.

[soccer-rpm-exercise]soccer teams $A$, $B$, and $C$, play each other
once. Each match is between two teams, and can be won, drawn, or lost.
Each team has a fixed, unknown degree of quality—an integer ranging from
0 to 3—and the outcome of a match depends probabilistically on the
difference in quality between the two teams.

1.  Construct a relational probability model to describe this domain,
    and suggest numerical values for all the necessary probability
    distributions.

2.  Construct the equivalent Bayesian network for the three matches.

3.  Suppose that in the first two matches $A$ beats $B$ and draws with
    $C$. Using an exact inference algorithm of your choice, compute the
    posterior distribution for the outcome of the third match.

4.  Suppose there are $n$ teams in the league and we have the results
    for all but the last match. How does the complexity of predicting
    the last game vary with $n$?

5.  Investigate the application of MCMC to this problem. How quickly
    does it converge in practice and how well does it scale?

[^1]: This is the most common name, but there are many synonyms,
    including , , , and . In statistics, the term refers to a somewhat
    broader class that includes Bayesian networks. An extension of
    Bayesian networks called a or is covered in .

[^2]: There is also a general topological criterion called for deciding
    whether a set of nodes $\mbf{X}$ is conditionally independent of
    another set $\mbf{Y}$, given a third set $\Z$. The criterion is
    rather complicated and is not needed for deriving the algorithms in
    this chapter, so we omit it. Details may be found in or . gives a
    more intuitive method of ascertaining d-separation.

[^3]: It follows that inference in linear Gaussian networks takes only
    $O(n^3)$ time in the worst case, regardless of the network topology.
    In , we see that inference for networks of discrete variables is
    NP-hard.

[^4]: An expression such as $\sum_e P(a,e)$ means to sum $P(A=a, E=e)$
    for all possible values of $e$. When $E$ is Boolean, there is an
    ambiguity in that $P(e)$ is used to mean both $P(E={true})$ and
    $P(E=e)$, but it should be clear from context which is intended; in
    particular, in the context of a sum the latter is intended.

[^5]: Ideally, we would like to use a sampling distribution equal to the
    true posterior $P(\z\given \e)$, to take all the evidence into
    account. This cannot be done efficiently, however. If it could, then
    we could approximate the desired probability to arbitrary accuracy
    with a polynomial number of samples. It can be shown that no such
    polynomial-time approximation scheme can exist.

[^6]: A game theorist would advise a dishonest customer to avoid
    detection by occasionally recommending a good book from a
    competitor. See .

[^7]: The name *relational probability model* was given by
    to a slightly different representation, but the underlying ideas are
    the same.

[^8]: Some technical conditions must be observed to guarantee that the
    RPM defines a proper distribution. First, the dependencies must be
    *acyclic*, otherwise the resulting Bayesian network
    will have cycles and will not define a proper distribution. Second,
    the dependencies must be *well-founded*, that is, there
    can be no infinite ancestor chains, such as might arise from
    recursive dependencies. Under some circumstances (see ), a
    fixed-point calculation yields a well-defined probability model for
    a recursive RPM.

[^9]: A distribution ${LogNormal}[\mu,\sigma^2](x)$ is equivalent to a
    distribution $N[\mu,\sigma^2](x)$ over $\log_e(x)$.

[^10]: I. J. Good was chief statistician for Turing’s code-breaking team
    in . In *2001: A Space Odyssey* @Clarke:1968b, Good and
    Minsky are credited with making the breakthrough that led to the
    development of the HAL 9000 computer.

[^11]: The title of the original version of the article was “Pearl for
    swine.”
Learning Probabilistic Models {#bayesian-learning-chapter}
=============================

pointed out the prevalence of uncertainty in real environments. Agents
can handle uncertainty by using the methods of probability and decision
theory, but first they must learn their probabilistic theories of the
world from experience. This chapter explains how they can do that, by
formulating the learning task itself as a process of probabilistic
inference (). We will see that a Bayesian view of learning is extremely
powerful, providing general solutions to the problems of noise,
overfitting, and optimal prediction. It also takes into account the fact
that a less-than-omniscient agent can never be certain about which
theory of the world is correct, yet must still make decisions by using
some theory of the world.

We describe methods for learning probability models—primarily Bayesian
networks—in Sections [parametric-learning-section] and [em-section].
Some of the material in this chapter is fairly mathematical, although
the general lessons can be understood without plunging into the details.
It may benefit the reader to review Chapters [probability-chapter]
and [bayes-nets-chapter] and peek at .

Statistical Learning {#statistical-learning-section}
--------------------

The key concepts in this chapter, just as in , are and . Here, the data
are —that is, instantiations of some or all of the random variables
describing the domain. The hypotheses in this chapter are probabilistic
theories of how the domain works, including logical theories as a
special case.

Consider a simple example. Our favorite Surprise candy comes in two
flavors: cherry (yum) and lime (ugh). The manufacturer has a peculiar
sense of humor and wraps each piece of candy in the same opaque wrapper,
regardless of flavor. The candy is sold in very large bags, of which
there are known to be five kinds—again, indistinguishable from the
outside:

> $h_1$: 100% cherry,\
> $h_2$: 75% cherry + 25% lime,\
> $h_3$: 50% cherry + 50% lime,\
> $h_4$: 25% cherry + 75% lime,\
> $h_5$: 100% lime  .

Given a new bag of candy, the random variable $H$ (for
*hypothesis*) denotes the type of the bag, with possible
values $h_1$ through $h_5$. $H$ is not directly observable, of course.
As the pieces of candy are opened and inspected, data are
revealed—$\Datum_1$, $\Datum_2$, $\ldots$, $\Datum_{\Ncount}$, where
each $\Datum_i$ is a random variable with possible values ${cherry}$
and ${lime}$. The basic task faced by the agent is to predict the
flavor of the next piece of candy.[^1] Despite its apparent triviality,
this scenario serves to introduce many of the major issues. The agent
really does need to infer a theory of its world, albeit a very simple
one.

simply calculates the probability of each hypothesis, given the data,
and makes predictions on that basis. That is, the predictions are made
by using *all* the hypotheses, weighted by their
probabilities, rather than by using just a single “best” hypothesis. In
this way, learning is reduced to probabilistic inference. Let $\Data$
represent all the data, with observed value $\data$; then the
probability of each hypothesis is obtained by Bayes’ rule:

$$P(h_i\given \data) = \alpha P(\data\given h_i) P(h_i)\ .
\label{hypothesis-posterior-equation}$$

Now, suppose we want to make a prediction about an unknown quantity $X$.
Then we have

$$\pv(X\given \data) = \sum\limits_i \pv(X\given \data,h_i) \pv(h_i\given \data)
= \sum\limits_i \pv(X\given h_i) P(h_i\given \data)\ ,
\label{bayes-prediction-equation}$$

where we have assumed that each hypothesis determines a probability
distribution over $X$. This equation shows that predictions are weighted
averages over the predictions of the individual hypotheses. The
hypotheses themselves are essentially “intermediaries” between the raw
data and the predictions. The key quantities in the Bayesian approach
are the , $P(h_i)$, and the of the data under each hypothesis,
$P(\data\given h_i)$.

For our candy example, we will assume for the time being that the prior
distribution over $h_1,\ldots,h_5$ is given by
$\<{0.1},{0.2},{0.4},{0.2},{0.1}\>$, as advertised by the manufacturer.
The likelihood of the data is calculated under the assumption that the
observations are (see ), so that

$$P(\data\given h_i) = \prod_j P(\datum_j\given h_i)\ .
\label{iid-likelihood-equation}$$

For example, suppose the bag is really an all-lime bag ($h_5$) and the
first 10 candies are all lime; then $P(\data\given h_3)$ is
${0.5}^{{10}}$, because half the candies in an $h_3$ bag are lime.[^2]
(a) shows how the posterior probabilities of the five hypotheses change
as the sequence of 10 lime candies is observed. Notice that the
probabilities start out at their prior values, so $h_3$ is initially the
most likely choice and remains so after 1 lime candy is unwrapped. After
2 lime candies are unwrapped, $h_4$ is most likely; after 3 or more,
$h_5$ (the dreaded all-lime bag) is the most likely. After 10 in a row,
we are fairly certain of our fate. (b) shows the predicted probability
that the next candy is lime, based on . As we would expect, it increases
monotonically toward 1.

[bayes-candy-figure]

The example shows that

the Bayesian prediction eventually agrees with the true hypothesis.

This is characteristic of Bayesian learning. For any fixed prior that
does not rule out the true hypothesis, the posterior probability of any
false hypothesis will, under certain technical conditions, eventually
vanish. This happens simply because the probability of generating
“uncharacteristic” data indefinitely is vanishingly small. (This point
is analogous to one made in the discussion of PAC learning in .) More
important, the Bayesian prediction is *optimal*, whether
the data set be small or large. Given the hypothesis prior, any other
prediction is expected to be correct less often.

The optimality of Bayesian learning comes at a price, of course. For
real learning problems, the hypothesis space is usually very large or
infinite, as we saw in . In some cases, the summation in (or
integration, in the continuous case) can be carried out tractably, but
in most cases we must resort to approximate or simplified methods.

A very common approximation—one that is usually adopted in science—is to
make predictions based on a single *most probable*
hypothesis—that is, an $h_i$ that maximizes $P(h_i\given \data)$. This
is often called a or MAP (pronounced “em-ay-pee”) hypothesis.
Predictions made according to an MAP hypothesis $\hmap$ are
approximately Bayesian to the extent that $\pv(X\given \data) \approx
\pv(X\given \hmap)$. In our candy example, $\hmap\eq h_5$ after three
lime candies in a row, so the MAP learner then predicts that the fourth
candy is lime with probability 1.0—a much more dangerous prediction than
the Bayesian prediction of 0.8 shown in (b). As more data arrive, the
MAP and Bayesian predictions become closer, because the competitors to
the MAP hypothesis become less and less probable.

Although our example doesn’t show it, finding MAP hypotheses is often
much easier than Bayesian learning, because it requires solving an
optimization problem instead of a large summation (or integration)
problem. We will see examples of this later in the chapter.

In both Bayesian learning and MAP learning, the hypothesis prior
$P(h_i)$ plays an important role. We saw in that can occur when the
hypothesis space is too expressive, so that it contains many hypotheses
that fit the data set well. Rather than placing an arbitrary limit on
the hypotheses to be considered, Bayesian and MAP learning methods use
the prior to *penalize complexity*. Typically, more complex
hypotheses have a lower prior probability—in part because there are
usually many more complex hypotheses than simple hypotheses. On the
other hand, more complex hypotheses have a greater capacity to fit the
data. (In the extreme case, a lookup table can reproduce the data
exactly with probability 1.) Hence, the hypothesis prior embodies a
tradeoff between the complexity of a hypothesis and its degree of fit to
the data.

We can see the effect of this tradeoff most clearly in the logical case,
where $H$ contains only *deterministic* hypotheses. In that
case, $P(\data\given h_i)$ is 1 if $h_i$ is consistent and 0 otherwise.
Looking at , we see that $\hmap$ will then be the

simplest logical theory that is consistent with the data.

Therefore, maximum *a posteriori* learning provides a
natural embodiment of Ockham’s razor.

Another insight into the tradeoff between complexity and degree of fit
is obtained by taking the logarithm of . Choosing $\hmap$ to maximize
$P(\data\given h_i) P(h_i)$ is equivalent to minimizing
$$-\log_2 P(\data\given h_i) - \log_2 P(h_i)\ .$$ Using the connection
between information encoding and probability that we introduced in , we
see that the ${-}\,\log_2 P(h_i)$ term equals the number of bits
required to specify the hypothesis $h_i$. Furthermore,
${-}\,\log_2 P(\data\given h_i)$ is the additional number of bits
required to specify the data, given the hypothesis. (To see this,
consider that no bits are required if the hypothesis predicts the data
exactly—as with $h_5$ and the string of lime candies—and
$\log_2 1\eq 0$.) Hence, MAP learning is choosing the hypothesis that
provides maximum *compression* of the data. The same task
is addressed more directly by the , or MDL, learning
method[MAP-MDL-page]. Whereas MAP learning expresses simplicity by
assigning higher probabilities to simpler hypotheses, MDL expresses it
directly by counting the bits in a binary encoding of the hypotheses and
data.

A final simplification is provided by assuming a prior over the space of
hypotheses. In that case, MAP learning reduces to choosing an $h_i$ that
maximizes $P(\data\given h_i)$. This is called a (ML) hypothesis,
$\hml$. Maximum-likelihood learning is very common in statistics, a
discipline in which many researchers distrust the subjective nature of
hypothesis priors. It is a reasonable approach when there is no reason
to prefer one hypothesis over another *a priori*—for
example, when all hypotheses are equally complex. It provides a good
approximation to Bayesian and MAP learning when the data set is large,
because the data swamps the prior distribution over hypotheses, but it
has problems (as we shall see) with small data sets.

Learning with Complete Data {#parametric-learning-section}
---------------------------

The general task of learning a probability model, given data that are
assumed to be generated from that model, is called . (The term applied
originally to probability density functions for continuous variables,
but is used now for discrete distributions too.)

This section covers the simplest case, where we have . Data are complete
when each data point contains values for every variable in the
probability model being learned. We focus on —finding the numerical
parameters for a probability model whose structure is fixed. For
example, we might be interested in learning the conditional
probabilities in a Bayesian network with a given structure. We will also
look briefly at the problem of learning structure and at nonparametric
density estimation.

### Maximum-likelihood parameter learning: Discrete models

Suppose we buy a bag of lime and cherry candy from a new manufacturer
whose lime–cherry proportions are completely unknown; the fraction could
be anywhere between 0 and 1. In that case, we have a continuum of
hypotheses. The in this case, which we call $\theta$, is the proportion
of cherry candies, and the hypothesis is $h_{\theta}$. (The proportion
of limes is just $1-\theta$.) If we assume that all proportions are
equally likely *a priori*, then a maximum-likelihood
approach is reasonable. If we model the situation with a Bayesian
network, we need just one random variable, ${Flavor}$ (the flavor of a
randomly chosen candy from the bag). It has values ${cherry}$ and
${lime}$, where the probability of ${cherry}$ is $\theta$ (see (a)).
Now suppose we unwrap $\Ncount$ candies, of which $c$ are cherries and
$\ell\eq \Ncount-c$ are limes. According to , the likelihood of this
particular data set is
$$P(\data \given h_{\theta}) = \prod_{j\eq 1}^{\Ncount} P(\datum_j\given h_{\theta}) =
  \theta^c\cdot (1-\theta)^{\ell}\ .$$ The maximum-likelihood hypothesis
is given by the value of $\theta$ that maximizes this expression. The
same value is obtained by maximizing the [log-likelihood-page],
$$L(\data \given h_{\theta}) = \log P(\data \given h_{\theta}) = \sum_{j\eq 1}^{\Ncount} \log P(\datum_j\given h_{\theta}) =
  c\log\theta +  \ell\log(1-\theta)\ .$$ (By taking logarithms, we
reduce the product to a sum over the data, which is usually easier to
maximize.) To find the maximum-likelihood value of $\theta$, we
differentiate $L$ with respect to $\theta$ and set the resulting
expression to zero:
$$\frac{dL(\data \given h_{\theta})}{d\theta} = \frac{c}{\theta} -
  \frac{\ell}{1-\theta} = 0 \qquad \implies \quad \theta =
  \frac{c}{c+\ell} = \frac{c}{\Ncount}\ .$$ In English, then, the
maximum-likelihood hypothesis $\hml$ asserts that the actual proportion
of cherries in the bag is equal to the observed proportion in the
candies unwrapped so far!

It appears that we have done a lot of work to discover the obvious. In
fact, though, we have laid out one standard method for
maximum-likelihood parameter learning, a method with broad
applicability:

1.  Write down an expression for the likelihood of the data as a
    function of the parameter(s).

2.  Write down the derivative of the log likelihood with respect to each
    parameter.

3.  Find the parameter values such that the derivatives are zero.

The trickiest step is usually the last. In our example, it was trivial,
but we will see that in many cases we need to resort to iterative
solution algorithms or other numerical optimization techniques, as
described in . The example also illustrates a significant problem with
maximum-likelihood learning in general:

when the data set is small enough that some events have not yet been
observed—for instance, no cherry candies—the maximum-likelihood
hypothesis assigns zero probability to those events.

Various tricks are used to avoid this problem, such as initializing the
counts for each event to 1 instead of 0.

[ml-networks-figure]

Let us look at another example. Suppose this new candy manufacturer
wants to give a little hint to the consumer and uses candy wrappers
colored red and green. The ${Wrapper}$ for each candy is selected
*probabilistically*, according to some unknown conditional
distribution, depending on the flavor. The corresponding probability
model is shown in (b). Notice that it has three parameters: $\theta$,
$\theta_1$, and $\theta_2$. With these parameters, the likelihood of
seeing, say, a cherry candy in a green wrapper can be obtained from the
standard semantics for Bayesian networks ():

$$\begin{aligned}
\lefteqn{P({Flavor}\eq {cherry},{Wrapper}\eq {green}\given h_{\theta,\theta_1,\theta_2})}\\
 & = & P({Flavor}\eq {cherry}\given h_{\theta,\theta_1,\theta_2})P({Wrapper}\eq
 {green}\given {Flavor}\eq {cherry},h_{\theta,\theta_1,\theta_2}) \\
 & = & \theta \cdot (1-\theta_1)\ .\end{aligned}$$

Now we unwrap $\Ncount$ candies, of which $c$ are cherries and $\ell$
are limes. The wrapper counts are as follows: $r_c$ of the cherries have
red wrappers and $g_c$ have green, while $r_{\ell}$ of the limes have
red and $g_{\ell}$ have green. The likelihood of the data is given by
$$P(\data\given h_{\theta,\theta_1,\theta_2}) = 
  \theta^c (1-\theta)^{\ell} \cdot \theta_1^{r_c}(1-\theta_1)^{g_c}
  \cdot \theta_2^{r_{\ell}}(1-\theta_2)^{g_{\ell}}\ .$$ This looks
pretty horrible, but taking logarithms helps:

$$L =  [c\log \theta + \ell\log (1-\theta) ] + 
     [r_c\log \theta_1 + g_c\log(1-\theta_1) ] + 
     [r_{\ell}\log \theta_2 + g_{\ell}\log(1-\theta_2) ]\ .$$

The benefit of taking logs is clear: the log likelihood is the sum of
three terms, each of which contains a single parameter. When we take
derivatives with respect to each parameter and set them to zero, we get
three independent equations, each containing just one parameter:
$$\begin{array}{rclcl}
\frac{\partial L}{\partial\theta} &=& \frac{c}{\theta} - \frac{\ell}{1-\theta} = 0                  & \qquad \implies & \theta = \frac{c}{c+\ell} \\
\frac{\partial L}{\partial\theta_1} &=& \frac{r_c}{\theta_1} - \frac{g_c}{1-\theta_1} = 0           & \qquad \implies & \theta_1 = \frac{r_c}{r_c+g_c} \\
\frac{\partial L}{\partial\theta_2} &=& \frac{r_{\ell}}{\theta_2} - \frac{g_{\ell}}{1-\theta_2} = 0 & \qquad \implies & \theta_2 = \frac{r_{\ell}}{r_{\ell}+g_{\ell}}\ .
\end{array}$$ The solution for $\theta$ is the same as before. The
solution for $\theta_1$, the probability that a cherry candy has a red
wrapper, is the observed fraction of cherry candies with red wrappers,
and similarly for $\theta_2$.

These results are very comforting, and it is easy to see that they can
be extended to any Bayesian network whose conditional probabilities are
represented as tables. The most important point is that,

with complete data, the maximum-likelihood parameter learning problem
for a Bayesian network decomposes into separate learning problems, one
for each parameter.

(See for the nontabulated case, where each parameter affects several
conditional probabilities.) The second point is that the parameter
values for a variable, given its parents, are just the observed
frequencies of the variable values for each setting of the parent
values. As before, we must be careful to avoid zeroes when the data set
is small.

### Naive Bayes models

Probably the most common Bayesian network model used in machine learning
is the model first introduced on . In this model, the “class” variable
$C$ (which is to be predicted) is the root and the “attribute” variables
$X_i$ are the leaves. The model is “naive” because it assumes that the
attributes are conditionally independent of each other, given the class.
(The model in (b) is a naive Bayes model with class ${Flavor}$ and
just one attribute, .) Assuming Boolean variables, the parameters are
$$\theta\eq P(C\eq {true}), \theta_{i1}\eq P(X_i\eq {true}\given C\eq {true}),
\theta_{i2}\eq P(X_i\eq {true}\given C\eq {false}).$$ The
maximum-likelihood parameter values are found in exactly the same way as
for (b). Once the model has been trained in this way, it can be used to
classify new examples for which the class variable $C$ is unobserved.
With observed attribute values $x_1,\ldots,x_n$, the probability of each
class is given by
$$\pv(C\given x_1,\ldots,x_n) = \alpha\; \pv(C)\prod_i \pv(x_i\given C)\ .$$
A deterministic prediction can be obtained by choosing the most likely
class. shows the learning curve for this method when it is applied to
the restaurant problem from . The method learns fairly well but not as
well as decision-tree learning; this is presumably because the true
hypothesis—which is a decision tree—is not representable exactly using a
naive Bayes model. Naive Bayes learning turns out to do surprisingly
well in a wide range of applications; the boosted version () is one of
the most effective general-purpose learning algorithms. Naive Bayes
learning scales well to very large problems: with $n$ Boolean
attributes, there are just $2n+1$ parameters, and

no search is required to find $\hml$, the maximum-likelihood naive Bayes
hypothesis.

Finally, naive Bayes learning systems have no difficulty with noisy or
missing data and can give probabilistic predictions when appropriate.

[restaurant-naive-bayes-curve-figure]

### Maximum-likelihood parameter learning: Continuous models

Continuous probability models such as the model were introduced in .
Because continuous variables are ubiquitous in real-world applications,
it is important to know how to learn the parameters of continuous models
from data. The principles for maximum-likelihood learning are identical
in the continuous and discrete cases.

Let us begin with a very simple case: learning the parameters of a
Gaussian density function on a single variable. That is, the data are
generated as follows:
$$P(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \ .$$
The parameters of this model are the mean $\mu$ and the standard
deviation $\sigma$. (Notice that the normalizing “constant” depends on
$\sigma$, so we cannot ignore it.) Let the observed values be
$x_1,\ldots,x_{\Ncount}$. Then the log likelihood is
$$L = \sum_{j\eq 1}^{\Ncount} \log \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x_j-\mu)^2}{2\sigma^2}}
    = \Ncount(-\log\sqrt{2\pi}-\log\sigma) - \sum_{j\eq 1}^{\Ncount}
    \frac{(x_j-\mu)^2}{2\sigma^2}\ .$$ Setting the derivatives to zero
as usual, we obtain

$$\begin{array}{rclcl}
\frac{\partial L}{\partial\mu} &=& -\frac{1}{\sigma^2}\sum_{j=1}^{\Ncount} (x_j-\mu) = 0
                  & \qquad \implies & \mu = \frac{\sum_j x_j}{\Ncount} \\
\frac{\partial L}{\partial\sigma} &=& -\frac{\Ncount}{\sigma}+\frac{1}{\sigma^3}\sum_{j=1}^{\Ncount} (x_j-\mu)^2 = 0
                  & \qquad \implies & \sigma = \sqrt{\frac{\sum_j (x_j-\mu)^2}{\Ncount}}\ .
\end{array}
\label{ml-gaussian-equation}$$

That is, the maximum-likelihood value of the mean is the sample average
and the maximum-likelihood value of the standard deviation is the square
root of the sample variance. Again, these are comforting results that
confirm “commonsense” practice.

[regression-figure]

Now consider a linear Gaussian model with one continuous parent $X$ and
a continuous child $Y$. As explained on , $Y$ has a Gaussian
distribution whose mean depends linearly on the value of $X$ and whose
standard deviation is fixed. To learn the conditional distribution
$P(Y\given X)$, we can maximize the conditional likelihood

$$P(y\given x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(y-(\theta_1 x+\theta_2))^2}{2\sigma^2}} \ .
\label{linear-gaussian-likelihood-equation}$$

Here, the parameters are $\theta_1$, $\theta_2$, and $\sigma$. The data
are a collection of $(x_j,y_j)$ pairs, as illustrated in . Using the
usual methods (), we can find the maximum-likelihood values of the
parameters. The point here is different. If we consider just the
parameters $\theta_1$ and $\theta_2$ that define the linear relationship
between $x$ and $y$, it becomes clear that maximizing the log likelihood
with respect to these parameters is the same as
*minimizing* the numerator $(y-(\theta_1 x +\theta_2))^2$
in the exponent of . This is the $L_2$ loss, the squared error between
the actual value $y$ and the prediction $\theta_1 x+\theta_2$. This is
the quantity minimized by the standard procedure described in . Now we
can understand why: minimizing the sum of squared errors gives the
maximum-likelihood straight-line model, *provided that the data
are generated with Gaussian noise of fixed
variance*.[regression-page]

### Bayesian parameter learning

Maximum-likelihood learning gives rise to some very simple procedures,
but it has some serious deficiencies with small data sets. For example,
after seeing one cherry candy, the maximum-likelihood hypothesis is that
the bag is 100% cherry (i.e., $\theta\eq
1.0$). Unless one’s hypothesis prior is that bags must be either all
cherry or all lime, this is not a reasonable conclusion. It is more
likely that the bag is a mixture of lime and cherry. The Bayesian
approach to parameter learning starts by defining a prior probability
distribution over the possible hypotheses. We call this the . Then, as
data arrives, the posterior probability distribution is updated.

[beta-distributions-figure]

The candy example in (a) has one parameter, $\theta$: the probability
that a randomly selected piece of candy is cherry-flavored. In the
Bayesian view, $\theta$ is the (unknown) value of a random variable
$\Theta$ that defines the hypothesis space; the hypothesis prior is just
the prior distribution $\pv(\Theta)$. Thus, $P(\Theta\eq \theta)$ is the
prior probability that the bag has a fraction $\theta$ of cherry
candies.

If the parameter $\theta$ can be any value between 0 and 1, then
$\pv(\Theta)$ must be a continuous distribution that is nonzero only
between 0 and 1 and that integrates to 1. The uniform density
$P(\theta)={Uniform}[0,1](\theta)$ is one candidate. (See .) It turns
out that the uniform density is a member of the family of . Each beta
distribution is defined by two [^3] $a$ and $b$ such that

$$\BetaDist[a,b](\theta) = \alpha\; \theta^{a-1}(1-\theta)^{b-1}\ ,
\label{beta-equation}$$

for $\theta$ in the range $[0,1]$. The normalization constant $\alpha$,
which makes the distribution integrate to 1, depends on $a$ and $b$.
(See .) shows what the distribution looks like for various values of $a$
and $b$. The mean value of the distribution is $a/(a+b)$, so larger
values of $a$ suggest a belief that $\Theta$ is closer to 1 than to 0.
Larger values of $a+b$ make the distribution more peaked, suggesting
greater certainty about the value of $\Theta$. Thus, the beta family
provides a useful range of possibilities for the hypothesis prior.

Besides its flexibility, the beta family has another wonderful property:
if $\Theta$ has a prior $\BetaDist[a,b]$, then, after a data point is
observed, the posterior distribution for $\Theta$ is also a beta
distribution. In other words, $\BetaDist$ is closed under update. The
beta family is called the for the family of distributions for a Boolean
variable.[^4] Let’s see how this works. Suppose we observe a cherry
candy; then we have

$$\begin{aligned}
  P(\theta\given \Datum_1\eq {cherry}) & = & \alpha\; P(\Datum_1\eq {cherry} \given  \theta) P(\theta) \\
                               & = & \alpha'\; \theta \cdot \BetaDist[a,b](\theta)
                                 = \alpha'\; \theta \cdot \theta^{a-1}(1-\theta)^{b-1} \\
                               & = & \alpha'\; \theta^{a}(1-\theta)^{b-1} = \BetaDist[a+1,b](\theta)\ .\end{aligned}$$

Thus, after seeing a cherry candy, we simply increment the $a$ parameter
to get the posterior; similarly, after seeing a lime candy, we increment
the $b$ parameter. Thus, we can view the $a$ and $b$ hyperparameters as
, in the sense that a prior $\BetaDist[a,b]$ behaves exactly as if we
had started out with a uniform prior $\BetaDist[1,1]$ and seen $a-1$
actual cherry candies and $b-1$ actual lime candies.

By examining a sequence of beta distributions for increasing values of
$a$ and $b$, keeping the proportions fixed, we can see vividly how the
posterior distribution over the parameter $\Theta$ changes as data
arrive. For example, suppose the actual bag of candy is 75% cherry. (b)
shows the sequence $\BetaDist[3,1]$, $\BetaDist[6,2]$,
$\BetaDist[{30},{10}]$. Clearly, the distribution is converging to a
narrow peak around the true value of $\Theta$. For large data sets,
then, Bayesian learning (at least in this case) converges to the same
answer as maximum-likelihood learning.

[bayesian-learning-network-figure]

Now let us consider a more complicated case. The network in (b) has
three parameters, $\theta$, $\theta_1$, and $\theta_2$, where $\theta_1$
is the probability of a red wrapper on a cherry candy and $\theta_2$ is
the probability of a red wrapper on a lime candy. The Bayesian
hypothesis prior must cover all three parameters—that is, we need to
specify $\pv(\Theta,\Theta_1,\Theta_2)$. Usually, we assume :
$$\pv(\Theta,\Theta_1,\Theta_2) = \pv(\Theta)\pv(\Theta_1)\pv(\Theta_2)\ .$$
With this assumption, each parameter can have its own beta distribution
that is updated separately as data arrive. shows how we can incorporate
the hypothesis prior and any data into one Bayesian network. The nodes
$\Theta,\Theta_1,\Theta_2$ have no parents. But each time we make an
observation of a wrapper and corresponding flavor of a piece of candy,
we add a node ${Flavor}_i$, which is dependent on the flavor parameter
$\Theta$:
$$P({Flavor}{}_i\eq {cherry} \given  \Theta\eq\theta) = \theta\ .$$
We also add a node ${Wrapper}_i$, which is dependent on $\Theta_1$ and
$\Theta_2$:

P(~i~ ~i~,~11~) = ~1~\
P(~i~ ~i~,~22~) = ~2~ .

Now, the entire Bayesian learning process can be formulated as an
*inference* problem. We add new evidence nodes, then query
the unknown nodes (in this case, $\Theta, \Theta_1, \Theta_2$). This
formulation of learning and prediction makes it clear that Bayesian
learning requires no extra “principles of learning.” Furthermore,

there is, in essence, just one learning algorithm

—the inference algorithm for Bayesian networks. Of course, the nature of
these networks is somewhat different from those of because of the
potentially huge number of evidence variables representing the training
set and the prevalence of continuous-valued parameter variables.

### Learning Bayes net structures {#complete-data-structure-learning-section}

So far, we have assumed that the structure of the Bayes net is given and
we are just trying to learn the parameters. The structure of the network
represents basic causal knowledge about the domain that is often easy
for an expert, or even a naive user, to supply. In some cases, however,
the causal model may be unavailable or subject to dispute—for example,
certain corporations have long claimed that smoking does not cause
cancer—so it is important to understand how the structure of a Bayes net
can be learned from data. This section gives a brief sketch of the main
ideas.

The most obvious approach is to *search* for a good model.
We can start with a model containing no links and begin adding parents
for each node, fitting the parameters with the methods we have just
covered and measuring the accuracy of the resulting model.
Alternatively, we can start with an initial guess at the structure and
use hill-climbing or simulated annealing search to make modifications,
retuning the parameters after each change in the structure.
Modifications can include reversing, adding, or deleting links. We must
not introduce cycles in the process, so many algorithms assume that an
ordering is given for the variables, and that a node can have parents
only among those nodes that come earlier in the ordering (just as in the
construction process in ). For full generality, we also need to search
over possible orderings.

There are two alternative methods for deciding when a good structure has
been found. The first is to test whether the conditional independence
assertions implicit in the structure are actually satisfied in the data.
For example, the use of a naive Bayes model for the restaurant problem
assumes that
$$\pv({Fri}/{Sat}, {Bar} \given  {WillWait}) = \pv({Fri}/{Sat} \given  {WillWait})
  \pv({Bar} \given  {WillWait})$$ and we can check in the data that
the same equation holds between the corresponding conditional
frequencies. But even if the structure describes the true causal nature
of the domain, statistical fluctuations in the data set mean that the
equation will never be satisfied *exactly*, so we need to
perform a suitable statistical test to see if there is sufficient
evidence that the independence hypothesis is violated. The complexity of
the resulting network will depend on the threshold used for this
test—the stricter the independence test, the more links will be added
and the greater the danger of overfitting.

An approach more consistent with the ideas in this chapter is to assess
the degree to which the proposed model explains the data (in a
probabilistic sense). We must be careful how we measure this, however.
If we just try to find the maximum-likelihood hypothesis, we will end up
with a fully connected network, because adding more parents to a node
cannot decrease the likelihood (). We are forced to penalize model
complexity in some way. The MAP (or MDL) approach simply subtracts a
penalty from the likelihood of each structure (after parameter tuning)
before comparing different structures. The Bayesian approach places a
joint prior over structures and parameters. There are usually far too
many structures to sum over (superexponential in the number of
variables), so most practitioners use MCMC to sample over structures.

Penalizing complexity (whether by MAP or Bayesian methods) introduces an
important connection between the optimal structure and the nature of the
representation for the conditional distributions in the network. With
tabular distributions, the complexity penalty for a node’s distribution
grows exponentially with the number of parents, but with, say, noisy-OR
distributions, it grows only linearly. This means that learning with
noisy-OR (or other compactly parameterized) models tends to produce
learned structures with more parents than does learning with tabular
distributions.

### Density estimation with nonparametric models

It is possible to learn a probability model without making any
assumptions about its structure and parameterization by adopting the
nonparametric methods of . The task of is typically done in continuous
domains, such as that shown in (a). The figure shows a probability
density function on a space defined by two continuous variables. In (b)
we see a sample of data points from this density function. The question
is, can we recover the model from the samples?

First we will consider $k$- models. (In we saw nearest-neighbor models
for classification and regression; here we see them for density
estimation.) Given a sample of data points, to estimate the unknown
probability density at a query point $\x$ we can simply measure the
density of the data points in the neighborhood of $\x$. (b) shows two
query points (small squares). For each query point we have drawn the
smallest circle that encloses 10 neighbors—the 10-nearest-neighborhood.
We can see that the central circle is large, meaning there is a low
density there, and the circle on the right is small, meaning there is a
high density there. In we show three plots of density estimation using
$k$-nearest-neighbors, for different values of $k$. It seems clear that
(b) is about right, while (a) is too spiky ($k$ is too small) and (c) is
too smooth ($k$ is too big).

[knn-circles-figure]

[mixture-knn-figure]

[mixture-kernel-figure]

Another possibility is to use , as we did for locally weighted
regression. To apply a kernel model to density estimation, assume that
each data point generates its own little density function, using a
Gaussian kernel. The estimated density at a query point $\x$ is then the
average density as given by each kernel function:
$$P(\x) = \frac{1}{N} \sum_{j=1}^{N} \kernel(\x,\x_j)\ .$$ We will
assume spherical Gaussians with standard deviation $w$ along each axis:
$$\kernel(\x,\x_j) = \frac{1}{(w^2\sqrt{2\pi})^d}
    e^{- \frac{D(\sx,\sx_j)^2}{2w^2}} \ ,$$ where $d$ is the number of
dimensions in $\x$ and $D$ is the Euclidean distance function. We still
have the problem of choosing a suitable value for kernel width $w$;
shows values that are too small, just right, and too large. A good value
of $w$ can be chosen by using cross-validation.

Learning with Hidden Variables: The EM Algorithm {#em-section}
------------------------------------------------

The preceding section dealt with the fully observable case. Many
real-world problems have (sometimes called ), which are not observable
in the data that are available for learning. For example, medical
records often include the observed symptoms, the physician’s diagnosis,
the treatment applied, and perhaps the outcome of the treatment, but
they seldom contain a direct observation of the disease itself! (Note
that the *diagnosis* is not the *disease*; it
is a causal consequence of the observed symptoms, which are in turn
caused by the disease.) One might ask, “If the disease is not observed,
why not construct a model without it?” The answer appears in , which
shows a small, fictitious diagnostic model for heart disease. There are
three observable predisposing factors and three observable symptoms
(which are too depressing to name). Assume that each variable has three
possible values (e.g., ${none}$, ${moderate}$, and ${severe}$).
Removing the hidden variable from the network in (a) yields the network
in (b); the total number of parameters increases from 78 to 708. Thus,

latent variables can dramatically reduce the number of parameters
required to specify a Bayesian network.

This, in turn, can dramatically reduce the amount of data needed to
learn the parameters.

[313-comparison-figure]

Hidden variables are important, but they do complicate the learning
problem. In (a), for example, it is not obvious how to learn the
conditional distribution for ${HeartDisease}$, given its parents,
because we do not know the value of ${HeartDisease}$ in each case; the
same problem arises in learning the distributions for the symptoms. This
section describes an algorithm called , or EM, that solves this problem
in a very general way. We will show three examples and then provide a
general description. The algorithm seems like magic at first, but once
the intuition has been developed, one can find applications for EM in a
huge range of learning problems.

### Unsupervised clustering: Learning mixtures of Gaussians {#clustering-section}

is the problem of discerning multiple categories in a collection of
objects. The problem is unsupervised because the category labels are not
given. For example, suppose we record the spectra of a hundred thousand
stars; are there different *types* of stars revealed by the
spectra, and, if so, how many types and what are their characteristics?
We are all familiar with terms such as “red giant” and “white dwarf,”
but the stars do not carry these labels on their hats—astronomers had to
perform unsupervised clustering to identify these categories. Other
examples include the identification of species, genera, orders, and so
on in the Linnæan taxonomy and the creation of natural
kinds for ordinary objects (see ).

Unsupervised clustering begins with data. (b) shows 500 data points,
each of which specifies the values of two continuous attributes. The
data points might correspond to stars, and the attributes might
correspond to spectral intensities at two particular frequencies. Next,
we need to understand what kind of probability distribution might have
generated the data. Clustering presumes that the data are generated from
a , $P$. Such a distribution has $k$ , each of which is a distribution
in its own right. A data point is generated by first choosing a
component and then generating a sample from that component. Let the
random variable $C$ denote the component, with values $1,\ldots,k$; then
the mixture distribution is given by
$$P(\x) = \sum_{i\eq1}^k P(C\eq i) \; P(\x\given C\eq i)\ ,$$ where $\x$
refers to the values of the attributes for a data point. For continuous
data, a natural choice for the component distributions is the
multivariate Gaussian, which gives the so-called family of
distributions. The parameters of a mixture of Gaussians are
$w_i\eq P(C\eq i)$ (the weight of each component), $\bmu_i$ (the mean of
each component), and $\bSigma_i$ (the covariance of each component). (a)
shows a mixture of three Gaussians; this mixture is in fact the source
of the data in (b) as well as being the model shown in (a) on .

[mixture-EM-figure]

The unsupervised problem, then, is to recover a mixture model like the
one in (a) from raw data like that in (b). Clearly, if we
*knew* which component generated each data point, then it
would be easy to recover the component Gaussians: we could just select
all the data points from a given component and then apply (a
multivariate version of) () for fitting the parameters of a Gaussian to
a set of data. On the other hand, if we *knew* the
parameters of each component, then we could, at least in a probabilistic
sense, assign each data point to a component. The problem is that we
know neither the assignments nor the parameters.

The basic idea of EM in this context is to *pretend* that
we know the parameters of the model and then to infer the probability
that each data point belongs to each component. After that, we refit the
components to the data, where each component is fitted to the entire
data set with each point weighted by the probability that it belongs to
that component. The process iterates until convergence. Essentially, we
are “completing” the data by inferring probability distributions over
the hidden variables—which component each data point belongs to—based on
the current model. For the mixture of Gaussians, we initialize the
mixture-model parameters arbitrarily and then iterate the following two
steps:

1.  **E-step**: Compute the probabilities
    $p_{ij}\eq P(C\eq i \given  \x_j)$, the probability that datum
    $\x_j$ was generated by component $i$. By Bayes’ rule, we have
    $p_{ij}\eq \alpha P(\x_j \given  C\eq i) P(C\eq i)$. The term
    $P(\x_j \given  C\eq i)$ is just the probability at $\x_j$ of the
    $i$th Gaussian, and the term $P(C\eq i)$ is just the weight
    parameter for the $i$th Gaussian. Define $n_i\eq \sum_j p_{ij}$, the
    effective number of data points currently assigned to component $i$.

2.  **M-step**: Compute the new mean, covariance, and
    component weights using the following steps in sequence:

    $$\begin{aligned}
    \bmu_i & \leftarrow & \sum_j p_{ij}\x_j/n_i\\
    \bSigma_i & \leftarrow & \sum_j p_{ij}(\x_j-\bmu_i)(\x_j-\bmu_i)\transpose/n_i\\
    w_i & \leftarrow & n_i/N\end{aligned}$$

where $N$ is the total number of data points. The E-step, or
*expectation* step, can be viewed as computing the expected
values $p_{ij}$ of the hidden $Z_{ij}$, where $Z_{ij}$ is 1 if datum
$\x_j$ was generated by the $i$th component and 0 otherwise. The M-step,
or *maximization* step, finds the new values of the
parameters that maximize the log likelihood of the data, given the
expected values of the hidden indicator variables.

The final model that EM learns when it is applied to the data in (a) is
shown in (c); it is virtually indistinguishable from the original model
from which the data were generated. (a) plots the log likelihood of the
data according to the current model as EM progresses.

There are two points to notice. First, the log likelihood for the final
learned model slightly *exceeds* that of the original
model, from which the data were generated. This might seem surprising,
but it simply reflects the fact that the data were generated randomly
and might not provide an exact reflection of the underlying model. The
second point is that

EM increases the log likelihood of the data at every iteration.

This fact can be proved in general. Furthermore, under certain
conditions (that hold in ost cases), EM can be proven to reach a local
maximum in likelihood. (In rare cases, it could reach a saddle point or
even a local minimum.) In this sense, EM resembles a gradient-based
hill-climbing algorithm, but notice that it has no “step size”
parameter.

[mixture-progress-figure]

Things do not always go as well as (a) might suggest. It can happen, for
example, that one Gaussian component shrinks so that it covers just a
single data point. Then its variance will go to zero and its likelihood
will go to infinity! Another problem is that two components can “merge,”
acquiring identical means and variances and sharing their data points.
These kinds of degenerate local maxima are serious problems, especially
in high dimensions. One solution is to place priors on the model
parameters and to apply the MAP version of EM. Another is to restart a
component with new random parameters if it gets too small or too close
to another component. Sensible initialization also helps.

### Learning Bayesian networks with hidden variables

[mixture-networks-figure]

To learn a Bayesian network with hidden variables, we apply the same
insights that worked for mixtures of Gaussians. represents a situation
in which there are two bags of candies that have been mixed together.
Candies are described by three features: in addition to the ${Flavor}$
and the ${Wrapper}$, some candies have a ${Hole}$ in the middle and
some do not. The distribution of candies in each bag is described by a
model: the features are independent, given the bag, but the conditional
probability distribution for each feature depends on the bag. The
parameters are as follows: $\theta$ is the prior probability that a
candy comes from Bag 1; $\theta_{F1}$ and $\theta_{F2}$ are the
probabilities that the flavor is cherry, given that the candy comes from
Bag 1 or Bag 2 respectively; $\theta_{W1}$ and $\theta_{W2}$ give the
probabilities that the wrapper is red; and $\theta_{H1}$ and
$\theta_{H2}$ give the probabilities that the candy has a hole. Notice
that the overall model is a mixture model. (In fact, we can also model
the mixture of Gaussians as a Bayesian network, as shown in (b).) In the
figure, the bag is a hidden variable because, once the candies have been
mixed together, we no longer know which bag each candy came from. In
such a case, can we recover the descriptions of the two bags by
observing candies from the mixture? Let us work through an iteration of
EM for this problem. First, let’s look at the data. We generated 1000
samples from a model whose true parameters are as follows:

$$\theta\eq {0.5},\ \;\theta_{F1}\eq \theta_{W1}\eq \theta_{H1}\eq {0.8},\ \;\theta_{F2}\eq \theta_{W2}\eq \theta_{H2}\eq {0.3}\ . 
\label{candy-true-equation}$$

That is, the candies are equally likely to come from either bag; the
first is mostly cherries with red wrappers and holes; the second is
mostly limes with green wrappers and no holes. The counts for the eight
possible kinds of candy are as follows:

[candy-counts-page]

We start by initializing the parameters. For numerical simplicity, we
arbitrarily choose[^5]

$$\theta^{(0)}\eq {0.6},\ \;\theta_{F1}^{(0)}\eq \theta_{W1}^{(0)}\eq \theta_{H1}^{(0)}\eq {0.6},\ \;\theta_{F2}^{(0)}\eq \theta_{W2}^{(0)} \eq \theta_{H2}^{(0)}\eq {0.4}\ . 
\label{candy-64-equation}$$

First, let us work on the $\theta$ parameter. In the fully observable
case, we would estimate this directly from the *observed*
counts of candies from bags 1 and 2. Because the bag is a hidden
variable, we calculate the *expected* counts instead. The
expected count $\hat{\Ncount}({Bag}\eq 1)$ is the sum, over all
candies, of the probability that the candy came from bag 1:
$$\theta^{(1)} = \hat{\Ncount}({Bag}\eq 1)/N = \sum_{j\eq 1}^{\Ncount} P({Bag}\eq 1\given {flavor}{}_j,{wrapper}{}_j,{holes}{}_j)/N\ .$$
These probabilities can be computed by any inference algorithm for
Bayesian networks. For a model such as the one in our example, we can do
the inference “by hand,” using Bayes’ rule and applying conditional
independence: $$\theta^{(1)} = \frac{1}{N} \sum_{j\eq 1}^{\Ncount} 
         \frac{P({\scriptstyle {flavor}{}_j\given {Bag}\eq 1})P({\scriptstyle {wrapper}{}_j\given {Bag}\eq 1})P({\scriptstyle {holes}{}_j\given {Bag}\eq 1})P({\scriptstyle {Bag}\eq 1})}
              {\sum_i P({\scriptstyle {flavor}{}_j\given {Bag}\eq i})P({\scriptstyle {wrapper}{}_j\given {Bag}\eq i})P({\scriptstyle {holes}{}_j\given {Bag}\eq i})P({\scriptstyle {Bag}\eq i})}\ .$$
Applying this formula to, say, the 273 red-wrapped cherry candies with
holes, we get a contribution of
$$\frac{{273}}{{1000}}\cdot\frac{\theta_{F1}^{(0)} \theta_{W1}^{(0)} \theta_{H1}^{(0)} \theta^{(0)}}
                              {\theta_{F1}^{(0)} \theta_{W1}^{(0)} \theta_{H1}^{(0)} \theta^{(0)} +
                               \theta_{F2}^{(0)} \theta_{W2}^{(0)} \theta_{H2}^{(0)} (1- \theta^{(0)})}
 \approx {0.22797}\ .$$ Continuing with the other seven kinds of candy
in the table of counts, we obtain $\theta^{(1)}\eq {0.6124}$.

Now let us consider the other parameters, such as $\theta_{F1}$. In the
fully observable case, we would estimate this directly from the
*observed* counts of cherry and lime candies from bag 1.
The *expected* count of cherry candies from bag 1 is given
by
$$\sum_{j: {Flavor}{}_j\eq {cherry}} \!\!\!\!\!\!\!\! P({Bag}\eq 1 \given  {Flavor}{}_j\eq {cherry},{wrapper}{}_j,{holes}{}_j)\ .$$
Again, these probabilities can be calculated by any Bayes net algorithm.
Completing this process, we obtain the new values of all the parameters:

$$\begin{array}{l}
\theta^{(1)}\eq {0.6124},\ \theta_{F1}^{(1)}\eq {0.6684},\ \theta_{W1}^{(1)}\eq {0.6483},\ \theta_{H1}^{(1)}\eq {0.6558}, \\
                       \theta_{F2}^{(1)}\eq {0.3887},
 \theta_{W2}^{(1)} \eq {0.3817},\ \theta_{H2}^{(1)}\eq {0.3827}\ . 
\end{array}
\label{candy-64-update-equation}$$

The log likelihood of the data increases from about $-{2044}$ initially
to about $-{2021}$ after the first iteration, as shown in (b). That is,
the update improves the likelihood itself by a factor of about
$e^{{23}}\approx {10}^{{10}}$. By the tenth iteration, the learned model
is a better fit than the original model ($L\eq
-{1982.214}$). Thereafter, progress becomes very slow. This is not
uncommon with EM, and many practical systems combine EM with a
gradient-based algorithm such as Newton–Raphson (see ) for the last
phase of learning.

The general lesson from this example is that

the parameter updates for Bayesian network learning with hidden
variables are directly available from the results of inference on each
example. Moreover, only *local* posterior probabilities are
needed for each parameter.

Here, “local” means that the CPT for each variable $X_i$ can be learned
from posterior probabilities involving just $X_i$ and its parents
$\U_i$. Defining $\theta_{ijk}$ to be the CPT parameter
$P(X_i\eq x_{ij} \given  \U_i\eq \u_{ik})$, the update is given by the
normalized expected counts as follows:
$$\theta_{ijk} \leftarrow \hat{\Ncount}(X_i\eq x_{ij},\U_i\eq \u_{ik}) / \hat{\Ncount}(\U_i\eq \u_{ik})\ .$$
The expected counts are obtained by summing over the examples, computing
the probabilities $P(X_i\eq x_{ij},\U_i\eq \u_{ik})$ for each by using
any Bayes net inference algorithm. For the exact algorithms—including
variable elimination—all these probabilities are obtainable directly as
a by-product of standard inference, with no need for extra computations
specific to learning. Moreover, the information needed for learning is
available *locally* for each parameter.

### Learning hidden Markov models

Our final application of EM involves learning the transition
probabilities in hidden Markov models (HMMs). Recall from that a hidden
Markov model can be represented by a dynamic Bayes net with a single
discrete state variable, as illustrated in . Each data point consists of
an observation *sequence* of finite length, so the problem
is to learn the transition probabilities from a set of observation
sequences (or from just one long sequence).

[dbn-unrolling-repeat-figure]

We have already worked out how to learn Bayes nets, but there is one
complication: in Bayes nets, each parameter is distinct; in a hidden
Markov model, on the other hand, the individual transition probabilities
from state $i$ to state $j$ at time $t$,
$\theta_{ijt}\eq P(X_{t+1}\eq j \given  X_{t}\eq i)$, are
*repeated* across time—that is,
$\theta_{ijt} \eq \theta_{ij}$ for all $t$. To estimate the transition
probability from state $i$ to state $j$, we simply calculate the
expected proportion of times that the system undergoes a transition to
state $j$ when in state $i$:
$$\theta_{ij} \leftarrow \sum_t \hat{\Ncount}(X_{t+1}\eq j, X_{t}\eq i) / \sum_t \hat{\Ncount}(X_{t}\eq i)\ .$$
The expected counts are computed by an HMM inference algorithm. The
algorithm shown in can be modified very easily to compute the necessary
probabilities. One important point is that the probabilities required
are obtained by rather than ; that is, we need to pay attention to
subsequent evidence in estimating the probability that a particular
transition occurred. The evidence in a murder case is usually obtained
*after* the crime (i.e., the transition from state $i$ to
state $j$) has taken place.

### The general form of the EM algorithm

We have seen several instances of the EM algorithm. Each involves
computing expected values of hidden variables for each example and then
recomputing the parameters, using the expected values as if they were
observed values. Let $\x$ be all the observed values in all the
examples, let $\Z$ denote all the hidden variables for all the examples,
and let $\btheta$ be all the parameters for the probability model. Then
the EM algorithm is
$$\btheta^{(i+1)} = \argmax_{\sbtheta} \sum_{\sz} P(\Z\eq \z \given  \x, \btheta^{(i)})
                 L(\x , \Z\eq \z\given  \btheta)\ .$$ This equation is
the EM algorithm in a nutshell. The E-step is the computation of the
summation, which is the expectation of the log likelihood of the
“completed” data with respect to the distribution
$P(\Z\eq \z \given  \x, \btheta^{(i)})$, which is the posterior over the
hidden variables, given the data. The M-step is the maximization of this
expected log likelihood with respect to the parameters. For mixtures of
Gaussians, the hidden variables are the $Z_{ij}$s, where $Z_{ij}$ is 1
if example $j$ was generated by component $i$. For Bayes nets, $Z_{ij}$
is the value of unobserved variable $X_i$ in example $j$. For HMMs,
$Z_{jt}$ is the state of the sequence in example $j$ at time $t$.
Starting from the general form, it is possible to derive an EM algorithm
for a specific application once the appropriate hidden variables have
been identified.

As soon as we understand the general idea of EM, it becomes easy to
derive all sorts of variants and improvements. For example, in many
cases the E-step—the computation of posteriors over the hidden
variables—is intractable, as in large Bayes nets. It turns out that one
can use an *approximate* E-step and still obtain an
effective learning algorithm. With a sampling algorithm such as MCMC
(see ), the learning process is very intuitive: each state
(configuration of hidden and observed variables) visited by MCMC is
treated exactly as if it were a complete observation. Thus, the
parameters can be updated directly after each MCMC transition. Other
forms of approximate inference, such as variational and loopy methods,
have also proved effective for learning very large networks.

### Learning Bayes net structures with hidden variables

In , we discussed the problem of learning Bayes net structures with
complete data. When unobserved variables may be influencing the data
that are observed, things get more difficult. In the simplest case, a
human expert might tell the learning algorithm that certain hidden
variables exist, leaving it to the algorithm to find a place for them in
the network structure. For example, an algorithm might try to learn the
structure shown in (a) on , given the information that
${HeartDisease}$ (a three-valued variable) should be included in the
model. As in the complete-data case, the overall algorithm has an outer
loop that searches over structures and an inner loop that fits the
network parameters given the structure.

If the learning algorithm is not told which hidden variables exist, then
there are two choices: either pretend that the data is really
complete—which may force the algorithm to learn a parameter-intensive
model such as the one in (b)—or *invent* new hidden
variables in order to simplify the model. The latter approach can be
implemented by including new modification choices in the structure
search: in addition to modifying links, the algorithm can add or delete
a hidden variable or change its arity. Of course, the algorithm will not
know that the new variable it has invented is called ${HeartDisease}$;
nor will it have meaningful names for the values. Fortunately, newly
invented hidden variables will usually be connected to preexisting
variables, so a human expert can often inspect the local conditional
distributions involving the new variable and ascertain its meaning.

As in the complete-data case, pure maximum-likelihood structure learning
will result in a completely connected network (moreover, one with no
hidden variables), so some form of complexity penalty is required. We
can also apply MCMC to sample many possible network structures, thereby
approximating Bayesian learning. For example, we can learn mixtures of
Gaussians with an unknown number of components by sampling over the
number; the approximate posterior distribution for the number of
Gaussians is given by the sampling frequencies of the MCMC process.

For the complete-data case, the inner loop to learn the parameters is
very fast—just a matter of extracting conditional frequencies from the
data set. When there are hidden variables, the inner loop may involve
many iterations of EM or a gradient-based algorithm, and each iteration
involves the calculation of posteriors in a Bayes net, which is itself
an NP-hard problem. To date, this approach has proved impractical for
learning complex models. One possible improvement is the so-called
algorithm, which operates in much the same way as ordinary (parametric)
EM except that the algorithm can update the structure as well as the
parameters. Just as ordinary EM uses the current parameters to compute
the expected counts in the E-step and then applies those counts in the
M-step to choose new parameters, structural EM uses the current
structure to compute expected counts and then applies those counts in
the M-step to evaluate the likelihood for potential new structures.
(This contrasts with the outer-loop/inner-loop method, which computes
new expected counts for each potential structure.) In this way,
structural EM may make several structural alterations to the network
without once recomputing the expected counts, and is capable of learning
nontrivial Bayes net structures. Nonetheless, much work remains to be
done before we can say that the structure-learning problem is solved.

Statistical learning methods range from simple calculation of averages
to the construction of complex models such as Bayesian networks. They
have applications throughout computer science, engineering,
computational biology, neuroscience, psychology, and physics. This
chapter has presented some of the basic ideas and given a flavor of the
mathematical underpinnings. The main points are as follows:

-   methods formulate learning as a form of probabilistic inference,
    using the observations to update a prior distribution over
    hypotheses. This approach provides a good way to implement Ockham’s
    razor, but quickly becomes intractable for complex hypothesis
    spaces.

-   (MAP) learning selects a single most likely hypothesis given the
    data. The hypothesis prior is still used and the method is often
    more tractable than full Bayesian learning.

-   learning simply selects the hypothesis that maximizes the likelihood
    of the data; it is equivalent to MAP learning with a uniform prior.
    In simple cases such as linear regression and fully observable
    Bayesian networks, maximum-likelihood solutions can be found easily
    in closed form. learning is a particularly effective technique that
    scales well.

-   When some variables are hidden, local maximum likelihood solutions
    can be found using the EM algorithm. Applications include clustering
    using mixtures of Gaussians, learning Bayesian networks, and
    learning hidden Markov models.

-   Learning the structure of Bayesian networks is an example of . This
    usually involves a discrete search in the space of structures. Some
    method is required for trading off model complexity against degree
    of fit.

-   represent a distribution using the collection of data points. Thus,
    the number of parameters grows with the training set.
    Nearest-neighbors methods look at the examples nearest
    to the point in question, whereas methods form a distance-weighted
    combination of all the examples.

Statistical learning continues to be a very active area of research.
Enormous strides have been made in both theory and practice, to the
point where it is possible to learn almost any model for which exact or
approximate inference is feasible.

The application of statistical learning techniques in AI was an active
area of research in the early years \<see\>Duda+Hart:1973
but became separated from mainstream AI as the latter field concentrated
on symbolic methods. A resurgence of interest occurred shortly after the
introduction of Bayesian network models in the late 1980s; at roughly
the same time, a statistical view of neural network learning began to
emerge. In the late 1990s, there was a noticeable convergence of
interests in machine learning, statistics, and neural networks, centered
on methods for creating large probabilistic models from data.

The naive Bayes model is one of the oldest and simplest forms of
Bayesian network, dating back to the 1950s. Its origins were mentioned
in . Its surprising success is partially explained by . A boosted form
of naive Bayes learning won the first KDD Cup data mining
competition @Elkan:1997. gives an excellent introduction to the general
problem of Bayes net learning. Bayesian parameter learning with
Dirichlet priors for Bayesian networks was discussed by . The software
package @Gilks+al:1994 incorporates many of these ideas and provides a
very powerful tool for formulating and learning complex probability
models. The first algorithms for learning Bayes net structures used
conditional independence tests @Pearl:1988 [@Pearl+Verma:1991].
developed a comprehensive approach embodied in the  package for Bayes
net learning. Algorithmic improvements since then led to a clear victory
in the 2001 KDD Cup data mining competition for a Bayes net learning
method @Cheng+al:2002. (The specific task here was a bioinformatics
problem with 139,351 features!) A structure-learning approach based on
maximizing likelihood was developed by and improved by . Several
algorithmic advances since that time have led to quite respectable
performance in the complete-data case @Moore+Wong:2003
[@Teyssier+Koller:2005]. One important component is an efficient data
structure, the AD-tree, for caching counts over all possible
combinations of variables and values @Moore+Lee:1997. pointed out the
influence of the representation of local conditional distributions on
the learned structure.

The general problem of learning probability models with hidden variables
and missing data was addressed by , who described the general idea of
what was later called EM and gave several examples. Further impetus came
from the Baum–Welch algorithm for HMM learning @Baum+Petrie:1966, which
is a special case of EM. The paper by Dempster, Laird, and
Rubin [-@Dempster+al:1977], which presented the EM algorithm in general
form and analyzed its convergence, is one of the most cited papers in
both computer science and statistics. (Dempster himself views EM as a
schema rather than an algorithm, since a good deal of mathematical work
may be required before it can be applied to a new family of
distributions.) devote an entire book to the algorithm and its
properties. The specific problem of learning mixture models, including
mixtures of Gaussians, is covered by . Within AI, the first successful
system that used EM for mixture modeling was @Cheeseman+al:1988
[@Cheeseman+Stutz:1996]. has been applied to a number of real-world
scientific classification tasks, including the discovery of new types of
stars from spectral data @Goebel+al:1989 and new classes of proteins and
introns in DNA/protein sequence databases @Hunter+States:1992.

For maximum-likelihood parameter learning in Bayes nets with hidden
variables, EM and gradient-based methods were introduced around the same
time by , , and . The structural EM algorithm was developed by and
applied to maximum-likelihood learning of Bayes net structures with
latent variables. . describe Bayesian structure learning.

The ability to learn the structure of Bayesian networks is closely
connected to the issue of recovering *causal* information
from data. That is, is it possible to learn Bayes nets in such a way
that the recovered network structure indicates real causal influences?
For many years, statisticians avoided this question, believing that
observational data (as opposed to data generated from experimental
trials) could yield only correlational information—after all, any two
variables that appear related might in fact be influenced by a third,
unknown causal factor rather than influencing each other directly. has
presented convincing arguments to the contrary, showing that there are
in fact many cases where causality can be ascertained and developing the
formalism to express causes and the effects of intervention as well as
ordinary conditional probabilities.

Nonparametric density estimation, also called density estimation, was
investigated initially by and . Since that time, a huge literature has
developed investigating the properties of various estimators. gives a
thorough introduction. There is also a rapidly growing literature on
nonparametric Bayesian methods, originating with the seminal work of on
the , which can be thought of as a distribution over Dirichlet
distributions. These methods are particularly useful for mixtures with
unknown numbers of components. and provide useful tutorials on the many
applications of these ideas to statistical learning. The text by covers
the , which gives a way of defining prior distributions over the space
of continuous functions.

The material in this chapter brings together work from the fields of
statistics and pattern recognition, so the story has been told many
times in many ways. Good texts on Bayesian statistics include those by ,
, and . and provide an excellent introduction to statistical machine
learning. For pattern classification, the classic text for many years
has been , now updated @Duda+al:2001. The annual NIPS (Neural
Information Processing Conference) conference, whose proceedings are
published as the series *Advances in Neural Information Processing
Systems*, is now dominated by Bayesian papers. Papers on learning
Bayesian networks also appear in the *Uncertainty in AI*
and *Machine Learning* conferences and in several
statistics conferences. Journals specific to neural networks include
*Neural Computation*, *Neural Networks*, and
the *IEEE Transactions on Neural Networks*. Specifically
Bayesian venues include the Valencia International Meetings on Bayesian
Statistics and the journal *Bayesian Analysis*.

[bayes-candy-exercise] The data used for on can be viewed as being
generated by $h_5$. For each of the other four hypotheses, generate a
data set of length 100 and plot the corresponding graphs for
$P(h_i\given \datum_1,\ldots,\datum_N)$ and
$P(\Datum_{N+1}\eq {lime}\given \datum_1,\ldots,\datum_N)$. Comment on
your results.

Repeat , this time plotting the values of
$P(\Datum_{N+1}\eq {lime}\given \hmap)$ and
$P(\Datum_{N+1}\eq {lime}\given \hml)$.

[candy-trade-exercise] Suppose that Ann’s utilities for cherry and lime
candies are $c_A$ and $\ell_A$, whereas Bob’s utilities are $c_B$ and
$\ell_B$. (But once Ann has unwrapped a piece of candy, Bob won’t buy
it.) Presumably, if Bob likes lime candies much more than Ann, it would
be wise for Ann to sell her bag of candies once she is sufficiently sure
of its lime content. On the other hand, if Ann unwraps too many candies
in the process, the bag will be worth less. Discuss the problem of
determining the optimal point at which to sell the bag. Determine the
expected utility of the optimal procedure, given the prior distribution
from .

Two statisticians go to the doctor and are both given the same
prognosis: A 40% chance that the problem is the deadly disease $A$, and
a 60% chance of the fatal disease $B$. Fortunately, there are anti-$A$
and anti-$B$ drugs that are inexpensive, 100% effective, and free of
side-effects. The statisticians have the choice of taking one drug,
both, or neither. What will the first statistician (an avid Bayesian)
do? How about the second statistician, who always uses the maximum
likelihood hypothesis?

The doctor does some research and discovers that disease $B$ actually
comes in two versions, dextro-$B$ and levo-$B$, which are equally likely
and equally treatable by the anti-$B$ drug. Now that there are three
hypotheses, what will the two statisticians do?

[BNB-exercise] Explain how to apply the boosting method of to naive
Bayes learning. Test the performance of the resulting algorithm on the
restaurant learning problem.

[linear-regression-exercise] Consider $N$ data points $(x_j,y_j)$, where
the $y_j$s are generated from the $x_j$s according to the linear
Gaussian model in . Find the values of $\theta_1$, $\theta_2$, and
$\sigma$ that maximize the conditional log likelihood of the data.

[noisy-OR-ML-exercise] Consider the noisy-OR model for fever described
in . Explain how to apply maximum-likelihood learning to fit the
parameters of such a model to a set of complete data.
(*Hint*: use the chain rule for partial derivatives.)

[beta-integration-exercise] This exercise investigates properties of the
Beta distribution defined in .

1.  By integrating over the range $[0,1]$, show that the normalization
    constant for the distribution $\BetaDist[a,b]$ is given by
    $\alpha = \Gamma(a+b)/\Gamma(a)\Gamma(b)$ where $\Gamma(x)$ is the ,
    defined by $\Gamma(x+1)\eq x\cdot\Gamma(x)$ and $\Gamma(1)\eq 1$.
    (For integer $x$, $\Gamma(x+1)\eq x!$.)

2.  Show that the mean is $a/(a+b)$.

3.  Find the mode(s) (the most likely value(s) of $\theta$).

4.  Describe the distribution $\BetaDist[\epsilon,\epsilon]$ for very
    small $\epsilon$. What happens as such a distribution is updated?

[ML-parents-exercise] Consider an arbitrary Bayesian network, a complete
data set for that network, and the likelihood for the data set according
to the network. Give a simple proof that the likelihood of the data
cannot decrease if we add a new link to the network and recompute the
maximum-likelihood parameter values.

Consider a single Boolean random variable $Y$ (the “classification”).
Let the prior probability $P(Y\eq true)$ be $\pi$. Let’s try to find
$\pi$, given a training set $D\eq (y_1,\ldots,y_N)$ with $N$ independent
samples of $Y$. Furthermore, suppose $p$ of the $N$ are positive and $n$
of the $N$ are negative.

1.  Write down an expression for the likelihood of $D$ (i.e., the
    probability of seeing this particular sequence of examples, given a
    fixed value of $\pi$) in terms of $\pi$, $p$, and $n$.

2.  By differentiating the log likelihood $L$, find the value of $\pi$
    that maximizes the likelihood.

3.  Now suppose we add in $k$ Boolean random variables
    $X_1, X_2,\ldots,X_k$ (the “attributes”) that describe each sample,
    and suppose we assume that the attributes are conditionally
    independent of each other given the goal $Y$. Draw the Bayes net
    corresponding to this assumption.

4.  Write down the likelihood for the data including the attributes,
    using the following additional notation:

    -   $\alpha_i$ is $P(X_i\eq true | Y\eq true)$.

    -   $\beta_i$ is $P(X_i\eq true | Y\eq false)$.

    -   $p_i^+$ is the count of samples for which $X_i\eq true$ and
        $Y\eq true$.

    -   $n_i^+$ is the count of samples for which $X_i\eq false$ and
        $Y\eq true$.

    -   $p_i^-$ is the count of samples for which $X_i\eq true$ and
        $Y\eq false$.

    -   $n_i^-$ is the count of samples for which $X_i\eq false$ and
        $Y\eq false$.

    [*Hint*: consider first the probability of seeing a
    single example with specified values for $X_1, X_2,\ldots,X_k$ and
    $Y$.]

5.  By differentiating the log likelihood $L$, find the values of
    $\alpha_i$ and $\beta_i$ (in terms of the various counts) that
    maximize the likelihood and say in words what these values
    represent.

6.  Let $k = 2$, and consider a data set with 4 all four possible
    examples of thexor function. Compute the maximum
    likelihood estimates of $\pi$, $\alpha_1$, $\alpha_2$, $\beta_1$,
    and $\beta_2$.

7.  Given these estimates of $\pi$, $\alpha_1$, $\alpha_2$, $\beta_1$,
    and $\beta_2$, what are the posterior probabilities
    $P(Y\eq true | x_1,x_2)$ for each example?

Consider the application of EM to learn the parameters for the network
in (a), given the true parameters in .

1.  Explain why the EM algorithm would not work if there were just two
    attributes in the model rather than three.

2.  Show the calculations for the first iteration of EM starting from .

3.  What happens if we start with all the parameters set to the same
    value $p$? (*Hint*: you may find it helpful to
    investigate this empirically before deriving the general result.)

4.  Write out an expression for the log likelihood of the tabulated
    candy data on in terms of the parameters, calculate the partial
    derivatives with respect to each parameter, and investigate the
    nature of the fixed point reached in part (c).

[^1]: Statistically sophisticated readers will recognize this scenario
    as a variant of the setup. We find urns and balls less compelling
    than candy; furthermore, candy lends itself to other tasks, such as
    deciding whether to trade the bag with a friend—see .

[^2]: We stated earlier that the bags of candy are very large;
    otherwise, the i.i.d. assumption fails to hold. Technically, it is
    more correct (but less hygienic) to rewrap each candy after
    inspection and return it to the bag.

[^3]: They are called hyperparameters because they parameterize a
    distribution over $\theta$, which is itself a parameter.

[^4]: Other conjugate priors include the family for the parameters of a
    discrete multivalued distribution and the family for the parameters
    of a Gaussian distribution. See .

[^5]: It is better in practice to choose them randomly, to avoid local
    maxima due to symmetry.
Making Complex Decisions {#complex-decisions-chapter}
========================

In this chapter, we address the computational issues involved in making
decisions in a stochastic environment. Whereas was concerned with
one-shot or episodic decision problems, in which the utility of each
action’s outcome was well known, we are concerned here with , in which
the agent’s utility depends on a sequence of decisions. Sequential
decision problems incorporate utilities, uncertainty, and sensing, and
include search and planning problems as special cases. explains how
sequential decision problems are defined, and
Sections [value-iteration-section] and [policy-iteration-section]
explain how they can be solved to produce optimal behavior that balances
the risks and rewards of acting in an uncertain environment. extends
these ideas to the case of partially observable environments, and
develops a complete design for decision-theoretic agents in partially
observable environments, combining dynamic Bayesian networks from with
decision networks from .

The second part of the chapter covers environments with multiple agents.
In such environments, the notion of optimal behavior is complicated by
the interactions among the agents. introduces the main ideas of ,
including the idea that rational agents might need to behave randomly.
looks at how multiagent systems can be designed so that multiple agents
can achieve a common goal.

Sequential Decision Problems {#mdp-section}
----------------------------

[sequential-decision-world-figure]

Suppose that an agent is situated in the $4\stimes 3$ environment shown
in (a). Beginning in the start state, it must choose an action at each
time step. The interaction with the environment terminates when the
agent reaches one of the goal states, marked +1 or
–1. Just as for search problems, the actions available to
the agent in each state are given by $\noprog{Actions}(s)$, sometimes
abbreviated to $A(s)$; in the $4\stimes 3$ environment, the actions in
every state are *Up*, *Down*,
*Left*, and *Right*. We assume for now that
the environment is , so that the agent always knows where it is.

If the environment were deterministic, a solution would be easy:
[*Up, Up, Right, Right, Right*]. Unfortunately, the
environment won’t always go along with this solution, because the
actions are unreliable. The particular model of stochastic motion that
we adopt is illustrated in (b). Each action achieves the intended effect
with probability 0.8, but the rest of the time, the action moves the
agent at right angles to the intended direction. Furthermore, if the
agent bumps into a wall, it stays in the same square. For example, from
the start square (1,1), the action *Up* moves the agent to
(1,2) with probability 0.8, but with probability 0.1, it moves right to
(2,1), and with probability 0.1, it moves left, bumps into the wall, and
stays in (1,1). In such an environment, the sequence
$[{Up},{Up},{Right},{Right},{Right}]$ goes up around the
barrier and reaches the goal state at (4,3) with probability
${0.8}^5 \eq {0.32768}$. There is also a small chance of accidentally
reaching the goal by going the other way around with probability
${0.1}^4 \times {0.8}$, for a grand total of 0.32776. (See also .)

As in , the (or just “model,” whenever no confusion can arise) describes
the outcome of each action in each state. Here, the outcome is
stochastic, so we write $\transprob{s}{a}{s'}$ to denote the probability
of reaching state $s'$ if action $a$ is done in state $s$. We will
assume that transitions are in the sense of , that is, the probability
of reaching $s'$ from $s$ depends only on $s$ and not on the history of
earlier states. For now, you can think of $\transprob{s}{a}{s'}$ as a
big three-dimensional table containing probabilities. Later, in , we
will see that the transition model can be represented as a , just as in
.

To complete the definition of the task environment, we must specify the
utility function for the agent. Because the decision problem is
sequential, the utility function will depend on a sequence of states—an
—rather than on a single state. Later in this section, we investigate
how such utility functions can be specified in general; for now, we
simply stipulate that in each state $s$, the agent receives a $R(s)$,
which may be positive or negative, but must be bounded. For our
particular example, the reward is $- {0.04}$ in all states except the
terminal states (which have rewards +1 and –1). The utility of an
environment history is just (for now) the *sum* of the
rewards received. For example, if the agent reaches the +1 state after
10 steps, its total utility will be 0.6. The negative reward of –0.04
gives the agent an incentive to reach (4,3) quickly, so our environment
is a stochastic generalization of the search problems of . Another way
of saying this is that the agent does not enjoy living in this
environment and so wants to leave as soon as possible.

To sum up: a sequential decision problem for a fully observable,
stochastic environment with a Markovian transition model and additive
rewards is called a , or , and consists of a set of states (with an
initial state $s_0$); a set $\noprog{Actions}(s)$ of actions in each
state; a transition model $\transprob{s}{a}{s'}$; and a reward function
$R(s)$.[^1]

The next question is, what does a solution to the problem look like? We
have seen that any fixed action sequence won’t solve the problem,
because the agent might end up in a state other than the goal.
Therefore, a solution must specify what the agent should do for
*any* state that the agent might reach. A solution of this
kind is called a . It is traditional to denote a policy by $\pi$, and
$\pi(s)$ is the action recommended by the policy $\pi$ for state $s$. If
the agent has a complete policy, then no matter what the outcome of any
action, the agent will always know what to do next.

Each time a given policy is executed starting from the initial state,
the stochastic nature of the environment may lead to a different
environment history. The quality of a policy is therefore measured by
the *expected* utility of the possible environment
histories generated by that policy. An is a policy that yields the
highest expected utility. We use $\pistar$ to denote an optimal policy.
Given $\pistar$, the agent decides what to do by consulting its current
percept, which tells it the current state $s$, and then executing the
action $\pistar(s)$. A policy represents the agent function explicitly
and is therefore a description of a simple reflex agent, computed from
the information used for a utility-based agent.

An optimal policy for the world of is shown in (a). Notice that, because
the cost of taking a step is fairly small compared with the penalty for
ending up in (4,2) by accident, the optimal policy for the state (3,1)
is conservative. The policy recommends taking the long way round, rather
than taking the shortcut and thereby risking entering (4,2).

The balance of risk and reward changes depending on the value of $R(s)$
for the nonterminal states. (b) shows optimal policies for four
different ranges of $R(s)$. When $R(s) \le
-{1.6284}$, life is so painful that the agent heads straight for the
nearest exit, even if the exit is worth –1. When $-{0.4278} \le R(s)
\le -{0.0850}$, life is quite unpleasant; the agent takes the shortest
route to the +1 state and is willing to risk falling into the –1 state
by accident. In particular, the agent takes the shortcut from (3,1).
When life is only slightly dreary ($-{0.0221} < R(s)< 0$), the optimal
policy takes *no risks at all*. In (4,1) and (3,2), the
agent heads directly away from the –1 state so that it cannot fall in by
accident, even though this means banging its head against the wall quite
a few times. Finally, if $R(s)>0$, then life is positively enjoyable and
the agent avoids *both* exits. As long as the actions in
(4,1), (3,2), and (3,3) are as shown, every policy is optimal, and the
agent obtains infinite total reward because it never enters a terminal
state. Surprisingly, it turns out that there are six other optimal
policies for various ranges of $R(s)$; asks you to find them.

[sequential-decision-policies-figure]

The careful balancing of risk and reward is a characteristic of MDPs
that does not arise in deterministic search problems; moreover, it is a
characteristic of many real-world decision problems. For this reason,
MDPs have been studied in several fields, including AI, operations
research, economics, and control theory. Dozens of algorithms have been
proposed for calculating optimal policies. In
sections [value-iteration-section] and [policy-iteration-section] we
describe two of the most important algorithm families. First, however,
we must complete our investigation of utilities and policies for
sequential decision problems.

### Utilities over time

In the MDP example in , the performance of the agent was measured by a
sum of rewards for the states visited. This choice of performance
measure is not arbitrary, but it is not the only possibility for the
utility function on environment histories, which we write as
$U_h([s_0,s_1,\ldots,s_n])$. Our analysis draws on () and is somewhat
technical; the impatient reader may wish to skip to the next section.

The first question to answer is whether there is a or an for decision
making. A finite horizon means that there is a *fixed* time
$N$ after which nothing matters—the game is over, so to speak. Thus,
$U_h([s_0,s_1,\ldots,s_{N+k}])\eq U_h([s_0,s_1,\ldots,s_{N}])$ for all
$k>0$. For example, suppose an agent starts at (3,1) in the $4\stimes 3$
world of , and suppose that $N\eq
3$. Then, to have any chance of reaching the +1 state, the agent must
head directly for it, and the optimal action is to go *Up*.
On the other hand, if $N\eq {100}$, then there is plenty of time to take
the safe route by going *Left*.

So, with a finite horizon, the optimal action in a given state could
change over time.

We say that the optimal policy for a finite horizon is . With no fixed
time limit, on the other hand, there is no reason to behave differently
in the same state at different times. Hence, the optimal action depends
only on the current state, and the optimal policy is . Policies for the
infinite-horizon case are therefore simpler than those for the
finite-horizon case, and we deal mainly with the infinite-horizon case
in this chapter. (We will see later that for partially observable
environments, the infinite-horizon case is not so simple.) Note that
“infinite horizon” does not necessarily mean that all state sequences
are infinite; it just means that there is no fixed deadline. In
particular, there can be finite state sequences in an infinite-horizon
MDP containing a terminal state.

The next question we must decide is how to calculate the utility of
state sequences. In the terminology of multiattribute utility theory,
each state $s_i$ can be viewed as an of the state sequence
$[s_0,s_1,s_2\ldots]$. To obtain a simple expression in terms of the
attributes, we will need to make some sort of preference-independence
assumption. The most natural assumption is that the agent’s preferences
between state sequences are [preference-stationarity-page]. Stationarity
for preferences means the following: if two state sequences
$[s_0,s_1,s_2,\ldots]$ and $[s_0',s_1',s_2',\ldots]$ begin with the same
state (i.e., $s_0 \eq
s_0'$), then the two sequences should be preference-ordered the same way
as the sequences $[s_1,s_2,\ldots]$ and $[s_1',s_2',\ldots]$. In
English, this means that if you prefer one future to another starting
tomorrow, then you should still prefer that future if it were to start
today instead. Stationarity is a fairly innocuous-looking assumption
with very strong consequences: it turns out that under stationarity
there are just two coherent ways to assign utilities to sequences:

1.  : The utility of a state sequence is
    $$U_h([s_0,s_1,s_2,\ldots]) = R(s_0) + R(s_1) + R(s_2) + \cdots \ .$$
    The $4\stimes 3$ world in uses additive rewards. Notice that
    additivity was used implicitly in our use of path cost functions in
    heuristic search algorithms ().

2.  : The utility of a state sequence is
    $$U_h([s_0,s_1,s_2,\ldots]) = R(s_0) + \gamma R(s_1) + \gamma^2 R(s_2) + \cdots\ ,$$
    where the $\gamma$ is a number between 0 and 1. The discount factor
    describes the preference of an agent for current rewards over future
    rewards. When $\gamma$ is close to 0, rewards in the distant future
    are viewed as insignificant. When $\gamma$ is 1, discounted rewards
    are exactly equivalent to additive rewards, so additive rewards are
    a special case of discounted rewards. Discounting appears to be a
    good model of both animal and human preferences over time. A
    discount factor of $\gamma$ is equivalent to an interest rate of
    $(1/\gamma) - 1$.

For reasons that will shortly become clear, we assume discounted rewards
in the remainder of the chapter, although sometimes we allow
$\gamma\eq 1$.

Lurking beneath our choice of infinite horizons is a problem: if the
environment does not contain a terminal state, or if the agent never
reaches one, then all environment histories will be infinitely long, and
utilities with additive, undiscounted rewards will generally be
infinite. While we can agree that$+\infty$ is better than $-\infty$,
comparing two state sequences with $+\infty$ utility is more difficult.
There are three solutions, two of which we have seen already:

1.  With discounted rewards, the utility of an infinite sequence is
    *finite*. In fact, if $\gamma<1$ and rewards are
    bounded by $\pm \Rmax$, we have

    $$U_h([s_0,s_1,s_2,\ldots]) = \sum_{t\eq 0}^\infty \gamma^t R(s_t) \leq
     \sum_{t\eq 0}^\infty \gamma^t \Rmax = \Rmax/(1-\gamma)\ ,
    \label{rmax-equation}$$

    using the standard formula for the sum of an infinite geometric
    series.

2.  If the environment contains terminal states *and if the agent
    is guaranteed to get to one eventually*, then we will never
    need to compare infinite sequences. A policy that is guaranteed to
    reach a terminal state is called a . With proper policies, we can
    use $\gamma\eq 1$ (i.e., additive rewards). The first three policies
    shown in (b) are proper, but the fourth is improper. It gains
    infinite total reward by staying away from the terminal states when
    the reward for the nonterminal states is positive. The existence of
    improper policies can cause the standard algorithms for solving MDPs
    to fail with additive rewards, and so provides a good reason for
    using discounted rewards.

3.  Infinite sequences can be compared in terms of the obtained per time
    step. Suppose that square (1,1) in the $4\stimes 3$ world has a
    reward of 0.1 while the other nonterminal states have a reward of
    0.01. Then a policy that does its best to stay in (1,1) will have
    higher average reward than one that stays elsewhere. Average reward
    is a useful criterion for some problems, but the analysis of
    average-reward algorithms is beyond the scope of this book.

In sum, discounted rewards present the fewest difficulties in evaluating
state sequences.

### Optimal policies and the utilities of states {#optimal-policy-section}

Having decided that the utility of a given state sequence is the sum of
discounted rewards obtained during the sequence, we can compare policies
by comparing the *expected* utilities obtained when
executing them. We assume the agent is in some initial state $s$ and
define $S_t$ (a random variable) to be the state the agent reaches at
time $t$ when executing a particular policy $\pi$. (Obviously,
$S_0\eq s$, the state the agent is in now.) The probability distribution
over state sequences $S_1,S_2,\ldots,$ is determined by the initial
state $s$, the policy $\pi$, and the transition model for the
environment.

The expected utility obtained by executing $\pi$ starting in $s$ is
given by

$$U^{\pi}(s) = E\left[\sum_{t\eq 0}^\infty \gamma^t R(S_t) \right]\ ,
\label{utility-definition-equation}$$

where the expectation is with respect to the probability distribution
over state sequences determined by $s$ and $\pi$. Now, out of all the
policies the agent could choose to execute starting in $s$, one (or
more) will have higher expected utilities than all the others. We’ll use
$\pistar_s$ to denote one of these policies:

$$\pistar_s = \argmax_{\pi} U^{\pi}(s)\ .$$

Remember that $\pistar_s$ is a policy, so it recommends an action for
every state; its connection with $s$ in particular is that it’s an
optimal policy when $s$ is the starting state. A remarkable consequence
of using discounted utilities with infinite horizons is that the optimal
policy is *independent* of the starting state. (Of course,
the *action sequence* won’t be independent; remember that a
policy is a function specifying an action for each state.) This fact
seems intuitively obvious: if policy $\pistar_a$ is optimal starting in
$a$ and policy $\pistar_b$ is optimal starting in $b$, then, when they
reach a third state $c$, there’s no good reason for them to disagree
with each other, or with $\pistar_c$, about what to do next.[^2] So we
can simply write $\pistar$ for an optimal policy.

Given this definition, the true utility of a state is just
$U^{\pistar}(s)$—that is, the expected sum of discounted rewards if the
agent executes an optimal policy. We write this as $U(s)$, matching the
notation used in for the utility of an outcome. Notice that $U(s)$ and
$R(s)$ are quite different quantities; $R(s)$ is the “short term” reward
for being in $s$, whereas $U(s)$ is the “long term” total reward from
$s$ onward. shows the utilities for the $4\stimes 3$ world. Notice that
the utilities are higher for states closer to the +1 exit, because fewer
steps are required to reach the exit.

[sequential-decision-values-figure]

The utility function $U(s)$ allows the agent to select actions by using
the principle of maximum expected utility from —that is, choose the
action that maximizes the expected utility of the subsequent state:

$$\pistar(s) = \argmax_{a\in A(s)} \sum\limits_{s'} \transprob{s}{a}{s'} U(s')\ .
\label{optimal-policy-equation}$$

The next two sections describe algorithms for finding optimal policies.

Value Iteration {#value-iteration-section}
---------------

In this section, we present an algorithm, called , for calculating an
optimal policy. The basic idea is to calculate the utility of each state
and then use the state utilities to select an optimal action in each
state.

### The Bellman equation for utilities

defined the utility of being in a state as the expected sum of
discounted rewards from that point onwards. From this, it follows that
there is a direct relationship between the utility of a state and the
utility of its neighbors:

the utility of a state is the immediate reward for that state plus the
expected discounted utility of the next state, assuming that the agent
chooses the optimal action.

That is, the utility of a state is given by

$$U(s) = R(s) + \gamma\, \max_{a\in A(s)} \sum\limits_{s'} \transprob{s}{a}{s'} U(s')\ .
\label{utility-dp-equation}$$

This is called the , after Richard Bellman [-@Bellman:1957]. The
utilities of the states—defined by as the expected utility of subsequent
state sequences—are solutions of the set of Bellman equations. In fact,
they are the *unique* solutions, as we show in .

Let us look at one of the Bellman equations for the $4\stimes 3$ world.
The equation for the state (1,1) is $$\begin{array}{rl}
U(1,1) = -{0.04} + \gamma\,\max[ & {0.8} U(1,2) + {0.1} U(2,1) + {0.1} U(1,1), \hfill\qquad ({Up})\\
                              & {0.9} U(1,1) + {0.1} U(1,2),               \hfill\qquad ({Left})\\
                              & {0.9} U(1,1) + {0.1} U(2,1),               \hfill\qquad ({Down})\\
                            & {0.8} U(2,1) + {0.1} U(1,2) + {0.1} U(1,1)\ ].\hfill\qquad ({Right})
\end{array}$$ When we plug in the numbers from , we find that
*Up* is the best action.

### The value iteration algorithm

The Bellman equation is the basis of the value iteration algorithm for
solving MDPs. If there are $n$ possible states, then there are $n$
Bellman equations, one for each state. The $n$ equations contain $n$
unknowns—the utilities of the states. So we would like to solve these
simultaneous equations to find the utilities. There is one problem: the
equations are *nonlinear*, because the “$\max$” operator is
not a linear operator. Whereas systems of linear equations can be solved
quickly using linear algebra techniques, systems of nonlinear equations
are more problematic. One thing to try is an *iterative*
approach. We start with arbitrary initial values for the utilities,
calculate the right-hand side of the equation, and plug it into the
left-hand side—thereby updating the utility of each state from the
utilities of its neighbors. We repeat this until we reach an
equilibrium. Let $U_i(s)$ be the utility value for state $s$ at the
$i$th iteration. The iteration step, called a , looks like this:

$$U_{i+1}(s) \leftarrow R(s) + \gamma \max_{a\in A(s)} \sum\limits_{s'} \transprob{s}{a}{s'} U_i(s')\ ,
\label{vi-update-equation}$$

where the update is assumed to be applied simultaneously to all the
states at each iteration. If we apply the Bellman update infinitely
often, we are guaranteed to reach an equilibrium (see ), in which case
the final utility values must be solutions to the Bellman equations. In
fact, they are also the *unique* solutions, and the
corresponding policy (obtained using ) is optimal. The algorithm, called
, is shown in .

[value-iteration-algorithm]

[vi-progress+bounds-figure]

We can apply value iteration to the $4\stimes 3$ world in (a). Starting
with initial values of zero, the utilities evolve as shown in (a).
Notice how the states at different distances from (4,3) accumulate
negative reward until a path is found to (4,3), whereupon the utilities
start to increase. We can think of the value iteration algorithm as
*propagating information* through the state space by means
of local updates.

### Convergence of value iteration {#vi-convergence-section}

We said that value iteration eventually converges to a unique set of
solutions of the Bellman equations. In this section, we explain why this
happens. We introduce some useful mathematical ideas along the way, and
we obtain some methods for assessing the error in the utility function
returned when the algorithm is terminated early; this is useful because
it means that we don’t have to run forever. This section is quite
technical.

The basic concept used in showing that value iteration converges is the
notion of a . Roughly speaking, a contraction is a function of one
argument that, when applied to two different inputs in turn, produces
two output values that are “closer together,” by at least some constant
factor, than the original inputs. For example, the function “divide by
two” is a contraction, because, after we divide any two numbers by two,
their difference is halved. Notice that the “divide by two” function has
a fixed point, namely zero, that is unchanged by the application of the
function. From this example, we can discern two important properties of
contractions:

-   A contraction has only one fixed point; if there were two fixed
    points they would not get closer together when the function was
    applied, so it would not be a contraction.

-   When the function is applied to any argument, the value must get
    closer to the fixed point (because the fixed point does not move),
    so repeated application of a contraction always reaches the fixed
    point in the limit.

Now, suppose we view the Bellman update () as an operator $B$ that is
applied simultaneously to update the utility of every state. Let $U_i$
denote the vector of utilities for all the states at the $i$th
iteration. Then the Bellman update equation can be written as
$$U_{i+1} \leftarrow B\,U_i\ .$$ Next, we need a way to measure
distances between utility vectors. We will use the , which measures the
“length” of a vector by the absolute value of its biggest component:
$$||U|| = \max_s |U(s)|\ .$$ With this definition, the “distance”
between two vectors, $||U-U'||$, is the maximum difference between any
two corresponding elements. The main result of this section is the
following:

Let $U_i$ and $U'_i$ be any two utility vectors. Then we have

$$||B\,U_i - B\,U'_i|| \leq \gamma\, ||U_i - U'_i ||\ .
\label{vi-contraction-equation}$$

That is, the Bellman update is a contraction by a factor of $\gamma$ on
the space of utility vectors.

( provides some guidance on proving this claim.) Hence, from the
properties of contractions in general, it follows that value iteration
always converges to a unique solution of the Bellman equations whenever
$\gamma < 1$.

We can also use the contraction property to analyze the
*rate* of convergence to a solution. In particular, we can
replace $U'_i$ in with the *true* utilities $U$, for which
$B\,U\eq U$. Then we obtain the inequality
$$||B\,U_i - U|| \leq \gamma\, ||U_i - U ||\ .$$ So, if we view
$||U_i - U ||$ as the *error* in the estimate $U_i$, we see
that the error is reduced by a factor of at least $\gamma$ on each
iteration. This means that value iteration converges exponentially fast.
We can calculate the number of iterations required to reach a specified
error bound $\epsilon$ as follows: First, recall from that the utilities
of all states are bounded by $\pm \Rmax/(1-\gamma)$. This means that the
maximum initial error $||U_0 - U || \leq 2\Rmax/(1-\gamma)$. Suppose we
run for $N$ iterations to reach an error of at most $\epsilon$. Then,
because the error is reduced by at least $\gamma$ each time, we require
$\gamma^N\cdot 2\Rmax/(1-\gamma) \leq \epsilon$. Taking logs, we find
$$N\eq \ceiling{\log(2\Rmax/\epsilon(1-\gamma))/\log(1/\gamma)}$$
iterations suffice. (b) shows how $N$ varies with $\gamma$, for
different values of the ratio $\epsilon/\Rmax$. The good news is that,
because of the exponentially fast convergence, $N$ does not depend much
on the ratio $\epsilon/\Rmax$. The bad news is that $N$ grows rapidly as
$\gamma$ becomes close to 1. We can get fast convergence if we make
$\gamma$ small, but this effectively gives the agent a short horizon and
could miss the long-term effects of the agent’s actions.

The error bound in the preceding paragraph gives some idea of the
factors influencing the run time of the algorithm, but is sometimes
overly conservative as a method of deciding when to stop the iteration.
For the latter purpose, we can use a bound relating the error to the
size of the Bellman update on any given iteration. From the contraction
property (), it can be shown that if the update is small (i.e., no
state’s utility changes by much), then the error, compared with the true
utility function, also is small. More precisely,

$$\mbox{if}\quad ||U_{i+1} - U_i||<\epsilon(1-\gamma)/\gamma\quad\mbox{then}\quad ||U_{i+1} - U||<\epsilon\ .
\label{vi-termination-equation}$$

This is the termination condition used in the algorithm of .

So far, we have analyzed the error in the utility function returned by
the value iteration algorithm.

What the agent really cares about, however, is how well it will do if it
makes its decisions on the basis of this utility function.

Suppose that after $i$ iterations of value iteration, the agent has an
estimate $U_i$ of the true utility $U$ and obtains the MEU policy
$\pi_i$ based on one-step look-ahead using $U_i$ (as in ). Will the
resulting behavior be nearly as good as the optimal behavior? This is a
crucial question for any real agent, and it turns out that the answer is
yes. $U^{\pi_i}(s)$ is the utility obtained if $\pi_i$ is executed
starting in $s$, and the $||U^{\pi_i} - U||$ is the most the agent can
lose by executing $\pi_i$ instead of the optimal policy $\pistar$. The
policy loss of $\pi_i$ is connected to the error in $U_i$ by the
following inequality:

$$\mbox{if}\quad ||U_i - U||<\epsilon\quad\mbox{then}\quad ||U^{\pi_i} - U||<2\epsilon\gamma/(1-\gamma)\ .
\label{policy-loss-bound-equation}$$

In practice, it often occurs that $\pi_i$ becomes optimal long before
$U_i$ has converged. shows how the maximum error in $U_i$ and the policy
loss approach zero as the value iteration process proceeds for the
$4\stimes 3$ environment with $\gamma\eq {0.9}$. The policy $\pi_i$ is
optimal when $i\eq 4$, even though the maximum error in $U_i$ is still
0.46.

[vi-error+loss-figure]

Now we have everything we need to use value iteration in practice. We
know that it converges to the correct utilities, we can bound the error
in the utility estimates if we stop after a finite number of iterations,
and we can bound the policy loss that results from executing the
corresponding MEU policy. As a final note, all of the results in this
section depend on discounting with $\gamma < 1$. If $\gamma\eq 1$ and
the environment contains terminal states, then a similar set of
convergence results and error bounds can be derived whenever certain
technical conditions are satisfied.

Policy Iteration {#policy-iteration-section}
----------------

In the previous section, we observed that it is possible to get an
optimal policy even when the utility function estimate is inaccurate. If
one action is clearly better than all others, then the exact magnitude
of the utilities on the states involved need not be precise. This
insight suggests an alternative way to find optimal policies. The
algorithm alternates the following two steps, beginning from some
initial policy $\pi_0$:

-   : given a policy $\pi_i$, calculate $U_i\eq U^{\pi_i}$, the utility
    of each state if $\pi_i$ were to be executed.

-   : Calculate a new MEU policy $\pi_{i+1}$, using one-step look-ahead
    based on $U_i$ (as in ).

The algorithm terminates when the policy improvement step yields no
change in the utilities. At this point, we know that the utility
function $U_i$ is a fixed point of the Bellman update, so it is a
solution to the Bellman equations, and $\pi_i$ must be an optimal
policy. Because there are only finitely many policies for a finite state
space, and each iteration can be shown to yield a better policy, policy
iteration must terminate. The algorithm is shown in .

[policy-iteration-algorithm]

The policy improvement step is obviously straightforward, but how do we
implement the routine? It turns out that doing so is much simpler than
solving the standard Bellman equations (which is what value iteration
does), because the action in each state is fixed by the policy. At the
$i$th iteration, the policy $\pi_i$ specifies the action $\pi_i(s)$ in
state $s$. This means that we have a simplified version of the Bellman
equation ([utility-dp-equation]) relating the utility of $s$ (under
$\pi_i$) to the utilities of its neighbors:

$$U_i(s) = R(s) + \gamma\, \sum\limits_{s'} \transprob{s}{\pi_i(s)}{s'}
U_i(s')\ .
\label{policy-evaluation-equation}$$

For example, suppose $\pi_i$ is the policy shown in (a). Then we have
$\pi_i(1,1)\eq {Up}$, $\pi_i(1,2)\eq {Up}$, and so on, and the
simplified Bellman equations are

$$\begin{aligned}
U_i(1,1) &=& -0.04 + {0.8} U_i(1,2) + {0.1} U_i(1,1) + {0.1} U_i(2,1) \ ,\\
U_i(1,2) &=& -0.04 + {0.8} U_i(1,3) + {0.2} U_i(1,2) \ ,\\
&\vdots &\end{aligned}$$

The important point is that these equations are *linear*,
because the “$\max$” operator has been removed. For $n$ states, we have
$n$ linear equations with $n$ unknowns, which can be solved exactly in
time $O(n^3)$ by standard linear algebra methods.

For small state spaces, policy evaluation using exact solution methods
is often the most efficient approach. For large state spaces, $O(n^3)$
time might be prohibitive. Fortunately, it is not necessary to do
*exact* policy evaluation. Instead, we can perform some
number of simplified value iteration steps (simplified because the
policy is fixed) to give a reasonably good approximation of the
utilities. The simplified Bellman update for this process is
$$U_{i+1}(s) \leftarrow R(s) + \gamma \sum\limits_{s'} \transprob{s}{\pi_i(s)}{s'} U_i(s') \ ,$$
and this is repeated $k$ times to produce the next utility estimate. The
resulting algorithm is called .[MPI-page] It is often much more
efficient than standard policy iteration or value iteration.

The algorithms we have described so far require updating the utility or
policy for all states at once. It turns out that this is not strictly
necessary. In fact, on each iteration, we can pick *any
subset* of states and apply *either* kind of
updating (policy improvement or simplified value iteration) to that
subset. This very general algorithm is called . Given certain conditions
on the initial policy and initial utility function, asynchronous policy
iteration is guaranteed to converge to an optimal policy. The freedom to
choose any states to work on means that we can design much more
efficient heuristic algorithms—for example, algorithms that concentrate
on updating the values of states that are likely to be reached by a good
policy. This makes a lot of sense in real life: if one has no intention
of throwing oneself off a cliff, one should not spend time worrying
about the exact value of the resulting states.

Partially Observable MDPs {#pomdp-section}
-------------------------

The description of Markov decision processes in assumed that the
environment was . With this assumption, the agent always knows which
state it is in. This, combined with the Markov assumption for the
transition model, means that the optimal policy depends only on the
current state. When the environment is only , the situation is, one
might say, much less clear. The agent does not necessarily know which
state it is in, so it cannot execute the action $\pi(s)$ recommended for
that state. Furthermore, the utility of a state $s$ and the optimal
action in $s$ depend not just on $s$, but also on *how much the
agent knows* when it is in $s$. For these reasons, (or
POMDPs—pronounced “pom-dee-pees”) are usually viewed as much more
difficult than ordinary MDPs. We cannot avoid POMDPs, however, because
the real world is one.

### Definition of POMDPs

To get a handle on POMDPs, we must first define them properly. A POMDP
has the same elements as an MDP—the transition model
$\transprob{s}{a}{s'}$, actions $A(s)$, and reward function $R(s)$—but,
like the partially observable search problems of , it also has a
$\observation{s}{e}$. Here, as in , the sensor model specifies the
probability of perceiving evidence $e$ in state $s$.[^3] For example, we
can convert the $4\stimes 3$ world of into a POMDP by adding a noisy or
partial sensor instead of assuming that the agent knows its location
exactly. Such a sensor might measure the *number of adjacent
walls*, which happens to be 2 in all the nonterminal squares
except for those in the third column, where the value is 1; a noisy
version might give the wrong value with probability 0.1[4x3-pomdp-page].

In Chapters [advanced-search-chapter] and [advanced-planning-chapter],
we studied nondeterministic and partially observable planning problems
and identified the —the set of actual states the agent might be in—as a
key concept for describing and calculating solutions. In POMDPs, the
belief state $b$ becomes a *probability distribution* over
all possible states, just as in . For example, the initial belief state
for the $4\stimes 3$ POMDP could be the uniform distribution over the
nine nonterminal states, i.e.,
$\<\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},0,0\>$.
We write $b(s)$ for the probability assigned to the actual state $s$ by
belief state $b$. The agent can calculate its current belief state as
the conditional probability distribution over the actual states given
the sequence of percepts and actions so far. This is essentially the
task described in . The basic recursive filtering equation
([filtering-equation] on ) shows how to calculate the new belief state
from the previous belief state and the new evidence. For POMDPs, we also
have an action to consider, but the result is essentially the same. If
$b(s)$ was the previous belief state, and the agent does action $a$ and
then perceives evidence $e$, then the new belief state is given by
$$b'(s') = \alpha\, \observation{s'}{e} \sum_s \transprob{s}{a}{s'} b(s) \ ,$$
where $\alpha$ is a normalizing constant that makes the belief state sum
to 1. By analogy with the update operator for filtering (), we can write
this as

$$b' = \noprog{Forward}(b,a,e)\ .
\label{pomdp-filtering-equation}$$

In the $4\stimes 3$ POMDP, suppose the agent moves and its sensor
reports 1 adjacent wall; then it’s quite likely (although not
guaranteed, because both the motion and the sensor are noisy) that the
agent is now in (3,1). asks you to calculate the exact probability
values for the new belief state.

The fundamental insight required to understand POMDPs is this:

the optimal action depends only on the agent’s current belief state.

That is, the optimal policy can be described by a mapping $\pistar(b)$
from belief states to actions. It does *not* depend on the
*actual* state the agent is in. This is a good thing,
because the agent does not know its actual state; all it knows is the
belief state. Hence, the decision cycle of a POMDP agent can be broken
down into the following three steps:

1.  Given the current belief state $b$, execute the action
    $a\eq \pistar(b)$.

2.  Receive percept $e$.

3.  Set the current belief state to $\noprog{Forward}(b,a,e)$ and
    repeat.

Now we can think of POMDPs as requiring a search in belief-state space,
just like the methods for sensorless and contingency problems in . The
main difference is that the POMDP belief-state space is
*continuous*, because a POMDP belief state is a probability
distribution. For example, a belief state for the $4\stimes 3$ world is
a point in an 11-dimensional continuous space. An action changes the
belief state, not just the physical state. Hence, the action is
evaluated at least in part according to the information the agent
acquires as a result. POMDPs therefore include the value of information
() as one component of the decision problem.

Let’s look more carefully at the outcome of actions. In particular,
let’s calculate the probability that an agent in belief state $b$
reaches belief state $b'$ after executing action $a$. Now, if we knew
the action *and the subsequent percept*, then would provide
a *deterministic* update to the belief state:
$b' = \noprog{Forward}(b,a,e)$. Of course, the subsequent percept is not
yet known, so the agent might arrive in one of several possible belief
states $b'$, depending on the percept that is received. The probability
of perceiving $e$, given that $a$ was performed starting in belief state
$b$, is given by summing over all the actual states $s'$ that the agent
might reach:

$$\begin{aligned}
  P(e|a,b) &=& \sum_{s'} P(e|a,s',b) P(s'|a,b) \\
         &=& \sum_{s'} \observation{s'}{e} P(s'|a,b) \\
         &=& \sum_{s'} \observation{s'}{e} \sum_s \transprob{s}{a}{s'} b(s)\ .\end{aligned}$$

Let us write the probability of reaching $b'$ from $b$, given action
$a$, as $\pomdptransprob{b}{a}{b'})$. Then that gives us

$$\begin{aligned}
\pomdptransprob{b}{a}{b'} &=&  P(b'|a,b) = \sum_e P(b'|e,a,b) P(e|a,b) \nonumber \\
             &=&  \sum_e P(b'|e,a,b) \sum_{s'} \observation{s'}{e} \sum_s \transprob{s}{a}{s'} b(s)\ ,
\label{pomdp-mdp-transition-equation}\end{aligned}$$

where $P(b'|e,a,b)$ is 1 if $b'\eq \noprog{Forward}(b,a,e)$ and 0
otherwise.

can be viewed as defining a transition model for the belief-state space.
We can also define a reward function for belief states (i.e., the
expected reward for the actual states the agent might be in):
$$\rho(b) = \sum_s b(s)R(s)\ .$$ Together, $\pomdptransprob{b}{a}{b'}$
and $\rho(b)$ define an *observable* MDP on the space of
belief states. Furthermore, it can be shown that an optimal policy for
this MDP, $\pistar(b)$, is also an optimal policy for the original
POMDP. In other words,

solving a POMDP on a physical state space can be reduced to solving an
MDP on the corresponding belief-state space.

This fact is perhaps less surprising if we remember that the belief
state is always observable to the agent, by definition.

Notice that, although we have reduced POMDPs to MDPs, the MDP we obtain
has a continuous (and usually high-dimensional) state space. None of the
MDP algorithms described in Sections [value-iteration-section]
and [policy-iteration-section] applies directly to such MDPs. The next
two subsections describe a value iteration algorithm designed
specifically for POMDPs and an online decision-making algorithm, similar
to those developed for games in .

### Value iteration for POMDPs

described a value iteration algorithm that computed one utility value
for each state. With infinitely many belief states, we need to be more
creative. Consider an optimal policy $\pistar$ and its application in a
specific belief state $b$: the policy generates an action, then, for
each subsequent percept, the belief state is updated and a new action is
generated, and so on. For this specific $b$, therefore, the policy is
exactly equivalent to a , as defined in for nondeterministic and
partially observable problems. Instead of thinking about policies, let
us think about conditional plans and how the expected utility of
executing a fixed conditional plan varies with the initial belief state.
We make two observations:

1.  Let the utility of executing a *fixed* conditional plan
    $p$ starting in physical state $s$ be $\alpha_p(s)$. Then the
    expected utility of executing $p$ in belief state $b$ is just
    $\sum_s b(s)\alpha_p(s)$, or $b \cdot \alpha_p$ if we think of them
    both as vectors. Hence, the expected utility of a fixed conditional
    plan varies *linearly* with $b$; that is, it
    corresponds to a hyperplane in belief space.

2.  At any given belief state $b$, the optimal policy will choose to
    execute the conditional plan with highest expected utility; and the
    expected utility of $b$ under the optimal policy is just the utility
    of that conditional plan:
    $$U(b) = U^{\pistar}(b) = \max_{p} b \cdot \alpha_p \ .$$ If the
    optimal policy $\pistar$ chooses to execute $p$ starting at $b$,
    then it is reasonable to expect that it might choose to execute $p$
    in belief states that are very close to $b$; in fact, if we bound
    the depth of the conditional plans, then there are only finitely
    many such plans and the continuous space of belief states will
    generally be divided into *regions*, each corresponding
    to a particular conditional plan that is optimal in that region.

From these two observations, we see that the utility function $U(b)$ on
belief states, being the maximum of a collection of hyperplanes, will be
*piecewise linear* and *convex*.

To illustrate this, we use a simple two-state world[2state-pomdp-page].
The states are labeled 0 and 1, with $R(0)\eq 0$ and $R(1)\eq 1$. There
are two actions: stays put with probability 0.9 and switches to the
other state with probability 0.9. For now we will assume the discount
factor $\gamma\eq 1$. The sensor reports the correct state with
probability 0.6. Obviously, the agent should when it thinks it’s in
state 1 and when it thinks it’s in state 0.

[alpha-vectors-figure]

The advantage of a two-state world is that the belief space can be
viewed as one-dimensional, because the two probabilities must sum to 1.
In (a), the $x$-axis represents the belief state, defined by $b(1)$, the
probability of being in state 1. Now let us consider the one-step plans
$[{Stay}]$ and $[{Go}]$, each of which receives the reward for the
current state followed by the (discounted) reward for the state reached
after the action:

$$\begin{aligned}
\alpha_{[{Stay}]}(0) &=& R(0) + \gamma(0.9 R(0) + 0.1 R(1)) = 0.1\\
\alpha_{[{Stay}]}(1) &=& R(1) + \gamma(0.9 R(1) + 0.1 R(0)) = 1.9\\
\alpha_{[{Go}]}(0) &=& R(0) + \gamma(0.9 R(1) + 0.1 R(0)) = 0.9\\
\alpha_{[{Go}]}(1) &=& R(1) + \gamma(0.9 R(0) + 0.1 R(1)) = 1.1\end{aligned}$$

The hyperplanes (lines, in this case) for $b \cdot \alpha_{[{Stay}]}$
and $b \cdot \alpha_{[{Go}]}$ are shown in (a) and their maximum is
shown in bold. The bold line therefore represents the utility function
for the finite-horizon problem that allows just one action, and in each
“piece” of the piecewise linear utility function the optimal action is
the first action of the corresponding conditional plan. In this case,
the optimal one-step policy is to when $b(1)>0.5$ and otherwise.

Once we have utilities $\alpha_p(s)$ for all the conditional plans $p$
of depth 1 in each physical state $s$, we can compute the utilities for
conditional plans of depth 2 by considering each possible first action,
each possible subsequent percept, and then each way of choosing a
depth-1 plan to execute for each percept:

$$\begin{aligned}
\lefteqn{[{Stay};\mbf{ if }{Percept}\eq 0\mbf{ then }{Stay}\mbf{ else }{Stay}]}\\
\lefteqn{[{Stay};\mbf{ if }{Percept}\eq 0\mbf{ then }{Stay}\mbf{ else }{Go}] \ldots} \end{aligned}$$

There are eight distinct depth-2 plans in all, and their utilities are
shown in (b). Notice that four of the plans, shown as dashed lines, are
suboptimal across the entire belief space—we say these plans are , and
they need not be considered further. There are four undominated plans,
each of which is optimal in a specific region, as shown in (c). The
regions partition the belief-state space.

[pomdp-vi-algorithm]

We repeat the process for depth 3, and so on. In general, let $p$ be a
depth-$d$ conditional plan whose initial action is $a$ and whose
depth-$d-1$ subplan for percept $e$ is $p.e$; then

$$\alpha_p(s) = R(s) + \gamma\left( \sum_{s'} \transprob{s}{a}{s'} \sum_{e} \observation{s'}{e} \alpha_{p.e}(s')\right)\ .
\label{pomdp-vi-equation}$$

This recursion naturally gives us a value iteration algorithm, which is
sketched in . The structure of the algorithm and its error analysis are
similar to those of the basic value iteration algorithm in on ; the main
difference is that instead of computing one utility number for each
state, maintains a collection of undominated plans with their utility
hyperplanes. The algorithm’s complexity depends primarily on how many
plans get generated. Given $|A|$ actions and $|E|$ possible
observations, it is easy to show that there are $|A|^{O(|E|^{d-1})}$
distinct depth-$d$ plans. Even for the lowly two-state world with
$d\eq 8$, the exact number is $2^{255}$. The elimination of dominated
plans is essential for reducing this doubly exponential growth: the
number of undominated plans with $d\eq 8$ is just 144. The utility
function for these 144 plans is shown in (d).

Notice that even though state 0 has lower utility than state 1, the
intermediate belief states have even lower utility because the agent
lacks the information needed to choose a good action. This is why
information has value in the sense defined in and optimal policies in
POMDPs often include information-gathering actions.

Given such a utility function, an executable policy can be extracted by
looking at which hyperplane is optimal at any given belief state $b$ and
executing the first action of the corresponding plan. In (d), the
corresponding optimal policy is still the same as for depth-1 plans:
when $b(1)>0.5$ and otherwise.

In practice, the value iteration algorithm in is hopelessly inefficient
for larger problems—even the $4\stimes 3$ POMDP is too hard. The main
reason is that, given $n$ conditional plans at level $d$, the algorithm
constructs $|A|\cdot n^{|E|}$ conditional plans at level $d+1$ before
eliminating the dominated ones. Since the 1970s, when this algorithm was
developed, there have been several advances including more efficient
forms of value iteration and various kinds of policy iteration
algorithms. Some of these are discussed in the notes at the end of the
chapter. For general POMDPs, however, finding optimal policies is very
difficult (PSPACE-hard, in fact—i.e., very hard indeed). Problems with a
few dozen states are often infeasible. The next section describes a
different, approximate method for solving POMDPs, one based on
look-ahead search.

### Online agents for POMDPs {#ddn-section}

In this section, we outline a simple approach to agent design for
partially observable, stochastic environments. The basic elements of the
design are already familiar:

-   The transition and sensor models are represented by a (DBN), as
    described in .

-   The dynamic Bayesian network is extended with decision and utility
    nodes, as used in in . The resulting model is called a , or DDN.

-   A filtering algorithm is used to incorporate each new percept and
    action and to update the belief state representation.

-   Decisions are made by projecting forward possible action sequences
    and choosing the best one.

DBNs are in the terminology of ; they typically have an exponential
complexity advantage over atomic representations and can model quite
substantial real-world problems. The agent design is therefore a
practical implementation of the sketched in .

[generic-ddn-figure]

In the DBN, the single state $S_t$ becomes a set of state variables
$\X_t$, and there may be multiple evidence variables $\E_t$. We will use
$A_t$ to refer to the action at time $t$, so the transition model
becomes $\pv(\X_{t+1}|\X_t,A_t)$ and the sensor model becomes
$\pv(\E_t|\X_t)$. We will use $R_t$ to refer to the reward received at
time $t$ and $U_t$ to refer to the utility of the state at time $t$.
(Both of these are random variables.) With this notation, a dynamic
decision network looks like the one shown in .

Dynamic decision networks can be used as inputs for any POMDP algorithm,
including those for value and policy iteration methods. In this section,
we focus on look-ahead methods that project action sequences forward
from the current belief state in much the same way as do the
game-playing algorithms of . The network in has been projected three
steps into the future; the current and future decisions $A$ and the
future observations $\E$ and rewards $R$ are all unknown. Notice that
the network includes nodes for the *rewards* for $\X_{t+1}$
and $\X_{t+2}$, but the *utility* for $\X_{t+3}$. This is
because the agent must maximize the (discounted) sum of all future
rewards, and $U(\X_{t+3})$ represents the reward for $\X_{t+3}$ and all
subsequent rewards. As in , we assume that $U$ is available only in some
approximate form: if exact utility values were available, look-ahead
beyond depth 1 would be unnecessary.

[pomdp-tree-figure]

shows part of the search tree corresponding to the three-step look-ahead
DDN in . Each of the triangular nodes is a belief state in which the
agent makes a decision $A_{t+i}$ for $i\eq 0,1,2,\ldots$. The round
(chance) nodes correspond to choices by the environment, namely, what
evidence $\E_{t+i}$ arrives. Notice that there are no chance nodes
corresponding to the action outcomes; this is because the belief-state
update for an action is deterministic regardless of the actual outcome.

The belief state at each triangular node can be computed by applying a
filtering algorithm to the sequence of percepts and actions leading to
it. In this way, the algorithm takes into account the fact that, for
decision $A_{t+i}$, the agent *will* have available
percepts $\mbf{E}_{t+1},\, \ldots,\, \mbf{E}_{t+i}$, even though at time
$t$ it does not know what those percepts will be. In this way, a
decision-theoretic agent automatically takes into account the value of
information and will execute information-gathering actions where
appropriate.

A decision can be extracted from the search tree by backing up the
utility values from the leaves, taking an average at the chance nodes
and taking the maximum at the decision nodes. This is similar to the
algorithm for game trees with chance nodes, except that (1) there can
also be rewards at non-leaf states and (2) the decision nodes correspond
to belief states rather than actual states. The time complexity of an
exhaustive search to depth $d$ is $O(|A|^d\cdot |\E|^d)$, where $|A|$ is
the number of available actions and $|\E|$ is the number of possible
percepts. (Notice that this is far less than the number of depth-$d$
conditional plans generated by value iteration.) For problems in which
the discount factor $\gamma$ is not too close to 1, a shallow search is
often good enough to give near-optimal decisions. It is also possible to
approximate the averaging step at the chance nodes, by sampling from the
set of possible percepts instead of summing over all possible percepts.
There are various other ways of finding good approximate solutions
quickly, but we defer them to .

Decision-theoretic agents based on dynamic decision networks have a
number of advantages compared with other, simpler agent designs
presented in earlier chapters. In particular, they handle partially
observable, uncertain environments and can easily revise their “plans”
to handle unexpected evidence. With appropriate sensor models, they can
handle sensor failure and can plan to gather information. They exhibit
“graceful degradation” under time pressure and in complex environments,
using various approximation techniques. So what is missing? One defect
of our DDN-based algorithm is its reliance on forward search through
state space, rather than using the hierarchical and other advanced
planning techniques described in . There have been attempts to extend
these techniques into the probabilistic domain, but so far they have
proved to be inefficient. A second, related problem is the basically
propositional nature of the DDN language. We would like to be able to
extend some of the ideas for first-order probabilistic languages to the
problem of decision making. Current research has shown that this
extension is possible and has significant benefits, as discussed in the
notes at the end of the chapter.

Decisions with Multiple Agents: Game Theory {#game-theory-section}
-------------------------------------------

This chapter has concentrated on making decisions in uncertain
environments. But what if the uncertainty is due to other agents and the
decisions they make? And what if the decisions of those agents are in
turn influenced by our decisions? We addressed this question once
before, when we studied games in . There, however, we were primarily
concerned with turn-taking games in fully observable environments, for
which minimax search can be used to find optimal moves. In this section
we study the aspects of that analyze games with simultaneous moves and
other sources of partial observability. (Game theorists use the terms
and rather than fully and partially observable.) Game theory can be used
in at least two ways:

1.  **Agent design**: Game theory can analyze the agent’s
    decisions and compute the expected utility for each decision (under
    the assumption that other agents are acting optimally according to
    game theory). For example, in the game [morra-page], two players,
    $O$ and $E$, simultaneously display one or two fingers. Let the
    total number of fingers be $f$. If $f$ is odd, $O$ collects $f$
    dollars from $E$; and if $f$ is even, $E$ collects $f$ dollars from
    $O$. Game theory can determine the best strategy against a rational
    player and the expected return for each player.[^4]

2.  **Mechanism design**: When an environment is inhabited
    by many agents, it might be possible to define the rules of the
    environment (i.e., the game that the agents must play) so that the
    collective good of all agents is maximized when each agent adopts
    the game-theoretic solution that maximizes its own utility. For
    example, game theory can help design the protocols for a collection
    of Internet traffic routers so that each router has an incentive to
    act in such a way that global throughput is maximized. Mechanism
    design can also be used to construct intelligent that solve complex
    problems in a distributed fashion.

### Single-move games

We start by considering a restricted set of games: ones where all
players take action simultaneously and the result of the game is based
on this single set of actions. (Actually, it is not crucial that the
actions take place at exactly the same time; what matters is that no
player has knowledge of the other players’ choices.) The restriction to
a single move (and the very use of the word “game”) might make this seem
trivial, but in fact, game theory is serious business. It is used in
decision-making situations including the auctioning of oil drilling
rights and wireless frequency spectrum rights, bankruptcy proceedings,
product development and pricing decisions, and national
defense—situations involving billions of dollars and hundreds of
thousands of lives. A single-move game is defined by three components:

-   or agents who will be making decisions. Two-player games have
    received the most attention, although $n$-player games for $n > 2$
    are also common. We give players capitalized names, like
    *Alice* and *Bob* or $O$ and $E$.

-   that the players can choose. We will give actions lowercase names,
    like ${one}$ or ${testify}$. The players may or may not have the
    same set of actions available.

-   A that gives the utility to each player for each combination of
    actions by all the players. For single-move games the payoff
    function can be represented by a matrix, a representation known as
    the (also called ). The payoff matrix for two-finger Morra is as
    follows:

                                *O: one *   *O: two *
      ------------------------ ------------------------ ------------------------
      *E: one *         $E=+2, O=-2$             $E=-3, O=+3$
      *E: two *         $E=-3, O=+3$             $E=+4, O=-4$

    For example, the lower-right corner shows that when player $O$
    chooses action *two* and $E$ also chooses
    *two*, the payoff is +4 for $E$ and $-4$ for $O$.

Each player in a game must adopt and then execute a (which is the name
used in game theory for a *policy*). A is a deterministic
policy; for a single-move game, a pure strategy is just a single action.
For many games an agent can do better with a , which is a randomized
policy that selects actions according to a probability distribution. The
mixed strategy that chooses action $a$ with probability $p$ and action
$b$ otherwise is written $[p \colon a; (1-p)
\colon b]$. For example, a mixed strategy for two-finger Morra might be
$[{0.5} \colon {one}; {0.5} \colon {two}]$. A is an assignment of a
strategy to each player; given the strategy profile, the game’s is a
numeric value for each player.

A to a game is a strategy profile in which each player adopts a rational
strategy. We will see that the most important issue in game theory is to
define what “rational” means when each agent chooses only part of the
strategy profile that determines the outcome. It is important to realize
that outcomes are actual results of playing a game, while solutions are
theoretical constructs used to analyze a game. We will see that some
games have a solution only in mixed strategies. But that does not mean
that a player must literally be adopting a mixed strategy to be
rational.

Consider the following story: Two alleged burglars, Alice and Bob, are
caught red-handed near the scene of a burglary and are interrogated
separately. A prosecutor offers each a deal: if you testify against your
partner as the leader of a burglary ring, you’ll go free for being the
cooperative one, while your partner will serve 10 years in prison.
However, if you both testify against each other, you’ll both get 5
years. Alice and Bob also know that if both refuse to testify they will
serve only 1 year each for the lesser charge of possessing stolen
property. Now Alice and Bob face the so-called : should they testify or
refuse? Being rational agents, Alice and Bob each want to maximize their
own expected utility. Let’s assume that Alice is callously unconcerned
about her partner’s fate, so her utility decreases in proportion to the
number of years she will spend in prison, regardless of what happens to
Bob. Bob feels exactly the same way. To help reach a rational decision,
they both construct the following payoff matrix:

                             ${Alice}$:${testify}$   ${Alice}$:${refuse}$
  ------------------------- --------------------------- --------------------------
  ${Bob}$:${testify}$        $A = -5, B = -5$        $A = -{10}, B = \,\, 0$
  ${Bob}$:${refuse}$      $A = \,\, 0, B = -{10}$        $A = -1, B = -1$

Alice analyzes the payoff matrix as follows: “Suppose Bob testifies.
Then I get 5 years if I testify and 10 years if I don’t, so in that case
testifying is better. On the other hand, if Bob refuses, then I get 0
years if I testify and 1 year if I refuse, so in that case as well
testifying is better. So in either case, it’s better for me to testify,
so that’s what I must do.”

Alice has discovered that ${testify}$ is a for the game. We say that a
strategy $s$ for player $p$ strategy $s'$ if the outcome for $s$ is
better for $p$ than the outcome for $s'$, for every choice of strategies
by the other player(s). Strategy $s$ $s'$ if $s$ is better than $s'$ on
at least one strategy profile and no worse on any other. A dominant
strategy is a strategy that dominates all others. It is irrational to
play a dominated strategy, and irrational not to play a dominant
strategy if one exists. Being rational, Alice chooses the dominant
strategy. We need just a bit more terminology: we say that an outcome is
[^5] if there is no other outcome that all players would prefer. An
outcome is by another outcome if all players would prefer the other
outcome.

If Alice is clever as well as rational, she will continue to reason as
follows: Bob’s dominant strategy is also to testify. Therefore, he will
testify and we will both get five years. When each player has a dominant
strategy, the combination of those strategies is called a . In general,
a strategy profile forms an if no player can benefit by switching
strategies, given that every other player sticks with the same strategy.
An equilibrium is essentially a in the space of policies; it is the top
of a peak that slopes downward along every dimension, where a dimension
corresponds to a player’s strategy choices.

The mathematician John Nash (1928–) proved that

every game has at least one equilibrium.

The general concept of equilibrium is now called in his honor. Clearly,
a dominant strategy equilibrium is a Nash equilibrium (), but some games
have Nash equilibria but no dominant strategies.

The *dilemma* in the prisoner’s dilemma is that the
equilibrium outcome is worse for both players than the outcome they
would get if they both refused to testify. In other words,
$({testify},
{testify})$ is Pareto dominated by the (-1, -1) outcome of
$({refuse}, {refuse})$. Is there any way for Alice and Bob to arrive
at the (-1, -1) outcome? It is certainly an *allowable*
option for both of them to refuse to testify, but is is hard to see how
rational agents can get there, given the definition of the game. Either
player contemplating playing ${refuse}$ will realize that he or she
would do better by playing ${testify}$. That is the attractive power
of an equilibrium point. Game theorists agree that being a Nash
equilibrium is a necessary condition for being a solution—although they
disagree whether it is a sufficient condition.

It is easy enough to get to the $({refuse}, {refuse})$ solution if
we modify the game. For example, we could change to a in which the
players know that they will meet again. Or the agents might have moral
beliefs that encourage cooperation and fairness. That means they have a
different utility function, necessitating a different payoff matrix,
making it a different game. We will see later that agents with limited
computational powers, rather than the ability to reason absolutely
rationally, can reach non-equilibrium outcomes, as can an agent that
knows that the other agent has limited rationality. In each case, we are
considering a different game than the one described by the payoff matrix
above.

Now let’s look at a game that has no dominant strategy. Acme, a video
game console manufacturer, has to decide whether its next game machine
will use Blu-ray discs or DVDs. Meanwhile, the video game software
producer Best needs to decide whether to produce its next game on
Blu-ray or DVD. The profits for both will be positive if they agree and
negative if they disagree, as shown in the following payoff matrix:

                             ${Acme}$:${bluray}$   ${Acme}$:${dvd}$
  ------------------------- ------------------------- ----------------------
  ${Best}$:${bluray}$         $A=+9, B=+9 $            $A=-4, B=-1$
  ${Best}$:${dvd}$             $A=-3,B=-1$             $A=+5, B=+5$

There is no dominant strategy equilibrium for this game, but there are
*two* Nash equilibria: (*bluray, bluray*) and
(*dvd, dvd*). We know these are Nash equilibria because if
either player unilaterally moves to a different strategy, that player
will be worse off. Now the agents have a problem:

there are multiple acceptable solutions, but if each agent aims for a
different solution, then both agents will suffer.

How can they agree on a solution? One answer is that both should choose
the Pareto-optimal solution (*bluray, bluray*); that is, we
can restrict the definition of “solution” to the unique Pareto-optimal
Nash equilibrium *provided that one exists*. Every game has
at least one Pareto-optimal solution, but a game might have several, or
they might not be equilibrium points. For example, if (*bluray,
bluray*) had payoff (5, 5), then there would be two equal
Pareto-optimal equilibrium points. To choose between them the agents can
either guess or *communicate*, which can be done either by
establishing a convention that orders the solutions before the game
begins or by negotiating to reach a mutually beneficial solution during
the game (which would mean including communicative actions as part of a
sequential game). Communication thus arises in game theory for exactly
the same reasons that it arose in multiagent planning in . Games in
which players need to communicate like this are called .

A game can have more than one Nash equilibrium; how do we know that
every game must have at least one? Some games have no
*pure-strategy* Nash equilibria. Consider, for example, any
pure-strategy profile for two-finger Morra (). If the total number of
fingers is even, then $O$ will want to switch; on the other hand (so to
speak), if the total is odd, then $E$ will want to switch. Therefore, no
pure strategy profile can be an equilibrium and we must look to mixed
strategies instead.

But *which* mixed strategy? In 1928, von Neumann developed
a method for finding the *optimal* mixed strategy for
two-player, —games in which the sum of the payoffs is always zero.[^6]
Clearly, Morra is such a game. For two-player, zero-sum games, we know
that the payoffs are equal and opposite, so we need consider the payoffs
of only one player, who will be the maximizer (just as in ). For Morra,
we pick the even player $E$ to be the maximizer, so we can define the
payoff matrix by the values $U_E(e,o)$—the payoff to $E$ if $E$ does $e$
and $O$ does $o$. (For convenience we call player $E$ “her” and $O$
“him.”) Von Neumann’s method is called the the technique, and it works
as follows:

-   Suppose we change the rules as follows: first $E$ picks her strategy
    and reveals it to $O$. Then $O$ picks his strategy, with knowledge
    of $E$’s strategy. Finally, we evaluate the expected payoff of the
    game based on the chosen strategies. This gives us a turn-taking
    game to which we can apply the standard algorithm from . Let’s
    suppose this gives an outcome $U_{E,O}$. Clearly, this game favors
    $O$, so the true utility $U$ of the original game (from $E$’s point
    of view) is *at least* $U_{E,O}$. For example, if we
    just look at pure strategies, the minimax game tree has a root value
    of $-3$ (see (a)), so we know that $U\geq -3$.

-   Now suppose we change the rules to force $O$ to reveal his strategy
    first, followed by $E$. Then the minimax value of this game is
    $U_{O,E}$, and because this game favors $E$ we know that $U$ is
    *at most* $U_{O,E}$. With pure strategies, the value is
    $+2$ (see (b)), so we know $U\leq +2$.

Combining these two arguments, we see that the true utility $U$ of the
solution to the original game must satisfy
$$U_{E,O} \leq U \leq U_{O,E} \qquad\mbox{or in this case,}\qquad
-3 \leq U \leq 2 \ .$$ To pinpoint the value of $U$, we need to turn our
analysis to mixed strategies. First, observe the following:

once the first player has revealed his or her strategy, the second
player might as well choose a pure strategy.

The reason is simple: if the second player plays a mixed strategy,
$[p\colon {one}; (1-p)\colon {two}]$, its expected utility is a
linear combination $(p\cdot u_{{one}} + (1-p)\cdot u_{{two}})$ of
the utilities of the pure strategies, $u_{{one}}$ and $u_{{two}}$.
This linear combination can never be better than the better of
$u_{{one}}$ and $u_{{two}}$, so the second player can just choose
the better one.

[morra-trees-figure]

With this observation in mind, the minimax trees can be thought of as
having infinitely many branches at the root, corresponding to the
infinitely many mixed strategies the first player can choose. Each of
these leads to a node with two branches corresponding to the pure
strategies for the second player. We can depict these infinite trees
finitely by having one “parameterized” choice at the root:

-   If $E$ chooses first, the situation is as shown in (c). $E$ chooses
    the strategy $[p\colon {one}; (1-p)\colon {two}]$ at the root,
    and then $O$ chooses a pure strategy (and hence a move) given the
    value of $p$. If $O$ chooses ${one}$, the expected payoff (to $E$)
    is $2p-3(1-p)\eq
    5p-3$; if $O$ chooses ${two}$, the expected payoff is
    $-3p+4(1-p)\eq
    4-7p$. We can draw these two payoffs as straight lines on a graph,
    where $p$ ranges from 0 to 1 on the $x$-axis, as shown in (e). $O$,
    the minimizer, will always choose the lower of the two lines, as
    shown by the heavy lines in the figure. Therefore, the best that $E$
    can do at the root is to choose $p$ to be at the intersection point,
    which is where $$5p-3 = 4-7p \qquad \implies \qquad p=7/{12}\ .$$
    The utility for $E$ at this point is $U_{E,O}\eq -1/{12}$.

-   If $O$ moves first, the situation is as shown in (d). $O$ chooses
    the strategy $[q\colon {one}; (1-q)\colon {two}]$ at the root,
    and then $E$ chooses a move given the value of $q$. The payoffs are
    $2q-3(1-q)\eq 5q-3$ and $-3q+4(1-q)\eq 4-7q$.[^7] Again, (f) shows
    that the best $O$ can do at the root is to choose the intersection
    point: $$5q-3 = 4-7q \qquad \implies \qquad q=7/{12}\ .$$ The
    utility for $E$ at this point is $U_{O,E}\eq -1/{12}$.

Now we know that the true utility of the original game lies between
$-1/{12}$ and $-1/{12}$, that is, it is exactly $-1/{12}$! (The moral is
that it is better to be $O$ than $E$ if you are playing this game.)
Furthermore, the true utility is attained by the mixed strategy
$[7/{12}\colon {one};
5/{12}\colon {two}]$, which should be played by both players. This
strategy is called the of the game, and is a Nash equilibrium. Note that
each component strategy in an equilibrium mixed strategy has the same
expected utility. In this case, both ${one}$ and ${two}$ have the
same expected utility, $-1/{12}$, as the mixed strategy itself.

Our result for two-finger Morra is an example of the general result by
von Neumann:

every two-player zero-sum game has a maximin equilibrium when you allow
mixed strategies.

Furthermore, every Nash equilibrium in a zero-sum game is a maximin for
both players. A player who adopts the maximin strategy has two
guarantees: First, no other strategy can do better against an opponent
who plays well (although some other strategies might be better at
exploiting an opponent who makes irrational mistakes). Second, the
player continues to do just as well even if the strategy is revealed to
the opponent.

The general algorithm for finding maximin equilibria in zero-sum games
is somewhat more involved than Figures [morra-trees-figure](e) and (f)
might suggest. When there are $n$ possible actions, a mixed strategy is
a point in $n$-dimensional space and the lines become hyperplanes. It’s
also possible for some pure strategies for the second player to be
dominated by others, so that they are not optimal against
*any* strategy for the first player. After removing all
such strategies (which might have to be done repeatedly), the optimal
choice at the root is the highest (or lowest) intersection point of the
remaining hyperplanes. Finding this choice is an example of a problem:
maximizing an objective function subject to linear constraints. Such
problems can be solved by standard techniques in time polynomial in the
number of actions (and in the number of bits used to specify the reward
function, if you want to get technical).

The question remains, what should a rational agent actually
*do* in playing a single game of Morra? The rational agent
will have derived the fact that
$[7/{12}\colon {one}; 5/{12}\colon {two}]$ is the maximin
equilibrium strategy, and will assume that this is mutual knowledge with
a rational opponent. The agent could use a 12-sided die or a random
number generator to pick randomly according to this mixed strategy, in
which case the expected payoff would be -1/12 for $E$. Or the agent
could just decide to play ${one}$, or ${two}$. In either case, the
expected payoff remains -1/12 for $E$. Curiously, unilaterally choosing
a particular action does not harm one’s expected payoff, but allowing
the other agent to know that one has made such a unilateral decision
*does* affect the expected payoff, because then the
opponent can adjust his strategy accordingly.

Finding equilibria in non-zero-sum games is somewhat more complicated.
The general approach has two steps: (1) Enumerate all possible subsets
of actions that might form mixed strategies. For example, first try all
strategy profiles where each player uses a single action, then those
where each player uses either one or two actions, and so on. This is
exponential in the number of actions, and so only applies to relatively
small games. (2) For each strategy profile enumerated in (1), check to
see if it is an equilibrium. This is done by solving a set of equations
and inequalities that are similar to the ones used in the zero-sum case.
For two players these equations are linear and can be solved with basic
linear programming techniques, but for three or more players they are
nonlinear and may be very difficult to solve.

### Repeated games

So far we have looked only at games that last a single move. The
simplest kind of multiple-move game is the , in which players face the
same choice repeatedly, but each time with knowledge of the history of
all players’ previous choices. A strategy profile for a repeated game
specifies an action choice for each player at each time step for every
possible history of previous choices. As with MDPs, payoffs are additive
over time.

Let’s consider the repeated version of the prisoner’s dilemma. Will
Alice and Bob work together and refuse to testify, knowing they will
meet again? The answer depends on the details of the engagement. For
example, suppose Alice and Bob know that they must play exactly 100
rounds of prisoner’s dilemma. Then they both know that the 100th round
will not be a repeated game—that is, its outcome can have no effect on
future rounds—and therefore they will both choose the dominant strategy,
${testify}$, in that round. But once the 100th round is determined,
the 99th round can have no effect on subsequent rounds, so it too will
have a dominant strategy equilibrium at $({testify},{testify})$. By
induction, both players will choose ${testify}$ on every round,
earning a total jail sentence of 500 years each.

We can get different solutions by changing the rules of the interaction.
For example, suppose that after each round there is a 99% chance that
the players will meet again. Then the expected number of rounds is still
100, but neither player knows for sure which round will be the last.
Under these conditions, more cooperative behavior is possible. For
example, one equilibrium strategy is for each player to ${refuse}$
unless the other player has ever played ${testify}$. This strategy
could be called . Suppose both players have adopted this strategy, and
this is mutual knowledge. Then as long as neither player has played
${testify}$, then at any point in time the expected future total
payoff for each player is
$$\sum_{t=0}^\infty {0.99}^{t}\cdot (-1) = -{100}\ .$$ A player who
deviates from the strategy and chooses ${testify}$ will gain a score
of 0 rather than $-1$ on the very next move, but from then on both
players will play ${testify}$ and the player’s total expected future
payoff becomes
$$0 + \sum_{t=1}^{\infty} {0.99}^{t}\cdot (-{5}) = -{495}\ .$$
Therefore, at every step, there is no incentive to deviate from
$({refuse},{refuse})$. Perpetual punishment is the “mutually assured
destruction” strategy of the prisoner’s dilemma: once either player
decides to ${testify}$, it ensures that both players suffer a great
deal. But it works as a deterrent only if the other player believes you
have adopted this strategy—or at least that you might have adopted it.

Other strategies are more forgiving. The most famous, called , calls for
starting with ${refuse}$ and then echoing the other player’s previous
move on all subsequent moves. So Alice would refuse as long as Bob
refuses and would testify the move after Bob testified, but would go
back to refusing if Bob did. Although very simple, this strategy has
proven to be highly robust and effective against a wide variety of
strategies.

We can also get different solutions by changing the agents, rather than
changing the rules of engagement. Suppose the agents are finite-state
machines with $n$ states and they are playing a game with $m > n$ total
steps. The agents are thus incapable of representing the number of
remaining steps, and must treat it as an unknown. Therefore, they cannot
do the induction, and are free to arrive at the more favorable
(*refuse, refuse*) equilibrium. In this case, ignorance
*is* bliss—or rather, having your opponent believe that you
are ignorant is bliss. Your success in these repeated games depends on
the other player’s *perception* of you as a bully or a
simpleton, and not on your actual characteristics.

### Sequential games

In the general case, a game consists of a sequence of turns that need
not be all the same. Such games are best represented by a game tree,
which game theorists call the . The tree includes all the same
information we saw in : an initial state $S_0$, a function $(s)$ that
tells which player has the move, a function $(s)$ enumerating the
possible actions, a function $(s, a)$ that defines the transition to a
new state, and a partial function $(s, p)$, which is defined only on
terminal states, to give the payoff for each player.

To represent stochastic games, such as backgammon, we add a
distinguished player, *chance*, that can take random
actions. *Chance*’s “strategy” is part of the definition of
the game, specified as a probability distribution over actions (the
other players get to choose their own strategy). To represent games with
nondeterministic actions, such as billiards, we break the action into
two pieces: the player’s action itself has a deterministic result, and
then *chance* has a turn to react to the action in its own
capricious way. To represent simultaneous moves, as in the prisoner’s
dilemma or two-finger Morra, we impose an arbitrary order on the
players, but we have the option of asserting that the earlier player’s
actions are not observable to the subsequent players: e.g., Alice must
choose *refuse* or *testify* first, then Bob
chooses, but Bob does not know what choice Alice made at that time (we
can also represent the fact that the move is revealed later). However,
we assume the players always remember all their *own*
previous actions; this assumption is called .

The key idea of extensive form that sets it apart from the game trees of
is the representation of partial observability. We saw in that a player
in a partially observable game such as Kriegspiel can create a game tree
over the space of . With that tree, we saw that in some cases a player
can find a sequence of moves (a strategy) that leads to a forced
checkmate regardless of what actual state we started in, and regardless
of what strategy the opponent uses. However, the techniques of could not
tell a player what to do when there is no guaranteed checkmate. If the
player’s best strategy depends on the opponent’s strategy and vice
versa, then minimax (or alpha–beta) by itself cannot find a solution.
The extensive form *does* allow us to find solutions
because it represents the belief states (game theorists call them ) of
*all* players at once. From that representation we can find
equilibrium solutions, just as we did with normal-form games.

As a simple example of a sequential game, place two agents in the
$4\stimes 3$ world of and have them move simultaneously until one agent
reaches an exit square, and gets the payoff for that square. If we
specify that no movement occurs when the two agents try to move into the
same square simultaneously (a common problem at many traffic
intersections), then certain pure strategies can get stuck forever.
Thus, agents need a mixed strategy to perform well in this game:
randomly choose between moving ahead and staying put. This is exactly
what is done to resolve packet collisions in Ethernet networks.

[extensive-game-figure]

Next we’ll consider a very simple variant of poker. The deck has only
four cards, two aces and two kings. One card is dealt to each player.
The first player then has the option to *raise* the stakes
of the game from 1 point to 2, or to *check*. If player 1
checks, the game is over. If he raises, then player 2 has the option to
*call*, accepting that the game is worth 2 points, or
*fold*, conceding the 1 point. If the game does not end
with a fold, then the payoff depends on the cards: it is zero for both
players if they have the same card; otherwise the player with the king
pays the stakes to the player with the ace.

The extensive-form tree for this game is shown in . Nonterminal states
are shown as circles, with the player to move inside the circle; player
0 is *chance*. Each action is depicted as an arrow with a
label, corresponding to a *raise, check, call,* or
*fold*, or, for *chance*, the four possible
deals (“AK” means that player 1 gets an ace and player 2 a king).
Terminal states are rectangles labeled by their payoff to player 1 and
player 2. Information sets are shown as labeled dashed boxes; for
example, $I_{1,1}$ is the information set where it is player 1’s turn,
and he knows he has an ace (but does not know what player 2 has). In
information set $I_{2,1}$, it is player 2’s turn and she knows that she
has an ace and that player 1 has raised, but does not know what card
player 1 has. (Due to the limits of two-dimensional paper, this
information set is shown as two boxes rather than one.)

One way to solve an extensive game is to convert it to a normal-form
game. Recall that the normal form is a matrix, each row of which is
labeled with a pure strategy for player 1, and each column by a pure
strategy for player 2. In an extensive game a pure strategy for player
*i* corresponds to an action for each information set
involving that player. So in , one pure strategy for player 1 is “raise
when in $I_{1,1}$ (that is, when I have an ace), and check when in
$I_{1,2}$ (when I have a king).” In the payoff matrix below, this
strategy is called *rk*. Similarly, strategy
*cf* for player 2 means “call when I have an ace and fold
when I have a king.” Since this is a zero-sum game, the matrix below
gives only the payoff for player 1; player 2 always has the opposite
payoff:

                         2:*cc *   2:*cf *   2:*ff *   2:*fc *
  --------------------- ---------------------- ---------------------- ---------------------- ----------------------
  1:*rr*             0                     -1/6                    1                     7/6
  1:*kr*            -1/3                   -1/6                   5/6                    2/3
  1:*rk*            1/3             **0**            1/6                    1/2
  1:*kk*             0              **0**             0                      0

This game is so simple that it has two pure-strategy equilibria, shown
in bold: *cf* for player 2 and *rk* or
*kk* for player 1. But in general we can solve extensive
games by converting to normal form and then finding a solution (usually
a mixed strategy) using standard linear programming methods. That works
in theory. But if a player has $I$ information sets and $a$ actions per
set, then that player will have $a^I$ pure strategies. In other words,
the size of the normal-form matrix is exponential in the number of
information sets, so in practice the approach works only for very small
game trees, on the order of a dozen states. A game like Texas hold’em
poker has about $10^{18}$ states, making this approach completely
infeasible.

What are the alternatives? In we saw how alpha–beta search could handle
games of perfect information with huge game trees by generating the tree
incrementally, by pruning some branches, and by heuristically evaluating
nonterminal nodes. But that approach does not work well for games with
imperfect information, for two reasons: first, it is harder to prune,
because we need to consider mixed strategies that combine multiple
branches, not a pure strategy that always chooses the best branch.
Second, it is harder to heuristically evaluate a nonterminal node,
because we are dealing with information sets, not individual states.

come to the rescue with an alternative representation of extensive
games, called the , that is only linear in the size of the tree, rather
than exponential. Rather than represent strategies, it represents paths
through the tree; the number of paths is equal to the number of terminal
nodes. Standard linear programming methods can again be applied to this
representation. The resulting system can solve poker variants with
25,000 states in a minute or two. This is an exponential speedup over
the normal-form approach, but still falls far short of handling full
poker, with $10^{18}$ states.

If we can’t handle $10^{18}$ states, perhaps we can simplify the problem
by changing the game to a simpler form. For example, if I hold an ace
and am considering the possibility that the next card will give me a
pair of aces, then I don’t care about the suit of the next card; any
suit will do equally well. This suggests forming an of the game, one in
which suits are ignored. The resulting game tree will be smaller by a
factor of $4!\eq 24$. Suppose I can solve this smaller game; how will
the solution to that game relate to the original game? If no player is
going for a flush (or bluffing so), then the suits don’t matter to any
player, and the solution for the abstraction will also be a solution for
the original game. However, if any player is contemplating a flush, then
the abstraction will be only an approximate solution (but it is possible
to compute bounds on the error).

There are many opportunities for abstraction. For example, at the point
in a game where each player has two cards, if I hold a pair of queens,
then the other players’ hands could be abstracted into three classes:
*better* (only a pair of kings or a pair of aces),
*same* (pair of queens) or *worse* (everything
else). However, this abstraction might be too coarse. A better
abstraction would divide *worse* into, say, *medium
pair* (nines through jacks), *low pair*, and
*no pair*. These examples are abstractions of states; it is
also possible to abstract actions. For example, instead of having a bet
action for each integer from 1 to 1000, we could restrict the bets to
$10^0$, $10^1$, $10^2$ and $10^3$. Or we could cut out one of the rounds
of betting altogether. We can also abstract over chance nodes, by
considering only a subset of the possible deals. This is equivalent to
the rollout technique used in Go programs. Putting all these
abstractions together, we can reduce the $10^{18}$ states of poker to
$10^7$ states, a size that can be solved with current techniques.

Poker programs based on this approach can easily defeat novice and some
experienced human players, but are not yet at the level of master
players. Part of the problem is that the solution these programs
approximate—the equilibrium solution—is optimal only against an opponent
who also plays the equilibrium strategy. Against fallible human players
it is important to be able to exploit an opponent’s deviation from the
equilibrium strategy. As Gautam Rao (aka “The Count”), the world’s
leading online poker player, said @Billings+al:2003, “You have a very
strong program. Once you add opponent modeling to it, it will kill
everyone.” However, good models of human fallability remain elusive.

In a sense, extensive game form is the one of the most complete
representations we have seen so far: it can handle partially observable,
multiagent, stochastic, sequential, dynamic environments—most of the
hard cases from the list of environment properties on . However, there
are two limitations of game theory. First, it does not deal well with
continuous states and actions (although there have been some extensions
to the continuous case; for example, the theory of uses game theory to
solve problems where two companies choose prices for their products from
a continuous space). Second, game theory assumes the game is
*known*. Parts of the game may be specified as unobservable
to some of the players, but it must be known what parts are
unobservable. In cases in which the players learn the unknown structure
of the game over time, the model begins to break down. Let’s examine
each source of uncertainty, and whether each can be represented in game
theory.

*Actions:* There is no easy way to represent a game where
the players have to discover what actions are available. Consider the
game between computer virus writers and security experts. Part of the
problem is anticipating what action the virus writers will try next.

*Strategies:* Game theory is very good at representing the
idea that the other players’ strategies are initially unknown—as long as
we assume all agents are rational. The theory itself does not say what
to do when the other players are less than fully rational. The notion of
a partially addresses this point: it is an equilibrium with respect to a
player’s prior probability distribution over the other players’
strategies—in other words, it expresses a player’s beliefs about the
other players’ likely strategies.

*Chance:* If a game depends on the roll of a die, it is
easy enough to model a chance node with uniform distribution over the
outcomes. But what if it is possible that the die is unfair? We can
represent that with another chance node, higher up in the tree, with two
branches for “die is fair” and “die is unfair,” such that the
corresponding nodes in each branch are in the same information set (that
is, the players don’t know if the die is fair or not). And what if we
suspect the other opponent does know? Then we add *another*
chance node, with one branch representing the case where the opponent
does know, and one where he doesn’t.

*Utilities:* What if we don’t know our opponent’s
utilities? Again, that can be modeled with a chance node, such that the
other agent knows its own utilities in each branch, but we don’t. But
what if we don’t know our *own* utilities? For example, how
do I know if it is rational to order the Chef’s salad if I don’t know
how much I will like it? We can model that with yet another chance node
specifying an unobservable “intrinsic quality” of the salad.

Thus, we see that game theory is good at representing most sources of
uncertainty—but at the cost of doubling the size of the tree every time
we add another node; a habit which quickly leads to intractably large
trees. Because of these and other problems, game theory has been used
primarily to *analyze* environments that are at
equilibrium, rather than to *control* agents within an
environment. Next we shall see how it can help *design*
environments.

Mechanism Design {#mechanism-design-section}
----------------

In the previous section, we asked, “Given a game, what is a rational
strategy?” In this section, we ask, “Given that agents pick rational
strategies, what game should we design?” More specifically, we would
like to design a game whose solutions, consisting of each agent pursuing
its own rational strategy, result in the maximization of some global
utility function. This problem is called , or sometimes . Mechanism
design is a staple of economics and political science. Capitalism 101
says that if everyone tries to get rich, the total wealth of society
will increase. But the examples we will discuss show that proper
mechanism design is necessary to keep the invisible hand on track. For
collections of agents, mechanism design allows us to construct smart
systems out of a collection of more limited systems—even uncooperative
systems—in much the same way that teams of humans can achieve goals
beyond the reach of any individual.

Examples of mechanism design include auctioning off cheap airline
tickets, routing TCP packets between computers, deciding how medical
interns will be assigned to hospitals, and deciding how robotic soccer
players will cooperate with their teammates. Mechanism design became
more than an academic subject in the 1990s when several nations, faced
with the problem of auctioning off licenses to broadcast in various
frequency bands, lost hundreds of millions of dollars in potential
revenue as a result of poor mechanism design. Formally, a consists of
(1) a language for describing the set of allowable strategies that
agents may adopt, (2) a distinguished agent, called the , that collects
reports of strategy choices from the agents in the game, and (3) an
outcome rule, known to all agents, that the center uses to determine the
payoffs to each agent, given their strategy choices.

### Auctions

Let’s consider first. An auction is a mechanism for selling some goods
to members of a pool of bidders. For simplicity, we concentrate on
auctions with a single item for sale. Each bidder $i$ has a utility
value $v_i$ for having the item. In some cases, each bidder has a for
the item. For example, the first item sold on eBay was a broken laser
pointer, which sold for \$14.83 to a collector of broken laser pointers.
Thus, we know that the collector has $v_i \ge \$14.83$, but most other
people would have $v_j \ll \$14.83$. In other cases, such as auctioning
drilling rights for an oil tract, the item has a —the tract will produce
some amount of money, $X$, and all bidders value a dollar equally—but
there is uncertainty as to what the actual value of $X$ is. Different
bidders have different information, and hence different estimates of the
item’s true value. In either case, bidders end up with their own $v_i$.
Given $v_i$, each bidder gets a chance, at the appropriate time or times
in the auction, to make a bid $b_i$. The highest bid, $b_{max}$ wins the
item, but the price paid need not be $b_{max}$; that’s part of the
mechanism design.

The best-known auction mechanism is the ,[^8] or , in which the center
starts by asking for a minimum (or ) bid $b_{min}$. If some bidder is
willing to pay that amount, the center then asks for $b_{min} + d$, for
some increment $d$, and continues up from there. The auction ends when
nobody is willing to bid anymore; then the last bidder wins the item,
paying the price he bid.

How do we know if this is a good mechanism? One goal is to maximize
expected revenue for the seller. Another goal is to maximize a notion of
global utility. These goals overlap to some extent, because one aspect
of maximizing global utility is to ensure that the winner of the auction
is the agent who values the item the most (and thus is willing to pay
the most). We say an auction is if the goods go to the agent who values
them most. The ascending-bid auction is usually both efficient and
revenue maximizing, but if the reserve price is set too high, the bidder
who values it most may not bid, and if the reserve is set too low, the
seller loses net revenue.

Probably the most important things that an auction mechanism can do is
encourage a sufficient number of bidders to enter the game and
discourage them from engaging in . Collusion is an unfair or illegal
agreement by two or more bidders to manipulate prices. It can happen in
secret backroom deals or tacitly, within the rules of the mechanism.

For example, in 1999, Germany auctioned ten blocks of cell-phone
spectrum with a simultaneous auction (bids were taken on all ten blocks
at the same time), using the rule that any bid must be a minimum of a
10% raise over the previous bid on a block. There were only two credible
bidders, and the first, Mannesman, entered the bid of 20 million
deutschmark on blocks 1-5 and 18.18 million on blocks 6-10. Why 18.18M?
One of T-Mobile’s managers said they “interpreted Mannesman’s first bid
as an offer.” Both parties could compute that a 10% raise on 18.18M is
19.99M; thus Mannesman’s bid was interpreted as saying “we can each get
half the blocks for 20M; let’s not spoil it by bidding the prices up
higher.” And in fact T-Mobile bid 20M on blocks 6-10 and that was the
end of the bidding. The German government got less than they expected,
because the two competitors were able to use the bidding mechanism to
come to a tacit agreement on how not to compete. From the government’s
point of view, a better result could have been obtained by any of these
changes to the mechanism: a higher reserve price; a sealed-bid
first-price auction, so that the competitors could not communicate
through their bids; or incentives to bring in a third bidder. Perhaps
the 10% rule was an error in mechanism design, because it facilitated
the precise signaling from Mannesman to T-Mobile.

In general, both the seller and the global utility function benefit if
there are more bidders, although global utility can suffer if you count
the cost of wasted time of bidders that have no chance of winning. One
way to encourage more bidders is to make the mechanism easier for them.
After all, if it requires too much research or computation on the part
of the bidders, they may decide to take their money elsewhere. So it is
desirable that the bidders have a . Recall that “dominant” means that
the strategy works against all other strategies, which in turn means
that an agent can adopt it without regard for the other strategies. An
agent with a dominant strategy can just bid, without wasting time
contemplating other agents’ possible strategies. A mechanism where
agents have a dominant strategy is called a mechanism. If, as is usually
the case, that strategy involves the bidders revealing their true value,
$v_i$, then it is called a , or , auction; the term is also used. The
states that any mechanism can be transformed into an equivalent
truth-revealing mechanism, so part of mechanism design is finding these
equivalent mechanisms.

It turns out that the ascending-bid auction has most of the desirable
properties. The bidder with the highest value $v_i$ gets the goods at a
price of $b_o + d$, where $b_o$ is the highest bid among all the other
agents and $d$ is the auctioneer’s increment.[^9] Bidders have a simple
dominant strategy: keep bidding as long as the current cost is below
your $v_i$. The mechanism is not quite truth-revealing, because the
winning bidder reveals only that his $v_i \ge b_o + d$; we have a lower
bound on $v_i$ but not an exact amount.

A disadvantage (from the point of view of the seller) of the
ascending-bid auction is that it can discourage competition. Suppose
that in a bid for cell-phone spectrum there is one advantaged company
that everyone agrees would be able to leverage existing customers and
infrastructure, and thus can make a larger profit than anyone else.
Potential competitors can see that they have no chance in an
ascending-bid auction, because the advantaged company can always bid
higher. Thus, the competitors may not enter at all, and the advantaged
company ends up winning at the reserve price.

Another negative property of the English auction is its high
communication costs. Either the auction takes place in one room or all
bidders have to have high-speed, secure communication lines; in either
case they have to have the time available to go through several rounds
of bidding. An alternative mechanism, which requires much less
communication, is the . Each bidder makes a single bid and communicates
it to the auctioneer, without the other bidders seeing it. With this
mechanism, there is no longer a simple dominant strategy. If your value
is $v_i$ and you believe that the maximum of all the other agents’ bids
will be $b_o$, then you should bid $b_o + \epsilon$, for some small
$\epsilon$, if that is less than $v_i$. Thus, your bid depends on your
estimation of the other agents’ bids, requiring you to do more work.
Also, note that the agent with the highest $v_i$ might not win the
auction. This is offset by the fact that the auction is more
competitive, reducing the bias toward an advantaged bidder.

A small change in the mechanism for sealed-bid auctions produces the ,
also known as a .[^10] In such auctions, the winner pays the price of
the *second*-highest bid, $b_o$, rather than paying his own
bid. This simple modification completely eliminates the complex
deliberations required for standard (or ) sealed-bid auctions, because
the dominant strategy is now simply to bid $v_i$; the mechanism is
truth-revealing. Note that the utility of agent $i$ in terms of his bid
$b_i$, his value $v_i$, and the best bid among the other agents, $b_o$,
is
$$u_i = \left\{\begin{array}{l} (v_i - b_o) \quad\mbox{if } b_i > b_o \\
                                        0\quad \mbox{otherwise.}
                       \end{array}\right.$$ To see that $b_i = v_i$ is a
dominant strategy, note that when $(v_i - b_o)$ is positive, any bid
that wins the auction is optimal, and bidding $v_i$ in particular wins
the auction. On the other hand, when $(v_i - b_o)$ is negative, any bid
that loses the auction is optimal, and bidding $v_i$ in particular loses
the auction. So bidding $v_i$ is optimal for all possible values of
$b_o$, and in fact, $v_i$ is the only bid that has this property.
Because of its simplicity and the minimal computation requirements for
both seller and bidders, the Vickrey auction is widely used in
constructing distributed AI systems. Also, Internet search engines
conduct over a billion auctions a day to sell advertisements along with
their search results, and online auction sites handle \$100 billion a
year in goods, all using variants of the Vickrey auction. Note that the
expected value to the seller is $b_o$, which is the same expected return
as the limit of the English auction as the increment $d$ goes to zero.
This is actually a very general result: the states that, with a few
minor caveats, any auction mechanism where risk-neutral bidders have
values $v_i$ known only to themselves (but know a probability
distribution from which those values are sampled), will yield the same
expected revenue. This principle means that the various mechanisms are
not competing on the basis of revenue generation, but rather on other
qualities.

Although the second-price auction is truth-revealing, it turns out that
extending the idea to multiple goods and using a next-price auction is
not truth-revealing. Many Internet search engines use a mechanism where
they auction $k$ slots for ads on a page. The highest bidder wins the
top spot, the second highest gets the second spot, and so on. Each
winner pays the price bid by the next-lower bidder, with the
understanding that payment is made only if the searcher actually clicks
on the ad. The top slots are considered more valuable because they are
more likely to be noticed and clicked on. Imagine that three bidders,
$b_1, b_2$ and $b_3$, have valuations for a click of
$v_1\eq 200, v_2\eq 180,$ and $v_3\eq 100$, and that$k=2$ slots are
available, where it is known that the top spot is clicked on 5% of the
time and the bottom spot 2%. If all bidders bid truthfully, then $b_1$
wins the top slot and pays 180, and has an expected return of
$(200-180)\times 0.05 \eq 1$. The second slot goes to $b_2$. But $b_1$
can see that if she were to bid anything in the range 101–179, she would
concede the top slot to $b_2$, win the second slot, and yield an
expected return of $(200-100)\times .02
\eq 2$. Thus, $b_1$ can double her expected return by bidding less than
her true value in this case. In general, bidders in this multislot
auction must spend a lot of energy analyzing the bids of others to
determine their best strategy; there is no simple dominant strategy.
show that there is a unique truthful auction mechanism for this
multislot problem, in which the winner of slot $j$ pays the full price
for slot $j$ just for those additional clicks that are available at slot
$j$ and not at slot $j+1$. The winner pays the price for the lower slot
for the remaining clicks. In our example, $b_1$ would bid 200
truthfully, and would pay 180 for the additional $.05-.02\eq
.03$ clicks in the top slot, but would pay only the cost of the bottom
slot, 100, for the remaining .02 clicks. Thus, the total return to $b_1$
would be $(200-180)\times .03 + (200-100) \times .02
\eq 2.6$.

Another example of where auctions can come into play within AI is when a
collection of agents are deciding whether to cooperate on a joint plan.
show that this can be accomplished efficiently with an auction in which
the agents bid for roles in the joint plan.

### Common goods

Now let’s consider another type of game, in which countries set their
policy for controlling air pollution. Each country has a choice: they
can reduce pollution at a cost of -10 points for implementing the
necessary changes, or they can continue to pollute, which gives them a
net utility of -5 (in added health costs, etc.) and also contributes -1
points to every other country (because the air is shared across
countries). Clearly, the dominant strategy for each country is “continue
to pollute,” but if there are 100 countries and each follows this
policy, then each country gets a total utility of -104, whereas if every
country reduced pollution, they would each have a utility of -10. This
situation is called the : if nobody has to pay for using a common
resource, then it tends to be exploited in a way that leads to a lower
total utility for all agents. It is similar to the prisoner’s dilemma:
there is another solution to the game that is better for all parties,
but there appears to be no way for rational agents to arrive at that
solution.

The standard approach for dealing with the tragedy of the commons is to
change the mechanism to one that charges each agent for using the
commons. More generally, we need to ensure that all —effects on global
utility that are not recognized in the individual agents’
transactions—are made explicit. Setting the prices correctly is the
difficult part. In the limit, this approach amounts to creating a
mechanism in which each agent is effectively required to maximize global
utility, but can do so by making a local decision. For this example, a
carbon tax would be an example of a mechanism that charges for use of
the commons in a way that, if implemented well, maximizes global
utility.

As a final example, consider the problem of allocating some common
goods. Suppose a city decides it wants to install some free wireless
Internet transceivers. However, the number of transceivers they can
afford is less than the number of neighborhoods that want them. The city
wants to allocate the goods efficiently, to the neighborhoods that would
value them the most. That is, they want to maximize the global utility
$V = \sum_i v_i$. The problem is that if they just ask each neighborhood
council “how much do you value this free gift?” they would all have an
incentive to lie, and report a high value. It turns out there is a
mechanism, known as the , or , mechanism, that makes it a dominant
strategy for each agent to report its true utility and that achieves an
efficient allocation of the goods. The trick is that each agent pays a
tax equivalent to the loss in global utility that occurs because of the
agent’s presence in the game. The mechanism works like this:

1.  The center asks each agent to report its value for receiving an
    item. Call this $b_i$.

2.  The center allocates the goods to a subset of the bidders. We call
    this subset $A$, and use the notation $b_i(A)$ to mean the result to
    $i$ under this allocation: $b_i$ if $i$ is in $A$ (that is, $i$ is a
    winner), and 0 otherwise. The center chooses $A$ to maximize total
    reported utility $B = \sum_i b_i(A)$.

3.  The center calculates (for each $i$) the sum of the reported
    utilities for all the winners except $i$. We use the notation
    $B_{-i} = \sum_{j \neq i} b_j(A)$. The center also computes (for
    each $i$) the allocation that would maximize total global utility if
    $i$ were not in the game; call that sum $W_{-i}$.

4.  Each agent $i$ pays a tax equal to $W_{-i} - B_{-i}$.

In this example, the VCG rule means that each winner would pay a tax
equal to the highest reported value among the losers. That is, if I
report my value as 5, and that causes someone with value 2 to miss out
on an allocation, then I pay a tax of 2. All winners should be happy
because they pay a tax that is less than their value, and all losers are
as happy as they can be, because they value the goods less than the
required tax.

Why is it that this mechanism is truth-revealing? First, consider the
payoff to agent $i$, which is the value of getting an item, minus the
tax:

$$\begin{aligned}
&&  v_i(A) - (W_{-i} - B_{-i}) \ .  \label{vcg-equation}\end{aligned}$$

Here we distinguish the agent’s true utility, $v_i$, from his reported
utility $b_i$ (but we are trying to show that a dominant strategy is
$b_i \eq v_i$). Agent $i$ knows that the center will maximize global
utility using the reported values,
$$\sum_j b_j(A) = b_i(A) + \sum_{j\neq i} b_j(A)$$ whereas agent $i$
wants the center to maximize ([vcg-equation]), which can be rewritten as
$$v_i(A) + \sum_{j\neq i} b_j(A) - W_{-i} \ .$$ Since agent $i$ cannot
affect the value of $W_{-i}$ (it depends only on the other agents), the
only way $i$ can make the center optimize what $i$ wants is to report
the true utility, $b_i \eq v_i$.

This chapter shows how to use knowledge about the world to make
decisions even when the outcomes of an action are uncertain and the
rewards for acting might not be reaped until many actions have passed.
The main points are as follows:

-   Sequential decision problems in uncertain environments, also called
    , or MDPs, are defined by a specifying the probabilistic outcomes of
    actions and a specifying the reward in each state.

-   The utility of a state sequence is the sum of all the rewards over
    the sequence, possibly discounted over time. The solution of an MDP
    is a that associates a decision with every state that the agent
    might reach. An optimal policy maximizes the utility of the state
    sequences encountered when it is executed.

-   The utility of a state is the expected utility of the state
    sequences encountered when an optimal policy is executed, starting
    in that state. The algorithm for solving MDPs works by iteratively
    solving the equations relating the utility of each state to those of
    its neighbors.

-   alternates between calculating the utilities of states under the
    current policy and improving the current policy with respect to the
    current utilities.

-   Partially observable MDPs, or POMDPs, are much more difficult to
    solve than are MDPs. They can be solved by conversion to an MDP in
    the continuous space of belief states; both value iteration and
    policy iteration algorithms have been devised. Optimal behavior in
    POMDPs includes information gathering to reduce uncertainty and
    therefore make better decisions in the future.

-   A decision-theoretic agent can be constructed for POMDP
    environments. The agent uses a to represent the transition and
    sensor models, to update its belief state, and to project forward
    possible action sequences.

-   describes rational behavior for agents in situations in which
    multiple agents interact simultaneously. Solutions of games are
    —strategy profiles in which no agent has an incentive to deviate
    from the specified strategy.

-   can be used to set the rules by which agents will interact, in order
    to maximize some global utility through the operation of
    individually rational agents. Sometimes, mechanisms exist that
    achieve this goal without requiring each agent to consider the
    choices made by other agents.

We shall return to the world of MDPs and POMDP in , when we study
methods that allow an agent to improve its behavior from experience in
sequential, uncertain environments.

Richard Bellman developed the ideas underlying the modern approach to
sequential decision problems while working at the RAND Corporation
beginning in 1949. According to his autobiography @Bellman:1984, he
coined the exciting term “dynamic programming” to hide from a
research-phobic Secretary of Defense, Charles Wilson, the fact that his
group was doing mathematics. (This cannot be strictly true, because his
first paper using the term @Bellman:1952 appeared before Wilson became
Secretary of Defense in 1953.) Bellman’s book, *Dynamic
Programming* [-@Bellman:1957], gave the new field a solid
foundation and introduced the basic algorithmic approaches. Ron Howard’s
Ph.D. thesis [-@Howard:1960] introduced policy iteration and the idea of
average reward for solving infinite-horizon problems. Several additional
results were introduced by Bellman and Dreyfus [-@Bellman+Dreyfus:1962].
Modified policy iteration is due to and . Asynchronous policy iteration
was analyzed by , who also proved the policy loss bound in . The
analysis of discounting in terms of stationary preferences is due to
Koopmans [-@Koopmans:1972]. The texts by , , and provide a rigorous
introduction to sequential decision problems. describe results on the
computational complexity of MDPs.

Seminal work by and on reinforcement learning methods for solving MDPs
played a significant role in introducing MDPs into the AI community, as
did the later survey by . (Earlier work by contained many similar ideas,
but was not taken up to the same extent.) The connection between MDPs
and AI planning problems was made first by Sven , who showed how
probabilistic operators provide a compact representation for transition
models \<see also\>Wellman:1990b. Work by and attempted to
overcome the combinatorics of large state spaces by using a limited
search horizon and abstract states. Heuristics based on the value of
information can be used to select areas of the state space where a local
expansion of the horizon will yield a significant improvement in
decision quality. Agents using this approach can tailor their effort to
handle time pressure and generate some interesting behaviors such as
using familiar “beaten paths” to find their way around the state space
quickly without having to recompute optimal decisions at each point.

As one might expect, AI researchers have pushed MDPs in the direction of
more expressive representations that can accommodate much larger
problems than the traditional atomic representations based on transition
matrices. The use of a dynamic Bayesian network to represent transition
models was an obvious idea, but work on @Boutilier+al:2000b
[@Koller+Parr:2000; @Guestrin+al:2003b] extends the idea to structured
representations of the value function with provable improvements in
complexity.  @Boutilier+al:2001 [@Guestrin+al:2003] go one step further,
using structured representations to handle domains with many related
objects.

The observation that a partially observable MDP can be transformed into
a regular MDP over belief states is due to and . The first complete
algorithm for the exact solution of POMDPs—essentially the value
iteration algorithm presented in this chapter—was proposed by Edward
Sondik [-@Sondik:1971] in his Ph.D. thesis. (A later journal paper by
Smallwood and Sondik [-@Smallwood+Sondik:1973] contains some errors, but
is more accessible.) surveyed the first twenty-five years of POMDP
research, reaching somewhat pessimistic conclusions about the
feasibility of solving large problems. The first significant
contribution within AI was the Witness algorithm @Cassandra+al:1994
[@Kaelbling+al:1998], an improved version of POMDP value iteration.
Other algorithms soon followed, including an approach due to that
constructs a policy incrementally in the form of a finite-state
automaton. In this policy representation, the belief state corresponds
directly to a particular state in the automaton. More recent work in AI
has focused on value iteration methods that, at each iteration, generate
conditional plans and $\alpha$-vectors for a finite set of belief states
rather than for the entire belief space. proposed such an algorithm for
a fixed grid of points, an approach taken also by . An influential paper
by suggested generating reachable points by simulating trajectories in a
somewhat greedy fashion; observe that one need generate plans for only a
small, randomly selected subset of points to improve on the plans from
the previous iteration for all points in the set. Current point-based
methods—such as point-based policy iteration @Ji+al:2007—can generate
near-optimal solutions for POMDPs with thousands of states. Because
POMDPs are PSPACE-hard @Papadimitriou+Tsitsiklis:1987, further progress
may require taking advantage of various kinds of structure within a
factored representation.

The online approach—using look-ahead search to select an action for the
current belief state—was first examined by . The use of sampling at
chance nodes was explored analytically by and . The basic ideas for an
agent architecture using dynamic decision networks were proposed by .
The book *Planning and Control* by Dean and
Wellman [-@Dean+Wellman:1991] goes into much greater depth, making
connections between DBN/DDN models and the classical control literature
on filtering. Tatman and Shachter [-@Tatman+Shachter:1990] showed how to
apply dynamic programming algorithms to DDN models. explains various
ways in which such agents can be scaled up and identifies a number of
open research issues.

The roots of game theory can be traced back to proposals made in the
17th century by Christiaan Huygens and Gottfried Leibniz to study
competitive and cooperative human interactions scientifically and
mathematically. Throughout the 19th century, several leading economists
created simple mathematical examples to analyze particular examples of
competitive situations. The first formal results in game theory are due
to Zermelo [-@Zermelo:1913] (who had, the year before, suggested a form
of minimax search for games, albeit an incorrect one). Emile introduced
the notion of a mixed strategy. John von Neumann [-@VonNeumann:1928]
proved that every two-person, zero-sum game has a maximin equilibrium in
mixed strategies and a well-defined value. Von Neumann’s collaboration
with the economist Oskar Morgenstern led to the publication in 1944 of
the *Theory of Games and Economic Behavior*, the defining
book for game theory. Publication of the book was delayed by the wartime
paper shortage until a member of the Rockefeller family personally
subsidized its publication.

In 1950, at the age of 21, John Nash published his ideas concerning
equilibria in general (non-zero-sum) games. His definition of an
equilibrium solution, although originating in the work of , became known
as Nash equilibrium. After a long delay because of the schizophrenia he
suffered from 1959 onward, Nash was awarded the Nobel Memorial Prize in
Economics (along with Reinhart Selten and John Harsanyi) in 1994. The
Bayes–Nash equilibrium is described by and discussed by . Some issues in
the use of game theory for agent control are covered by .

The prisoner’s dilemma was invented as a classroom exercise by Albert W.
Tucker in 1950 (based on an example by Merrill Flood and Melvin Dresher)
and is covered extensively by and . Repeated games were introduced by ,
and games of partial information in extensive form by . The first
practical algorithm for sequential, partial-information games was
developed within AI by ; the paper by provides a readable introduction
to the field and describe a working system for representing and solving
sequential games.

The use of abstraction to reduce a game tree to a size that can be
solved with Koller’s technique is discussed by . show how to use
importance sampling to get a better estimate of the value of a strategy.
show that the abstraction approach is vulnerable to making systematic
errors in approximating the equilibrium solution, meaning that the whole
approach is on shaky ground: it works for some games but not others.
experiment with an opponent model in the form of a Bayesian network. It
plays five-card stud about as well as experienced humans.
@Zinkevich+al:2008 show how an approach that minimizes regret can find
approximate equilibria for abstractions with $10^{12}$ states, 100 times
more than previous methods.

Game theory and MDPs are combined in the theory of Markov games, also
called stochastic games @Littman:1994b [@Hu+Wellman:1998]. actually
described the value iteration algorithm independently of Bellman, but
his results were not widely appreciated, perhaps because they were
presented in the context of Markov games. Evolutionary game theory
@Smith:1982 [@Weibull:1995] looks at strategy drift over time: if your
opponent’s strategy is changing, how should you react? Textbooks on game
theory from an economics point of view include those by , , , and ;
concentrate on repeated games. From an AI perspective we have , , and .

The 2007 Nobel Memorial Prize in Economics went to Hurwicz, Maskin, and
Myerson “for having laid the foundations of mechanism design theory”
@Hurwicz:1973. The tragedy of the commons, a motivating problem for the
field, was presented by . The revelation principle is due to , and the
revenue equivalence theorem was developed independently by and . Two
economists, and , write about the multibillion-dollar spectrum auctions
they were involved in.

Mechanism design is used in multiagent planning @Hunsberger+Grosz:2000
[@Stone+al:2009] and scheduling @Rassenti+al:1982. gives a brief
overview with connections to the computer science literature, and
present a book-length treatment with applications to distributed AI.
Related work on distributed AI also goes under other names, including
collective intelligence @Tumer+Wolpert:2000 [@Segaran:2007] and
market-based control @Clearwater:1996. Since 2001 there has been an
annual Trading Agents Competition (TAC), in which agents try to make the
best profit on a series of auctions @Wellman+al:2001
[@Arunachalam+Sadeh:2005]. Papers on computational issues in auctions
often appear in the ACM Conferences on Electronic Commerce.

[mdp-model-exercise]For the $4\stimes 3$ world shown in , calculate
which squares can be reached from (1,1) by the action sequence
$[{Up},{Up},{Right},{Right},{Right}]$ and with what
probabilities. Explain how this computation is related to the prediction
task (see ) for a hidden Markov model.

[mdp-model-exercise]For the $4\stimes 3$ world shown in , calculate
which squares can be reached from (1,1) by the action sequence
$[{Right},{Right},{Right},{Up},{Up}]$ and with what
probabilities. Explain how this computation is related to the prediction
task (see ) for a hidden Markov model.

Select a specific member of the set of policies that are optimal for
$R(s)>0$ as shown in (b), and calculate the fraction of time the agent
spends in each state, in the limit, if the policy is executed forever.
(*Hint*: Construct the state-to-state transition
probability matrix corresponding to the policy and see .)

[nonseparable-exercise]Suppose that we define the utility of a state
sequence to be the *maximum* reward obtained in any state
in the sequence. Show that this utility function does not result in
stationary preferences between state sequences. Is it still possible to
define a utility function on states such that MEU decision making gives
optimal behavior?

Can any finite search problem be translated exactly into a Markov
decision problem such that an optimal solution of the latter is also an
optimal solution of the former? If so, explain *precisely*
how to translate the problem and how to translate the solution back; if
not, explain *precisely* why not (i.e., give a
counterexample).

[reward-equivalence-exercise] Sometimes MDPs are formulated with a
reward function $R(s,a)$ that depends on the action taken or with a
reward function $R(s,a,s')$ that also depends on the outcome state.

1.  Write the Bellman equations for these formulations.

2.  Show how an MDP with reward function $R(s,a,s')$ can be transformed
    into a different MDP with reward function $R(s,a)$, such that
    optimal policies in the new MDP correspond exactly to optimal
    policies in the original MDP.

3.  Now do the same to convert MDPs with $R(s,a)$ into MDPs with $R(s)$.

[threshold-cost-exercise]For the environment shown in , find all the
threshold values for $R(s)$ such that the optimal policy changes when
the threshold is crossed. You will need a way to calculate the optimal
policy and its value for fixed $R(s)$. (*Hint*: Prove that
the value of any fixed policy varies linearly with $R(s)$.)

[vi-contraction-exercise] on states that the Bellman operator is a
contraction.

1.  Show that, for any functions $f$ and $g$,
    $$|\max_a f(a) - \max_a g(a)| \leq \max_a |f(a) - g(a)|\ .$$

2.  Write out an expression for $|(B\,U_i - B\,U'_i)(s)|$ and then apply
    the result from (a) to complete the proof that the Bellman operator
    is a contraction.

This exercise considers two-player MDPs that correspond to zero-sum,
turn-taking games like those in . Let the players be $A$ and $B$, and
let $R(s)$ be the reward for player $A$ in state $s$. (The reward for
$B$ is always equal and opposite.)

1.  Let $U_A(s)$ be the utility of state $s$ when it is $A$’s turn to
    move in $s$, and let $U_B(s)$ be the utility of state $s$ when it is
    $B$’s turn to move in $s$. All rewards and utilities are calculated
    from $A$’s point of view (just as in a minimax game tree). Write
    down Bellman equations defining $U_A(s)$ and $U_B(s)$.

2.  Explain how to do two-player value iteration with these equations,
    and define a suitable termination criterion.

3.  Consider the game described in on . Draw the state space (rather
    than the game tree), showing the moves by $A$ as solid lines and
    moves by $B$ as dashed lines. Mark each state with $R(s)$. You will
    find it helpful to arrange the states $(s_A,s_B)$ on a
    two-dimensional grid, using $s_A$ and $s_B$ as “coordinates.”

4.  Now apply two-player value iteration to solve this game, and derive
    the optimal policy.

[grid-mdp-figure]

[3x3-mdp-exercise] Consider the $3 \times 3$ world shown in (a). The
transition model is the same as in the $4\stimes 3$ : 80% of the time
the agent goes in the direction it selects; the rest of the time it
moves at right angles to the intended direction.

Implement value iteration for this world for each value of $r$ below.
Use discounted rewards with a discount factor of 0.99. Show the policy
obtained in each case. Explain intuitively why the value of $r$ leads to
each policy.

1.  $r = 100$

2.  $r = -3$

3.  $r = 0$

4.  $r = +3$

[101x3-mdp-exercise] Consider the $101 \times 3$ world shown in (b). In
the start state the agent has a choice of two deterministic actions,
*Up* or *Down*, but in the other states the
agent has one deterministic action, *Right*. Assuming a
discounted reward function, for what values of the discount $\gamma$
should the agent choose *Up* and for which
*Down*? Compute the utility of each action as a function of
$\gamma$. (Note that this simple example actually reflects many
real-world situations in which one must weigh the value of an immediate
action versus the potential continual long-term consequences, such as
choosing to dump pollutants into a lake.)

Consider an undiscounted MDP having three states, (1, 2, 3), with
rewards $-1$, $-2$, $0$, respectively. State 3 is a terminal state. In
states 1 and 2 there are two possible actions: $a$ and $b$. The
transition model is as follows:

-   In state 1, action $a$ moves the agent to state 2 with probability
    0.8 and makes the agent stay put with probability 0.2.

-   In state 2, action $a$ moves the agent to state 1 with probability
    0.8 and makes the agent stay put with probability 0.2.

-   In either state 1 or state 2, action $b$ moves the agent to state 3
    with probability 0.1 and makes the agent stay put with probability
    0.9.

Answer the following questions:

1.  What can be determined *qualitatively* about the
    optimal policy in states 1 and 2?

2.  Apply policy iteration, showing each step in full, to determine the
    optimal policy and the values of states 1 and 2. Assume that the
    initial policy has action $b$ in both states.

3.  What happens to policy iteration if the initial policy has action
    $a$ in both states? Does discounting help? Does the optimal policy
    depend on the discount factor?

Consider the $4\stimes 3$ world shown in .

1.  Implement an environment simulator for this environment, such that
    the specific geography of the environment is easily altered. Some
    code for doing this is already in the online code repository.

2.  Create an agent that uses policy iteration, and measure its
    performance in the environment simulator from various starting
    states. Perform several experiments from each starting state, and
    compare the average total reward received per run with the utility
    of the state, as determined by your algorithm.

3.  Experiment with increasing the size of the environment. How does the
    run time for policy iteration vary with the size of the environment?

[policy-loss-exercise]How can the value determination algorithm be used
to calculate the expected loss experienced by an agent using a given set
of utility estimates and an estimated model , compared with an agent
using correct values?

[4x3-pomdp-exercise] Let the initial belief state $b_0$ for the
$4\stimes 3$ POMDP on be the uniform distribution over the nonterminal
states, i.e.,
$\<\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},\frac{1}{9},0,0\>$.
Calculate the exact belief state $b_1$ after the agent moves and its
sensor reports 1 adjacent wall. Also calculate $b_2$ assuming that the
same thing happens again.

What is the time complexity of $d$ steps of POMDP value iteration for a
sensorless environment?

[2state-pomdp-exercise] Consider a version of the two-state POMDP on in
which the sensor is 90% reliable in state 0 but provides no information
in state 1 (that is, it reports 0 or 1 with equal probability). Analyze,
either qualitatively or quantitatively, the utility function and the
optimal policy for this problem.

[dominant-equilibrium-exercise]Show that a dominant strategy equilibrium
is a Nash equilibrium, but not vice versa.

In the children’s game of rock–paper–scissors each player reveals at the
same time a choice of rock, paper, or scissors. Paper wraps rock, rock
blunts scissors, and scissors cut paper. In the extended version
rock–paper–scissors–fire–water, fire beats rock, paper, and scissors;
rock, paper, and scissors beat water; and water beats fire. Write out
the payoff matrix and find a mixed-strategy solution to this game.

Solve the game of *three*-finger Morra.

In the *Prisoner’s Dilemma*, consider the case where after
each round, Alice and Bob have probability $X$ meeting again. Suppose
both players choose the perpetual punishment strategy (where each will
choose ${refuse}$ unless the other player has ever played
${testify}$). Assume neither player has played ${testify}$ thus far.
What is the expected future total payoff for choosing to ${testify}$
versus ${refuse}$ when $X = .2$? How about when $X = .05$? For what
value of $X$ is the expected future total payoff the same whether one
chooses to ${testify}$ or ${refuse}$ in the current round?

The following payoff matrix, from by way of , shows a game between
politicians and the Federal Reserve.

                     Fed: contract   Fed: do nothing   Fed: expand
  ----------------- --------------- ----------------- -------------
  Pol: contract       $F=7, P=1$        $F=9,P=4$       $F=6,P=6$
  Pol: do nothing     $F=8, P=2$        $F=5,P=5$       $F=4,P=9$
  Pol: expand         $F=3, P=3$        $F=2,P=7$       $F=1,P=8$

Politicians can expand or contract fiscal policy, while the Fed can
expand or contract monetary policy. (And of course either side can
choose to do nothing.) Each side also has preferences for who should do
what—neither side wants to look like the bad guys. The payoffs shown are
simply the rank orderings: 9 for first choice through 1 for last choice.
Find the Nash equilibrium of the game in pure strategies. Is this a
Pareto-optimal solution? You might wish to analyze the policies of
recent administrations in this light.

A Dutch auction is similar in an English auction, but rather than
starting the bidding at a low price and increasing, in a Dutch auction
the seller starts at a high price and gradually lowers the price until
some buyer is willing to accept that price. (If multiple bidders accept
the price, one is arbitrarily chosen as the winner.) More formally, the
seller begins with a price $p$ and gradually lowers $p$ by increments of
$d$ until at least one buyer accepts the price. Assuming all bidders act
rationally, is it true that for arbitrarily small $d$, a Dutch auction
will always result in the bidder with the highest value for the item
obtaining the item? If so, show mathematically why. If not, explain how
it may be possible for the bidder with highest value for the item not to
obtain it.

Imagine an auction mechanism that is just like an ascending-bid auction,
except that at the end, the winning bidder, the one who bid $b_{max}$,
pays only $b_{max}/2$ rather than $b_{max}$. Assuming all agents are
rational, what is the expected revenue to the auctioneer for this
mechanism, compared with a standard ascending-bid auction?

Teams in the National Hockey League historically received 2 points for
winning a game and 0 for losing. If the game is tied, an overtime period
is played; if nobody wins in overtime, the game is a tie and each team
gets 1 point. But league officials felt that teams were playing too
conservatively in overtime (to avoid a loss), and it would be more
exciting if overtime produced a winner. So in 1999 the officials
experimented in mechanism design: the rules were changed, giving a team
that loses in overtime 1 point, not 0. It is still 2 points for a win
and 1 for a tie.

1.  Was hockey a zero-sum game before the rule change? After?

2.  Suppose that at a certain time $t$ in a game, the home team has
    probability $p$ of winning in regulation time, probability $0.78-p$
    of losing, and probability 0.22 of going into overtime, where they
    have probability $q$ of winning, $.9-q$ of losing, and .1 of tying.
    Give equations for the expected value for the home and visiting
    teams.

3.  Imagine that it were legal and ethical for the two teams to enter
    into a pact where they agree that they will skate to a tie in
    regulation time, and then both try in earnest to win in overtime.
    Under what conditions, in terms of $p$ and $q$, would it be rational
    for both teams to agree to this pact?

4.  report that since the rule change, the percentage of games with a
    winner in overtime went up 18.2%, as desired, but the percentage of
    overtime games also went up 3.6%. What does that suggest about
    possible collusion or conservative play after the rule change?

[^1]: Some definitions of MDPs allow the reward to depend on the action
    and outcome too, so the reward function is $R(s,a,s')$. This
    simplifies the description of some environments but does not change
    the problem in any fundamental way, as shown in .

[^2]: Although this seems obvious, it does not hold for finite-horizon
    policies or for other ways of combining rewards over time. The proof
    follows directly from the uniqueness of the utility function on
    states, as shown in .

[^3]: As with the reward function for MDPs, the sensor model can also
    depend on the action and outcome state, but again this change is not
    fundamental.

[^4]: Morra is a recreational version of an . In such games, an
    inspector chooses a day to inspect a facility (such as a restaurant
    or a biological weapons plant), and the facility operator chooses a
    day to hide all the nasty stuff. The inspector wins if the days are
    different, and the facility operator wins if they are the same.

[^5]: Pareto optimality is named after the economist Vilfredo Pareto
    (1848–1923).

[^6]: or a constant—see .

[^7]: It is a coincidence that these equations are the same as those for
    $p$; the coincidence arises because
    $U_E({one},{two})\eq U_E({two},{one})\eq
    -3$. This also explains why the optimal strategy is the same for
    both players.

[^8]: The word “auction” comes from the Latin *augere*, to
    increase.

[^9]: There is actually a small chance that the agent with highest $v_i$
    fails to get the goods, in the case in which\
    $b_o <
    v_i < b_o + d$. The chance of this can be made arbitrarily small by
    decreasing the increment $d$.

[^10]: Named after William Vickrey (1914–1996), who won the 1996 Nobel
    Prize in economics for this work and died of a heart attack three
    days later.
[learning-part]

Learning from Examples {#concept-learning-chapter}
======================

An agent is if it improves its performance on future tasks after making
observations about the world. Learning can range from the trivial, as
exhibited by jotting down a phone number, to the profound, as exhibited
by Albert Einstein, who inferred a new theory of the universe. In this
chapter we will concentrate on one class of learning problem, which
seems restricted but actually has vast applicability: from a collection
of input–output pairs, learn a function that predicts the output for new
inputs.

Why would we want an agent to learn? If the design of the agent can be
improved, why wouldn’t the designers just program in that improvement to
begin with? There are three main reasons. First, the designers cannot
anticipate all possible situations that the agent might find itself in.
For example, a robot designed to navigate mazes must learn the layout of
each new maze it encounters. Second, the designers cannot anticipate all
changes over time; a program designed to predict tomorrow’s stock market
prices must learn to adapt when conditions change from boom to bust.
Third, sometimes human programmers have no idea how to program a
solution themselves. For example, most people are good at recognizing
the faces of family members, but even the best programmers are unable to
program a computer to accomplish that task, except by using learning
algorithms. This chapter first gives an overview of the various forms of
learning, then describes one popular approach, decision-tree learning,
in , followed by a theoretical analysis of learning in . We look at
various learning systems used in practice: linear models, nonlinear
models (in particular, neural networks), nonparametric models, and
support vector machines. Finally we show how ensembles of models can
outperform a single model.

Forms of Learning {#learning-model-section}
-----------------

Any component of an agent can be improved by learning from data. The
improvements, and the techniques used to make them, depend on four major
factors:

-   Which *component* is to be improved.

-   What *prior knowledge* the agent already has.

-   What *representation* is used for the data and the
    component.

-   What *feedback* is available to learn from.

#### Components to be learned

described several agent designs. The components of these agents include:

1.  A direct mapping from conditions on the current state to actions.

2.  A means to infer relevant properties of the world from the percept
    sequence.

3.  Information about the way the world evolves and about the results of
    possible actions the agent can take.

4.  Utility information indicating the desirability of world states.

5.  Action-value information indicating the desirability of actions.

6.  Goals that describe classes of states whose achievement maximizes
    the agent’s utility.

Each of these components can be learned. Consider, for example, an agent
training to become a taxi driver. Every time the instructor shouts
“Brake!” the agent might learn a condition–action rule for when to brake
(component 1); the agent also learns every time the instructor does not
shout. By seeing many camera images that it is told contain buses, it
can learn to recognize them (2). By trying actions and observing the
results—for example, braking hard on a wet road—it can learn the effects
of its actions (3). Then, when it receives no tip from passengers who
have been thoroughly shaken up during the trip, it can learn a useful
component of its overall utility function (4).

#### Representation and prior knowledge

We have seen several examples of representations for agent components:
propositional and first-order logical sentences for the components in a
logical agent; Bayesian networks for the inferential components of a
decision-theoretic agent, and so on. Effective learning algorithms have
been devised for all of these representations. This chapter (and most of
current machine learning research) covers inputs that form a —a vector
of attribute values—and outputs that can be either a continuous
numerical value or a discrete value. covers functions and prior
knowledge composed of first-order logic sentences, and concentrates on
Bayesian networks.

There is another way to look at the various types of learning. We say
that learning a (possibly incorrect) general function or rule from
specific input–output pairs is called . We will see in that we can also
do or : going from a known general rule to a new rule that is logically
entailed, but is useful because it allows more efficient processing.

#### Feedback to learn from

There are three *types of feedback* that determine the
three main types of learning:

In the agent learns patterns in the input even though no explicit
feedback is supplied. The most common unsupervised learning task is :
detecting potentially useful clusters of input examples. For example, a
taxi agent might gradually develop a concept of “good traffic days” and
“bad traffic days” without ever being given labeled examples of each by
a teacher.

In the agent learns from a series of reinforcements—rewards or
punishments. For example, the lack of a tip at the end of the journey
gives the taxi agent an indication that it did something wrong. The two
points for a win at the end of a chess game tells the agent it did
something right. It is up to the agent to decide which of the actions
prior to the reinforcement were most responsible for it.

In the agent observes some example input–output pairs and learns a
function that maps from input to output. In component 1 above, the
inputs are percepts and the output are provided by a teacher who says
“Brake!” or “Turn left.” In component 2, the inputs are camera images
and the outputs again come from a teacher who says “that’s a bus.” In 3,
the theory of braking is a function from states and braking actions to
stopping distance in feet. In this case the output value is available
directly from the agent’s percepts (after the fact); the environment is
the teacher.

In practice, these distinction are not always so crisp. In we are given
a few labeled examples and must make what we can of a large collection
of unlabeled examples. Even the labels themselves may not be the
oracular truths that we hope for. Imagine that you are trying to build a
system to guess a person’s age from a photo. You gather some labeled
examples by snapping pictures of people and asking their age. That’s
supervised learning. But in reality some of the people lied about their
age. It’s not just that there is random noise in the data; rather the
inaccuracies are systematic, and to uncover them is an unsupervised
learning problem involving images, self-reported ages, and true
(unknown) ages. Thus, both noise and lack of labels create a continuum
between supervised and unsupervised learning.

Supervised Learning {#inductive-learning-section}
-------------------

The task of supervised learning is this:

> Given a of $\Ncount$ example input–output pairs
> $$(x_1, y_1), (x_2, y_2), \ldots (x_{\Ncount}, y_{\Ncount}) \ ,$$
> where each $y_j$ was generated by an unknown function $y = f(x)$,\
> discover a function $h$ that approximates the true function $f$.

Here $x$ and $y$ can be any value; they need not be numbers. The
function $h$ is a .[^1] Learning is a search through the space of
possible hypotheses for one that will perform well, even on new examples
beyond the training set. To measure the accuracy of a hypothesis we give
it a of examples that are distinct from the training set. We say a
hypothesis well if it correctly predicts the value of $y$ for novel
examples. Sometimes the function $f$ is stochastic—it is not strictly a
function of $x$, and what we have to learn is a conditional probability
distribution, $\pv(Y\given
x)$.

When the output $y$ is one of a finite set of values (such as
*sunny, cloudy* or *rainy*), the learning
problem is called , and is called Boolean or binary classification if
there are only two values. When $y$ is a number (such as tomorrow’s
temperature), the learning problem is called . (Technically, solving a
regression problem is finding a conditional expectation or average value
of $y$, because the probability that we have found
*exactly* the right real-valued number for $y$ is 0.)

[xy-plot-figure]

shows a familiar example: fitting a function of a single variable to
some data points. The examples are points in the $(x,y)$ plane, where
$y=f(x)$. We don’t know what $f$ is, but we will approximate it with a
function $h$ selected from a , $\Hyp$, which for this example we will
take to be the set of polynomials, such as $x^5 + 3x^2+2$. (a) shows
some data with an exact fit by a straight line (the polynomial
$0.4x + 3$). The line is called a hypothesis because it agrees with all
the data. (b) shows a high-degree polynomial that is also consistent
with the same data. This illustrates a fundamental problem in inductive
learning:

how do we choose from among multiple consistent hypotheses?

One answer is to prefer the *simplest* hypothesis
consistent with the data. This principle is called , after the
14th-century English philosopher William of Ockham, who used it to argue
sharply against all sorts of complications. Defining simplicity is not
easy, but it seems clear that a degree-1 polynomial is simpler than a
degree-7 polynomial, and thus (a) should be preferred to (b). We will
make this intuition more precise in .

​(c) shows a second data set. There is no consistent straight line for
this data set; in fact, it requires a degree-6 polynomial for an exact
fit. There are just 7 data points, so a polynomial with 7 parameters
does not seem to be finding any pattern in the data and we do not expect
it to generalize well. A straight line that is not consistent with any
of the data points, but might generalize fairly well for unseen values
of $x$, is also shown in (c).

In general, there is a tradeoff between complex hypotheses that fit the
training data well and simpler hypotheses that may generalize better.

In (d) we expand the hypothesis space $\Hyp$ to allow polynomials over
both $x$ and $\sin(x)$, and find that the data in (c) can be fitted
exactly by a simple function of the form $ax +
b + c\sin(x)$. This shows the importance of the choice of hypothesis
space. We say that a learning problem is if the hypothesis space
contains the true function. Unfortunately, we cannot always tell whether
a given learning problem is realizable, because the true function is not
known.

In some cases, an analyst looking at a problem is willing to make more
fine-grained distinctions about the hypothesis space, to say—even before
seeing any data—not just that a hypothesis is possible or impossible,
but rather how probable it is. Supervised learning can be done by
choosing the hypothesis $h^*$ that is most probable given the data:
$$h^* = \argmax_{h \in \Hyp} P(h|{data}) \ .$$ By Bayes’ rule this is
equivalent to $$h^* = \argmax_{h \in \Hyp} P({data}|h) \, P(h) \ .$$
Then we can say that the prior probability $P(h)$ is high for a degree-1
or -2 polynomial, lower for a degree-7 polynomial, and especially low
for degree-7 polynomials with large, sharp spikes as in (b). We allow
unusual-looking functions when the data say we really need them, but we
discourage them by giving them a low prior probability.

Why not let $\Hyp$ be the class of all Java programs, or Turing
machines? After all, every computable function can be represented by
some Turing machine, and that is the best we can do. One problem with
this idea is that it does not take into account the computational
complexity of learning.

There is a tradeoff between the expressiveness of a hypothesis space and
the complexity of finding a good hypothesis within that space.

For example, fitting a straight line to data is an easy computation;
fitting high-degree polynomials is somewhat harder; and fitting Turing
machines is in general undecidable. A second reason to prefer simple
hypothesis spaces is that presumably we will want to use $h$ after we
have learned it, and computing $h(x)$ when $h$ is a linear function is
guaranteed to be fast, while computing an arbitrary Turing machine
program is not even guaranteed to terminate. For these reasons, most
work on learning has focused on simple representations.

We will see that the expressiveness–complexity tradeoff is not as simple
as it first seems: it is often the case, as we saw with first-order
logic in , that an expressive language makes it possible for a
*simple* hypothesis to fit the data, whereas restricting
the expressiveness of the language means that any consistent hypothesis
must be very complex. For example, the rules of chess can be written in
a page or two of first-order logic, but require thousands of pages when
written in propositional logic.

Learning Decision Trees {#decision-tree-section}
-----------------------

Decision tree induction is one of the simplest and yet most successful
forms of machine learning. We first describe the representation—the
hypothesis space—and then show how to learn a good hypothesis.

### The decision tree representation

A represents a function that takes as input a vector of attribute values
and returns a “decision”—a single output value. The input and output
values can be discrete or continuous. For now we will concentrate on
problems where the inputs have discrete values and the output has
exactly two possible values; this is Boolean classification, where each
example input will be classified as true (a example) or false (a
example).

A decision tree reaches its decision by performing a sequence of tests.
Each internal node in the tree corresponds to a test of the value of one
of the input attributes, ${A}_i$, and the branches from the node are
labeled with the possible values of the attribute,
${A}_i\eq v_{i\dtvalue}$. Each leaf node in the tree specifies a value
to be returned by the function. The decision tree representation is
natural for humans; indeed, many “How To” manuals (e.g., for car repair)
are written entirely as a single decision tree stretching over hundreds
of pages.

As an example, we will build a decision tree to decide whether to wait
for a table at a restaurant. The aim here is to learn a definition for
the ${WillWait}$. First we list the attributes that we will consider
as part of the input:

1.  ${Alternate}$: whether there is a suitable alternative restaurant
    nearby.

2.  ${Bar}$: whether the restaurant has a comfortable bar area to wait
    in.

3.  ${Fri/Sat}$: true on Fridays and Saturdays.

4.  ${Hungry}$: whether we are hungry.

5.  ${Patrons}$: how many people are in the restaurant (values are
    ${None}$, ${Some}$, and ${Full}$).

6.  ${Price}$: the restaurant’s price range (, , ).

7.  ${Raining}$: whether it is raining outside.

8.  ${Reservation}$: whether we made a reservation.

9.  ${Type}$: the kind of restaurant (French, Italian, Thai, or
    burger).

10. ${WaitEstimate}$: the wait estimated by the host (0–10 minutes,
    10–30, 30–60, or $>$60).

[restaurant-tree-figure]

Note that every variable has a small set of possible values; the value
of ${WaitEstimate}$, for example, is not an integer, rather it is one
of the four discrete values 0–10, 10–30, 30–60, or $>$60. The decision
tree usually used by one of us (SR) for this domain is shown in . Notice
that the tree ignores the ${Price}$ and ${Type}$ attributes.
Examples are processed by the tree starting at the root and following
the appropriate branch until a leaf is reached. For instance, an example
with $\v{Patrons}\eq \v{Full}$ and $\v{WaitEstimate}\eq 0$–10 will be
classified as positive (i.e., yes, we will wait for a table).

### Expressiveness of decision trees

A Boolean decision tree is logically equivalent to the assertion that
the goal attribute is true if and only if the input attributes satisfy
one of the paths leading to a leaf with value ${true}$. Writing this
out in propositional logic, we have[logical-tree-page]
$${Goal} \lequiv ({Path}_1 \lor {Path}_2 \lor \cdots )\ ,$$ where
each ${Path}$ is a conjunction of attribute-value tests required to
follow that path. Thus, the whole expression is equivalent to
disjunctive normal form (see ), which means that any function in
propositional logic can be expressed as a decision tree. As an example,
the rightmost path in is
$${Path} = ({Patrons} \eq {Full} \land {WaitEstimate} \eq \mbox{0--10}) \ .$$
For a wide variety of problems, the decision tree format yields a nice,
concise result. But some functions cannot be represented concisely. For
example, the majority function, which returns true if and only if more
than half of the inputs are true, requires an exponentially large
decision tree. In other words, decision trees are good for some kinds of
functions and bad for others. Is there *any* kind of
representation that is efficient for *all* kinds of
functions? Unfortunately, the answer is no. We can show this in a
general way. Consider the set of all Boolean functions on $\Acount$
attributes. How many different functions are in this set? This is just
the number of different truth tables that we can write down, because the
function is defined by its truth table. A truth table over $\Acount$
attributes has $2^{\Acount}$ rows, one for each combination of values of
the attributes. We can consider the “answer” column of the table as a
$2^{\Acount}$-bit number that defines the function. That means there are
$2^{2^{\Acount}}$ different functions (and there will be more than that
number of trees, since more than one tree can compute the same
function). This is a scary number. For example, with just the ten
Boolean attributes of our restaurant problem there are $2^{1024}$ or
about $10^{308}$ different functions to choose from, and for 20
attributes there are over $10^{300,000}$. We will need some ingenious
algorithms to find good hypotheses in such a large space.

### Inducing decision trees from examples

[tb] [restaurant-example-table]

An example for a Boolean decision tree consists of an $(\x, y)$ pair,
where $\x$ is a vector of values for the input attributes, and $y$ is a
single Boolean output value. A training set of 12 examples is shown in .
The positive examples are the ones in which the goal ${WillWait}$ is
true $(\x_1,\x_3,\ldots)$; the negative examples are the ones in which
it is false $(\x_2, \x_5, \ldots)$.

We want a tree that is consistent with the examples and is as small as
possible. Unfortunately, no matter how we measure size, it is an
intractable problem to find the smallest consistent tree; there is no
way to efficiently search through the $2^{2^{\Acount}}$ trees. With some
simple heuristics, however, we can find a good approximate solution: a
small (but not smallest) consistent tree. The algorithm adopts a greedy
divide-and-conquer strategy: always test the most important attribute
first. This test divides the problem up into smaller subproblems that
can then be solved recursively. By “most important attribute,” we mean
the one that makes the most difference to the classification of an
example. That way, we hope to get to the correct classification with a
small number of tests, meaning that all paths in the tree will be short
and the tree as a whole will be shallow.

[restaurant-stub-figure]

​(a) shows that ${Type}$ is a poor attribute, because it leaves us
with four possible outcomes, each of which has the same number of
positive as negative examples. On the other hand, in (b) we see that
${Patrons}$ is a fairly important attribute, because if the value is
${None}$ or ${Some}$, then we are left with example sets for which
we can answer definitively (${No}$ and ${Yes}$, respectively). If
the value is ${Full}$, we are left with a mixed set of examples. In
general, after the first attribute test splits up the examples, each
outcome is a new decision tree learning problem in itself, with fewer
examples and one less attribute. There are four cases to consider for
these recursive problems:

1.  If the remaining examples are all positive (or all negative), then
    we are done: we can answer ${Yes}$ or ${No}$. (b) shows examples
    of this happening in the ${None}$ and ${Some}$ branches.

2.  If there are some positive and some negative examples, then choose
    the best attribute to split them. (b) shows ${Hungry}$ being used
    to split the remaining examples.

3.  If there are no examples left, it means that no example has been
    observed for this combination of attribute values, and we return a
    default value calculated from the plurality classification of all
    the examples that were used in constructing the node’s parent. These
    are passed along in the variable $\v{parent\_examples}$.

4.  If there are no attributes left, but both positive and negative
    examples, it means that these examples have exactly the same
    description, but different classifications. This can happen because
    there is an error or in the data; because the domain is
    nondeterministic; or because we can’t observe an attribute that
    would distinguish the examples. The best we can do is return the
    plurality classification of the remaining examples.

[DTL-algorithm]

[induced-restaurant-tree-figure]

[restaurant-dtl-curve-figure]

The algorithm is shown in . Note that the set of examples is crucial for
*constructing* the tree, but nowhere do the examples appear
in the tree itself. A tree consists of just tests on attributes in the
interior nodes, values of attributes on the branches, and output values
on the leaf nodes. The details of the function are given in . The output
of the learning algorithm on our sample training set is shown in . The
tree is clearly different from the original tree shown in . One might
conclude that the learning algorithm is not doing a very good job of
learning the correct function. This would be the wrong conclusion to
draw, however. The learning algorithm looks at the
*examples*, not at the correct function, and in fact, its
hypothesis (see ) not only is consistent with all the examples, but is
considerably simpler than the original tree! The learning algorithm has
no reason to include tests for ${Raining}$ and ${Reservation}$,
because it can classify all the examples without them. It has also
detected an interesting and previously unsuspected pattern: the first
author will wait for Thai food on weekends. It is also bound to make
some mistakes for cases where it has seen no examples. For example, it
has never seen a case where the wait is 0–10 minutes but the restaurant
is full. In that case it says not to wait when ${Hungry}$ is false,
but I (SR) would certainly wait. With more training examples the
learning program could correct this mistake.

We note there is a danger of over-interpreting the tree that the
algorithm selects. When there are several variables of similar
importance, the choice between them is somewhat arbitrary: with slightly
different input examples, a different variable would be chosen to split
on first, and the whole tree would look completely different. The
function computed by the tree would still be similar, but the structure
of the tree can vary widely.

We can evaluate the accuracy of a learning algorithm with a , as shown
in . We have 100 examples at our disposal, which we split into a
training set and a test set. We learn a hypothesis $h$ with the training
set and measure its accuracy with the test set. We do this starting with
a training set of size 1 and increasing one at a time up to size 99. For
each size we actually repeat the process of randomly splitting 20 times,
and average the results of the 20 trials. The curve shows that as the
training set size grows, the accuracy increases. (For this reason,
learning curves are also called .) In this graph we reach 95% accuracy,
and it looks like the curve might continue to increase with more data.

### Choosing attribute tests {#information-theory-section}

[importance-section]

The greedy search used in decision tree learning is designed to
approximately minimize the depth of the final tree. The idea is to pick
the attribute that goes as far as possible toward providing an exact
classification of the examples. A perfect attribute divides the examples
into sets, each of which are all positive or all negative and thus will
be leaves of the tree. The ${Patrons}$ attribute is not perfect, but
it is fairly good. A really useless attribute, such as ${Type}$,
leaves the example sets with roughly the same proportion of positive and
negative examples as the original set.

All we need, then, is a formal measure of “fairly good” and “really
useless” and we can implement the function of . We will use the notion
of information gain, which is defined in terms of , the fundamental
quantity in information theory @Shannon+Weaver:1949.

Entropy is a measure of the uncertainty of a random variable;
acquisition of information corresponds to a reduction in entropy. A
random variable with only one value—a coin that always comes up
heads—has no uncertainty and thus its entropy is defined as zero; thus,
we gain no information by observing its value. A flip of a fair coin is
equally likely to come up heads or tails, 0 or 1, and we will soon show
that this counts as “1 bit” of entropy. The roll of a fair
*four*-sided die has 2 bits of entropy, because it takes
two bits to describe one of four equally probable choices. Now consider
an unfair coin that comes up heads 99% of the time. Intuitively, this
coin has less uncertainty than the fair coin—if we guess heads we’ll be
wrong only 1% of the time—so we would like it to have an entropy measure
that is close to zero, but positive. In general, the entropy of a random
variable $V$ with values $v_{\dtvalue}$, each with probability
$P(v_{\dtvalue})$, is defined as
$$\mbox{Entropy:} \quad H(V) = \sum\limits_{{\dtvalue}} P(v_{\dtvalue})\log_2 \frac{1}{P(v_{\dtvalue})} = - \sum\limits_{{\dtvalue}}  P(v_{\dtvalue}) \log_2 P(v_{\dtvalue})\ .$$
We can check that the entropy of a fair coin flip is indeed 1 bit:
$$H({Fair}) =  - (0.5 \log_2 0.5 + 0.5 \log_2 0.5)   = 1\ \mbox{.}$$
If the coin is loaded to give 99% heads, we get
$$H({Loaded}) = - (0.99 \log_2 0.99 + 0.01 \log_2 0.01)   \approx 0.08\ \mbox{bits.}$$
It will help to define $\BinH(q)$ as the entropy of a Boolean random
variable that is true with probability $q$:
$$\BinH(q)\eq {-}(q\log_2 q + (1-q)\log_2 (1-q)) \ .$$ Thus,
$H({Loaded}) \eq \BinH(0.99) \approx 0.08$. Now let’s get back to
decision tree learning. If a training set contains $p$ positive examples
and $n$ negative examples, then the entropy of the goal attribute on the
whole set is $$H({Goal}) = \BinH\bigg(\frac{p}{p+n}\bigg) \ .$$ The
restaurant training set in has $p=n=6$, so the corresponding entropy is
$\BinH(0.5)$ or exactly 1 bit. A test on a single attribute $A$ might
give us only part of this 1 bit. We can measure exactly how much by
looking at the entropy remaining *after* the attribute
test.

An attribute $A$ with $\Vcount$ distinct values divides the training set
$E$ into subsets $E_1,\ldots, E_{\Vcount}$. Each subset $E_{\dtvalue}$
has $p_{\dtvalue}$ positive examples and $n_{\dtvalue}$ negative
examples, so if we go along that branch, we will need an additional
$\BinH({p_{\dtvalue}}/{(p_{\dtvalue}+n_{\dtvalue})})$ bits of
information to answer the question. A randomly chosen example from the
training set has the ${\dtvalue}$th value for the attribute with
probability $(p_{\dtvalue}+n_{\dtvalue})/(p+n)$, so the expected entropy
remaining after testing attribute $A$ is
$${Remainder}(A) = \sum\limits_{{\dtvalue}=1}^{\Vcount} {\textstyle
         \frac{p_{\dtvalue}+n_{\dtvalue}}{p+n} 
             \BinH(\frac{p_{\dtvalue}}{p_{\dtvalue}+n_{\dtvalue}})}\ .$$
The [info-gain-page] from the attribute test on $A$ is the expected
reduction in entropy: $${\textstyle
   {Gain}(A) = \BinH({\textstyle\frac{p}{p+n}}) - {Remainder}(A)\ .
}$$ In fact ${Gain}(A)$ is just what we need to implement the
function. Returning to the attributes considered in , we have
$${\textstyle
  {Gain}({Patrons}) = 1 - \left[\frac{2}{{12}} \BinH(\frac{0}{2}) + 
                       \frac{4}{{12}} \BinH(\frac{4}{4})
                      +\frac{6}{{12}} \BinH(\frac{2}{6}) \right]
\approx {0.541} \mbox{ bits,}
}$$ $${\textstyle
  {Gain}({Type}) = 1 - \left[{\frac{2}{{12}} \BinH(\frac{1}{2}) +
                    \frac{2}{{12}} \BinH(\frac{1}{2}) +
                    \frac{4}{{12}} \BinH(\frac{2}{4}) +
                    \frac{4}{{12}} \BinH(\frac{2}{4})} \right]     = {0} \mbox{ bits,}
}$$ confirming our intuition that ${Patrons}$ is a better attribute to
split on. In fact, ${Patrons}$ has the maximum gain of any of the
attributes and would be chosen by the decision-tree learning algorithm
as the root.

### Generalization and overfitting {#chi-squared-section}

On some problems, the algorithm will generate a large tree when there is
actually no pattern to be found. Consider the problem of trying to
predict whether the roll of a die will come up as 6 or not. Suppose that
experiments are carried out with various dice and that the attributes
describing each training example include the color of the die, its
weight, the time when the roll was done, and whether the experimenters
had their fingers crossed. If the dice are fair, the right thing to
learn is a tree with a single node that says “no,” But the algorithm
will seize on any pattern it can find in the input. If it turns out that
there are 2 rolls of a 7-gram blue die with fingers crossed and they
both come out 6, then the algorithm may construct a path that predicts 6
in that case. This problem is called . A general phenomenon, overfitting
occurs with all types of learners, even when the target function is not
at all random. In (b) and (c), we saw polynomial functions overfitting
the data. Overfitting becomes more likely as the hypothesis space and
the number of input attributes grows, and less likely as we increase the
number of training examples.

For decision trees, a technique called combats overfitting. Pruning
works by eliminating nodes that are not clearly relevant. We start with
a full tree, as generated by . We then look at a test node that has only
leaf nodes as descendants. If the test appears to be
irrelevant—detecting only noise in the data—then we eliminate the test,
replacing it with a leaf node. We repeat this process, considering each
test with only leaf descendants, until each one has either been pruned
or accepted as is.

The question is, how do we detect that a node is testing an irrelevant
attribute? Suppose we are at a node consisting of $p$ positive and $n$
negative examples. If the attribute is irrelevant, we would expect that
it would split the examples into subsets that each have roughly the same
proportion of positive examples as the whole set, $p/(p+n)$, and so the
information gain will be close to zero.[^2] Thus, the information gain
is a good clue to irrelevance. Now the question is, how large a gain
should we require in order to split on a particular attribute?

We can answer this question by using a statistical . Such a test begins
by assuming that there is no underlying pattern (the so-called ). Then
the actual data are analyzed to calculate the extent to which they
deviate from a perfect absence of pattern. If the degree of deviation is
statistically unlikely (usually taken to mean a 5% probability or less),
then that is considered to be good evidence for the presence of a
significant pattern in the data. The probabilities are calculated from
standard distributions of the amount of deviation one would expect to
see in random sampling.

In this case, the null hypothesis is that the attribute is irrelevant
and, hence, that the information gain for an infinitely large sample
would be zero. We need to calculate the probability that, under the null
hypothesis, a sample of size $v\eq n+p$ would exhibit the observed
deviation from the expected distribution of positive and negative
examples. We can measure the deviation by comparing the actual numbers
of positive and negative examples in each subset, $p_{\dtvalue}$ and
$n_{\dtvalue}$, with the expected numbers, $\hat p_{\dtvalue}$ and
$\hat n_{\dtvalue}$, assuming true irrelevance:
$$\hat p_{\dtvalue} = p\times\frac{p_{\dtvalue}+n_{\dtvalue}}{p+n} \qquad\qquad
  \hat n_{\dtvalue} = n\times\frac{p_{\dtvalue}+n_{\dtvalue}}{p+n}\ .$$
A convenient measure of the total deviation is given by
$$\ChiSquaredDeviation = \sum_{{\dtvalue}=1}^{\Vcount} \frac{(p_{\dtvalue}-\hat p_{\dtvalue})^2}{\hat p_{\dtvalue}} +
                   \frac{(n_{\dtvalue}-\hat n_{\dtvalue})^2}{\hat
                   n_{\dtvalue}}\ .$$ Under the null hypothesis, the
value of $\ChiSquaredDeviation$ is distributed according to the $\chi^2$
(chi-squared) distribution with $v-1$ degrees of freedom. We can use a
$\chi^2$ table or a standard statistical library routine to see if a
particular $\ChiSquaredDeviation$ value confirms or rejects the null
hypothesis. For example, consider the restaurant type attribute, with
four values and thus three degrees of freedom. A value of
$\ChiSquaredDeviation\eq 7.82$ or more would reject the null hypothesis
at the 5% level (and a value of $\ChiSquaredDeviation\eq 11.35$ or more
would reject at the 1% level). asks you to extend the algorithm to
implement this form of pruning, which is known as .

With pruning, noise in the examples can be tolerated. Errors in the
example’s label (e.g., an example $(\x, {Yes})$ that should be
$(\x, {No})$) give a linear increase in prediction error, whereas
errors in the descriptions of examples (e.g., ${Price}\eq \$$ when it
was actually ${Price}\eq \$\$$) have an asymptotic effect that gets
worse as the tree shrinks down to smaller sets. Pruned trees perform
significantly better than unpruned trees when the data contain a large
amount of noise. Also, the pruned trees are often much smaller and hence
easier to understand.

One final warning: You might think that $\chi^2$ pruning and information
gain look similar, so why not combine them using an approach called
—have the decision tree algorithm stop generating nodes when there is no
good attribute to split on, rather than going to all the trouble of
generating nodes and then pruning them away. The problem with early
stopping is that it stops us from recognizing situations where there is
no one good attribute, but there are combinations of attributes that are
informative. For example, consider the XOR function of two binary
attributes. If there are roughly equal number of examples for all four
combinations of input values, then neither attribute will be
informative, yet the correct thing to do is to split on one of the
attributes (it doesn’t matter which one), and then at the second level
we will get splits that are informative. Early stopping would miss this,
but generate-and-then-prune handles it correctly.

### Broadening the applicability of decision trees {#broadening-decision-tree-section}

In order to extend decision tree induction to a wider variety of
problems, a number of issues must be addressed. We will briefly mention
several, suggesting that a full understanding is best obtained by doing
the associated exercises:

In many domains, not all the attribute values will be known for every
example. The values might have gone unrecorded, or they might be too
expensive to obtain. This gives rise to two problems: First, given a
complete decision tree, how should one classify an example that is
missing one of the test attributes? Second, how should one modify the
information-gain formula when some examples have unknown values for the
attribute? These questions are addressed in .

When an attribute has many possible values, the information gain measure
gives an inappropriate indication of the attribute’s usefulness. In the
extreme case, an attribute such as ${ExactTime}$ has a different value
for every example, which means each subset of examples is a singleton
with a unique classification, and the information gain measure would
have its highest value for this attribute. But choosing this split first
is unlikely to yield the best tree. One solution is to use the ().
Another possibility is to allow a Boolean test of the form
$A\eq v_{\dtvalue}$, that is, picking out just one of the possible
values for an attribute, leaving the remaining values to possibly be
tested later in the tree.

Continuous or integer-valued attributes such as ${Height}$ and
${Weight}$, have an infinite set of possible values. Rather than
generate infinitely many branches, decision-tree learning algorithms
typically find the that gives the highest information gain. For example,
at a given node in the tree, it might be the case that testing on
${Weight} > {160}$ gives the most information. Efficient methods exist
for finding good split points: start by sorting the values of the
attribute, and then consider only split points that are between two
examples in sorted order that have different classifications, while
keeping track of the running totals of positive and negative examples on
each side of the split point. Splitting is the most expensive part of
real-world decision tree learning applications.

If we are trying to predict a numerical output value, such as the price
of an apartment, then we need a rather than a classification tree. A
regression tree has at each leaf a linear function of some subset of
numerical attributes, rather than a single value. For example, the
branch for two-bedroom apartments might end with a linear function of
square footage, number of bathrooms, and average income for the
neighborhood. The learning algorithm must decide when to stop splitting
and begin applying linear regression (see ) over the attributes.

A decision-tree learning system for real-world applications must be able
to handle all of these problems. Handling continuous-valued variables is
especially important, because both physical and financial processes
provide numerical data. Several commercial packages have been built that
meet these criteria, and they have been used to develop thousands of
fielded systems. In many areas of industry and commerce, decision trees
are usually the first method tried when a classification method is to be
extracted from a data set. One important property of decision trees is
that it is possible for a human to understand the reason for the output
of the learning algorithm. (Indeed, this is a *legal
requirement* for financial decisions that are subject to
anti-discrimination laws.) This is a property not shared by some other
representations, such as neural networks.

Evaluating and Choosing the Best Hypothesis {#best-hypothesis-section}
-------------------------------------------

We want to learn a hypothesis that fits the future data best. To make
that precise we need to define “future data” and “best.” We make the :
that there is a probability distribution over examples that remains
stationary over time. Each example data point (before we see it) is a
random variable $E_j$ whose observed value $e_j \eq (x_j,y_j)$ is
sampled from that distribution, and is independent of the previous
examples: $$\pv(E_j|E_{j-1},E_{j-2},\ldots) = \pv(E_j)\ ,$$ and each
example has an identical prior probability distribution:
$$\pv(E_j) = \pv(E_{j-1}) = \pv(E_{j-2}) = \cdots \ .$$ Examples that
satisfy these assumptions are called *independent and identically
distributed* or [iid-page]. An i.i.d. assumption connects the
past to the future; without some such connection, all bets are off—the
future could be anything. (We will see later that learning can still
occur if there are *slow* changes in the distribution.)

The next step is to define “best fit.” We define the of a hypothesis as
the proportion of mistakes it makes—the proportion of times that
$h(x) \neq y$ for an $(x, y)$ example. Now, just because a hypothesis
$h$ has a low error rate on the training set does not mean that it will
generalize well. A professor knows that an exam will not accurately
evaluate students if they have already seen the exam questions.
Similarly, to get an accurate evaluation of a hypothesis, we need to
test it on a set of examples it has not seen yet. The simplest approach
is the one we have seen already: randomly split the available data into
a training set from which the learning algorithm produces $h$ and a test
set on which the accuracy of $h$ is evaluated. This method, sometimes
called , has the disadvantage that it fails to use all the available
data; if we use half the data for the test set, then we are only
training on half the data, and we may get a poor hypothesis. On the
other hand, if we reserve only 10% of the data for the test set, then we
may, by statistical chance, get a poor estimate of the actual accuracy.

We can squeeze more out of the data and still get an accurate estimate
using a technique called . The idea is that each example serves double
duty—as training data and test data. First we split the data into $k$
equal subsets. We then perform $k$ rounds of learning; on each round
$1/k$ of the data is held out as a test set and the remaining examples
are used as training data. The average test set score of the $k$ rounds
should then be a better estimate than a single score. Popular values for
$k$ are 5 and 10—enough to give an estimate that is statistically likely
to be accurate, at a cost of 5 to 10 times longer computation time. The
extreme is $k=n$, also known as or .

Despite the best efforts of statistical methodologists, users frequently
invalidate their results by inadvertently at the test data. Peeking can
happen like this: A learning algorithm has various “knobs” that can be
twiddled to tune its behavior—for example, various different criteria
for choosing the next attribute in decision tree learning. The
researcher generates hypotheses for various different settings of the
knobs, measures their error rates on the test set, and reports the error
rate of the best hypothesis. Alas, peeking has occurred! The reason is
that the hypothesis was selected *on the basis of its test set
error rate*, so information about the test set has leaked into
the learning algorithm.

Peeking is a consequence of using test-set performance to both
*choose* a hypothesis and *evaluate* it. The
way to avoid this is to *really* hold the test set out—lock
it away until you are completely done with learning and simply wish to
obtain an independent evaluation of the final hypothesis. (And then, if
you don’t like the results $\ldots$ you have to obtain, and lock away, a
completely new test set if you want to go back and find a better
hypothesis.) If the test set is locked away, but you still want to
measure performance on unseen data as a way of selecting a good
hypothesis, then divide the available data (without the test set) into a
training set and a . The next section shows how to use validation sets
to find a good tradeoff between hypothesis complexity and goodness of
fit.

### Model selection: Complexity versus goodness of fit {#cross-validation-search-section}

In () we showed that higher-degree polynomials can fit the training data
better, but when the degree is too high they will overfit, and perform
poorly on validation data. Choosing the degree of the polynomial is an
instance of the problem of . You can think of the task of finding the
best hypothesis as two tasks: model selection defines the hypothesis
space and then finds the best hypothesis within that space.

[cross-validation-wrapper-algorithm]

[cross-validation-curve-figure]

In this section we explain how to select among models that are
parameterized by šize. For example, with polynomials we have
$\v{size}\eq 1$ for linear functions, $\v{size}\eq 2$ for quadratics,
and so on. For decision trees, the size could be the number of nodes in
the tree. In all cases we want to find the value of the $\v{size}$
parameter that best balances underfitting and overfitting to give the
best test set accuracy.

An algorithm to perform model selection and optimization is shown in .
It is a that takes a learning algorithm as an argument (, for example).
The wrapper enumerates models according to a parameter, šize. For each
size, it uses cross validation on Ľearner to compute the average error
rate on the training and test sets. We start with the smallest, simplest
models (which probably underfit the data), and iterate, considering more
complex models at each step, until the models start to overfit. In we
see typical curves: the training set error decreases monotonically
(although there may in general be slight random variation), while the
validation set error decreases at first, and then increases when the
model begins to overfit. The cross-validation procedure picks the value
of šize with the lowest validation set error; the bottom of the U-shaped
curve. We then generate a hypothesis of that šize, using all the data
(without holding out any of it). Finally, of course, we should evaluate
the returned hypothesis on a separate test set.

This approach requires that the learning algorithm accept a parameter,
šize, and deliver a hypothesis of that size. As we said, for decision
tree learning, the size can be the number of nodes. We can modify so
that it takes the number of nodes as an input, builds the tree
breadth-first rather than depth-first (but at each level it still
chooses the highest gain attribute first), and stops when it reaches the
desired number of nodes.

### From error rates to loss

So far, we have been trying to minimize error rate. This is clearly
better than maximizing error rate, but it is not the full story.
Consider the problem of classifying email messages as spam or non-spam.
It is worse to classify non-spam as spam (and thus potentially miss an
important message) then to classify spam as non-spam (and thus suffer a
few seconds of annoyance). So a classifier with a 1% error rate, where
almost all the errors were classifying spam as non-spam, would be better
than a classifier with only a 0.5% error rate, if most of those errors
were classifying non-spam as spam. We saw in that decision-makers should
maximize expected utility, and utility is what learners should maximize
as well. In machine learning it is traditional to express utilities by
means of a . The loss function $L(x, y, \hat{y})$ is defined as the
amount of utility lost by predicting $h(x) \eq \hat{y}$ when the correct
answer is $f(x) \eq y$:

$$\begin{aligned}
L(x, y, \hat{y}) &=& {Utility}(\mbox{result of using~} y \mbox{ given an input } x) \\
                 &-& {Utility}(\mbox{result of using~}\hat{y} \mbox{ given an input } x)\end{aligned}$$

This is the most general formulation of the loss function. Often a
simplified version is used, $L(y, \hat{y})$, that is independent of $x$.
We will use the simplified version for the rest of this chapter, which
means we can’t say that it is worse to misclassify a letter from Mom
than it is to misclassify a letter from our annoying cousin, but we can
say it is 10 times worse to classify non-spam as spam than vice-versa:
$$L({spam},{nospam}) = 1, \quad L({nospam},{spam}) = 10 \,.$$
Note that $L(y, y)$ is always zero; by definition there is no loss when
you guess exactly right. For functions with discrete outputs, we can
enumerate a loss value for each possible misclassification, but we can’t
enumerate all the possibilities for real-valued data. If $f(x)$ is
137.035999, we would be fairly happy with $h(x) =
137.036$, but just how happy should we be? In general small errors are
better than large ones; two functions that implement that idea are the
absolute value of the difference (called the $L_1$ loss), and the square
of the difference (called the $L_2$ loss). If we are content with the
idea of minimizing error rate, we can use the $L_{0/1}$ loss function,
which has a loss of 1 for an incorrect answer and is appropriate for
discrete-valued outputs:

  ---------------------- -------------------------------------------------------------------
  Absolute value loss:   $L_1(y, \hat{y}) = \mathopen| y-\hat{y} \mathclose|$
  Squared error loss:    $L_2(y, \hat{y}) = (y-\hat{y})^2$
  0/1 loss:              $L_{0/1}(y, \hat{y}) = 0 \mbox{~if~} y=\hat{y}, \mbox{~else~} 1 $
  ---------------------- -------------------------------------------------------------------

The learning agent can theoretically maximize its expected utility by
choosing the hypothesis that minimizes expected loss over all
input–output pairs it will see. It is meaningless to talk about this
expectation without defining a prior probability distribution,
$\pv(X, Y)$ over examples. Let $\mathcal{E}$ be the set of all possible
input–output examples. Then the expected for a hypothesis $h$ (with
respect to loss function $L$) is
$${GenLoss}_L(h) = \sum\limits_{(x,y) \in \mathcal{E}} L(y, h(x)) \, P(x,y) \ ,$$
and the best hypothesis, $h^*$, is the one with the minimum expected
generalization loss: $$h^* = \argmin_{h \in \Hyp} {GenLoss}_L(h) \ .$$
Because $P(x,y)$ is not known, the learning agent can only
*estimate* generalization loss with on a set of examples,
$E$:
$${EmpLoss}_{L,E}(h) = \frac{1}{N}\sum\limits_{(x,y) \in E} L(y, h(x))  \ .$$
The estimated best hypothesis $\hat{h}^*$ is then the one with minimum
empirical loss:
$$\hat{h}^* = \argmin_{h \in \Hyp} {EmpLoss}_{L,E}(h) \ .$$ There are
four reasons why $\hat{h}^*$ may differ from the true function, $f$:
unrealizability, variance, noise, and computational complexity. First,
$f$ may not be realizable—may not be in $\Hyp$—or may be present in such
a way that other hypotheses are preferred. Second, a learning algorithm
will return different hypotheses for different sets of examples, even if
those sets are drawn from the same true function $f$, and those
hypotheses will make different predictions on new examples. The higher
the variance among the predictions, the higher the probability of
significant error. Note that even when the problem is realizable, there
will still be random variance, but that variance decreases towards zero
as the number of training examples increases. Third, $f$ may be
nondeterministic or —it may return different values for $f(x)$ each time
$x$ occurs. By definition, noise cannot be predicted; in many cases, it
arises because the observed labels $y$ are the result of attributes of
the environment not listed in $x$. And finally, when $\Hyp$ is complex,
it can be computationally intractable to systematically search the whole
hypothesis space. The best we can do is a local search (hill climbing or
greedy search) that explores only part of the space. That gives us an
approximation error. Combining the sources of error, we’re left with an
estimation of an approximation of the true function $f$.

Traditional methods in statistics and the early years of machine
learning concentrated on , where the number of training examples ranged
from dozens to the low thousands. Here the generalization error mostly
comes from the approximation error of not having the true $f$ in the
hypothesis space, and from estimation error of not having enough
training examples to limit variance. In recent years there has been more
emphasis on , often with millions of examples. Here the generalization
error is dominated by limits of computation: there is enough data and a
rich enough model that we could find an $h$ that is very close to the
true $f$, but the computation to find it is too complex, so we settle
for a sub-optimal approximation.

### Regularization {#regularization-section}

In , we saw how to do model selection with cross-validation on model
size. An alternative approach is to search for a hypothesis that
directly minimizes the weighted sum of empirical loss and the complexity
of the hypothesis, which we will call the total cost:

$$\begin{aligned}
 {Cost}(h) &=& {EmpLoss}(h) + \lambda \, {Complexity}(h) \\
   \hat{h}^* &=& \argmin_{h \in \Hyp} {Cost}(h) \ . \end{aligned}$$

Here $\lambda$ is a parameter, a positive number that serves as a
conversion rate between loss and hypothesis complexity (which after all
are not measured on the same scale). This approach combines loss and
complexity into one metric, allowing us to find the best hypothesis all
at once. Unfortunately we still need to do a cross-validation search to
find the hypothesis that generalizes best, but this time it is with
different values of $\lambda$ rather than . We select the value of
$\lambda$ that gives us the best validation set score.

This process of explicitly penalizing complex hypotheses is called
(because it looks for a function that is more regular, or less complex).
Note that the cost function requires us to make two choices: the loss
function and the complexity measure, which is called a regularization
function. The choice of regularization function depends on the
hypothesis space. For example, a good regularization function for
polynomials is the sum of the squares of the coefficients—keeping the
sum small would guide us away from the wiggly polynomials in (b) and
(c). We will show an example of this type of regularization in .

Another way to simplify models is to reduce the dimensions that the
models work with. A process of can be performed to discard attributes
that appear to be irrelevant. $\chi^2$ pruning is a kind of feature
selection.

It is in fact possible to have the empirical loss and the complexity
measured on the same scale, without the conversion factor $\lambda$:
they can both be measured in bits. First encode the hypothesis as a
Turing machine program, and count the number of bits. Then count the
number of bits required to encode the data, where a correctly predicted
example costs zero bits and the cost of an incorrectly predicted example
depends on how large the error is. The or MDL hypothesis minimizes the
total number of bits required[MDL-page]. This works well in the limit,
but for smaller problems there is a difficulty in that the choice of
encoding for the program—for example, how best to encode a decision tree
as a bit string—affects the outcome. In (), we describe a probabilistic
interpretation of the MDL approach.

The Theory of Learning {#learning-theory-section}
----------------------

The main unanswered question in learning is this: How can we be sure
that our learning algorithm has produced a hypothesis that will predict
the correct value for previously unseen inputs? In formal terms, how do
we know that the hypothesis $h$ is close to the target function $f$ if
we don’t know what $f$ is? These questions have been pondered for
several centuries. In more recent decades, other questions have emerged:
how many examples do we need to get a good $h$? What hypothesis space
should we use? If the hypothesis space is very complex, can we even find
the best $h$, or do we have to settle for a local maximum in the space
of hypotheses? How complex should $h$ be? How do we avoid overfitting?
This section examines these questions.

We’ll start with the question of how many examples are needed for
learning. We saw from the learning curve for decision tree learning on
the restaurant problem ( on ) that improves with more training data.
Learning curves are useful, but they are specific to a particular
learning algorithm on a particular problem. Are there some more general
principles governing the number of examples needed in general? Questions
like this are addressed by , which lies at the intersection of AI,
statistics, and theoretical computer science. The underlying principle
is that

any hypothesis that is seriously wrong will almost certainly be “found
out” with high probability after a small number of examples, because it
will make an incorrect prediction. Thus, any hypothesis that is
consistent with a sufficiently large set of training examples is
unlikely to be seriously wrong: that is, it must be .

Any learning algorithm that returns hypotheses that are probably
approximately correct is called a algorithm; we can use this approach to
provide bounds on the performance of various learning algorithms.

PAC-learning theorems, like all theorems, are logical consequences of
axioms. When a *theorem* (as opposed to, say, a political
pundit) states something about the future based on the past, the axioms
have to provide the “juice” to make that connection. For PAC learning,
the juice is provided by the stationarity assumption introduced on ,
which says that future examples are going to be drawn from the same
fixed distribution $\pv(E)\eq
\pv(X,Y)$ as past examples. (Note that we do not have to know what
distribution that is, just that it doesn’t change.) In addition, to keep
things simple, we will assume that the true function $f$ is
deterministic and is a member of the hypothesis class $\Hyp$ that is
being considered.

The simplest PAC theorems deal with Boolean functions, for which the 0/1
loss is appropriate. The of a hypothesis $h$, defined informally
earlier, is defined formally here as the expected generalization error
for examples drawn from the stationary distribution:
$$\error(h) = {GenLoss}_{L_{0/1}}(h) = \sum_{x,y} L_{0/1}(y, h(x)) \, P(x,y) \ .$$
In other words, $\error(h)$ is the probability that $h$ misclassifies a
new example. This is the same quantity being measured experimentally by
the learning curves shown earlier.

A hypothesis $h$ is called if $\error(h)
\leq \epsilon$, where $\epsilon$ is a small constant. We will show that
we can find an $\Ncount$ such that, after seeing $\Ncount$ examples,
with high probability, all consistent hypotheses will be approximately
correct. One can think of an approximately correct hypothesis as being
“close” to the true function in hypothesis space: it lies inside what is
called the around the true function $f$. The hypothesis space outside
this ball is called $\Hbad$.

We can calculate the probability that a “seriously wrong” hypothesis
$h_b\in \Hbad$ is consistent with the first $\Ncount$ examples as
follows. We know that $\error(h_b) > \epsilon$. Thus, the probability
that it agrees with a given example is at most $ 1-\epsilon$. Since the
examples are independent, the bound for $\Ncount$ examples is
$$P(\mbox{\(h_b\) agrees with \(\Ncount\) examples}) \leq (1-\epsilon)^{\Ncount}\ .$$
The probability that $\Hbad$ contains at least one consistent hypothesis
is bounded by the sum of the individual probabilities:
$$P(\Hbad\mbox{ contains a consistent hypothesis}) 
                       \leq |\Hbad|(1-\epsilon)^{\Ncount}   \leq |\Hyp |(1-\epsilon)^{\Ncount}\ ,$$
where we have used the fact that $|\Hbad| \leq |\Hyp |$. We would like
to reduce the probability of this event below some small number
$\delta$: $$|\Hyp |(1-\epsilon)^N \leq \delta\ .$$ Given that
$1-\epsilon \leq e^{-\epsilon}$, we can achieve this if we allow the
algorithm to see

$$\Ncount \geq \frac{1}{\epsilon}\left({\ln\frac{1}{\delta} + \ln |\Hyp | }\right)
\label{sample-size-equation}$$

examples. Thus, if a learning algorithm returns a hypothesis that is
consistent with this many examples, then with probability at least
$1-\delta$, it has error at most $\epsilon$. In other words, it is
probably approximately correct. The number of required examples, as a
function of $\epsilon$ and $\delta$, is called the of the hypothesis
space.

As we saw earlier, if $\Hyp$ is the set of all Boolean functions on $n$
attributes, then $|\Hyp|=2^{2^n}$. Thus, the sample complexity of the
space grows as $2^n$. Because the number of possible examples is also
$2^n$, this suggests that PAC-learning in the class of all Boolean
functions requires seeing all, or nearly all, of the possible examples.
A moment’s thought reveals the reason for this: $\Hyp$ contains enough
hypotheses to classify any given set of examples in all possible ways.
In particular, for any set of $\Ncount$ examples, the set of hypotheses
consistent with those examples contains equal numbers of hypotheses that
predict $x_{\Ncount+1}$ to be positive and hypotheses that predict
$x_{\Ncount+1}$ to be negative.

To obtain real generalization to unseen examples, then, it seems we need
to restrict the hypothesis space $\Hyp$ in some way; but of course, if
we do restrict the space, we might eliminate the true function
altogether. There are three ways to escape this dilemma. The first,
which we will cover in , is to bring prior knowledge to bear on the
problem. The second, which we introduced in , is to insist that the
algorithm return not just any consistent hypothesis, but preferably a
simple one (as is done in decision tree learning). In cases where
finding simple consistent hypotheses is tractable, the sample complexity
results are generally better than for analyses based only on
consistency. The third escape, which we pursue next, is to focus on
learnable subsets of the entire hypothesis space of Boolean functions.
This approach relies on the assumption that the restricted language
contains a hypothesis $h$ that is close enough to the true function $f$;
the benefits are that the restricted hypothesis space allows for
effective generalization and is typically easier to search. We now
examine one such restricted language in more detail.

### PAC learning example: Learning decision lists

We now show how to apply PAC learning to a new hypothesis space: . A
decision list consists of a series of tests, each of which is a
conjunction of literals. If a test succeeds when applied to an example
description, the decision list specifies the value to be returned. If
the test fails, processing continues with the next test in the list.
Decision lists resemble decision trees, but their overall structure is
simpler: they branch only in one direction. In contrast, the individual
tests are more complex. shows a decision list that represents the
following hypothesis:
$${WillWait} \lequiv ({Patrons} = {Some}) \lor
({Patrons} = {Full} \land {Fri}/{Sat})\ .$$

[decision-list-figure]

If we allow tests of arbitrary size, then decision lists can represent
any Boolean function (). On the other hand, if we restrict the size of
each test to at most $k$ literals, then it is possible for the learning
algorithm to generalize successfully from a small number of examples. We
call this language . The example in is in 2-DL. It is easy to show ()
that $\kdl$ includes as a subset the language , the set of all decision
trees of depth at most $k$. It is important to remember that the
particular language referred to by $\kdl$ depends on the attributes used
to describe the examples. We will use the notation $\kdl(\Acount)$ to
denote a $\kdl$ language using $\Acount$ Boolean attributes.

The first task is to show that $\kdl$ is learnable—that is, that any
function in $\kdl$ can be approximated accurately after training on a
reasonable number of examples. To do this, we need to calculate the
number of hypotheses in the language. Let the language of
tests—conjunctions of at most $k$ literals using $\Acount$ attributes—be
${Conj}(\Acount,k)$. Because a decision list is constructed of tests,
and because each test can be attached to either a ${Yes}$ or a
${No}$ outcome or can be absent from the decision list, there are at
most $3^{|{Conj}(\Acount,k)|}$ distinct sets of component tests. Each
of these sets of tests can be in any order, so
$$|\kdl(\Acount)| \leq 3^{|{Conj}({\Acount},k)|} |{Conj}({\Acount},k)|!\ .$$
The number of conjunctions of $k$ literals from ${\Acount}$ attributes
is given by
$$|{Conj}({\Acount},k)| = \sum_{i=0}^k {{2{\Acount}} \choose i} = O({\Acount}^k)\ .$$
Hence, after some work, we obtain
$$|\kdl({\Acount})| = 2^{O({\Acount}^k\log_2({\Acount}^k))}\ .$$ We can
plug this into to show that the number of examples needed for
PAC-learning a $\kdl$ function is polynomial in ${\Acount}$:
$$N \geq \frac{1}{\epsilon}\left({\ln\frac{1}{\delta} + O({\Acount}^k\log_2({\Acount}^k)) }\right)\ .$$
Therefore, any algorithm that returns a consistent decision list will
PAC-learn a $\kdl$ function in a reasonable number of examples, for
small $k$.

[DLL-algorithm]

The next task is to find an efficient algorithm that returns a
consistent decision list. We will use a greedy algorithm called that
repeatedly finds a test that agrees exactly with some subset of the
training set. Once it finds such a test, it adds it to the decision list
under construction and removes the corresponding examples. It then
constructs the remainder of the decision list, using just the remaining
examples. This is repeated until there are no examples left. The
algorithm is shown in .

[restaurant-dll+dtl-curve-figure]

This algorithm does not specify the method for selecting the next test
to add to the decision list. Although the formal results given earlier
do not depend on the selection method, it would seem reasonable to
prefer small tests that match large sets of uniformly classified
examples, so that the overall decision list will be as compact as
possible. The simplest strategy is to find the smallest test $t$ that
matches any uniformly classified subset, regardless of the size of the
subset. Even this approach works quite well, as suggests.

Regression and Classification with Linear Models {#linear-regression-section}
------------------------------------------------

Now it is time to move on from decision trees and lists to a different
hypothesis space, one that has been used for hundred of years: the class
of of continuous-valued inputs. We’ll start with the simplest case:
regression with a univariate linear function, otherwise known as
“fitting a straight line.” covers the multivariate case. show how to
turn linear functions into classifiers by applying hard and soft
thresholds.

### Univariate linear regression {#univariate-regression-section}

A univariate linear function (a straight line) with input $x$ and output
$y$ has the form $y \eq w_1x + w_0$, where $w_0$ and $w_1$ are
real-valued coefficients to be learned. We use the letter $w$ because we
think of the coefficients as ; the value of $y$ is changed by changing
the relative weight of one term or another. We’ll define $\bw$ to be the
vector $[w_0, w_1]$, and define $$h_{\sw}(x) \eq w_1x + w_0 \ .$$

[xy-plot-figure2]

​(a) shows an example of a training set of $n$ points in the $x,y$
plane, each point representing the size in square feet and the price of
a house offered for sale. The task of finding the $h_{\sw}$ that best
fits these data is called . To fit a line to the data, all we have to do
is find the values of the weights $[w_0, w_1]$ that minimize the
empirical loss. It is traditional (going back to Gauss[^3]) to use the
squared loss function, $L_2$, summed over all the training examples:
$${Loss}(h_{\sw}) = \sum_{j\eq 1}^{\Ncount} L_2(y_j,h_{\sw}(x_j)) = \sum_{j\eq 1}^{\Ncount} (y_j - h_{\sw}(x_j))^2 = \sum_{j\eq 1}^{\Ncount}(y_j - (w_1 x_j + w_0))^2 \ .$$
We would like to find $\bw^* = \argmin_{\sw} {Loss}(h_{\sw})$. The sum
$ \sum_{j\eq 1}^{\Ncount} (y_j - (w_1x_j + w_0))^2 $ is minimized when
its partial derivatives with respect to $w_0$ and $w_1$ are zero:

$$\frac{\partial{}}{\partial w_0} \sum_{j\eq 1}^{\Ncount}(y_j - (w_1 x_j + w_0))^2 = 0 \mbox{   and   }
\frac{\partial{}}{\partial w_1} \sum_{j\eq 1}^{\Ncount}(y_j - (w_1 x_j + w_0))^2 = 0 \ .
\label{linear-regression-zero-gradient-equation}$$

These equations have a unique solution:

$$w_1 = \frac{\Ncount(\sum x_j y_j) - (\sum x_j)(\sum y_j)}{\Ncount(\sum x_j^2) - (\sum x_j)^2} ; \;\;
w_0 \eq(\sum y_j - w_1(\sum x_j)) / \Ncount \ . \label{linear-regression-equation}$$

For the example in (a), the solution is $w_1\eq
0.232$, $w_0 = 246$, and the line with those weights is shown as a
dashed line in the figure.

Many forms of learning involve adjusting weights to minimize a loss, so
it helps to have a mental picture of what’s going on in —the space
defined by all possible settings of the weights. For univariate linear
regression, the weight space defined by $w_0$ and $w_1$ is
two-dimensional, so we can graph the loss as a function of $w_0$ and
$w_1$ in a 3D plot (see (b)). We see that the loss function is , as
defined on ; this is true for *every* linear regression
problem with an $L_2$ loss function, and implies that there are no local
minima. In some sense that’s the end of the story for linear models; if
we need to fit lines to data, we apply .[^4]

To go beyond linear models, we will need to face the fact that the
equations defining minimum loss (as in ) will often have no closed-form
solution. Instead, we will face a general optimization search problem in
a continuous weight space. As indicated in (), such problems can be
addressed by a hill-climbing algorithm that follows the of the function
to be optimized. In this case, because we are trying to minimize the
loss, we will use . We choose any starting point in weight space—here, a
point in the ($w_0$, $w_1$) plane—and then move to a neighboring point
that is downhill, repeating until we converge on the minimum possible
loss:

$$\begin{aligned}
&&{\bw}\,\leftarrow\,\mbox{any point in the parameter space} \nonumber\\
&&\mbox{\k{loop} until convergence \k{do}} \nonumber\\
&&    \qquad\mbox{\k{for each} \(w_i\) \k{in} \(\bw\) \k{do}} \nonumber\\
&&        \qquad\qquad {w_i}\,\leftarrow\,{w_i - \alpha\, \frac{\partial {}}{\partial w_i} {Loss}(\bw)} 
        \label{gradient-descent-equation}\end{aligned}$$

The parameter $\alpha$, which we called the in , is usually called the
when we are trying to minimize loss in a learning problem. It can be a
fixed constant, or it can decay over time as the learning process
proceeds.

For univariate regression, the loss function is a quadratic function, so
the partial derivative will be a linear function. (The only calculus you
need to know is that $\frac{\partial {}}{\partial x} x^2 \eq 2x$ and
$\frac{\partial {}}{\partial x} x \eq 1$.) Let’s first work out the
partial derivatives—the slopes—in the simplified case of only one
training example, $(x, y)$:

$$\begin{aligned}
\frac{\partial {}}{\partial w_i} {Loss}(\bw) & =& \frac{\partial {}}{\partial w_i} (y - h_{\sw}(x))^2 \nonumber\\
  & = & 2 (y - h_{\sw}(x)) \times \frac{\partial {}}{\partial w_i}(y - h_{\sw}(x)) \nonumber\\
  & = & 2 (y - h_{\sw}(x)) \times \frac{\partial {}}{\partial w_i} (y - (w_1 x + w_0)) \ , \label{gradient-descent-one-point-equation}\end{aligned}$$

applying this to both $w_0$ and $w_1$ we get:
$$\frac{\partial {}}{\partial w_0} {Loss}(\bw)  =  - 2(y - h_{\sw}(x)) \, ; \qquad
\frac{\partial {}}{\partial w_1} {Loss}(\bw)  =  - 2(y - h_{\sw}(x))\stimes x \\$$
Then, plugging this back into , and folding the 2 into the unspecified
learning rate $\alpha$, we get the following learning rule for the
weights: $$w_0 \leftarrow w_0 + \alpha\,(y - h_{\sw}(x)) \, ; \quad
w_1 \leftarrow w_1 + \alpha\, (y - h_{\sw}(x)) \stimes x$$ These updates
make intuitive sense: if $h_{\sw}(x) > y$, i.e., the output of the
hypothesis is too large, reduce $w_0$ a bit, and reduce $w_1$ if $x$ was
a positive input but increase $w_1$ if $x$ was a negative input.

The preceding equations cover one training example. For $N$ training
examples, we want to minimize the sum of the individual losses for each
example. The derivative of a sum is the sum of the derivatives, so we
have:
$$w_0 \leftarrow w_0 + \alpha \sum_j (y_j - h_{\sw}(x_j)) \, ; \quad
w_1 \leftarrow w_1 + \alpha \sum_j (y_j - h_{\sw}(x_j)) \stimes x_j \ .$$
These updates constitute the learning rule for univariate linear
regression. Convergence to the unique global minimum is guaranteed (as
long as we pick $\alpha$ small enough) but may be very slow: we have to
cycle through all the training data for every step, and there may be
many steps.

There is another possibility, called , where we consider only a single
training point at a time, taking a step after each one using .
Stochastic gradient descent can be used in an online setting, where new
data are coming in one at a time, or offline, where we cycle through the
same data as many times as is necessary, taking a step after considering
each single example. It is often faster than batch gradient descent.
With a fixed learning rate $\alpha$, however, it does not guarantee
convergence; it can oscillate around the minimum without settling down.
In some cases, as we see later, a schedule of decreasing learning rates
(as in simulated annealing) does guarantee convergence.

### Multivariate linear regression {#multivariate-regression-section}

We can easily extend to problems, in which each example $\x_j$ is an
$\Acount$-element vector.[^5] Our hypothesis space is the set of
functions of the form
$$h_{sw}(\x_j) = w_0 + w_1 x_{j,1} + \cdots + w_{\Acount} x_{j,{\Acount}} = w_0 + \sum_i w_i x_{j,i} \,.$$
The $w_0$ term, the intercept, stands out as different from the others.
We can fix that by inventing a dummy input attribute, $x_{j,0}$, which
is defined as always equal to 1. Then $h$ is simply the dot product of
the weights and the input vector (or equivalently, the matrix product of
the transpose of the weights and the input vector):
$$h_{sw}(\x_j) = \bw \cdot \x_j  = \bw\transpose \x_j = \sum_i w_i x_{j,i} \,.$$
The best vector of weights, $\bw^*$, minimizes squared-error loss over
the examples:
$$\bw^* = \argmin_{\sw} \sum_j L_2( y_j, \bw \cdot \x_j) \,.$$
Multivariate linear regression is actually not much more complicated
than the univariate case we just covered. Gradient descent will reach
the (unique) minimum of the loss function; the update equation for each
weight $w_i$ is

$${w_i}\,\leftarrow\, w_i + \alpha\, \sum_j x_{j,i}(y_j - h_{\sw}(\x_j)) \ .
\label{multivariate-regression-update-equation}$$

It is also possible to solve analytically for the $\bw$ that minimizes
loss. Let $\y$ be the vector of outputs for the training examples, and
$\X$ be the , i.e., the matrix of inputs with one $\Acount$-dimensional
example per row. Then the solution
$$\bw^* = (\X\transpose \X)^{-1}\X\transpose \y$$ minimizes the squared
error.

With univariate linear regression we didn’t have to worry about
overfitting. But with multivariate linear regression in high-dimensional
spaces it is possible that some dimension that is actually irrelevant
appears by chance to be useful, resulting in .

Thus, it is common to use on multivariate linear functions to avoid
overfitting. Recall that with regularization we minimize the total cost
of a hypothesis, counting both the empirical loss and the complexity of
the hypothesis:
$${Cost}(h) = {EmpLoss}(h) + \lambda \, {Complexity}(h) \ .$$ For
linear functions the complexity can be specified as a function of the
weights. We can consider a family of regularization functions:
$${Complexity}(h_{\sw}) = L_q(\bw) = \sum_i {|w_i|}^q \ .$$ As with
loss functions,[^6] with $q\eq 1$ we have $L_1$ regularization, which
minimizes the sum of the absolute values; with $q\eq 2$, $L_2$
regularization minimizes the sum of squares. Which regularization
function should you pick? That depends on the specific problem, but
$L_1$ regularization has an important advantage: it tends to produce a .
That is, it often sets many weights to zero, effectively declaring the
corresponding attributes to be irrelevant—just as does (although by a
different mechanism). Hypotheses that discard attributes can be easier
for a human to understand, and may be less likely to overfit.

[L1-diamond-figure]

gives an intuitive explanation of why $L_1$ regularization leads to
weights of zero, while $L_2$ regularization does not. Note that
minimizing ${Loss}(\bw) + \lambda
{Complexity}(\bw)$ is equivalent to minimizing ${Loss}(\bw)$ subject
to the constraint that ${Complexity}(\bw) \le c$, for some constant
$c$ that is related to $\lambda$. Now, in (a) the diamond-shaped box
represents the set of points $\bw$ in two-dimensional weight space that
have $L_1$ complexity less than $c$; our solution will have to be
somewhere inside this box. The concentric ovals represent contours of
the loss function, with the minimum loss at the center. We want to find
the point in the box that is closest to the minimum; you can see from
the diagram that, for an arbitrary position of the minimum and its
contours, it will be common for the corner of the box to find its way
closest to the minimum, just because the corners are pointy. And of
course the corners are the points that have a value of zero in some
dimension. In (b), we’ve done the same for the $L_2$ complexity measure,
which represents a circle rather than a diamond. Here you can see that,
in general, there is no reason for the intersection to appear on one of
the axes; thus $L_2$ regularization does not tend to produce zero
weights. The result is that the number of examples required to find a
good $h$ is linear in the number of irrelevant features for $L_2$
regularization, but only logarithmic with $L_1$ regularization.
Empirical evidence on many problems supports this analysis.

Another way to look at it is that $L_1$ regularization takes the
dimensional axes seriously, while $L_2$ treats them as arbitrary. The
$L_2$ function is spherical, which makes it rotationally invariant:
Imagine a set of points in a plane, measured by their $x$ and $y$
coordinates. Now imagine rotating the axes by $45\deg$. You’d get a
different set of $(x', y')$ values representing the same points. If you
apply $L_2$ regularization before and after rotating, you get exactly
the same point as the answer (although the point would be described with
the new $(x',y')$ coordinates). That is appropriate when the choice of
axes really is arbitrary—when it doesn’t matter whether your two
dimensions are distances north and east; or distances north-east and
south-east. With $L_1$ regularization you’d get a different answer,
because the $L_1$ function is not rotationally invariant. That is
appropriate when the axes are not interchangeable; it doesn’t make sense
to rotate “number of bathrooms” $45\deg$ towards “lot size.”

### Linear classifiers with a hard threshold {#linear-threshold-section}

Linear functions can be used to do classification as well as regression.
For example, (a) shows data points of two classes: earthquakes (which
are of interest to seismologists) and underground explosions (which are
of interest to arms control experts). Each point is defined by two input
values, $x_1$ and $x_2$, that refer to body and surface wave magnitudes
computed from the seismic signal. Given these training data, the task of
classification is to learn a hypothesis $h$ that will take new
$(x_1, x_2)$ points and return either 0 for earthquakes or 1 for
explosions.

[earthquake-figure]

A is a line (or a surface, in higher dimensions) that separates the two
classes. In (a), the decision boundary is a straight line. A linear
decision boundary is called a and data that admit such a separator are
called . The linear separator in this case is defined by
$$x_2 = 1.7x_1 - 4.9 \quad \mbox{or} \quad -4.9 + 1.7x_1 - x_2 = 0\ .$$
The explosions, which we want to classify with value 1, are to the right
of this line with higher values of $x_1$ and lower values of $x_2$, so
they are points for which $-4.9 + 1.7x_1 - x_2 > 0$, while earthquakes
have $-4.9 + 1.7x_1 -
x_2 < 0$. Using the convention of a dummy input $x_0\eq 1$, we can write
the classification hypothesis as
$$h_{\sw}(\x) = 1 \mbox{ if } \bw \cdot \x \geq 0 \mbox{ and } 0 \mbox{ otherwise.}$$
Alternatively, we can think of $h$ as the result of passing the linear
function $\bw\cdot \x$ through a :
$$h_{\sw}(\x) = {Threshold}(\bw \cdot \x) \mbox{ where } {Threshold}(z) \eq 1 \mbox{ if } z \geq 0 \mbox{ and } 0 \mbox{ otherwise.}$$
The threshold function is shown in (a).

Now that the hypothesis $h_{\sw}(\x)$ has a well-defined mathematical
form, we can think about choosing the weights $\bw$ to minimize the
loss. In , we did this both in closed form (by setting the gradient to
zero and solving for the weights) and by gradient descent in weight
space. Here, we cannot do either of those things because the gradient is
zero almost everywhere in weight space except at those points where
$\bw\cdot\x\eq 0$, and at those points the gradient is undefined.

There is, however, a simple weight update rule that converges to a
solution—that is, a linear separator that classifies the data
perfectly–provided the data are linearly separable. For a single example
$(\x,y)$, we have

$${w_i}\,\leftarrow\, w_i + \alpha\, (y - h_{\sw}(\x)) \stimes x_i
\label{perceptron-update-equation}$$

which is essentially identical to the , the update rule for linear
regression! This rule is called the , for reasons that will become clear
in . Because we are considering a 0/1 classification problem, however,
the behavior is somewhat different. Both the true value $y$ and the
hypothesis output $h_{\sw}(\x)$ are either 0 or 1, so there are three
possibilities:

-   If the output is correct, i.e., $y\eq h_{\sw}(\x)$, then the weights
    are not changed.

-   If $y$ is 1 but $h_{\sw}(\x)$ is 0, then $w_i$ is
    *increased* when the corresponding input $x_i$ is
    positive and *decreased* when $x_i$ is negative. This
    makes sense, because we want to make $\bw\cdot\x$ bigger so that
    $h_{\sw}(\x)$ outputs a 1.

-   If $y$ is 0 but $h_{\sw}(\x)$ is 1, then $w_i$ is
    *decreased* when the corresponding input $x_i$ is
    positive and *increased* when $x_i$ is negative. This
    makes sense, because we want to make $\bw\cdot\x$ smaller so that
    $h_{\sw}(\x)$ outputs a 0.

Typically the learning rule is applied one example at a time, choosing
examples at random (as in stochastic gradient descent). (a) shows a for
this learning rule applied to the earthquake/explosion data shown in
(a). A training curve measures the classifier performance on a fixed
training set as the learning process proceeds on that same training set.
The curve shows the update rule converging to a zero-error linear
separator. The “convergence” process isn’t exactly pretty, but it always
works. This particular run takes 657 steps to converge, for a data set
with 63 examples, so each example is presented roughly 10 times on
average. Typically, the variation across runs is very large.

[perceptron-earthquake-figure]

We have said that the perceptron learning rule converges to a perfect
linear separator when the data points are linearly separable, but what
if they are not? This situation is all too common in the real world. For
example, (b) adds back in the data points left out by when they plotted
the data shown in (a). In (b), we show the perceptron learning rule
failing to converge even after 10,000 steps: even though it hits the
minimum-error solution (three errors) many times, the algorithm keeps
changing the weights. In general, the perceptron rule may not converge
to a stable solution for fixed learning rate $\alpha$, but if $\alpha$
decays as $O(1/t)$ where $t$ is the iteration number, then the rule can
be shown to converge to a minimum-error solution when examples are
presented in a random sequence.[^7][stochastic-convergence-page] It can
also be shown that finding the minimum-error solution is NP-hard, so one
expects that many presentations of the examples will be required for
convergence to be achieved. (b) shows the training process with a
learning rate schedule $\alpha(t)\eq 1000/(1000+t)$: convergence is not
perfect after 100,000 iterations, but it is much better than the
fixed-$\alpha$ case.

### Linear classification with logistic regression {#logistic-regression-section}

[threshold-functions-figure]

We have seen that passing the output of a linear function through the
threshold function creates a linear classifier; yet the hard nature of
the threshold causes some problems: the hypothesis $h_{\sw}(\x)$ is not
differentiable and is in fact a discontinuous function of its inputs and
its weights; this makes learning with the perceptron rule a very
unpredictable adventure. Furthermore, the linear classifier always
announces a completely confident prediction of 1 or 0, even for examples
that are very close to the boundary; in many situations, we really need
more gradated predictions.

All of these issues can be resolved to a large extent by softening the
threshold function—approximating the hard threshold with a continuous,
differentiable function. In (), we saw two functions that look like soft
thresholds: the integral of the standard normal distribution (used for
the probit model) and the logistic function (used for the logit model).
Although the two functions are very similar in shape, the logistic
function[logistic-function-page]
$${Logistic}(z) = \frac{1}{1 + e^{-z}}$$ has more convenient
mathematical properties. The function is shown in (b). With the logistic
function replacing the threshold function, we now have
$$h_{\sw}(\x) = {Logistic}(\bw \cdot \x) = \frac{1}{1 + e^{-\sw \cdot \sx}} \ .$$
An example of such a hypothesis for the two-input earthquake/explosion
problem is shown in (c). Notice that the output, being a number between
0 and 1, can be interpreted as a *probability* of belonging
to the class labeled 1. The hypothesis forms a soft boundary in the
input space and gives a probability of 0.5 for any input at the center
of the boundary region, and approaches 0 or 1 as we move away from the
boundary.

The process of fitting the weights of this model to minimize loss on a
data set is called . There is no easy closed-form solution to find the
optimal value of $\bw$ with this model, but the gradient descent
computation is straightforward. Because our hypotheses no longer output
just 0 or 1, we will use the $L_2$ loss function; also, to keep the
formulas readable, we’ll use $g$ to stand for the logistic function,
with $g'$ its derivative.

For a single example $(\x,y)$, the derivation of the gradient is the
same as for linear regression () up to the point where the actual form
of $h$ is inserted. (For this derivation, we will need the :
$\partial g(f(x))/\partial x \eq g'(f(x))\,\partial f(x)/\partial x$.)
We have

$$\begin{aligned}
\frac{\partial {}}{\partial w_i} {Loss}(\bw) & =& \frac{\partial {}}{\partial w_i} (y - h_{\sw}(\x))^2 \\
  & = & 2 (y - h_{\sw}(\x)) \times \frac{\partial {}}{\partial w_i}(y - h_{\sw}(\x)) \\
  & = & -2 (y - h_{\sw}(\x)) \times g'(\bw\cdot\x) \stimes \frac{\partial {}}{\partial w_i} \bw\cdot\x \\
  & = & -2 (y - h_{\sw}(\x)) \times g'(\bw\cdot\x) \stimes x_i \ .\end{aligned}$$

The derivative $g'$ of the logistic function satisfies
$g'(z) \eq  g(z) (1-g(z))$, so we have
$$g'(\bw\cdot\x) = g(\bw\cdot\x)(1- g(\bw\cdot\x)) = h_{\sw}(\x)(1- h_{\sw}(\x))$$
so the weight update for minimizing the loss is

$${w_i}\,\leftarrow\, w_i + \alpha\, (y - h_{\sw}(\x))\stimes
   h_{\sw}(\x)(1- h_{\sw}(\x))\stimes x_i\ .
\label{logistic-update-equation}$$

Repeating the experiments of with logistic regression instead of the
linear threshold classifier, we obtain the results shown in . In (a),
the linearly separable case, logistic regression is somewhat slower to
converge, but behaves much more predictably. In (b) and (c), where the
data are noisy and nonseparable, logistic regression converges far more
quickly and reliably. These advantages tend to carry over into
real-world applications and logistic regression has become one of the
most popular classification techniques for problems in medicine,
marketing and survey analysis, credit scoring, public health, and other
applications.

[logistic-earthquake-figure]

Artificial Neural Networks {#nn-section}
--------------------------

We turn now to what seems to be a somewhat unrelated topic: the brain.
In fact, as we will see, the technical ideas we have discussed so far in
this chapter turn out to be useful in building mathematical models of
the brain’s activity; conversely, thinking about the brain has helped in
extending the scope of the technical ideas.

touched briefly on the basic findings of neuroscience—in particular, the
hypothesis that mental activity consists primarily of electrochemical
activity in networks of brain cells called . ( on showed a schematic
diagram of a typical neuron.) Inspired by this hypothesis, some of the
earliest AI work aimed to create artificial . (Other names for the field
include , , and .) shows a simple mathematical model of the neuron
devised by . Roughly speaking, it “fires” when a linear combination of
its inputs exceeds some (hard or soft) threshold—that is, it implements
a linear classifier of the kind described in the preceding section. A
neural network is just a collection of units connected together; the
properties of the network are determined by its topology and the
properties of the “neurons.”

Since 1943, much more detailed and realistic models have been developed,
both for neurons and for larger systems in the brain, leading to the
modern field of . On the other hand, researchers in AI and statistics
became interested in the more abstract properties of neural networks,
such as their ability to perform distributed computation, to tolerate
noisy inputs, and to learn. Although we understand now that other kinds
of systems—including Bayesian networks—have these properties, neural
networks remain one of the most popular and effective forms of learning
system and are worthy of study in their own right.

### Neural network structures

[neuron-unit-figure]

Neural networks are composed of nodes or (see ) connected by directed .
A link from unit $\nninput$ to unit $\nnunit$ serves to propagate the
$a_{\nninput}$ from $\nninput$ to $\nnunit$.[^8] Each link also has a
numeric $\w{\nninput}{\nnunit}$ associated with it, which determines the
strength and sign of the connection. Just as in linear regression
models, each unit has a dummy input $a_0\eq 1$ with an associated weight
$\w{0}{\nnunit}$. Each unit $\nnunit$ first computes a weighted sum of
its inputs:
$${in}{}_{\nnunit} = \sum_{\nninput\eq 0}^n \w{\nninput}{\nnunit} a_{\nninput}\ .$$
Then it applies an $g$ to this sum to derive the output:

$$a_{\nnunit} = g({in}{}_{\nnunit}) = g\left(\sum_{\nninput\eq 0}^n \w{\nninput}{\nnunit} a_{\nninput}\right)\ .
\label{perceptron-output-equation}$$

The activation function $g$ is typically either a hard threshold ((a)),
in which case the unit is called a , or a logistic function ((b)), in
which case the term is sometimes used. Both of these nonlinear
activation function ensure the important property that the entire
network of units can represent a nonlinear function (see ). As mentioned
in the discussion of logistic regression (), the logistic activation
function has the added advantage of being differentiable.

Having decided on the mathematical model for individual “neurons,” the
next task is to connect them together to form a network. There are two
fundamentally distinct ways to do this. A has connections only in one
direction—that is, it forms a directed acyclic graph. Every node
receives input from “upstream” nodes and delivers output to “downstream”
nodes; there are no loops. A feed-forward network represents a function
of its current input; thus, it has no internal state other than the
weights themselves. A , on the other hand, feeds its outputs back into
its own inputs. This means that the activation levels of the network
form a dynamical system that may reach a stable state or exhibit
oscillations or even chaotic behavior. Moreover, the response of the
network to a given input depends on its initial state, which may depend
on previous inputs. Hence, recurrent networks (unlike feed-forward
networks) can support short-term memory. This makes them more
interesting as models of the brain, but also more difficult to
understand. This section will concentrate on feed-forward networks; some
pointers for further reading on recurrent networks are given at the end
of the chapter.

Feed-forward networks are usually arranged in , such that each unit
receives input only from units in the immediately preceding layer. In
the next two subsections, we will look at single-layer networks, in
which every unit connects directly from the network’s inputs to its
outputs, and multilayer networks, which have one or more layers of that
are not connected to the outputs of the network. So far in this chapter,
we have considered only learning problems with a single output variable
$y$, but neural networks are often used in cases where multiple outputs
are appropriate. For example, if we want to train a network to add two
input bits, each a 0 or a 1, we will need one output for the sum bit and
one for the carry bit. Also, when the learning problem involves
classification into more than two classes—for example, when learning to
categorize images of handwritten digits—it is common to use one output
unit for each class.

### Single-layer feed-forward neural networks (perceptrons) {#perceptron-learning-section}

A network with all the inputs connected directly to the outputs is
called a , or a . shows a simple two-input, two-output perceptron
network. With such a network, we might hope to learn the two-bit adder
function, for example. Here are all the training data we will need:

|c|c||c|c| $\quad x_1 \quad $ & $\quad x_2 \quad $ &
$\quad y_3 $ (carry) & $\quad y_4$ (sum)\
0 & 0 & 0 & 0\
0 & 1 & 0 & 1\
1 & 0 & 0 & 1\
1 & 1 & 1 & 0\

The first thing to notice is that a perceptron network with $m$ outputs
is really $m$ separate networks, because each weight affects only one of
the outputs. Thus, there will be $m$ separate training processes.
Furthermore, depending on the type of activation function used, the
training processes will be either the ( on ) or gradient descent rule
for the ( on ).

[neural-net-figure]

If you try either method on the two-bit-adder data, something
interesting happens. Unit 3 learns the carry function easily, but unit 4
completely fails to learn the sum function. No, unit 4 is not defective!
The problem is with the sum function itself. We saw in that linear
classifiers (whether hard or soft) can represent linear decision
boundaries in the input space. This works fine for the carry function,
which is a logical and (see (a)). The sum function,
however, is an xor (exclusive or) of the two
inputs. As (c) illustrates, this function is not linearly separable so
the perceptron cannot learn it.

[perceptron-linear-figure]

The linearly separable functions constitute just a small fraction of all
Boolean functions; asks you to quantify this fraction. The inability of
perceptrons to learn even such simple functions as xor was
a significant setback to the nascent neural network community in the
1960s. Perceptrons are far from useless, however. noted that logistic
regression (i.e., training a sigmoid perceptron) is even today a very
popular and effective tool. Moreover, a perceptron can represent some
quite “complex” Boolean functions very compactly. For example, the ,
which outputs a 1 only if more than half of its $n$ inputs are 1, can be
represented by a perceptron with each $w_{i} \eq 1$ and with
$w_0\eq {-}n/2$. A decision tree would need exponentially many nodes to
represent this function.

shows the learning curve for a perceptron on two different problems. On
the left, we show the curve for learning the majority function with 11
Boolean inputs (i.e., the function outputs a 1 if 6 or more inputs are
1). As we would expect, the perceptron learns the function quite
quickly, because the majority function is linearly separable. On the
other hand, the decision-tree learner makes no progress, because the
majority function is very hard (although not impossible) to represent as
a decision tree. On the right, we have the restaurant example. The
solution problem is easily represented as a decision tree, but is not
linearly separable. The best plane through the data correctly classifies
only 65%.

[perceptron-learning-figure]

### Multilayer feed-forward neural networks

@McCulloch+Pitts:1943 were well aware that a single threshold unit would
not solve all their problems. In fact, their paper proves that such a
unit can represent the basic Boolean functions and,
or, and not and then goes on to argue that any
desired functionality can be obtained by connecting large numbers of
units into (possibly recurrent) networks of arbitrary depth. The problem
was that nobody knew how to train such networks.

This turns out to be an easy problem if we think of a network the right
way: as a function $h_{\sw}(\x)$ parameterized by the weights $\bw$.
Consider the simple network shown in (b), which has two input units, two
hidden units, and two output unit. (In addition, each unit has a dummy
input fixed at 1.) Given an input vector $\x\eq (x_1,x_2)$, the
activations of the input units are set to $(a_1, a_2)\eq (x_1,x_2)$. The
output at unit 5 is given by

$$\begin{aligned}
a_5 & = & g(\w{0,5} + \w{3}{5}\,a_3 + \w{4}{5}\,a_4)  \nonumber  \\
    & = & g(\w{0,5} + \w{3}{5}\,g(\w{0}{3} + \w{1}{3}\,a_1 + \w{2}{3}\,a_2) + 
              \w{4}{5}\,g(w_{0}{4} + \w{1}{4}\,a_1 + \w{2}{4}\,a_2)) \nonumber \\
    & = & g(\w{0,5} + \w{3}{5}\,g(\w{0}{3} + \w{1}{3}\,x_1 + \w{2}{3}\,x_2) + 
              \w{4}{5}\,g(w_{0}{4} + \w{1}{4}\,x_1 + \w{2}{4}\,x_2)) \nonumber .
\label{nn-output-equation}\end{aligned}$$

Thus, we have the output expressed as a function of the inputs and the
weights. A similar expression holds for unit 6. As long as we can
calculate the derivatives of such expressions with respect to the
weights, we can use the gradient-descent loss-minimization method to
train the network. shows exactly how to do this. And because the
function represented by a network can be highly nonlinear—composed, as
it is, of nested nonlinear soft threshold functions—we can see neural
networks as a tool for doing .

[nn-bump-figure]

Before delving into learning rules, let us look at the ways in which
networks generate complicated functions. First, remember that each unit
in a sigmoid network represents a soft threshold in its input space, as
shown in (c) (). With one hidden layer and one output layer, as in (b),
each output unit computes a soft-thresholded linear combination of
several such functions. For example, by adding two opposite-facing soft
threshold functions and thresholding the result, we can obtain a “ridge”
function as shown in (a). Combining two such ridges at right angles to
each other (i.e., combining the outputs from four hidden units), we
obtain a “bump” as shown in (b).

With more hidden units, we can produce more bumps of different sizes in
more places. In fact, with a single, sufficiently large hidden layer, it
is possible to represent any continuous function of the inputs with
arbitrary accuracy; with two layers, even discontinuous functions can be
represented.[^9] Unfortunately, for any *particular*
network structure, it is harder to characterize exactly which functions
can be represented and which ones cannot.

### Learning in multilayer networks {#backprop-section}

First, let us dispense with one minor complication arising in multilayer
networks: interactions among the learning problems when the network has
multiple outputs. In such cases, we should think of the network as
implementing a vector function $\h_{\sw}$ rather than a scalar function
$h_{\sw}$; for example, the network in (b) returns a vector $[a_5,a_6]$.
Similarly, the target output will be a vector $\y$. Whereas a perceptron
network decomposes into $m$ separate learning problems for an $m$-output
problem, this decomposition fails in a multilayer network. For example,
both $a_5$ and $a_6$ in (b) depend on all of the input-layer weights, so
updates to those weights will depend on errors in both $a_5$ and $a_6$.
Fortunately, this dependency is very simple in the case of any loss
function that is *additive* across the components of the
error vector $\y-\h_{\sw}(\x)$. For the $L_2$ loss, we have, for any
weight $w$,

$$\frac{\partial{}}{\partial w}{Loss}(\bw) = \frac{\partial{}}{\partial w}|\y-\h_{\sw}(\x)|^2 
= \frac{\partial{}}{\partial w} \sum_{\nnoutput} (y_{\nnoutput} - a_{\nnoutput})^2 = \sum_{\nnoutput} \frac{\partial{}}{\partial w} (y_{\nnoutput} - a_{\nnoutput})^2
\label{backprop-gradient-sum-equation}$$

where the index ${\nnoutput}$ ranges over nodes in the output layer.
Each term in the final summation is just the gradient of the loss for
the ${\nnoutput}$th output, computed as if the other outputs did not
exist. Hence, we can decompose an $m$-output learning problem into $m$
learning problems, provided we remember to add up the gradient
contributions from each of them when updating the weights.

The major complication comes from the addition of hidden layers to the
network. Whereas the error $\y-\h_{\sw}$ at the output layer is clear,
the error at the hidden layers seems mysterious because the training
data do not say what value the hidden nodes should have. Fortunately, it
turns out that we can the error from the output layer to the hidden
layers. The back-propagation process emerges directly from a derivation
of the overall error gradient. First, we will describe the process with
an intuitive justification; then, we will show the derivation.

At the output layer, the weight-update rule is identical to . We have
multiple output units, so let ${Err}{}_{\nnoutput}$ be the
${\nnoutput}$th component of the error vector $\y-\h_{\sw}$. We will
also find it convenient to define a modified error
$\Delta_{\nnoutput} \eq {Err}{}_{\nnoutput} \times g'({in}{}_{\nnoutput})$,
so that the weight-update rule becomes

$$\w{{\nnunit}}{{\nnoutput}} \leftarrow \w{{\nnunit}}{{\nnoutput}} +
 \alpha \times a_{\nnunit} \times \Delta_{\nnoutput}\ .
\label{nn-w-update-equation}$$

To update the connections between the input units and the hidden units,
we need to define a quantity analogous to the error term for output
nodes. Here is where we do the error back-propagation. The idea is that
hidden node ${\nnunit}$ is “responsible” for some fraction of the error
$\Delta_{\nnoutput}$ in each of the output nodes to which it connects.
Thus, the $\Delta_{\nnoutput}$ values are divided according to the
strength of the connection between the hidden node and the output node
and are propagated back to provide the $\Delta_{\nnunit}$ values for the
hidden layer. The propagation rule for the $\Delta$ values is the
following:

$$\Delta_{\nnunit} = g'({in}{}_{\nnunit}) \sum_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} \Delta_{\nnoutput}\ .
\label{nn-delta-equation}$$

Now the weight-update rule for the weights between the inputs and the
hidden layer is essentially identical to the update rule for the output
layer:
$$\w{{\nninput}}{{\nnunit}} \leftarrow \w{{\nninput}}{{\nnunit}} +
 \alpha \times a_{\nninput} \times \Delta_{\nnunit}\ .$$ The
back-propagation process can be summarized as follows:

-   Compute the $\Delta$ values for the output units, using the observed
    error.

-   Starting with output layer, repeat the following for each layer in
    the network, until the earliest hidden layer is reached:

    -   Propagate the $\Delta$ values back to the previous layer.

    -   Update the weights between the two layers.

The detailed algorithm is shown in .

[back-prop-learning-algorithm]

For the mathematically inclined, we will now derive the back-propagation
equations from first principles. The derivation is quite similar to the
gradient calculation for logistic regression (leading up to on ), except
that we have to use the chain rule more than once.

Following , we compute just the gradient for
${Loss}_{\nnoutput} = (y_{\nnoutput} - a_{\nnoutput})^2$ at the
$\nnoutput$th output. The gradient of this loss with respect to weights
connecting the hidden layer to the output layer will be zero except for
weights $\w{{\nnunit}}{{\nnoutput}}$ that connect to the $\nnoutput$th
output unit. For those weights, we have

$$\begin{aligned}
\frac{\partial {Loss}_{\nnoutput}}{\partial \w{{\nnunit}}{{\nnoutput}}} 
  & = & - 2(y_{\nnoutput} - a_{\nnoutput}) \frac{\partial a_{\nnoutput}}{\partial \w{{\nnunit}}{{\nnoutput}}} 
      = - 2(y_{\nnoutput} - a_{\nnoutput}) \frac{\partial g({in}{}_{\nnoutput})}{\partial \w{{\nnunit}}{{\nnoutput}}} \\
  & = & - 2(y_{\nnoutput} - a_{\nnoutput}) g'({in}{}_{\nnoutput})\frac{\partial {in}{}_{\nnoutput}}{\partial \w{{\nnunit}}{{\nnoutput}}}
      = - 2(y_{\nnoutput} - a_{\nnoutput}) g'({in}{}_{\nnoutput})\frac{\partial}{\partial\w{{\nnunit}}{{\nnoutput}}}\left(\sum_{\nnunit}\w{{\nnunit}}{{\nnoutput}} a_{\nnunit} \right) \\
  & = & - 2(y_{\nnoutput} - a_{\nnoutput}) g'({in}{}_{\nnoutput}) a_{\nnunit} = - a_{\nnunit} \Delta_{\nnoutput}\ ,\end{aligned}$$

with $\Delta_{\nnoutput}$ defined as before. To obtain the gradient with
respect to the $\w{{\nninput}}{{\nnunit}}$ weights connecting the input
layer to the hidden \<layer, we have to expand out the activations
$a_{\nnunit}$ and reapply the chain rule. We will show the derivation in
gory detail because it is interesting to see how the derivative operator
propagates back through the network:

$$\begin{aligned}
\frac{\partial {Loss}_{\nnoutput}}{\partial \w{{\nninput}}{{\nnunit}}} 
  & = & - 2 (y_{\nnoutput} - a_{\nnoutput}) \frac{\partial a_{\nnoutput}}{\partial \w{{\nninput}}{{\nnunit}}} 
      = - 2 (y_{\nnoutput} - a_{\nnoutput}) \frac{\partial g({in}{}_{\nnoutput})}{\partial \w{{\nninput}}{{\nnunit}}} \\
  & = & - 2 (y_{\nnoutput} - a_{\nnoutput}) g'({in}{}_{\nnoutput})\frac{\partial {in}{}_{\nnoutput}}{\partial \w{{\nninput}}{{\nnunit}}}
      = - 2 \Delta_{\nnoutput}\frac{\partial}{\partial\w{{\nninput}}{{\nnunit}}}\left(\sum_{\nnunit}\w{{\nnunit}}{{\nnoutput}} a_{\nnunit} \right) \\
  & = & - 2 \Delta_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} \frac{\partial a_{\nnunit}}{\partial\w{{\nninput}}{{\nnunit}}}
      = - 2 \Delta_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} \frac{\partial g({in}{}_{\nnunit})}{\partial\w{{\nninput}}{{\nnunit}}} \\
  & = & - 2 \Delta_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} g'({in}{}_{\nnunit})\frac{\partial {in}{}_{\nnunit}}{\partial \w{{\nninput}}{{\nnunit}}}\\
  & = & - 2 \Delta_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} g'({in}{}_{\nnunit})\frac{\partial}{\partial \w{{\nninput}}{{\nnunit}}}\left(\sum_{\nninput}\w{{\nninput}}{{\nnunit}} a_{\nninput} \right) \\
  & = & - 2 \Delta_{\nnoutput} \w{{\nnunit}}{{\nnoutput}} g'({in}{}_{\nnunit}) a_{\nninput} = - a_{\nninput} \Delta_{\nnunit}\ ,\end{aligned}$$

where $\Delta_{\nnunit}$ is defined as before. Thus, we obtain the
update rules obtained earlier from intuitive considerations. It is also
clear that the process can be continued for networks with more than one
hidden layer, which justifies the general algorithm given in .

[restaurant-back-prop-figure]

Having made it through (or skipped over) all the mathematics, let’s see
how a single-hidden-layer network performs on the restaurant problem.
First, we need to determine the structure of the network. We have 10
attributes describing each example, so we will need 10 input units.
Should we have one hidden layer or two? How many nodes in each layer?
Should they be fully connected? There is no good theory that will tell
us the answer. (See the next section.) As always, we can use
cross-validation: try several different structures and see which one
works best. It turns out that a network with one hidden layer containing
four nodes is about right for this problem. In , we show two curves. The
first is a training curve showing the mean squared error on a given
training set of 100 restaurant examples during the weight-updating
process. This demonstrates that the network does indeed converge to a
perfect fit to the training data. The second curve is the standard
learning curve for the restaurant data. The neural network does learn
well, although not quite as fast as decision-tree learning; this is
perhaps not surprising, because the data were generated from a simple
decision tree in the first place.

Neural networks are capable of far more complex learning tasks of
course, although it must be said that a certain amount of twiddling is
needed to get the network structure right and to achieve convergence to
something close to the global optimum in weight space. There are
literally tens of thousands of published applications of neural
networks. looks at one such application in more depth.

### Learning neural network structures {#network-structure-page}

So far, we have considered the problem of learning weights, given a
fixed network structure; just as with Bayesian networks, we also need to
understand how to find the best network structure. If we choose a
network that is too big, it will be able to memorize all the examples by
forming a large lookup table, but will not necessarily generalize well
to inputs that have not been seen before.[^10] In other words, like all
statistical models, neural networks are subject to when there are too
many parameters in the model. We saw this in (), where the
high-parameter models in (b) and (c) fit all the data, but might not
generalize as well as the low-parameter models in (a) and (d).

If we stick to fully connected networks, the only choices to be made
concern the number of hidden layers and their sizes. The usual approach
is to try several and keep the best. The techniques of are needed if we
are to avoid at the test set. That is, we choose the network
architecture that gives the highest prediction accuracy on the
validation sets.

If we want to consider networks that are not fully connected, then we
need to find some effective search method through the very large space
of possible connection topologies. The algorithm begins with a fully
connected network and removes connections from it. After the network is
trained for the first time, an information-theoretic approach identifies
an optimal selection of connections that can be dropped. The network is
then retrained, and if its performance has not decreased then the
process is repeated. In addition to removing connections, it is also
possible to remove units that are not contributing much to the result.

Several algorithms have been proposed for growing a larger network from
a smaller one. One, the algorithm, resembles decision-list learning. The
idea is to start with a single unit that does its best to produce the
correct output on as many of the training examples as possible.
Subsequent units are added to take care of the examples that the first
unit got wrong. The algorithm adds only as many units as are needed to
cover all the examples.

Nonparametric Models {#nonparametric-section}
--------------------

Linear regression and neural networks use the training data to estimate
a fixed set of parameters $\bw$. That defines our hypothesis
$h_{\sw}(\x)$, and at that point we can throw away the training data,
because they are all summarized by $\bw$. A learning model that
summarizes data with a set of parameters of fixed size (independent of
the number of training examples) is called a .

No matter how much data you throw at a parametric model, it won’t change
its mind about how many parameters it needs. When data sets are small,
it makes sense to have a strong restriction on the allowable hypotheses,
to avoid overfitting. But when there are thousands or millions or
billions of examples to learn from, it seems like a better idea to let
the data speak for themselves rather than forcing them to speak through
a tiny vector of parameters. If the data say that the correct answer is
a very wiggly function, we shouldn’t restrict ourselves to linear or
slightly wiggly functions.

A is one that cannot be characterized by a bounded set of parameters.
For example, suppose that each hypothesis we generate simply retains
within itself all of the training examples and uses all of them to
predict the next example. Such a hypothesis family would be
nonparametric because the effective number of parameters is unbounded—it
grows with the number of examples. This approach is called or . The
simplest instance-based learning method is : take all the training
examples, put them in a lookup table, and then when asked for $h(\x)$,
see if $\x$ is in the table; if it is, return the corresponding $y$. The
problem with this method is that it does not generalize well: when $\x$
is not in the table all it can do is return some default value.

### Nearest neighbor models

We can improve on table lookup with a slight variation: given a query
$\x_q$, find the $k$ examples that are *nearest* to $\x_q$.
This is called lookup. We’ll use the notation ${NN}(k, \x_q)$ to
denote the set of $k$ nearest neighbors.

To do classification, first find ${NN}(k, \x_q)$, then take the
plurality vote of the neighbors (which is the majority vote in the case
of binary classification). To avoid ties, $k$ is always chosen to be an
odd number. To do regression, we can take the mean or median of the $k$
neighbors, or we can solve a linear regression problem on the neighbors.

$k\eq 1$$k \eq 5$ [earthquake-nn-figure]

In , we show the decision boundary of $k$-nearest-neighbors
classification for $k\eq$ 1 and 5 on the earthquake data set from .
Nonparametric methods are still subject to underfitting and overfitting,
just like parametric methods. In this case 1-nearest neighbors is
overfitting; it reacts too much to the black outlier in the upper right
and the white outlier at (5.4, 3.7). The 5-nearest-neighbors decision
boundary is good; higher $k$ would underfit. As usual, cross-validation
can be used to select the best value of $k$.

The very word “nearest” implies a distance metric. How do we measure the
distance from a query point $\x_q$ to an example point $\x_j$?
Typically, distances are measured with a or $L^p$ norm, defined as
$$L^p(\x_j, \x_q) = (\sum_i | x_{j,i} - x_{q,i} |^p)^{1/p} \ .$$ With
$p\eq 2$ this is Euclidean distance and with $p\eq 1$ it is Manhattan
distance. With Boolean attribute values, the number of attributes on
which the two points differ is called the . Often $p\eq 2$ is used if
the dimensions are measuring similar properties, such as the width,
height and depth of parts on a conveyor belt, and Manhattan distance is
used if they are dissimilar, such as age, weight, and gender of a
patient. Note that if we use the raw numbers from each dimension then
the total distance will be affected by a change in scale in any
dimension. That is, if we change dimension $i$ from measurements in
centimeters to miles while keeping the other dimensions the same, we’ll
get different nearest neighbors. To avoid this, it is common to apply to
the measurements in each dimension. One simple approach is to compute
the mean $\mu_i$ and standard deviation $\sigma_i$ of the values in each
dimension, and rescale them so that $x_{j,i}$ becomes
$(x_{j,i} - \mu_i)/\sigma_i$. A more complex metric known as the takes
into account the covariance between dimensions.

In low-dimensional spaces with plenty of data, nearest neighbors works
very well: we are likely to have enough nearby data points to get a good
answer. But as the number of dimensions rises we encounter a problem:
the nearest neighbors in high-dimensional spaces are usually not very
near! Consider $k$-nearest-neighbors on a data set of ${\Ncount}$ points
uniformly distributed throughout the interior of an
$\Acount$-dimensional unit hypercube. We’ll define the $k$-neighborhood
of a point as the smallest hypercube that contains the $k$-nearest
neighbors. Let $\ell$ be the average side length of a neighborhood. Then
the volume of the neighborhood (which contains $k$ points) is
$\ell^{\Acount}$ and the volume of the full cube (which contains
${\Ncount}$ points) is 1. So, on average,
$\ell^{\Acount}\eq k/{\Ncount}$. Taking ${\Acount}$th roots of both
sides we get $\ell = (k/{\Ncount})^{1/{\Acount}}$.

To be concrete, let $k\eq 10$ and ${\Ncount}\eq 1,000,000$. In two
dimensions (${\Acount}\eq 2$; a unit square), the average neighborhood
has $\ell\eq{0.003}$, a small fraction of the unit square, and in 3
dimensions $\ell$ is just 2% of the edge length of the unit cube. But by
the time we get to 17 dimensions, $\ell$ is half the edge length of the
unit hypercube, and in 200 dimensions it is 94%. This problem has been
called the .

Another way to look at it: consider the points that fall within a thin
shell making up the outer 1% of the unit hypercube. These are outliers;
in general it will be hard to find a good value for them because we will
be extrapolating rather than interpolating. In one dimension, these
outliers are only 2% of the points on the unit line (those points where
$x < .01$ or $x > .99$), but in 200 dimensions, over 98% of the points
fall within this thin shell—almost all the points are outliers. You can
see an example of a poor nearest-neighbors fit on outliers if you look
ahead to (b).

[curse-dimensionality-figure]

The ${NN}(k, \x_q)$ function is conceptually trivial: given a set of
${\Ncount}$ examples and a query $\x_q$, iterate through the examples,
measure the distance to $\x_q$ from each one, and keep the best $k$. If
we are satisfied with an implementation that takes $O({\Ncount})$
execution time, then that is the end of the story. But instance-based
methods are designed for large data sets, so we would like an algorithm
with sublinear run time. Elementary analysis of algorithms tells us that
exact table lookup is $O({\Ncount})$ with a sequential table,
$O(\log {\Ncount})$ with a binary tree, and $O(1)$ with a hash table. We
will now see that binary trees and hash tables are also applicable for
finding nearest neighbors.

### Finding nearest neighbors with k-d trees

A balanced binary tree over data with an arbitrary number of dimensions
is called a , for k-dimensional tree. (In our notation, the number of
dimensions is $\Acount$, so they would be $\Acount$-d trees. The
construction of a k-d tree is similar to the construction of a
one-dimensional balanced binary tree. We start with a set of examples
and at the root node we split them along the $i$th dimension by testing
whether $x_i \le m$. We chose the value $m$ to be the median of the
examples along the $i$th dimension; thus half the examples will be in
the left branch of the tree and half in the right. We then recursively
make a tree for the left and right sets of examples, stopping when there
are fewer than two examples left. To choose a dimension to split on at
each node of the tree, one can simply select dimension $i \bmod \Acount$
at level $i$ of the tree. (Note that we may need to split on any given
dimension several times as we proceed down the tree.) Another strategy
is to split on the dimension that has the widest spread of values.

Exact lookup from a k-d tree is just like lookup from a binary tree
(with the slight complication that you need to pay attention to which
dimension you are testing at each node). But nearest neighbor lookup is
more complicated. As we go down the branches, splitting the examples in
half, in some cases we can discard the other half of the examples. But
not always. Sometimes the point we are querying for falls very close to
the dividing boundary. The query point itself might be on the left hand
side of the boundary, but one or more of the $k$ nearest neighbors might
actually be on the right-hand side. We have to test for this possibility
by computing the distance of the query point to the dividing boundary,
and then searching both sides if we can’t find $k$ examples on the left
that are closer than this distance. Because of this problem, k-d trees
are appropriate only when there are many more examples than dimensions,
preferably at least $2^{\Acount}$ examples. Thus, k-d trees work well
with up to 10 dimensions with thousands of examples or up to 20
dimensions with millions of examples. If we don’t have enough examples,
lookup is no faster than a linear scan of the entire data set.

### Locality-sensitive hashing

Hash tables have the potential to provide even faster lookup than binary
trees. But how can we find nearest neighbors using a hash table, when
hash codes rely on an *exact* match? Hash codes randomly
distribute values among the bins, but we want to have near points
grouped together in the same bin; we want a (LSH).

We can’t use hashes to solve ${NN}(k, \x_q)$ exactly, but with a
clever use of randomized algorithms, we can find an
*approximate* solution. First we define the problem: given
a data set of example points and a query point $\x_q$, find, with high
probability, an example point (or points) that is near $\x_q$. To be
more precise, we require that if there is a point $\x_j$ that is within
a radius $r$ of $\x_q$, then with high probability the algorithm will
find a point $\x_{j'}$ that is within distance $c\,r$ of $q$. If there
is no point within radius $r$ then the algorithm is allowed to report
failure. The values of $c$ and “high probability” are parameters of the
algorithm.

To solve approximate near neighbors, we will need a hash function
$g(\x)$ that has the property that, for any two points $\x_j$ and
$\x_{j'}$, the probability that they have the same hash code is small if
their distance is more than $c\, r$, and is high if their distance is
less than $r$. For simplicity we will treat each point as a bit string.
(Any features that are not Boolean can be encoded into a set of Boolean
features.)

The intuition we rely on is that if two points are close together in an
$\Acount$-dimensional space, then they will necessarily be close when
projected down onto a one-dimensional space (a line). In fact, we can
discretize the line into bins—hash buckets—so that, with high
probability, near points project down to exactly the same bin. Points
that are far away from each other will tend to project down into
different bins for most projections, but there will always be a few
projections that coincidentally project far-apart points into the same
bin. Thus, the bin for point $\x_q$ contains many (but not all) points
that are near to $\x_q$, as well as some points that are far away.

The trick of LSH is to create *multiple* random projections
and combine them. A random projection is just a random subset of the
bit-string representation. We choose $\ell$ different random projections
and create $\ell$ hash tables, $g_1(\x), \ldots,
g_{\ell}(\x)$. We then enter all the examples into each hash table. Then
when given a query point $\x_q$, we fetch the set of points in bin
$g_k(q)$ for each $k$, and union these sets together into a set of
candidate points, $C$. Then we compute the actual distance to $\x_q$ for
each of the points in $C$ and return the $k$ closest points. With high
probability, each of the points that are near to $\x_q$ will show up in
at least one of the bins, and although some far-away points will show up
as well, we can ignore those. With large real-world problems, such as
finding the near neighbors in a data set of 13 million Web images using
512 dimensions @Torralba+al:2008, locality-sensitive hashing needs to
examine only a few thousand images out of 13 million to find nearest
neighbors; a thousand-fold speedup over exhaustive or k-d tree
approaches.

### Nonparametric regression

[nonparametric-regression-figure]

Now we’ll look at nonparametric approaches to *regression*
rather than classification. shows an example of some different models.
In (a), we have perhaps the simplest method of all, known informally as
“connect-the-dots,” and superciliously as “piecewise-linear
nonparametric regression.” This model creates a function $h(x)$ that,
when given a query $x_q$, solves the ordinary linear regression problem
with just two points: the training examples immediately to the left and
right of $x_q$. When noise is low, this trivial method is actually not
too bad, which is why it is a standard feature of charting software in
spreadsheets. But when the data are noisy, the resulting function is
spiky, and does not generalize well.

$k$- ((b)) improves on connect-the-dots. Instead of using just the two
examples to the left and right of a query point $x_q$, we use the $k$
nearest neighbors (here 3). A larger value of $k$ tends to smooth out
the magnitude of the spikes, although the resulting function has
discontinuities. In (b), we have the $k$-nearest-neighbors average:
$h(x)$ is the mean value of the $k$ points, $\sum y_j/k$. Notice that at
the outlying points, near $x\eq 0$ and $x\eq 14$, the estimates are poor
because all the evidence comes from one side (the interior), and ignores
the trend. In (c), we have $k$-nearest-neighbor linear regression, which
finds the best line through the $k$ examples. This does a better job of
capturing trends at the outliers, but is still discontinuous. In both
(b) and (c), we’re left with the question of how to choose a good value
for $k$. The answer, as usual, is cross-validation.

((d)) gives us the advantages of nearest neighbors, without the
discontinuities. To avoid discontinuities in $h(x)$, we need to avoid
discontinuities in the set of examples we use to estimate $h(x)$. The
idea of locally weighted regression is that at each query point $x_q$,
the examples that are close to $x_q$ are weighted heavily, and the
examples that are farther away are weighted less heavily or not at all.
The decrease in weight over distance is always gradual, not sudden.

[kernel-figure]

We decide how much to weight each example with a function known as a . A
kernel function looks like a bump; in we see the specific kernel used to
generate (d). We can see that the weight provided by this kernel is
highest in the center and reaches zero at a distance of $\pm 5$. Can we
choose just any function for a kernel? No. First, note that we invoke a
kernel function $\kernel$ with $\kernel({Distance}(\x_j, \x_q))$,
where $\x_q$ is a query point that is a given distance from $\x_j$, and
we want to know how much to weight that distance. So $\kernel$ should be
symmetric around 0 and have a maximum at 0. The area under the kernel
must remain bounded as we go to $\pm \infty$. Other shapes, such as
Gaussians, have been used for kernels, but the latest research suggests
that the choice of shape doesn’t matter much. We do have to be careful
about the width of the kernel. Again, this is a parameter of the model
that is best chosen by cross-validation. Just as in choosing the $k$ for
nearest neighbors, if the kernels are too wide we’ll get underfitting
and if they are too narrow we’ll get overfitting. In (d), the value of
$k\eq 10$ gives a smooth curve that looks about right—but maybe it does
not pay enough attention to the outlier at $x\eq 6$; a narrower kernel
width would be more responsive to individual points.

Doing locally weighted regression with kernels is now straightforward.
For a given query point $\x_q$ we solve the following weighted
regression problem using gradient descent:
$$\bw^* = \argmin_{\sw} \sum_{j} \kernel({Distance}(\x_q, \x_j)) \, (y_j - \bw \cdot \x_j)^2  \ ,$$
where ${Distance}$ is any of the distance metrics discussed for
nearest neighbors. Then the answer is $h(\x_q) \eq \bw^* \cdot \x_q$.

Note that we need to solve a new regression problem for
*every* query point—that’s what it means to be
*local*. (In ordinary linear regression, we solved the
regression problem once, globally, and then used the same $h_{\sw}$ for
any query point.) Mitigating against this extra work is the fact that
each regression problem will be easier to solve, because it involves
only the examples with nonzero weight—the examples whose kernels overlap
the query point. When kernel widths are small, this may be just a few
points.

Most nonparametric models have the advantage that it is easy to do
leave-one-out cross-validation without having to recompute everything.
With a $k$-nearest-neighbors model, for instance, when given a test
example $(\x,y)$ we retrieve the $k$ nearest neighbors once, compute the
per-example loss $L(y,h(\x))$ from them, and record that as the
leave-one-out result for every example that is not one of the neighbors.
Then we retrieve the $k+1$ nearest neighbors and record distinct results
for leaving out each of the $k$ neighbors. With ${\Ncount}$ examples the
whole process is $O(k)$, not $O(k{\Ncount})$.

Support Vector Machines {#kernel-machine-section}
-----------------------

The or SVM framework is currently the most popular approach for
“off-the-shelf” supervised learning: if you don’t have any specialized
prior knowledge about a domain, then the SVM is an excellent method to
try first. There are three properties that make SVMs attractive:

1.  SVMs construct a —a decision boundary with the largest possible
    distance to example points. This helps them generalize well.

2.  SVMs create a linear separating hyperplane, but they have the
    ability to embed the data into a higher-dimensional space, using the
    so-called . Often, data that are not linearly separable in the
    original input space are easily separable in the higher-dimensional
    space. The high-dimensional linear separator is actually nonlinear
    in the original space. This means the hypothesis space is greatly
    expanded over methods that use strictly linear representations.

3.  SVMs are a nonparametric method—they retain training examples and
    potentially need to store them all. On the other hand, in practice
    they often end up retaining only a small fraction of the number of
    examples—sometimes as few as a small constant times the number of
    dimensions. Thus SVMs combine the advantages of nonparametric and
    parametric models: they have the flexibility to represent complex
    functions, but they are resistant to overfitting.

[kernel-machine-margin-figure2]

You could say that SVMs are successful because of one key insight and
one neat trick. We will cover each in turn. In (a), we have a binary
classification problem with three candidate decision boundaries, each a
linear separator. Each of them is consistent with all the examples, so
from the point of view of 0/1 loss, each would be equally good. Logistic
regression would find some separating line; the exact location of the
line depends on *all* the example points. The key insight
of SVMs is that some examples are more important than others, and that
paying attention to them can lead to better generalization.

Consider the lowest of the three separating lines in (a). It comes very
close to 5 of the black examples. Although it classifies all the
examples correctly, and thus minimizes loss, it should make you nervous
that so many examples are close to the line; it may be that other black
examples will turn out to fall on the other side of the line.

SVMs address this issue: Instead of minimizing expected *empirical
loss* on the training data, SVMs attempt to minimize expected
*generalization* loss. We don’t know where the
as-yet-unseen points may fall, but under the probabilistic assumption
that they are drawn from the same distribution as the previously seen
examples, there are some arguments from computational learning theory ()
suggesting that we minimize generalization loss by choosing the
separator that is farthest away from the examples we have seen so far.
We call this separator, shown in (b) the . The is the width of the area
bounded by dashed lines in the figure—twice the distance from the
separator to the nearest example point.

Now, how do we find this separator? Before showing the equations, some
notation: Traditionally SVMs use the convention that class labels are +1
and -1, instead of the +1 and 0 we have been using so far. Also, where
we put the intercept into the weight vector $\bw$ (and a corresponding
dummy 1 value into $x_{j,0}$), SVMs do not do that; they keep the
intercept as a separate parameter, $b$. With that in mind, the separator
is defined as the set of points $\{ \x :
\bw \cdot \x + b \eq 0 \}$. We could search the space of $\bw$ and $b$
with gradient descent to find the parameters that maximize the margin
while correctly classifying all the examples.

However, it turns out there is another approach to solving this problem.
We won’t show the details, but will just say that there is an
alternative representation called the dual representation, in which the
optimal solution is found by solving

$$\argmax_{\alpha}\sum_j \alpha_j - \frac{1}{2}\sum_{j,k} \alpha_j \alpha_k y_j y_k (\x_j \cdot \x_k)
\label{kernel-qp-equation}$$

subject to the constraints $\alpha_j \geq 0$ and
$\sum_j \alpha_j y_j\eq 0$. This is a optimization problem, for which
there are good software packages. Once we have found the vector $\alpha$
we can get back to $\bw$ with the equation
$\bw\eq \sum_j \alpha_j \x_j$, or we can stay in the dual
representation. There are three important properties of . First, the
expression is convex; it has a single global maximum that can be found
efficiently. Second,

the data enter the expression only in the form of dot products of pairs
of points.

This second property is also true of the equation for the separator
itself; once the optimal $\alpha_j$ have been calculated, it is

$$h(\x) = \mbox{sign}\left(\sum_{j} \alpha_j y_j (\x\cdot\x_j) - b\right)\ .
\label{kernel-separator-equation}$$

A final important property is that the weights $\alpha_j$ associated
with each data point are *zero* except for the —the points
closest to the separator. (They are called “support” vectors because
they “hold up” the separating plane.) Because there are usually many
fewer support vectors than examples, SVMs gain some of the advantages of
parametric models.

What if the examples are not linearly separable? (a) shows an input
space defined by attributes $\x\eq (x_1,x_2)$, with positive examples
($y\eq +1$) inside a circular region and negative examples ($y\eq -1$)
outside. Clearly, there is no linear separator for this problem. Now,
suppose we re-express the input data—i.e., we map each input vector $\x$
to a new vector of feature values, $F(\x)$. In particular, let us use
the three features

$$f_1\eq x_1^2\ ,\qquad f_2\eq x_2^2\ ,\qquad f_3\eq \sqrt{2}x_1 x_2\ .
\label{kernel-feature-space-equation}$$

We will see shortly where these came from, but for now, just look at
what happens. (b) shows the data in the new, three-dimensional space
defined by the three features; the data are *linearly
separable* in this space! This phenomenon is actually fairly
general: if data are mapped into a space of sufficiently high dimension,
then they will almost always be linearly separable—if you look at a set
of points from enough directions, you’ll find a way to make them line
up. Here, we used only three dimensions;[^11] asks you to show that four
dimensions suffice for linearly separating a circle anywhere in the
plane (not just at the origin), and five dimensions suffice to linearly
separate any ellipse. In general (with some special cases excepted) if
we have ${\Ncount}$ data points then they will always be separable in
spaces of ${\Ncount}-1$ dimensions or more ().

[kernel-machine-figure]

Now, we would not usually expect to find a linear separator in the input
space $\x$, but we can find linear separators in the high-dimensional
feature space $F(\x)$ simply by replacing $\x_j \cdot \x_k$ in with
$F(\x_j) \cdot F(\x_k)$. This by itself is not remarkable—replacing $\x$
by $F(\x)$ in *any* learning algorithm has the required
effect—but the dot product has some special properties. It turns out
that $F(\x_j) \cdot F(\x_k)$ can often be computed without first
computing $F$ for each point. In our three-dimensional feature space
defined by , a little bit of algebra shows that
$$F(\x_j) \cdot F(\x_k) = (\x_j \cdot \x_k)^2\ .$$ (That’s why the
$\sqrt{2}$ is in $f_3$.) The expression $(\x_j
\cdot \x_k)^2$ is called a ,[^12] and is usually written as
$K(\x_j,\x_k)$. The kernel function can be applied to pairs of input
data to evaluate dot products in some corresponding feature space. So,
we can find linear separators in the higher-dimensional feature space
$F(\x)$ simply by replacing $\x_j \cdot \x_k$ in with a kernel function
$K(\x_j,\x_k)$. Thus, we can learn in the higher-dimensional space, but
we compute only kernel functions rather than the full list of features
for each data point.

The next step is to see that there’s nothing special about the kernel
$K(\x_j,\x_k)\eq (\x_j \cdot
\x_k)^2$. It corresponds to a particular higher-dimensional feature
space, but other kernel functions correspond to other feature spaces. A
venerable result in mathematics,  [-@Mercer:1909], tells us that any
“reasonable”[^13] kernel function corresponds to *some*
feature space. These feature spaces can be very large, even for
innocuous-looking kernels. For example, the ,
$K(\x_j,\x_k)\eq (1+ \x_j \cdot
\x_k)^d$, corresponds to a feature space whose dimension is exponential
in $d$.

This then is the clever : Plugging these kernels into ,

optimal linear separators can be found efficiently in feature spaces
with billions of (or, in some cases, infinitely many) dimensions.

The resulting linear separators, when mapped back to the original input
space, can correspond to arbitrarily wiggly, nonlinear decision
boundaries between the positive and negative examples.

In the case of inherently noisy data, we may not want a linear separator
in some high-dimensional space. Rather, we’d like a decision surface in
a lower-dimensional space that does not cleanly separate the classes,
but reflects the reality of the noisy data. That is possible with the
classifier, which allows examples to fall on the wrong side of the
decision boundary, but assigns them a penalty proportional to the
distance required to move them back on the correct side.

The kernel method can be applied not only with learning algorithms that
find optimal linear separators, but also with any other algorithm that
can be reformulated to work only with dot products of pairs of data
points, as in Equations [kernel-qp-equation]
and [kernel-separator-equation]. Once this is done, the dot product is
replaced by a kernel function and we have a version of the algorithm.
This can be done easily for $k$-nearest-neighbors and perceptron
learning (), among others.

Ensemble Learning {#ensemble-section}
-----------------

So far we have looked at learning methods in which a single hypothesis,
chosen from a hypothesis space, is used to make predictions. The idea of
methods is to select a collection, or , of hypotheses from the
hypothesis space and combine their predictions. For example, during
cross-validation we might generate twenty different decision trees, and
have them vote on the best classification for a new example.

The motivation for ensemble learning is simple. Consider an ensemble of
$\Ecount\eq 5$ hypotheses and suppose that we combine their predictions
using simple majority voting. For the ensemble to misclassify a new
example, *at least three of the five hypotheses have to
misclassify it*. The hope is that this is much less likely than a
misclassification by a single hypothesis. Suppose we assume that each
hypothesis $h_k$ in the ensemble has an error of $p$—that is, the
probability that a randomly chosen example is misclassified by $h_k$ is
$p$. Furthermore, suppose we assume that the errors made by each
hypothesis are *independent*. In that case, if $p$ is
small, then the probability of a large number of misclassifications
occurring is minuscule. For example, a simple calculation () shows that
using an ensemble of five hypotheses reduces an error rate of 1 in 10
down to an error rate of less than 1 in 100. Now, obviously the
assumption of independence is unreasonable, because hypotheses are
likely to be misled in the same way by any misleading aspects of the
training data. But if the hypotheses are at least a little bit
different, thereby reducing the correlation between their errors, then
ensemble learning can be very useful.

[ensemble-expressiveness-figure]

Another way to think about the ensemble idea is as a generic way of
enlarging the hypothesis space. That is, think of the ensemble itself as
a hypothesis and the new hypothesis space as the set of all possible
ensembles constructable from hypotheses in the original space. shows how
this can result in a more expressive hypothesis space. If the original
hypothesis space allows for a simple and efficient learning algorithm,
then the ensemble method provides a way to learn a much more expressive
class of hypotheses without incurring much additional computational or
algorithmic complexity.

The most widely used ensemble method is called . To understand how it
works, we need first to explain the idea of a . In such a training set,
each example has an associated weight $w_j\geq
0$. The higher the weight of an example, the higher is the importance
attached to it during the learning of a hypothesis. It is
straightforward to modify the learning algorithms we have seen so far to
operate with weighted training sets.[^14]

Boosting starts with $w_j\eq 1$ for all the examples (i.e., a normal
training set). From this set, it generates the first hypothesis, $h_1$.
This hypothesis will classify some of the training examples correctly
and some incorrectly. We would like the next hypothesis to do better on
the misclassified examples, so we increase their weights while
decreasing the weights of the correctly classified examples. From this
new weighted training set, we generate hypothesis $h_2$. The process
continues in this way until we have generated $\Ecount$ hypotheses,
where $\Ecount$ is an input to the boosting algorithm. The final
ensemble hypothesis is a weighted-majority combination of all the
$\Ecount$ hypotheses, each weighted according to how well it performed
on the training set. shows how the algorithm works conceptually. There
are many variants of the basic boosting idea, with different ways of
adjusting the weights and combining the hypotheses. One specific
algorithm, called , is shown in . has a very important property: if the
input learning algorithm is a algorithm—which means that always returns
a hypothesis with accuracy on the training set that is slightly better
than random guessing (i.e., 50%$ + \epsilon$ for Boolean
classification)—then will return a hypothesis that *classifies the
training data perfectly* for large enough $\Ecount$. Thus, the
algorithm *boosts* the accuracy of the original learning
algorithm on the training data. This result holds no matter how
inexpressive the original hypothesis space and no matter how complex the
function being learned.

[boosting-figure]

[adaboost-algorithm]

[boosting-performance-figure]

Let us see how well boosting does on the restaurant data. We will choose
as our original hypothesis space the class of , which are decision trees
with just one test, at the root. The lower curve in (a) shows that
unboosted decision stumps are not very effective for this data set,
reaching a prediction performance of only 81% on 100 training examples.
When boosting is applied (with $\Ecount\eq 5$), the performance is
better, reaching 93% after 100 examples.

An interesting thing happens as the ensemble size $\Ecount$ increases.
(b) shows the training set performance (on 100 examples) as a function
of $\Ecount$. Notice that the error reaches zero when $\Ecount$ is 20;
that is, a weighted-majority combination of 20 decision stumps suffices
to fit the 100 examples exactly. As more stumps are added to the
ensemble, the error remains at zero. The graph also shows that

the test set performance continues to increase long after the training
set error has reached zero.

At $\Ecount = {20}$, the test performance is 0.95 (or 0.05 error), and
the performance increases to 0.98 as late as $\Ecount = {137}$, before
gradually dropping to 0.95.

This finding, which is quite robust across data sets and hypothesis
spaces, came as quite a surprise when it was first noticed. Ockham’s
razor tells us not to make hypotheses more complex than necessary, but
the graph tells us that the predictions *improve* as the
ensemble hypothesis gets more complex! Various explanations have been
proposed for this. One view is that boosting approximates (see ), which
can be shown to be an optimal learning algorithm, and the approximation
improves as more hypotheses are added. Another possible explanation is
that the addition of further hypotheses enables the ensemble to be
*more definite* in its distinction between positive and
negative examples, which helps it when it comes to classifying new
examples.

### Online Learning

So far, everything we have done in this chapter has relied on the
assumption that the data are i.i.d. (independent and
identically distributed). On the one hand, that is a sensible
assumption: if the future bears no resemblance to the past, then how can
we predict anything? On the other hand, it is too strong an assumption:
it is rare that our inputs have captured all the information that would
make the future truly independent of the past.

In this section we examine what to do when the data are not i.i.d.; when
they can change over time. In this case, it matters *when*
we make a prediction, so we will adopt the perspective called : an agent
receives an input $x_j$ from nature, predicts the corresponding $y_j$,
and then is told the correct answer. Then the process repeats with
$x_{j+1}$, and so on. One might think this task is hopeless—if nature is
adversarial, all the predictions may be wrong. It turns out that there
are some guarantees we can make.

Let us consider the situation where our input consists of predictions
from a panel of experts. For example, each day a set of ${\Ccount}$
pundits predicts whether the stock market will go up or down, and our
task is to pool those predictions and make our own. One way to do this
is to keep track of how well each expert performs, and choose to believe
them in proportion to their past performance. This is called the . We
can described it more formally:

1.  Initialize a set of weights $\{w_1,\ldots,w_{\Ccount}\}$ all to 1.

2.  Receive the predictions $\{\hat{y}_1,\ldots, \hat{y}_{\Ccount}\}$
    from the experts.

3.  Randomly choose an expert $k^*$, in proportion to its weight:
    $P(k) \eq w_k/(\sum_{k'} w_{k'})$.

4.  Predict $\hat{y}_{k^*}$.

5.  Receive the correct answer $y$.

6.  For each expert $k$ such that $\hat{y}_k \neq y$, update

Here $\beta$ is a number, $0 < \beta < 1$, that tells how much to
penalize an expert for each mistake.

We measure the success of this algorithm in terms of , which is defined
as the number of additional mistakes we make compared to the expert who,
in hindsight, had the best prediction record. Let ${\Mcount}^*$ be the
number of mistakes made by the best expert. Then the number of mistakes,
${\Mcount}$, made by the random weighted majority algorithm, is bounded
by[^15]
$${\Mcount} < \frac{{\Mcount}^* \ln (1 / \beta) + \ln {\Ccount}}{1 - \beta} \ .$$
This bound holds for *any* sequence of examples, even ones
chosen by adversaries trying to do their worst. To be specific, when
there are ${\Ccount}\eq 10$ experts, if we choose $\beta\eq 1/2$ then
our number of mistakes is bounded by $1.39{\Mcount}^* + 4.6$, and if
$\beta\eq
3/4$ by $1.15{\Mcount}^* + 9.2$. In general, if $\beta$ is close to 1
then we are responsive to change over the long run; if the best expert
changes, we will pick up on it before too long. However, we pay a
penalty at the beginning, when we start with all experts trusted
equally; we may accept the advice of the bad experts for too long. When
$\beta$ is closer to 0, these two factors are reversed. Note that we can
choose $\beta$ to get asymptotically close to ${\Mcount}^*$ in the long
run; this is called (because the average amount of regret per trial
tends to 0 as the number of trials increases).

Online learning is helpful when the data may be changing rapidly over
time. It is also useful for applications that involve a large collection
of data that is constantly growing, even if changes are gradual. For
example, with a database of millions of Web images, you wouldn’t want to
train, say, a linear regression model on all the data, and then retrain
from scratch every time a new image is added. It would be more practical
to have an online algorithm that allows images to be added
incrementally. For most learning algorithms based on minimizing loss,
there is an online version based on minimizing regret. It is a bonus
that many of these online algorithms come with guaranteed bounds on
regret.

To some observers, it is surprising that there are such tight bounds on
how well we can do compared to a panel of experts. To others, the really
surprising thing is that when panels of human experts
congregate—predicting stock market prices, sports outcomes, or political
contests—the viewing public is so willing to listen to them pontificate
and so unwilling to quantify their error rates.

Practical Machine Learning
--------------------------

We have introduced a wide range of machine learning techniques, each
illustrated with simple learning tasks. In this section, we consider two
aspects of practical machine learning. The first involves finding
algorithms capable of learning to recognize handwritten digits and
squeezing every last drop of predictive performance out of them. The
second involves anything but—pointing out that obtaining, cleaning, and
representing the data can be at least as important as algorithm
engineering.

### Case study: Handwritten digit recognition {#lecun-section}

Recognizing handwritten digits is an important problem with many
applications, including automated sorting of mail by postal code,
automated reading of checks and tax returns, and data entry for
hand-held computers. It is an area where rapid progress has been made,
in part because of better learning algorithms and in part because of the
availability of better training sets. The National Institute of Science
and Technology () has archived a database of 60,000 labeled digits, each
${20}\stimes{{20}}\eq{{400}}$ pixels with 8-bit grayscale values. It has
become one of the standard benchmark problems for comparing new learning
algorithms. Some example digits are shown in .

[digit-examples-figure]

Many different learning approaches have been tried. One of the first,
and probably the simplest, is the **3-nearest-neighbor**
classifier, which also has the advantage of requiring no training time.
As a memory-based algorithm, however, it must store all 60,000 images,
and its run time performance is slow. It achieved a test error rate of
2.4%.

A **single-hidden-layer neural network** was designed for
this problem with 400 input units (one per pixel) and 10 output units
(one per class). Using cross-validation, it was found that roughly 300
hidden units gave the best performance. With full interconnections
between layers, there were a total of 123,300 weights. This network
achieved a 1.6% error rate.

A series of **specialized neural networks** called LeNet
were devised to take advantage of the structure of the problem—that the
input consists of pixels in a two–dimensional array, and that small
changes in the position or slant of an image are unimportant. Each
network had an input layer of ${32}\stimes{{32}}$ units, onto which the
${20}\stimes{{20}}$ pixels were centered so that each input unit is
presented with a local neighborhood of pixels. This was followed by
three layers of hidden units. Each layer consisted of several planes of
$n\stimes{n}$ arrays, where $n$ is smaller than the previous layer so
that the network is down-sampling the input, and where the weights of
every unit in a plane are constrained to be identical, so that the plane
is acting as a feature detector: it can pick out a feature such as a
long vertical line or a short semi-circular arc. The output layer had 10
units. Many versions of this architecture were tried; a representative
one had hidden layers with 768, 192, and 30 units, respectively. The
training set was augmented by applying affine transformations to the
actual inputs: shifting, slightly rotating, and scaling the images. (Of
course, the transformations have to be small, or else a 6 will be
transformed into a 9!) The best error rate achieved by LeNet was 0.9%.

A **boosted neural network** combined three copies of the
LeNet architecture, with the second one trained on a mix of patterns
that the first one got 50% wrong, and the third one trained on patterns
for which the first two disagreed. During testing, the three nets voted
with the majority ruling. The test error rate was 0.7%.

A (see ) with 25,000 support vectors achieved an error rate of 1.1%.
This is remarkable because the SVM technique, like the simple
nearest-neighbor approach, required almost no thought or iterated
experimentation on the part of the developer, yet it still came close to
the performance of LeNet, which had had years of development. Indeed,
the support vector machine makes no use of the structure of the problem,
and would perform just as well if the pixels were presented in a
permuted order.

A starts with a regular SVM and then improves it with a technique that
is designed to take advantage of the structure of the problem. Instead
of allowing products of all pixel pairs, this approach concentrates on
kernels formed from pairs of nearby pixels. It also augments the
training set with transformations of the examples, just as LeNet did. A
virtual SVM achieved the best error rate recorded to date, 0.56%.

**Shape matching** is a technique from computer vision used
to align corresponding parts of two different images of
objects @Belongie+al:2002. The idea is to pick out a set of points in
each of the two images, and then compute, for each point in the first
image, which point in the second image it corresponds to. From this
alignment, we then compute a transformation between the images. The
transformation gives us a measure of the distance between the images.
This distance measure is better motivated than just counting the number
of differing pixels, and it turns out that a 3–nearest neighbor
algorithm using this distance measure performs very well. Training on
only 20,000 of the 60,000 digits, and using 100 sample points per image
extracted from a Canny edge detector, a shape matching classifier
achieved 0.63% test error.

**Humans** are estimated to have an error rate of about
0.2% on this problem. This figure is somewhat suspect because humans
have not been tested as extensively as have machine learning algorithms.
On a similar data set of digits from the Postal Service, human errors
were at 2.5%.

The following figure summarizes the error rates, run time performance,
memory requirements, and amount of training time for the seven
algorithms we have discussed. It also adds another measure, the
percentage of digits that must be rejected to achieve 0.5% error. For
example, if the SVM is allowed to reject 1.8% of the inputs—that is,
pass them on for someone else to make the final judgment—then its error
rate on the remaining 98.2% of the inputs is reduced from 1.1% to 0.5%.

The following table summarizes the error rate and some of the other
characteristics of the seven techniques we have discussed.

  -------------------------------- ------ -------- ------- --------- ------ --------- -------
                                     3      300             Boosted          Virtual   Shape
                                     NN    Hidden   LeNet    LeNet    SVM      SVM     Match
  Error rate (pct.)                 2.4     1.6      0.9      0.7     1.1     0.56     0.63
  Run time (millisec/digit)         1000     10      30       50      2000     200    
  Memory requirements (Mbyte)        12     .49     .012      .21      11             
  Training time (days)               0       7       14       30       10             
  % rejected to reach 0.5% error    8.1     3.2      1.8      0.5     1.8             
  -------------------------------- ------ -------- ------- --------- ------ --------- -------

### Case study: Word senses and house prices

In a textbook we need to deal with simple, toy data to get the ideas
across: a small data set, usually in two dimensions. But in practical
applications of machine learning, the data set is usually large,
multidimensional, and messy. The data are not handed to the analyst in a
prepackaged set of $(\x, y)$ values; rather the analyst needs to go out
and acquire the right data. There is a task to be accomplished, and most
of the engineering problem is deciding what data are necessary to
accomplish the task; a smaller part is choosing and implementing an
appropriate machine learning method to process the data. shows a typical
real-world example,

[banko-figure]

comparing five learning algorithms on the task of word-sense
classification[wsd-page] (given a sentence such as “The bank folded,”
classify the word “bank” as “money-bank” or “river-bank”). The point is
that machine learning researchers have focused mainly on the vertical
direction: Can I invent a new learning algorithm that performs better
than previously published algorithms on a standard training set of 1
million words? But the graph shows there is more room for improvement in
the horizontal direction: instead of inventing a new algorithm, all I
need to do is gather 10 million words of training data; even the
*worst* algorithm at 10 million words is performing better
than the *best* algorithm at 1 million. As we gather even
more data, the curves continue to rise, dwarfing the differences between
algorithms.

Consider another problem: the task of estimating the true value of
houses that are for sale. In we showed a toy version of this problem,
doing linear regression of house size to asking price. You probably
noticed many limitations of this model. First, it is measuring the wrong
thing: we want to estimate the selling price of a house, not the asking
price. To solve this task we’ll need data on actual sales. But that
doesn’t mean we should throw away the data about asking price—we can use
it as one of the input features. Besides the size of the house, we’ll
need more information: the number of rooms, bedrooms and bathrooms;
whether the kitchen and bathrooms have been recently remodeled; the age
of the house; we’ll also need information about the lot, and the
neighborhood. But how do we define neighborhood? By zip code? What if
part of one zip code is on the “wrong” side of the highway or train
tracks, and the other part is desirable? What about the school district?
Should the *name* of the school district be a feature, or
the *average test scores*? In addition to deciding what
features to include, we will have to deal with missing data; different
areas have different customs on what data are reported, and individual
cases will always be missing some data. If the data you want are not
available, perhaps you can set up a social networking site to encourage
people to share and correct data. In the end, this process of deciding
what features to use, and how to use them, is just as important as
choosing between linear regression, decision trees, or some other form
of learning.

That said, one *does* have to pick a method (or methods)
for a problem. There is no guaranteed way to pick the best method, but
there are some rough guidelines. Decision trees are good when there are
a lot of discrete features and you believe that many of them may be
irrelevant. Nonparametric methods are good when you have a lot of data
and no prior knowledge, and when you don’t want to worry too much about
choosing just the right features (as long as there are fewer than 20 or
so). However, nonparametric methods usually give you a function $h$ that
is more expensive to run. Support vector machines are often considered
the best method to try first, provided the data set is not too large.

This chapter has concentrated on inductive learning of functions from
examples. The main points were as follows:

-   Learning takes many forms, depending on the nature of the agent, the
    component to be improved, and the available feedback.

-   If the available feedback provides the correct answer for example
    inputs, then the learning problem is called . The task is to learn a
    function $y = h(x)$. Learning a discrete-valued function is called ;
    learning a continuous function is called .

-   Inductive learning involves finding a hypothesis that agrees well
    with the examples. suggests choosing the simplest consistent
    hypothesis. The difficulty of this task depends on the chosen
    representation.

-   can represent all Boolean functions. The heuristic provides an
    efficient method for finding a simple, consistent decision tree.

-   The performance of a learning algorithm is measured by the , which
    shows the prediction accuracy on the as a function of the size.

-   When there are multiple models to choose from, can be used to select
    a model that will generalize well.

-   Sometimes not all errors are equal. A tells us how bad each error
    is; the goal is then to minimize loss over a validation set.

-   analyzes the sample complexity and computational complexity of
    inductive learning. There is a tradeoff between the expressiveness
    of the hypothesis language and the ease of learning.

-   is a widely used model. The optimal parameters of a linear
    regression model can be found by gradient descent search, or
    computed exactly.

-   A linear classifier with a hard threshold—also known as a —can be
    trained by a simple weight update rule to fit data that are . In
    other cases, the rule fails to converge.

-   replaces the perceptron’s hard threshold with a soft threshold
    defined by a logistic function. Gradient descent works well even for
    noisy data that are not linearly separable.

-   represent complex nonlinear functions with a network of
    linear-threshold units. termMultilayer feed-forward
    neural networks can represent any function, given enough units. The
    algorithm implements a gradient descent in parameter space to
    minimize the output error.

-   use all the data to make each prediction, rather than trying to
    summarize the data first with a few parameters. Examples include and
    .

-   find linear separators with to improve the generalization
    performance of the classifier. implicitly transform the input data
    into a high-dimensional space where a linear separator may exist,
    even if the original data are non-separable.

-   Ensemble methods such as often perform better than individual
    methods. In we can aggregate the opinions of experts to come
    arbitrarily close to the best expert’s performance, even when the
    distribution of the data is constantly shifting.

outlined the history of philosophical investigations into inductive
learning. William of Ockham[^16] (1280–1349), the most influential
philosopher of his century and a major contributor to medieval
epistemology, logic, and metaphysics, is credited with a statement
called “Ockham’s Razor”—in Latin, *Entia non sunt multiplicanda
praeter necessitatem*, and in English, “Entities are not to be
multiplied beyond necessity.” Unfortunately, this laudable piece of
advice is nowhere to be found in his writings in precisely these words
(although he did say “Pluralitas non est ponenda sine necessitate,” or
“plurality shouldn’t be posited without necessity”). A similar sentiment
was expressed by Aristotle in 350 b.c. in
*Physics* book I, chapter VI: “For the more limited, if
adequate, is always preferable.”

The first notable use of decision trees was in EPAM, the “Elementary
Perceiver And Memorizer” @Feigenbaum:1961, which was a simulation of
human concept learning. ID3 @Quinlan:1979 added the crucial idea of
choosing the attribute with maximum entropy; it is the basis for the
decision tree algorithm in this chapter. Information theory was
developed by Claude Shannon to aid in the study of
communication @Shannon+Weaver:1949. (Shannon also contributed one of the
earliest examples of machine learning, a mechanical mouse named Theseus
that learned to navigate through a maze by trial and error.) The
$\chi^2$ method of tree pruning was described by
Quinlan [-@Quinlan:1986]. C4.5, an industrial-strength decision tree
package, can be found in Quinlan [-@Quinlan:1993a]. An independent
tradition of decision tree learning exists in the statistical
literature. *Classification and Regression
Trees* @Breiman+al:1984, known as the “CART book,” is the
principal reference.

was first introduced by , and in a form close to what we show by and .
The regularization procedure is due to . introduce a journal issue
devoted to the problem of feature selection. and discuss the advantages
of using large amounts of data. It was Robert Mercer, a speech
researcher who said in 1985 “There is no data like more data.”
@Lyman+Varian:2003 estimate that about 5 exabytes ($5\times 10^{18}$
bytes) of data was produced in 2002, and that the rate of production is
doubling every 3 years.

Theoretical analysis of learning algorithms began with the work of on .
This approach was motivated in part by models of scientific discovery
from the philosophy of science @Popper:1962, but has been applied mainly
to the problem of learning grammars from example
sentences @Osherson+al:1986.

Whereas the identification-in-the-limit approach concentrates on
eventual convergence, the study of or , developed independently by
Solomonoff [-@Solomonoff:1964; -@Solomonoff:2009] and
Kolmogorov [-@Kolmogorov:1965], attempts to provide a formal definition
for the notion of simplicity used in Ockham’s razor. To escape the
problem that simplicity depends on the way in which information is
represented, it is proposed that simplicity be measured by the length of
the shortest program for a universal Turing machine that correctly
reproduces the observed data. Although there are many possible universal
Turing machines, and hence many possible “shortest” programs, these
programs differ in length by at most a constant that is independent of
the amount of data. This beautiful insight, which essentially shows that
*any* initial representation bias will eventually be
overcome by the data itself, is marred only by the undecidability of
computing the length of the shortest program. Approximate measures such
as the , or MDL @Rissanen:1984 [@Rissanen:2007] can be used instead and
have produced excellent results in practice. The text by Li and
Vitanyi [-@Li+Vitanyi:1993] is the best source for Kolmogorov
complexity.

The theory of PAC-learning was inaugurated by Leslie
Valiant [-@Valiant:1984]. His work stressed the importance of
computational and sample complexity. With Michael
Kearns [-@Kearns:1990], Valiant showed that several concept classes
cannot be PAC-learned tractably, even though sufficient information is
available in the examples. Some positive results were obtained for
classes such as decision lists @Rivest:1987.

An independent tradition of sample-complexity analysis has existed in
statistics, beginning with the work on  @Vapnik+Chervonenkis:1971. The
so-called provides a measure roughly analogous to, but more general
than, the $\ln|\Hyp|$ measure obtained from PAC analysis. The VC
dimension can be applied to continuous function classes, to which
standard PAC analysis does not apply. PAC-learning theory and VC theory
were first connected by the “four Germans” (none of whom actually is
German): Blumer, Ehrenfeucht, Haussler, and Warmuth [-@Blumer+al:1989].

Linear regression with squared error loss goes back to and , who were
both working on predicting orbits around the sun. The modern use of
multivariate regression for machine learning is covered in texts such as
. analyzed the differences between $L_1$ and $L_2$ regularization.

The term comes from Pierre-François Verhulst (1804–1849), a statistician
who used the curve to model population growth with limited resources, a
more realistic model than the unconstrained geometric growth proposed by
Thomas Malthus. Verhulst called it the *courbe logistique*,
because of its relation to the logarithmic curve. The term is due to
Francis Galton, nineteenth century statistician, cousin of Charles
Darwin, and initiator of the fields of meteorology, fingerprint
analysis, and statistical correlation, who used it in the sense of
regression to the mean. The term comes from Richard .

Logistic regression can be solved with gradient descent, or with the
Newton-Raphson method @Newton:1664 [@Raphson:1690]. A variant of the
Newton method called is sometimes used for large-dimensional problems;
the L stands for “limited memory,” meaning that it avoids creating the
full matrices all at once, and instead creates parts of them on the fly.
BFGS are authors’ initials @Byrd+al:1995.

Nearest-neighbors models date back at least to and have been a standard
tool in statistics and pattern recognition ever since. Within AI, they
were popularized by , who investigated methods for adapting the distance
metric to the data. developed a way to localize the metric to each point
in the space, depending on the distribution of data around that point.
introduced locality-sensitive hashing, which has revolutionized the
retrieval of similar objects in high-dimensional spaces, particularly in
computer vision. provide a recent survey of LSH and related methods.

The ideas behind kernel machines come from (who also introduced the ),
but the full development of the theory is due to Vapnik and his
colleagues @Boser+al:1992. SVMs were made practical with the
introduction of the soft-margin classifier for handling noisy data in a
paper that won the 2008 ACM Theory and Practice Award
@Cortes+Vapnik:1995, and of the Sequential Minimal Optimization (SMO)
algorithm for efficiently solving SVM problems using quadratic
programming @Platt:1999. SVMs have proven to be very popular and
effective for tasks such as text categorization @Joachims:2001,
computational genomics @Cristianini+Hahn:2007, and natural language
processing, such as the handwritten digit recognition of . As part of
this process, many new kernels have been designed that work with
strings, trees, and other nonnumerical data types. A related technique
that also uses the kernel trick to implicitly represent an exponential
feature space is the @Freund+Schapire:1999 [@Collins+Duffy:2002].
Textbooks on SVMs include and . A friendlier exposition appears in the
*AI Magazine* article by . show some of the limitations of
SVMs and other local, nonparametric methods for learning functions that
have a global structure but do not have local smoothness.

Ensemble learning is an increasingly popular technique for improving the
performance of learning algorithms.  @Breiman:1996, the first effective
method, combines hypotheses learned from multiple data sets, each
generated by subsampling the original data set. The method described in
this chapter originated with theoretical work by Schapire
[-@Schapire:1990]. The algorithm was developed by and analyzed
theoretically by . explain boosting from a statistician’s viewpoint.
Online learning is covered in a survey by and a book by . introduce the
idea of confidence-weighted online learning for classification: in
addition to keeping a weight for each parameter, they also maintain a
measure of confidence, so that a new example can have a large effect on
features that were rarely seen before (and thus had low confidence) and
a small effect on common features that have already been well-estimated.

The literature on neural networks is rather too large (approximately
150,000 papers to date) to cover in detail. Cowan and
Sharp [-@Cowan+Sharp:1988; -@Cowan+Sharp:1988a] survey the early
history, beginning with the work of McCulloch and
Pitts [-@McCulloch+Pitts:1943]. (As mentioned in , John McCarthy has
pointed to the work of Nicolas as the earliest mathematical model of
neural learning.) Norbert Wiener, a pioneer of cybernetics and control
theory @Wiener:1948, worked with McCulloch and Pitts and influenced a
number of young researchers including Marvin Minsky, who may have been
the first to develop a working neural network in hardware in
1951 \<see\>[pp. ix–x]Minsky+Papert:1988. wrote a research
report titled *Intelligent Machinery* that begins with the
sentence “I propose to investigate the question as to whether it is
possible for machinery to show intelligent behaviour” and goes on to
describe a recurrent neural network architecture he called “B-type
unorganized machines” and an approach to training them. Unfortunately,
the report went unpublished until 1969, and was all but ignored until
recently.

Frank Rosenblatt [-@Rosenblatt:1957] invented the modern “perceptron”
and proved the perceptron convergence theorem [-@Rosenblatt:1960],
although it had been foreshadowed by purely mathematical work outside
the context of neural networks @Agmon:1954 [@Motzkin+Schoenberg:1954].
Some early work was also done on multilayer networks, including
 @Gamba+al:1961 and  @Widrow:1962. *Learning Machines*
@Nilsson:1965 covers much of this early work and more. The subsequent
demise of early perceptron research efforts was hastened (or, the
authors later claimed, merely explained) by the book
*Perceptrons* @Minsky+Papert:1969, which lamented the
field’s lack of mathematical rigor. The book pointed out that
single-layer perceptrons could represent only linearly separable
concepts and noted the lack of effective learning algorithms for
multilayer networks.

The papers in @Hinton+Anderson:1981, based on a conference in San Diego
in 1979, can be regarded as marking a renaissance of connectionism. The
two-volume “PDP” (Parallel Distributed Processing) anthology
@Rumelhart+al:1986 and a short article in *Nature*
@Rumelhart+al:1986a attracted a great deal of attention—indeed, the
number of papers on “neural networks” multiplied by a factor of 200
between 1980–84 and 1990–94. The analysis of neural networks using the
physical theory of magnetic spin glasses @Amit+al:1985 tightened the
links between statistical mechanics and neural network theory—providing
not only useful mathematical insights but also
*respectability*. The back-propagation technique had been
invented quite early @Bryson+Ho:1969 but it was rediscovered several
times @Werbos:1974 [@Parker:1985].

The probabilistic interpretation of neural networks has several sources,
including and . The role of the sigmoid function is discussed by .
Bayesian parameter learning for neural networks was proposed by and is
explored further by . The capacity of neural networks to represent
functions was investigated by Cybenko [-@Cybenko:1988; -@Cybenko:1989],
who showed that two hidden layers are enough to represent any function
and a single layer is enough to represent any *continuous*
function. The “optimal brain damage” method for removing useless
connections is by LeCun et al. [-@LeCun+al:1989], and Sietsma and Dow
[-@Sietsma+Dow:1988] show how to remove useless units. The tiling
algorithm for growing larger structures is due to Mézard and Nadal
[-@Mezard+Nadal:1989]. survey a number of algorithms for handwritten
digit recognition. Improved error rates since then were reported by for
shape matching and for virtual support vectors. At the time of writing,
the best test error rate reported is 0.39% by using a convolutional
neural network.

The complexity of neural network learning has been investigated by
researchers in computational learning theory. Early computational
results were obtained by Judd [-@Judd:1990], who showed that the general
problem of finding a set of weights consistent with a set of examples is
, even under very restrictive assumptions. Some of the first sample
complexity results were obtained by , who showed that the number of
examples required for effective learning grows as roughly $W\log W$,
where $W$ is the number of weights.[^17] Since then, a much more
sophisticated theory has been developed @Anthony+Bartlett:1999,
including the important result that the representational capacity of a
network depends on the *size* of the weights as well as on
their number, a result that should not be surprising in the light of our
discussion of regularization.

The most popular kind of neural network that we did not cover is the ,
or RBF, network. A radial basis function combines a weighted collection
of kernels (usually Gaussians, of course) to do function approximation.
RBF networks can be trained in two phases: first, an unsupervised
clustering approach is used to train the parameters of the Gaussians—the
means and variances—are trained, as in . In the second phase, the
relative weights of the Gaussians are determined. This is a system of
linear equations, which we know how to solve directly. Thus, both phases
of RBF training have a nice benefit: the first phase is unsupervised,
and thus does not require labeled training data, and the second phase,
although supervised, is efficient. See for more details.

, in which units are linked in cycles, were mentioned in the chapter but
not explored in depth. @Hopfield:1982 are probably the best-understood
class of recurrent networks. They use *bidirectional*
connections with *symmetric* weights (i.e.,
$\w{i}{j} = \w{j}{i}$), all of the units are both input and output
units, the activation function $g$ is the sign function, and the
activation levels can only be $\pm 1$. A Hopfield network functions as
an : after the network trains on a set of examples, a new stimulus will
cause it to settle into an activation pattern corresponding to the
example in the training set that *most closely resembles*
the new stimulus. For example, if the training set consists of a set of
photographs, and the new stimulus is a small piece of one of the
photographs, then the network activation levels will reproduce the
photograph from which the piece was taken. Notice that the original
photographs are not stored separately in the network; each weight is a
partial encoding of all the photographs. One of the most interesting
theoretical results is that Hopfield networks can reliably store up to
${0.138} N$ training examples, where $N$ is the number of units in the
network.

@Hinton+Sejnowski:1983 [@Hinton+Sejnowski:1986] also use symmetric
weights, but include hidden units. In addition, they use a
*stochastic* activation function, such that the probability
of the output being 1 is some function of the total weighted input.
Boltzmann machines therefore undergo state transitions that resemble a
simulated annealing search (see ) for the configuration that best
approximates the training set. It turns out that Boltzmann machines are
very closely related to a special case of Bayesian networks evaluated
with a stochastic simulation algorithm. (See .)

For neural nets, , , and are the leading texts. The field of
computational neuroscience is covered by .

The approach taken in this chapter was influenced by the excellent
course notes of David Cohn, Tom Mitchell, Andrew Moore, and Andrew Ng.
There are several top-notch textbooks in Machine Learning @Mitchell:1997
[@Bishop:2007] and in the closely allied and overlapping fields of
pattern recognition @Ripley:1996 [@Duda+al:2001], statistics
@Wasserman:2004 [@Hastie+al:2009], data mining @Hand+al:2001
[@Witten+Frank:2005], computational learning theory
@Kearns+Vazirani:1994 [@Vapnik:1998] and information theory
@Shannon+Weaver:1949 [@MacKay:2002; @Cover+Thomas:2006]. Other books
concentrate on implementations @Segaran:2007 [@Marsland:2009] and
comparisons of algorithms @Michie+al:1994. Current research in machine
learning is published in the annual proceedings of the International
Conference on Machine Learning (ICML) and the conference on Neural
Information Processing Systems (NIPS), in *Machine
Learning* and the *Journal of Machine Learning
Research*, and in mainstream AI journals.

[infant-language-exercise]Consider the problem faced by an infant
learning to speak and understand a language. Explain how this process
fits into the general learning model. Describe the percepts and actions
of the infant, and the types of learning the infant must do. Describe
the subfunctions the infant is trying to learn in terms of inputs and
outputs, and available example data.

Repeat for the case of learning to play tennis (or some other sport with
which you are familiar). Is this supervised learning or reinforcement
learning?

Draw a decision tree for the problem of deciding whether to move forward
at a road intersection, given that the light has just turned green.

We never test the same attribute twice along one path in a decision
tree. Why not?

Suppose we generate a training set from a decision tree and then apply
decision-tree learning to that training set. Is it the case that the
learning algorithm will eventually return the correct tree as the
training-set size goes to infinity? Why or why not?

[leaf-classification-exercise]In the recursive construction of decision
trees, it sometimes happens that a mixed set of positive and negative
examples remains at a leaf node, even after all the attributes have been
used. Suppose that we have $p$ positive examples and $n$ negative
examples.

1.  Show that the solution used by , which picks the majority
    classification, minimizes the absolute error over the set of
    examples at the leaf.

2.  Show that the $p/(p+n)$ minimizes the sum of squared errors.

[nonnegative-gain-exercise]Suppose that an attribute splits the set of
examples $E$ into subsets $E_{\dtvalue}$ and that each subset has
$p_{\dtvalue}$ positive examples and $n_{\dtvalue}$ negative examples.
Show that the attribute has strictly positive information gain unless
the ratio $p_{\dtvalue}/(p_{\dtvalue}+n_{\dtvalue})$ is the same for all
${\dtvalue}$.

Consider the following data set comprised of three binary input
attributes ($A_1, A_2$, and $A_3$) and one binary output:

   **Example**   $A_1$   $A_2$   $A_3$   Output $y$
  -------------------------- ------- ------- ------- ------------
            $\x_1$              1       0       0         0
            $\x_2$              1       0       1         0
            $\x_3$              0       1       0         0
            $\x_4$              1       1       1         1
            $\x_5$              1       1       0         1

Use the algorithm in () to learn a decision tree for these data. Show
the computations made to determine the attribute to split at each node.

Construct a data set (set of examples with attributes and
classifications) that would cause the decision-tree learning algorithm
to find a non-minimal-sized tree. Show the tree constructed by the
algorithm and the minimal-sized tree that you can generate by hand.

A decision *graph* is a generalization of a decision tree
that allows nodes (i.e., attributes used for splits) to have multiple
parents, rather than just a single parent. The resulting graph must
still be acyclic. Now, consider the XOR function of *three*
binary input attributes, which produces the value 1 if and only if an
odd number of the three input attributes has value 1.

1.  Draw a minimal-sized decision *tree* for the
    three-input XOR function.

2.  Draw a minimal-sized decision *graph* for the
    three-input XOR function.

[pruning-DTL-exercise]This exercise considers $\chi^2$ pruning of
decision trees ().

1.  Create a data set with two input attributes, such that the
    information gain at the root of the tree for both attributes is
    zero, but there is a decision tree of depth 2 that is consistent
    with all the data. What would $\chi^2$ pruning do on this data set
    if applied bottom up? If applied top down?

2.  Modify to include $\chi^2$-pruning. You might wish to consult
    Quinlan [-@Quinlan:1986] or for details.

[missing-value-DTL-exercise]The standard algorithm described in the
chapter does not handle cases in which some examples have missing
attribute values.

1.  First, we need to find a way to classify such examples, given a
    decision tree that includes tests on the attributes for which values
    can be missing. Suppose that an example $\x$ has a missing value for
    attribute $A$ and that the decision tree tests for $A$ at a node
    that $\x$ reaches. One way to handle this case is to pretend that
    the example has *all* possible values for the
    attribute, but to weight each value according to its frequency among
    all of the examples that reach that node in the decision tree. The
    classification algorithm should follow all branches at any node for
    which a value is missing and should multiply the weights along each
    path. Write a modified classification algorithm for decision trees
    that has this behavior.

2.  Now modify the information-gain calculation so that in any given
    collection of examples $C$ at a given node in the tree during the
    construction process, the examples with missing values for any of
    the remaining attributes are given “as-if” values according to the
    frequencies of those values in the set $C$.

[gain-ratio-DTL-exercise]In , we noted that attributes with many
different possible values can cause problems with the gain measure. Such
attributes tend to split the examples into numerous small classes or
even singleton classes, thereby appearing to be highly relevant
according to the gain measure. The criterion selects attributes
according to the ratio between their gain and their intrinsic
information content—that is, the amount of information contained in the
answer to the question, “What is the value of this attribute?” The
gain-ratio criterion therefore tries to measure how efficiently an
attribute provides information on the correct classification of an
example. Write a mathematical expression for the information content of
an attribute, and implement the gain ratio criterion in .

Suppose you are running a learning experiment on a new algorithm for
Boolean classification. You have a data set consisting of 100 positive
and 100 negative examples. You plan to use leave-one-out
cross-validation and compare your algorithm to a baseline function, a
simple majority classifier. (A majority classifier is given a set of
training data and then always outputs the class that is in the majority
in the training set, regardless of the input.) You expect the majority
classifier to score about 50% on leave-one-out cross-validation, but to
your surprise, it scores zero every time. Can you explain why?

Suppose that a learning algorithm is trying to find a consistent
hypothesis when the classifications of examples are actually random.
There are $n$ Boolean attributes, and examples are drawn uniformly from
the set of $2^n$ possible examples. Calculate the number of examples
required before the probability of finding a contradiction in the data
reaches 0.5.

Construct a *decision list* to classify the data below.
Select tests to be as small as possible (in terms of attributes),
breaking ties among tests with the same number of attributes by
selecting the one that classifies the greatest number of examples
correctly. If multiple tests have the same number of attributes and
classify the same number of examples, then break the tie using
attributes with lower index numbers (e.g., select $A_1$ over $A_2$).

            $A_1$   $A_2$   $A_3$   $A_4$   $y$
  -------- ------- ------- ------- ------- -----
   $\x_1$     1       0       0       0      1
   $\x_2$     1       0       1       1      1
   $\x_3$     0       1       0       0      1
   $\x_4$     0       1       1       0      0
   $\x_5$     1       1       0       1      1
   $\x_6$     0       1       0       1      0
   $\x_7$     0       0       1       1      1
   $\x_8$     0       0       1       0      0

Prove that a decision list can represent the same function as a decision
tree while using at most as many rules as there are leaves in the
decision tree for that function. Give an example of a function
represented by a decision list using strictly fewer rules than the
number of leaves in a minimal-sized decision tree for that same
function.

[DL-expressivity-exercise]This exercise concerns the expressiveness of
decision lists ().

1.  Show that decision lists can represent any Boolean function, if the
    size of the tests is not limited.

2.  Show that if the tests can contain at most $k$ literals each, then
    decision lists can represent any function that can be represented by
    a decision tree of depth $k$.

[knn-mean-mode] Suppose a $7$-nearest-neighbors regression search
returns $ \{7, 6, 8, 4, 7, 11, 100\} $ as the 7 nearest $y$ values for a
given $x$ value. What is the value of $\hat{y}$ that minimizes the $L_1$
loss function on this data? There is a common name in statistics for
this value as a function of the $y$ values; what is it? Answer the same
two questions for the $L_2$ loss function.

[knn-mean-mode] Suppose a $7$-nearest-neighbors regression search
returns $ \{4, 2, 8, 4, 9, 11, 100\} $ as the 7 nearest $y$ values for a
given $x$ value. What is the value of $\hat{y}$ that minimizes the $L_1$
loss function on this data? There is a common name in statistics for
this value as a function of the $y$ values; what is it? Answer the same
two questions for the $L_2$ loss function.

[svm-ellipse-exercise] showed how a circle at the origin can be linearly
separated by mapping from the features $(x_1, x_2)$ to the two
dimensions $(x_1^2, x_2^2)$. But what if the circle is not located at
the origin? What if it is an ellipse, not a circle? The general equation
for a circle (and hence the decision boundary) is $(x_1-a)^2 +
(x_2-b)^2 - r^2\eq 0$, and the general equation for an ellipse is
$c(x_1-a)^2 + d(x_2-b)^2 - 1 \eq 0$.

1.  Expand out the equation for the circle and show what the weights
    $w_i$ would be for the decision boundary in the four-dimensional
    feature space $(x_1, x_2, x_1^2, x_2^2)$. Explain why this means
    that any circle is linearly separable in this space.

2.  Do the same for ellipses in the five-dimensional feature space
    $(x_1, x_2, x_1^2, x_2^2, x_1 x_2)$.

[svm-exercise] Construct a support vector machine that computes the
xor function. Use values of +1 and –1 (instead of 1 and 0)
for both inputs and outputs, so that an example looks like $([-1, 1],
1)$ or $([-1, -1], -1)$. Map the input $[x_1,x_2]$ into a space
consisting of $x_1$ and $x_1\,x_2$. Draw the four input points in this
space, and the maximal margin separator. What is the margin? Now draw
the separating line back in the original Euclidean input space.

[ensemble-error-exercise] Consider an ensemble learning algorithm that
uses simple majority voting among ${\Ecount}$ learned hypotheses.
Suppose that each hypothesis has error $\epsilon$ and that the errors
made by each hypothesis are independent of the others’. Calculate a
formula for the error of the ensemble algorithm in terms of ${\Ecount}$
and $\epsilon$, and evaluate it for the cases where ${\Ecount}\eq 5$,
10, and 20 and $\epsilon\eq {0.1}$, 0.2, and 0.4. If the independence
assumption is removed, is it possible for the ensemble error to be
*worse* than $\epsilon$?

Construct by hand a neural network that computes the xor
function of two inputs. Make sure to specify what sort of units you are
using.

A simple perceptron cannot represent xor (or, generally,
the parity function of its inputs). Describe what happens to the weights
of a four-input, hard-threshold perceptron, beginning with all weights
set to 0.1, as examples of the parity function arrive.

[linear-separability-exercise] Recall from that there are
$2^{2^{\Acount}}$ distinct Boolean functions of ${\Acount}$ inputs. How
many of these are representable by a threshold perceptron?

Consider the following set of examples, each with six inputs and one
target output:

  ------- --- --- --- --- --- --- --- --- --- --- --- --- --- ---
  $I_1$    1   1   1   1   1   1   1   0   0   0   0   0   0   0
  $I_2$    0   0   0   1   1   0   0   1   1   0   1   0   1   1
  $I_3$    1   1   1   0   1   0   0   1   1   0   0   0   1   1
  $I_4$    0   1   0   0   1   0   0   1   0   1   1   1   0   1
  $I_5$    0   0   1   1   0   1   1   0   1   1   0   0   1   0
  $I_6$    0   0   0   1   0   1   0   1   1   0   1   1   1   0
  $T$      1   1   1   1   1   1   0   1   0   0   0   0   0   0
  ------- --- --- --- --- --- --- --- --- --- --- --- --- --- ---

1.  Run the perceptron learning rule on these data and show the final
    weights.

2.  Run the decision tree learning rule, and show the resulting decision
    tree.

3.  Comment on your results.

[perceptron-ML-gradient-exercise] () noted that the output of the
logistic function could be interpreted as a *probability*
$p$ assigned by the model to the proposition that $f(\x)\eq 1$; the
probability that $f(\x)\eq 0$ is therefore $1-p$. Write down the
probability $p$ as a function of $\x$ and calculate the derivative of
$\log p$ with respect to each weight $w_i$. Repeat the process for
$\log (1-p)$. These calculations give a learning rule for minimizing the
negative-log-likelihood loss function for a probabilistic hypothesis.
Comment on any resemblance to other learning rules in the chapter.

[linear-nn-exercise]Suppose you had a neural network with linear
activation functions. That is, for each unit the output is some constant
$c$ times the weighted sum of the inputs.

1.  Assume that the network has one hidden layer. For a given assignment
    to the weights $\bw$, write down equations for the value of the
    units in the output layer as a function of $\bw$ and the input layer
    $\x$, without any explicit mention of the output of the hidden
    layer. Show that there is a network with no hidden units that
    computes the same function.

2.  Repeat the calculation in part (a), but this time do it for a
    network with any number of hidden layers.

3.  Suppose a network with one hidden layer and linear activation
    functions has $n$ input and output nodes and $h$ hidden nodes. What
    effect does the transformation in part (a) to a network with no
    hidden layers have on the total number of weights? Discuss in
    particular the case $h \ll n$.

Implement a data structure for layered, feed-forward neural networks,
remembering to provide the information needed for both forward
evaluation and backward propagation. Using this data structure, write a
function that takes an example and a network and computes the
appropriate output values.

Suppose that a training set contains only a single example, repeated 100
times. In 80 of the 100 cases, the single output value is 1; in the
other 20, it is 0. What will a back-propagation network predict for this
example, assuming that it has been trained and reaches a global optimum?
(*Hint:* to find the global optimum, differentiate the
error function and set it to zero.)

The neural network whose learning performance is measured in has four
hidden nodes. This number was chosen somewhat arbitrarily. Use a
cross-validation method to find the best number of hidden nodes.

[embedding-separability-exercise] Consider the problem of separating $N$
data points into positive and negative examples using a linear
separator. Clearly, this can always be done for $N\eq2$ points on a line
of dimension $d\eq1$, regardless of how the points are labeled or where
they are located (unless the points are in the same place).

1.  Show that it can always be done for $N\eq3$ points on a plane of
    dimension $d\eq2$, unless they are collinear.

2.  Show that it cannot always be done for $N\eq4$ points on a plane of
    dimension $d\eq2$.

3.  Show that it can always be done for $N\eq4$ points in a space of
    dimension $d\eq3$, unless they are coplanar.

4.  Show that it cannot always be done for $N\eq5$ points in a space of
    dimension $d\eq3$.

5.  The ambitious student may wish to prove that $N$ points in general
    position (but not $N+1$) are linearly separable in a space of
    dimension $N-1$.

[^1]: A note on notation: except where noted, we will use $j$ to index
    the $\Ncount$ examples; $x_j$ will always be the input and $y_j$ the
    output. In cases where the input is specifically a vector of
    attribute values (beginning with ), we will use $\x_j$ for the $j$th
    example and we will use $i$ to index the $\Acount$ attributes of
    each example. The elements of $\x_j$ are written
    $x_{j,1}, x_{j,2}, \ldots, x_{j,{\Acount}}$.

[^2]: The gain will be strictly positive except for the unlikely case
    where all the proportions are *exactly* the same. (See
    .)

[^3]: Gauss showed that if the $y_j$ values have normally distributed
    noise, then the most likely values of $w_1$ and $w_0$ are obtained
    by minimizing the sum of the squares of the errors.

[^4]: With some caveats: the $L_2$ loss function is appropriate when
    there is normally-distributed noise that is independent of $x$; all
    results rely on the stationarity assumption; etc.

[^5]: The reader may wish to consult for a brief summary of linear
    algebra.

[^6]: It is perhaps confusing that $L_1$ and $L_2$ are used for both
    loss functions and regularization functions. They need not be used
    in pairs: you could use $L_2$ loss with $L_1$ regularization, or
    vice versa.

[^7]: Technically, we require that
    $\sum_{t\eq 1}^{\infty}\alpha(t) \eq\infty$ and
    $\sum_{t\eq 1}^{\infty}\alpha^2(t) <\infty$. The decay
    $\alpha(t)\eq O(1/t)$ satisfies these conditions.

[^8]: A note on notation: for this section, we are forced to suspend our
    usual conventions. Input attributes are still indexed by $i$ , so
    that an “external” activation $a_{\nninput}$ is given by input
    $x_{\nninput}$; but index $j$ will refer to internal units rather
    than examples. Throughout this section, the mathematical derivations
    concern a single generic example $\x$, omitting the usual summations
    over examples to obtain results for the whole data set.

[^9]: The proof is complex, but the main point is that the required
    number of hidden units grows exponentially with the number of
    inputs. For example, $2^n/n$ hidden units are needed to encode all
    Boolean functions of $n$ inputs.

[^10]: It has been observed that very large networks *do*
    generalize well *as long as the weights are kept
    small*. This restriction keeps the activation values in the
    *linear* region of the sigmoid function $g(x)$ where
    $x$ is close to zero. This, in turn, means that the network behaves
    like a linear function () with far fewer parameters.

[^11]: The reader may notice that we could have used just $f_1$ and
    $f_2$, but the 3D mapping illustrates the idea better.

[^12]: This usage of “kernel function” is slightly different from the
    kernels in locally weighted regression. Some SVM kernels are
    distance metrics, but not all are.

[^13]: Here, “reasonable” means that the matrix
    $\mbf{K}_{jk}\eq K(\x_j,\x_k)$ is positive definite.

[^14]: For learning algorithms in which this is not possible, one can
    instead create a where the $j$th example appears $w_j$ times, using
    randomization to handle fractional weights.

[^15]: See @Blum:1996 for the proof.

[^16]: The name is often misspelled as “Occam,” perhaps from the French
    rendering, “Guillaume d’Occam.”

[^17]: This approximately confirmed “Uncle Bernie’s rule.” The rule was
    named after Bernie Widrow, who recommended using roughly ten times
    as many examples as weights.
Constraint Satisfaction Problems {#csp-chapter}
================================

Chapters [search-chapter] and [advanced-search-chapter] explored the
idea that problems can be solved by searching in a space of . These
states can be evaluated by domain-specific heuristics and tested to see
whether they are goal states. From the point of view of the search
algorithm, however, each state is atomic, or indivisible—a black box
with no internal structure.

This chapter describes a way to solve a wide variety of problems more
efficiently. We use a for each state: a set of variables, each of which
has a value. A problem is solved when each variable has a value that
satisfies all the constraints on the variable. A problem described this
way is called a , or CSP.

CSP search algorithms take advantage of the structure of states and use
*general-purpose* rather than
*problem-specific* heuristics to enable the solution of
complex problems. The main idea is to eliminate large portions of the
search space all at once by identifying variable/value combinations that
violate the constraints.

Defining Constraint Satisfaction Problems {#csp-section}
-----------------------------------------

A constraint satisfaction problem consists of three components, $X, D,$
and $C$:

$X$ is a set of variables, $\{X_1,\ldots,X_n\}$.

$D$ is a set of domains, $\{D_1,\ldots,D_n\}$, one for each variable.

$C$ is a set of constraints that specify allowable combinations of
values.

Each domain $D_i$ consists of a set of allowable values,
$\{v_1, \ldots, v_k\}$ for variable $X_i$. Each constraint $C_i$
consists of a pair $\constraint{{scope}}{{rel}}$, where ${scope}$
is a tuple of variables that participate in the constraint and ${rel}$
is a relation that defines the values that those variables can take on.
A relation can be represented as an explicit list of all tuples of
values that satisfy the constraint, or as an abstract relation that
supports two operations: testing if a tuple is a member of the relation
and enumerating the members of the relation. For example, if $X_1$ and
$X_2$ both have the domain {A,B}, then the constraint saying the two
variables must have different values can be written as
$\constraint{(X_1, X_2)}{[(A, B), (B, A)]}$ or as
$\constraint{(X_1, X_2)}{X_1 \neq X_2}$.

To solve a CSP, we need to define a state space and the notion of a
solution. Each state in a CSP is defined by an of values to some or all
of the variables, $\{X_i=v_i, X_j=v_j, \ldots\}$. An assignment that
does not violate any constraints is called a or legal assignment. A is
one in which every variable is assigned, and a to a CSP is a consistent,
complete assignment. A is one that assigns values to only some of the
variables.

### Example problem: Map coloring {#map-coloring-section}

Suppose that, having tired of , we are looking at a map of showing each
of its states and territories ((a)). We are given the task of coloring
each region either red, green, or blue in such a way that no neighboring
regions have the same color. To formulate this as a CSP, we define the
variables to be the regions
$$X =  \{{WA}, {NT}, Q, {NSW}, V, {SA}, T\}\ .$$ The domain of
each variable is the set $D_i = \{{red},{green},{blue}\}$. The
constraints require neighboring regions to have distinct colors. Since
there are nine places where regions border, there are nine constraints:
$$\begin{array}{lll}
C & = & \{{SA}\neq{WA}, {SA}\neq{NT}, {SA}\neq Q, {SA}\neq{NSW}, {SA}\neq V, \\
  &   &   {WA}\neq{NT}, {NT}\neq Q, Q \neq {NSW}, {NSW}\neq V\}\ .
\end{array}$$ Here we are using abbreviations; ${SA}\neq{WA}$ is a
shortcut for $\constraint{\varlist{{SA}, {WA}}}{{SA}\neq{WA}}$,
where ${{SA}\neq{WA}}$ can be fully enumerated in turn as
$$\{({red},{green}),({red},{blue}),({green},{red}),({green},{blue}),({blue},{red}),({blue},{green})\}\ .$$
There are many possible solutions to this problem, such as
$$\{{WA}\eq {red},{NT}\eq {green},Q\eq {red},{NSW}\eq {green},V\eq {red},{SA}\eq
  {blue},T\eq {red}\ \}.$$ It can be helpful to visualize a CSP as a
, as shown in (b). The nodes of the graph correspond to variables of the
problem, and a link connects any two variables that participate in a
constraint.

[australia-figure]

Why formulate a problem as a CSP? One reason is that the CSPs yield a
natural representation for a wide variety of problems; if you already
have a CSP-solving system, it is often easier to solve a problem using
it than to design a custom solution using another search technique. In
addition, CSP solvers can be faster than state-space searchers because
the CSP solver can quickly eliminate large swatches of the search space.
For example, once we have chosen $\{{SA}\eq {blue}\}$ in the
Australia problem, we can conclude that none of the five neighboring
variables can take on the value ${blue}$. Without taking advantage of
constraint propagation, a search procedure would have to consider
$3^5\eq 243$ assignments for the five neighboring variables; with
constraint propagation we never have to consider ${blue}$ as a value,
so we have only $2^5\eq
32$ assignments to look at, a reduction of 87%.

In regular state-space search we can only ask: is this specific state a
goal? No? What about this one? With CSPs, once we find out that a
partial assignment is not a solution, we can immediately discard further
refinements of the partial assignment. Furthermore, we can see
*why* the assignment is not a solution—we see which
variables violate a constraint—so we can focus attention on the
variables that matter. As a result, many problems that are intractable
for regular state-space search can be solved quickly when formulated as
a CSP.

### Example problem: Job-shop scheduling {#csp-job-shop-scheduling-section}

Factories have the problem of scheduling a day’s worth of jobs, subject
to various constraints. In practice, many of these problems are solved
with CSP techniques. Consider the problem of scheduling the assembly of
a car. The whole job is composed of tasks, and we can model each task as
a variable, where the value of each variable is the time that the task
starts, expressed as an integer number of minutes. Constraints can
assert that one task must occur before another—for example, a wheel must
be installed before the hubcap is put on—and that only so many tasks can
go on at once. Constraints can also specify that a task takes a certain
amount of time to complete.

We consider a small part of the car assembly, consisting of 15 tasks:
install axles (front and back), affix all four wheels (right and left,
front and back), tighten nuts for each wheel, affix hubcaps, and inspect
the final assembly. We can represent the tasks with 15 variables:
$$\begin{array}{lll}
X &=& \{{Axle}_F, {Axle}_B, {Wheel}_{RF}, {Wheel}_{LF}, {Wheel}_{RB}, {Wheel}_{LB}, {Nuts}_{RF}, \\
   && {Nuts}_{LF}, {Nuts}_{RB}, {Nuts}_{LB}, {Cap}_{RF}, {Cap}_{LF}, {Cap}_{RB}, {Cap}_{LB}, {Inspect}\}\ .
 \end{array}$$ The value of each variable is the time that the task
starts. Next we represent between individual tasks. Whenever a task
$T_1$ must occur before task $T_2$, and task $T_1$ takes duration $d_1$
to complete, we add an arithmetic constraint of the form
$$T_1 + d_1 \le T_2 \ .$$ In our example, the axles have to be in place
before the wheels are put on, and it takes 10 minutes to install an
axle, so we write $$\begin{array}{l}
{Axle}_F + 10 \le {Wheel}_{RF}; \quad {Axle}_F + 10 \le {Wheel}_{LF}; \\
{Axle}_B + 10 \le {Wheel}_{RB}; \quad {Axle}_B + 10 \le {Wheel}_{LB} \ . 
\end{array}$$ Next we say that, for each wheel, we must affix the wheel
(which takes 1 minute), then tighten the nuts (2 minutes), and finally
attach the hubcap (1 minute, but not represented yet):
$$\begin{array}{l}
 {Wheel}_{RF} + 1 \le {Nuts}_{RF}; \quad {Nuts}_{RF} + 2 \le {Cap}_{RF}; \\
 {Wheel}_{LF} + 1 \le {Nuts}_{LF}; \quad {Nuts}_{LF} + 2 \le {Cap}_{LF}; \\
 {Wheel}_{RB} + 1 \le {Nuts}_{RB}; \quad {Nuts}_{RB} + 2 \le {Cap}_{RB}; \\
 {Wheel}_{LB} + 1 \le {Nuts}_{LB}; \quad {Nuts}_{LB} + 2 \le {Cap}_{LB} \ .
\end{array}$$ Suppose we have four workers to install wheels, but they
have to share one tool that helps put the axle in place. We need a to
say that ${Axle}_F$ and ${Axle}_B$ must not overlap in time; either
one comes first or the other does:
$$({Axle}_{F} + 10 \le {Axle}_{B}) \mbox{\quad{\bf or}\quad} 
  ({Axle}_{B} + 10 \le {Axle}_{F}) \ .$$ This looks like a more
complicated constraint, combining arithmetic and logic. But it still
reduces to a set of pairs of values that ${Axle}_{F}$ and
${Axle}_{F}$ can take on.

We also need to assert that the inspection comes last and takes 3
minutes. For every variable except ${Inspect}$ we add a constraint of
the form $X + d_X \le {Inspect}$. Finally, suppose there is a
requirement to get the whole assembly done in 30 minutes. We can achieve
that by limiting the domain of all variables:
$$D_i = \{1,2,3,\ldots,27\} \ .$$ This particular problem is trivial to
solve, but CSPs have been applied to job-shop scheduling problems like
this with thousands of variables. In some cases, there are complicated
constraints that are difficult to specify in the CSP formalism, and more
advanced planning techniques are used, as discussed in .

### Variations on the CSP formalism

The simplest kind of CSP involves variables that have , . Map-coloring
problems and scheduling with time limits are both of this kind. The
8-queens problem described in can also be viewed as a finite-domain CSP,
where the variables $Q_1,\ldots,Q_8$ are the positions of each queen in
columns $1,\ldots,8$ and each variable has the domain
$D_i = \{1,2,3,4,5,6,7,8\}$.

A discrete domain can be , such as the set of integers or strings. (If
we didn’t put a deadline on the job-scheduling problem, there would be
an infinite number of start times for each variable.) With infinite
domains, it is no longer possible to describe constraints by enumerating
all allowed combinations of values. Instead, a must be used that
understands constraints such as $T_1 + d_1 \le T_2$ directly, without
enumerating the set of pairs of allowable values for $(T_1, T_2)$.
Special solution algorithms (which we do not discuss here) exist for on
integer variables—that is, constraints, such as the one just given, in
which each variable appears only in linear form. It can be shown that no
algorithm exists for solving general on integer variables.

Constraint satisfaction problems with are common in the real world and
are widely studied in the field of operations research. For example, the
scheduling of experiments on the requires very precise timing of
observations; the start and finish of each observation and maneuver are
continuous-valued variables that must obey a variety of astronomical,
precedence, and power constraints. The best-known category of
continuous-domain CSPs is that of problems, where constraints must be
linear equalities or inequalities. Linear programming problems can be
solved in time polynomial in the number of variables. Problems with
different types of constraints and objective functions have also been
studied—quadratic programming, second-order conic programming, and so
on.

In addition to examining the types of variables that can appear in CSPs,
it is useful to look at the types of constraints. The simplest type is
the , which restricts the value of a single variable. For example, in
the map-coloring problem it could be the case that South Australians
won’t tolerate the color green; we can express that with the unary
constraint $\constraint{\varlist{{{SA}}}}{{SA}\neq {green}}$

A relates two variables. For example, ${SA}\neq {NSW}$ is a binary
constraint. A binary CSP is one with only binary constraints; it can be
represented as a constraint graph, as in (b).

We can also describe higher-order constraints, such as asserting that
the value of $Y$ is between $X$ and $Z$, with the ternary constraint
${Between}(X, Y, Z)$.

A constraint involving an arbitrary number of variables is called a .
(The name is traditional but confusing because it need not involve
*all* the variables in a problem). One of the most common
global constraints is $\v{Alldiff}$, which says that all of the
variables involved in the constraint must have different values. In
Sudoku problems (see ), all variables in a row or column must satisfy an
$\v{Alldiff}$ constraint. Another example is provided by puzzles. (See
(a).) Each letter in a cryptarithmetic puzzle represents a different
digit. For the case in (a), this would be represented as the global
constraint $\v{Alldiff}(F,T,U,W,R,O)$. The addition constraints on the
four columns of the puzzle can be written as the following $n$-ary
constraints:

O + O = R + 10C~10~\
C~10~ + W + W = U + 10C~100~\
C~100~ + T + T = O + 10C~1000~\
C~1000~ = F ,

where $C_{10}$, $C_{100}$, and $C_{1000}$ are auxiliary variables
representing the digit carried over into the tens, hundreds, or
thousands column. These constraints can be represented in a , such as
the one shown in (b). A hypergraph consists of ordinary nodes (the
circles in the figure) and hypernodes (the squares), which represent
$n$-ary constraints.

Alternatively, as asks you to prove, every finite-domain constraint can
be reduced to a set of binary constraints if enough auxiliary variables
are introduced, so we could transform any CSP into one with only binary
constraints; this makes the algorithms simpler. Another way to convert
an $n$-ary CSP to a binary one is the transformation: create a new graph
in which there will be one variable for each constraint in the original
graph, and one binary constraint for each pair of constraints in the
original graph that share variables. For example, if the original graph
has variables $\{X,Y,Z\}$ and constraints $\constraint{(X,Y,Z)}{C_1}$
and $\constraint{(X,Y)}{C_2}$ then the dual graph would have variables
$\{C_1,C_2\}$ with the binary constraint $\constraint{(X,Y)}{R_1}$,
where $(X,Y)$ are the shared variables and $R_1$ is a new relation that
defines the constraint between the shared variables, as specified by the
original $C_1$ and $C_2$.

There are however two reasons why we might prefer a global constraint
such as $\v{Alldiff}$ rather than a set of binary constraints. First, it
is easier and less error-prone to write the problem description using
$\v{Alldiff}$. Second, it is possible to design special-purpose
inference algorithms for global constraints that are not available for a
set of more primitive constraints. We describe these inference
algorithms in .

[cryptarithmetic-figure]

The constraints we have described so far have all been absolute
constraints, violation of which rules out a potential solution. Many
real-world CSPs include indicating which solutions are preferred. For
example, in a university class-scheduling problem there are absolute
constraints that no professor can teach two classes at the same time.
But we also may allow preference constraints: Prof. R might prefer
teaching in the morning, whereas Prof. N prefers teaching in the
afternoon. A schedule that has Prof. R teaching at 2 p.m. would still be
an allowable solution (unless Prof. R happens to be the department
chair) but would not be an optimal one. Preference constraints can often
be encoded as costs on individual variable assignments—for example,
assigning an afternoon slot for Prof. R costs 2 points against the
overall objective function, whereas a morning slot costs 1. With this
formulation, CSPs with preferences can be solved with optimization
search methods, either path-based or local. We call such a problem a ,
or COP. Linear programming problems do this kind of optimization.

Constraint Propagation: Inference in CSPs
-----------------------------------------

In regular state-space search, an algorithm can do only one thing:
search. In CSPs there is a choice: an algorithm can search (choose a new
variable assignment from several possibilities) or do a specific type of
called : using the constraints to reduce the number of legal values for
a variable, which in turn can reduce the legal values for another
variable, and so on. Constraint propagation may be intertwined with
search, or it may be done as a preprocessing step, before search starts.
Sometimes this preprocessing can solve the whole problem, so no search
is required at all.

The key idea is . If we treat each variable as a node in a graph (see
(b)) and each binary constraint as an arc, then the process of enforcing
local consistency in each part of the graph causes inconsistent values
to be eliminated throughout the graph. There are different types of
local consistency, which we now cover in turn.

### Node consistency

A single variable (corresponding to a node in the CSP network) is if all
the values in the variable’s domain satisfy the variable’s unary
constraints. For example, in the variant of the Australia map-coloring
problem () where South Australians dislike green, the variable ${SA}$
starts with domain $\{{red}, {green}, {blue}\}$, and we can make
it node consistent by eliminating ${green}$, leaving ${SA}$ with the
reduced domain $\{{red}, {blue}\}$. We say that a network is
node-consistent if every variable in the network is node-consistent.

It is always possible to eliminate all the unary constraints in a CSP by
running node consistency. It is also possible to transform all $n$-ary
constraints into binary ones (see ). Because of this, it is common to
define CSP solvers that work with only binary constraints; we make that
assumption for the rest of this chapter, except where noted.

### Arc consistency

A variable in a CSP is if every value in its domain satisfies the
variable’s binary constraints. More formally, $X_i$ is arc-consistent
with respect to another variable $X_j$ if for every value in the current
domain $D_i$ there is some value in the domain $D_j$ that satisfies the
binary constraint on the arc $(X_i,
X_j)$. A network is arc-consistent if every variable is arc consistent
with every other variable. For example, consider the constraint
$Y = X^2$ where the domain of both $X$ and $Y$ is the set of digits. We
can write this constraint explicitly as
$$\constraint{\varlist{{X, Y}}}{\{(0,0), (1,1), (2, 4), (3, 9))\}} \ .$$
To make $X$ arc-consistent with respect to $Y$, we reduce $X$’s domain
to $\{0,1,2,3\}$. If we also make $Y$ arc-consistent with respect to
$X$, then $Y$’s domain becomes $\{0,1,4,9\}$ and the whole CSP is
arc-consistent.

On the other hand, arc consistency can do nothing for the Australia
map-coloring problem. Consider the following inequality constraint on
$\varlist{{{SA}, {WA}}}$:
$${\{({red},{green}),({red},{blue}),({green},{red}),({green},{blue}),({blue},{red}),({blue},{green})\}}\ .$$
No matter what value you choose for ${SA}$ (or for ${WA}$), there is
a valid value for the other variable. So applying arc consistency has no
effect on the domains of either variable.

[ac3-algorithm]

The most popular algorithm for arc consistency is called AC-3 (see ). To
make every variable arc-consistent, the AC-3 algorithm maintains a queue
of arcs to consider. (Actually, the order of consideration is not
important, so the data structure is really a set, but tradition calls it
a queue.) Initially, the queue contains all the arcs in the CSP. AC-3
then pops off an arbitrary arc $(X_i,X_j)$ from the queue and makes
$X_i$ arc-consistent with respect to $X_j$. If this leaves $D_i$
unchanged, the algorithm just moves on to the next arc. But if this
revises $D_i$ (makes the domain smaller), then we add to the queue all
arcs $(X_k,X_i)$ where $X_k$ is a neighbor of $X_i$. We need to do that
because the change in $D_i$ might enable further reductions in the
domains of $D_k$, even if we have previously considered $X_k$. If $D_i$
is revised down to nothing, then we know the whole CSP has no consistent
solution, and AC-3 can immediately return failure. Otherwise, we keep
checking, trying to remove values from the domains of variables until no
more arcs are in the queue. At that point, we are left with a CSP that
is equivalent to the original CSP—they both have the same solutions—but
the arc-consistent CSP will in most cases be faster to search because
its variables have smaller domains.

The complexity of AC-3 can be analyzed as follows. Assume a CSP with $n$
variables, each with domain size at most $d$, and with $c$ binary
constraints (arcs). Each arc $(X_k,X_i)$ can be inserted in the queue
only $d$ times because $X_i$ has at most $d$ values to delete. Checking
consistency of an arc can be done in $O(d^2)$ time, so we get $O(cd^3)$
total worst-case time.[^1]

It is possible to extend the notion of arc consistency to handle $n$-ary
rather than just binary constraints; this is called generalized arc
consistency or sometimes hyperarc consistency, depending on the author.
A variable $X_i$ is with respect to an $n$-ary constraint if for every
value $v$ in the domain of $X_i$ there exists a tuple of values that is
a member of the constraint, has all its values taken from the domains of
the corresponding variables, and has its $X_i$ component equal to $v$.
For example, if all variables have the domain $\{0, 1, 2, 3\}$, then to
make the variable $X$ consistent with the constraint $X < Y < Z$, we
would have to eliminate 2 and 3 from the domain of $X$ because the
constraint cannot be satisfied when $X$ is 2 or 3.

### Path consistency

Arc consistency can go a long way toward reducing the domains of
variables, sometimes finding a solution (by reducing every domain to
size 1) and sometimes finding that the CSP cannot be solved (by reducing
some domain to size 0). But for other networks, arc consistency fails to
make enough inferences. Consider the map-coloring problem on Australia,
but with only two colors allowed, red and blue. Arc consistency can do
nothing because every variable is already arc consistent: each can be
red with blue at the other end of the arc (or vice versa). But clearly
there is no solution to the problem: because Western Australia, Northern
Territory and South Australia all touch each other, we need at least
three colors for them alone.

Arc consistency tightens down the domains (unary constraints) using the
arcs (binary constraints). To make progress on problems like map
coloring, we need a stronger notion of consistency. tightens the binary
constraints by using implicit constraints that are inferred by looking
at triples of variables.

A two-variable set $\{X_i,X_j\}$ is path-consistent with respect to a
third variable $X_m$ if, for every assignment $\{X_i=a, X_j=b\}$
consistent with the constraints on $\{X_i,X_j\}$, there is an assignment
to $X_m$ that satisfies the constraints on $\{X_i,X_m\}$ and
$\{X_m,X_j\}$. This is called path consistency because one can think of
it as looking at a path from $X_i$ to $X_j$ with $X_m$ in the middle.

Let’s see how path consistency fares in coloring the Australia map with
two colors. We will make the set $\{{WA},{SA}\}$ path consistent
with respect to ${NT}$. We start by enumerating the consistent
assignments to the set. In this case, there are only two:
$\{{WA} = {red},{SA} = {blue}\}$ and $\{{WA} =
{blue},{SA} = {red}\}$. We can see that with both of these
assignments ${NT}$ can be neither ${red}$ nor ${blue}$ (because it
would conflict with either ${WA}$ or ${SA}$). Because there is no
valid choice for ${NT}$, we eliminate both assignments, and we end up
with no valid assignments for $\{{WA},{SA}\}$. Therefore, we know
that there can be no solution to this problem. The PC-2 algorithm
@Mackworth:1977 achieves path consistency in much the same way that AC-3
achieves arc consistency. Because it is so similar, we do not show it
here.

### *K*-consistency {#k-consistency-page}

Stronger forms of propagation can be defined with the notion of . A CSP
is $k$-consistent if, for any set of $k-1$ variables and for any
consistent assignment to those variables, a consistent value can always
be assigned to any $k$th variable. 1-consistency says that, given the
empty set, we can make any set of one variable consistent: this is what
we called node consistency. 2-consistency is the same as arc
consistency. For binary constraint networks, 3-consistency is the same
as path consistency.

A CSP is if it is $k$-consistent and is also $(k-1)$-consistent,
$(k-2)$-consistent, $\ldots$ all the way down to 1-consistent. Now
suppose we have a CSP with $n$ nodes and make it strongly $n$-consistent
(i.e., strongly $k$-consistent for $k\eq
n$). We can then solve the problem as follows: First, we choose a
consistent value for $X_1$. We are then guaranteed to be able to choose
a value for $X_2$ because the graph is 2-consistent, for $X_3$ because
it is 3-consistent, and so on. For each variable $X_i$, we need only
search through the $d$ values in the domain to find a value consistent
with $X_1,\ldots,X_{i-1}$. We are guaranteed to find a solution in time
$O(n^2d)$. Of course, there is no free lunch: any algorithm for
establishing $n$-consistency must take time exponential in $n$ in the
worst case. Worse, $n$-consistency also requires space that is
exponential in $n$. The memory issue is even more severe than the time.
In practice, determining the appropriate level of consistency checking
is mostly an empirical science. It can be said practitioners commonly
compute 2-consistency and less commonly 3-consistency.

### Global constraints

[global-constraint-section]

Remember that a is one involving an arbitrary number of variables (but
not necessarily all variables). Global constraints occur frequently in
real problems and can be handled by special-purpose algorithms that are
more efficient than the general-purpose methods described so far. For
example, the $\v{Alldiff}$ constraint says that all the variables
involved must have distinct values (as in the cryptarithmetic problem
above and Sudoku puzzles below). One simple form of inconsistency
detection for $\v{Alldiff}$ constraints works as follows: if $m$
variables are involved in the constraint, and if they have $n$ possible
distinct values altogether, and $m>n$, then the constraint cannot be
satisfied.

This leads to the following simple algorithm: First, remove any variable
in the constraint that has a singleton domain, and delete that
variable’s value from the domains of the remaining variables. Repeat as
long as there are singleton variables. If at any point an empty domain
is produced or there are more variables than domain values left, then an
inconsistency has been detected.

This method can detect the inconsistency in the assignment
$\{{WA}\eq {red},$ ${NSW}\eq {red}\}$ for . Notice that the
variables ${SA}$, ${NT}$, and $Q$ are effectively connected by an
$\v{Alldiff}$ constraint because each pair must have two different
colors. After applying AC-3 with the partial assignment, the domain of
each variable is reduced to $\{{green},{blue}\}$. That is, we have
three variables and only two colors, so the $\v{Alldiff}$ constraint is
violated. Thus, a simple consistency procedure for a higher-order
constraint is sometimes more effective than applying arc consistency to
an equivalent set of binary constraints. There are more complex
inference algorithms for $\v{Alldiff}$
\<see\>vanHoeve+Katriel:2006 that propagate more
constraints but are more computationally expensive to run.

Another important higher-order constraint is the , sometimes called the
${atmost}$ constraint. For example, in a scheduling problem, let
$P_1,\ldots,P_4$ denote the numbers of personnel assigned to each of
four tasks. The constraint that no more than 10 personnel are assigned
in total is written as ${Atmost}({10},P_1,P_2,P_3,P_4)$. We can detect
an inconsistency simply by checking the sum of the minimum values of the
current domains; for example, if each variable has the domain
$\{3, 4, 5, 6\}$, the ${Atmost}$ constraint cannot be satisfied. We
can also enforce consistency by deleting the maximum value of any domain
if it is not consistent with the minimum values of the other domains.
Thus, if each variable in our example has the domain
$\{2, 3, 4, 5, 6\}$, the values 5 and 6 can be deleted from each domain.

For large resource-limited problems with integer values—such as
logistical problems involving moving thousands of people in hundreds of
vehicles—it is usually not possible to represent the domain of each
variable as a large set of integers and gradually reduce that set by
consistency-checking methods. Instead, domains are represented by upper
and lower bounds and are managed by . For example, in an
airline-scheduling problem, let’s suppose there are two flights, $F_1$
and $F_2$, for which the planes have capacities 165 and 385,
respectively. The initial domains for the numbers of passengers on each
flight are then
$$D_1 = [0,{165}] \quad\mbox{and}\quad D_2 = [0,{385}]\ .$$ Now suppose
we have the additional constraint that the two flights together must
carry 420 people: $F_1+F_2 = 420$. Propagating bounds constraints, we
reduce the domains to
$$D_1 = [{35},{165}] \quad\mbox{and}\quad D_2 = [{255},{385}]\ .$$ We
say that a CSP is if for every variable $X$, and for both the
lower-bound and upper-bound values of $X$, there exists some value of
$Y$ that satisfies the constraint between $X$ and $Y$ for every variable
$Y$. This kind of bounds propagation is widely used in practical
constraint problems.

### Sudoku example {#sudoku-section}

The popular puzzle has introduced millions of people to constraint
satisfaction problems, although they may not recognize it. A Sudoku
board consists of 81 squares, some of which are initially filled with
digits from 1 to 9. The puzzle is to fill in all the remaining squares
such that no digit appears twice in any row, column, or $3\stimes 3$ box
(see ). A row, column, or box is called a .

[sudoku-figure]

The Sudoku puzzles that are printed in newspapers and puzzle books have
the property that there is exactly one solution. Although some can be
tricky to solve by hand, taking tens of minutes, even the hardest Sudoku
problems yield to a CSP solver in less than 0.1 second.

A Sudoku puzzle can be considered a CSP with 81 variables, one for each
square. We use the variable names $\v{A1}$ through $\v{A9}$ for the top
row (left to right), down to $\v{I1}$ through $\v{I9}$ for the bottom
row. The empty squares have the domain $\{1,2,3,4,5,6,7,8,9\}$ and the
pre-filled squares have a domain consisting of a single value. In
addition, there are 27 different $\v{Alldiff}$ constraints: one for each
row, column, and box of 9 squares.

$$\begin{array}{l}
\v{Alldiff}(A1,A2,A3,A4,A5,A6,A7,A8,A9) \\
\v{Alldiff}(B1,B2,B3,B4,B5,B6,B7,B8,B9) \\
\cdots  \\
\v{Alldiff}(A1,B1,C1,D1,E1,F1,G1,H1,I1) \\
\v{Alldiff}(A2,B2,C2,D2,E2,F2,G2,H2,I2) \\
\cdots  \\
\v{Alldiff}(A1,A2,A3,B1,B2,B3,C1,C2,C3) \\
\v{Alldiff}(A4,A5,A6,B4,B5,B6,C4,C5,C6) \\
\cdots 
\end{array}$$ Let us see how far arc consistency can take us. Assume
that the $\v{Alldiff}$ constraints have been expanded into binary
constraints (such as $\v{A1} \neq \v{A2}$) so that we can apply the AC-3
algorithm directly. Consider variable $\v{E6}$ from (a)—the empty square
between the 2 and the 8 in the middle box. From the constraints in the
box, we can remove not only 2 and 8 but also 1 and 7 from $\v{E6}$’s
domain. From the constraints in its column, we can eliminate 5, 6, 2, 8,
9, and 3. That leaves $\v{E6}$ with a domain of $\{4\}$; in other words,
we know the answer for $\v{E6}$. Now consider variable $\v{I6}$—the
square in the bottom middle box surrounded by 1, 3, and 3. Applying arc
consistency in its column, we eliminate 5, 6, 2, 4 (since we now know
$\v{E6}$ must be 4), 8, 9, and 3. We eliminate 1 by arc consistency with
$\v{I5}$, and we are left with only the value 7 in the domain of
$\v{I6}$. Now there are 8 known values in column 6, so arc consistency
can infer that $\v{A6}$ must be 1. Inference continues along these
lines, and eventually, AC-3 can solve the entire puzzle—all the
variables have their domains reduced to a single value, as shown in (b).

Of course, Sudoku would soon lose its appeal if every puzzle could be
solved by a mechanical application of AC-3, and indeed AC-3 works only
for the easiest Sudoku puzzles. Slightly harder ones can be solved by
PC-2, but at a greater computational cost: there are 255,960 different
path constraints to consider in a Sudoku puzzle. To solve the hardest
puzzles and to make efficient progress, we will have to be more clever.

Indeed, the appeal of Sudoku puzzles for the human solver is the need to
be resourceful in applying more complex inference strategies.
Aficionados give them colorful names, such as “naked triples.” That
strategy works as follows: in any unit (row, column or box), find three
squares that each have a domain that contains the same three numbers or
a subset of those numbers. For example, the three domains might be
$\{1,8\}$, $\{3,8\}$, and $\{1, 3, 8\}$. From that we don’t know which
square contains 1, 3, or 8, but we do know that the three numbers must
be distributed among the three squares. Therefore we can remove 1, 3,
and 8 from the domains of every *other* square in the unit.

It is interesting to note how far we can go without saying much that is
specific to Sudoku. We do of course have to say that there are 81
variables, that their domains are the digits 1 to 9, and that there are
27 $\v{Alldiff}$ constraints. But beyond that, all the strategies—arc
consistency, path consistency, etc.—apply generally to all CSPs, not
just to Sudoku problems. Even naked triples is really a strategy for
enforcing consistency of $\v{Alldiff}$ constraints and has nothing to do
with Sudoku *per se*. This is the power of the CSP
formalism: for each new problem area, we only need to define the problem
in terms of constraints; then the general constraint-solving mechanisms
can take over.

Backtracking Search for CSPs {#csp-backtracking-section}
----------------------------

Sudoku problems are designed to be solved by inference over constraints.
But many other CSPs cannot be solved by inference alone; there comes a
time when we must search for a solution. In this section we look at
backtracking search algorithms that work on partial assignments; in the
next section we look at local search algorithms over complete
assignments.

We could apply a standard depth-limited search (from ). A state would be
a partial assignment, and an action would be adding
$\v{var} = \v{value}$ to the assignment. But for a CSP with $n$
variables of domain size $d$, we quickly notice something terrible: the
branching factor at the top level is $nd$ because any of $d$ values can
be assigned to any of $n$ variables. At the next level, the branching
factor is $(n-1)d$, and so on for $n$ levels. We generate a tree with
$n!\cdot d^n$ leaves, even though there are only $d^n$ possible complete
assignments!

Our seemingly reasonable but naive formulation ignores crucial property
common to all CSPs: . A problem is commutative if the order of
application of any given set of actions has no effect on the outcome.
CSPs are commutative because when assigning values to variables, we
reach the same partial assignment regardless of order. Therefore, we
need only consider a *single* variable at each node in the
search tree. For example, at the root node of a search tree for coloring
the map of Australia, we might make a choice between
${SA}\eq {red}$, ${SA}\eq
{green}$, and ${SA}\eq {blue}$, but we would never choose between
${SA}\eq {red}$ and ${WA}\eq {blue}$. With this restriction, the
number of leaves is $d^n$, as we would hope.

The term is used for a depth-first search that chooses values for one
variable at a time and backtracks when a variable has no legal values
left to assign. The algorithm is shown in . It repeatedly chooses an
unassigned variable, and then tries all values in the domain of that
variable in turn, trying to find a solution. If an inconsistency is
detected, then returns failure, causing the previous call to try another
value. Part of the search tree for the Australia problem is shown in ,
where we have assigned variables in the order ${WA},{NT},Q,\ldots$.
Because the representation of CSPs is standardized, there is no need to
supply with a domain-specific initial state, action function, transition
model, or goal test.

Notice that keeps only a single representation of a state and alters
that representation rather than creating new ones, as described on .

[backtracking-search-algorithm]

[australia-search-figure]

In we improved the poor performance of uninformed search algorithms by
supplying them with domain-specific heuristic functions derived from our
knowledge of the problem. It turns out that we can solve CSPs
efficiently *without* such domain-specific knowledge.
Instead, we can add some sophistication to the unspecified functions in
, using them to address the following questions:

1.  Which variable should be assigned next (), and in what order should
    its values be tried ()?

2.  What inferences should be performed at each step in the search ()?

3.  When the search arrives at an assignment that violates a constraint,
    can the search avoid repeating this failure?

The subsections that follow answer each of these questions in turn.

### Variable and value ordering

[csp-ordering-section]

The backtracking algorithm contains the line

 .

The simplest strategy for is to choose the next unassigned variable in
order, $\{X_1, X_2,
\ldots\}$. This static variable ordering seldom results in the most
efficient search. For example, after the assignments for ${WA}\eq
{red}$ and ${NT}\eq {green}$ in , there is only one possible value
for ${SA}$, so it makes sense to assign ${SA}\eq {blue}$ next
rather than assigning $Q$. In fact, after ${SA}$ is assigned, the
choices for $Q$, ${NSW}$, and $V$ are all forced. This intuitive
idea—choosing the variable with the fewest “legal” values—is called the
(MRV) heuristic. It also has been called the “most constrained variable”
or “fail-first” heuristic, the latter because it picks a variable that
is most likely to cause a failure soon, thereby pruning the search tree.
If some variable $X$ has no legal values left, the MRV heuristic will
select $X$ and failure will be detected immediately—avoiding pointless
searches through other variables. The MRV heuristic usually performs
better than a random or static ordering, sometimes by a factor of 1,000
or more, although the results vary widely depending on the problem.

The MRV heuristic doesn’t help at all in choosing the first region to
color in , because initially every region has three legal colors. In
this case, the comes in handy[degree-heuristic-page]. It attempts to
reduce the branching factor on future choices by selecting the variable
that is involved in the largest number of constraints on other
unassigned variables. In , ${SA}$ is the variable with highest degree,
5; the other variables have degree 2 or 3, except for $T$, which has
degree 0. In fact, once ${SA}$ is chosen, applying the degree
heuristic solves the problem without any false steps—you can choose
*any* consistent color at each choice point and still
arrive at a solution with no backtracking. The minimum-remaining-values
heuristic is usually a more powerful guide, but the degree heuristic can
be useful as a tie-breaker.

Once a variable has been selected, the algorithm must decide on the
order in which to examine its values. For this, the heuristic can be
effective in some cases. It prefers the value that rules out the fewest
choices for the neighboring variables in the constraint graph. For
example, suppose that in we have generated the partial assignment with
${WA}\eq {red}$ and ${NT}\eq {green}$ and that our next choice
is for $Q$. Blue would be a bad choice because it eliminates the last
legal value left for $Q$’s neighbor, ${SA}$. The
least-constraining-value heuristic therefore prefers red to blue. In
general, the heuristic is trying to leave the maximum flexibility for
subsequent variable assignments. Of course, if we are trying to find all
the solutions to a problem, not just the first one, then the ordering
does not matter because we have to consider every value anyway. The same
holds if there are no solutions to the problem.

Why should variable selection be fail-first, but value selection be
fail-last? It turns out that, for a wide variety of problems, a variable
ordering that chooses a variable with the minimum number of remaining
values helps minimize the number of nodes in the search tree by pruning
larger parts of the tree earlier. For value ordering, the trick is that
we only need one solution; therefore it makes sense to look for the most
likely values first. If we wanted to enumerate all solutions rather than
just find one, then value ordering would be irrelevant.

### Interleaving search and inference

So far we have seen how AC-3 and other algorithms can infer reductions
in the domain of variables *before* we begin the search.
But inference can be even more powerful in the course of a search: every
time we make a choice of a value for a variable, we have a brand-new
opportunity to infer new domain reductions on the neighboring variables.

One of the simplest forms of inference is called . Whenever a variable
$X$ is assigned, the forward-checking process establishes arc
consistency for it: for each unassigned variable $Y$ that is connected
to $X$ by a constraint, delete from $Y$’s domain any value that is
inconsistent with the value chosen for $X$. Because forward checking
only does arc consistency inferences, there is no reason to do forward
checking if we have already done arc consistency as a preprocessing
step.

shows the progress of backtracking search on the Australia CSP with
forward checking. There are two important points to notice about this
example. First, notice that after ${WA}\eq {red}$ and
$Q\eq {green}$ are assigned, the domains of ${NT}$ and ${SA}$ are
reduced to a single value; we have eliminated branching on these
variables altogether by propagating information from ${WA}$ and $Q$. A
second point to notice is that after $V\eq {blue}$, the domain of
${SA}$ is empty. Hence, forward checking has detected that the partial
assignment $\{{WA}\eq {red},Q\eq {green},V\eq {blue}\}$ is
inconsistent with the constraints of the problem, and the algorithm will
therefore backtrack immediately.

For many problems the search will be more effective if we combine the
MRV heuristic with forward checking. Consider after assigning
$\{{WA}\eq {red}\}$. Intuitively, it seems that that assignment
constrains its neighbors, ${NT}$ and ${SA}$, so we should handle
those variables next, and then all the other variables will fall into
place. That’s exactly what happens with MRV: ${NT}$ and ${SA}$ have
two values, so one of them is chosen first, then the other, then $Q$,
${NSW}$, and $V$ in order. Finally $T$ still has three values, and any
one of them works. We can view forward checking as an efficient way to
incrementally compute the information that the MRV heuristic needs to do
its job.

[australia-fc-figure]

Although forward checking detects many inconsistencies, it does not
detect all of them. The problem is that it makes the current variable
arc-consistent, but doesn’t look ahead and make all the other variables
arc-consistent. For example, consider the third row of . It shows that
when ${WA}$ is ${red}$ and $Q$ is ${green}$, both ${NT}$ and
${SA}$ are forced to be blue. Forward checking does not look far
enough ahead to notice that this is an inconsistency: ${NT}$ and
${SA}$ are adjacent and so cannot have the same value.

The algorithm called MAC (for ) detects this inconsistency. After a
variable $X_i$ is assigned a value, the procedure calls AC-3, but
instead of a queue of all arcs in the CSP, we start with only the arcs
$(X_j, X_i)$ for all $X_j$ that are unassigned variables that are
neighbors of $X_i$. From there, AC-3 does constraint propagation in the
usual way, and if any variable has its domain reduced to the empty set,
the call to AC-3 fails and we know to backtrack immediately. We can see
that MAC is strictly more powerful than forward checking because forward
checking does the same thing as MAC on the initial arcs in MAC’s queue;
but unlike MAC, forward checking does not recursively propagate
constraints when changes are made to the domains of variables.

### Intelligent backtracking: Looking backward {#csp-intelligent-backtracking-section}

The algorithm in has a very simple policy for what to do when a branch
of the search fails: back up to the preceding variable and try a
different value for it. This is called because the *most
recent* decision point is revisited. In this subsection, we
consider better possibilities.

Consider what happens when we apply simple backtracking in with a fixed
variable ordering $Q$, ${NSW}$, $V$, $T$, ${SA}$, ${WA}$,
${NT}$. Suppose we have generated the partial assignment
$\{Q\eq {red},{NSW}\eq {green},V\eq
{blue},T\eq {red}\}$. When we try the next variable, ${SA}$, we
see that every value violates a constraint. We back up to $T$ and try a
new color for Tasmania! Obviously this is silly—recoloring Tasmania
cannot possibly resolve the problem with South Australia.

A more intelligent approach to backtracking is to backtrack to a
variable that might fix the problem—a variable that was responsible for
making one of the possible values of ${SA}$ impossible. To do this, we
will keep track of a set of assignments that are in conflict with some
value for ${SA}$. The set (in this case
$\{Q\eq {red},{NSW}\eq {green},V\eq {blue},\}$), is called the
for ${SA}$. The method backtracks to the *most recent*
assignment in the conflict set; in this case, backjumping would jump
over Tasmania and try a new value for $V$. This method is easily
implemented by a modification to such that it accumulates the conflict
set while checking for a legal value to assign. If no legal value is
found, the algorithm should return the most recent element of the
conflict set along with the failure indicator.

The sharp-eyed reader will have noticed that forward checking can supply
the conflict set with no extra work: whenever forward checking based on
an assignment $X\eq x$ deletes a value from $Y$’s domain, it should add
$X \eq x$ to $Y$’s conflict set. If the last value is deleted from $Y$’s
domain, then the assignments in the conflict set of $Y$ are added to the
conflict set of $X$. Then, when we get to $Y$, we know immediately where
to backtrack if needed.

The eagle-eyed reader will have noticed something odd: backjumping
occurs when every value in a domain is in conflict with the current
assignment; but forward checking detects this event and prevents the
search from ever reaching such a node! In fact, it can be shown that
*every* branch pruned by backjumping is also pruned by
forward checking. Hence, simple backjumping is redundant in a
forward-checking search or, indeed, in a search that uses stronger
consistency checking, such as .

Despite the observations of the preceding paragraph, the idea behind
backjumping remains a good one: to backtrack based on the reasons for
failure. Backjumping notices failure when a variable’s domain becomes
empty, but in many cases a branch is doomed long before this occurs.
Consider again the partial assignment $\{{WA}\eq {red},{NSW}\eq
{red}\}$ (which, from our earlier discussion, is inconsistent).
Suppose we try $T\eq {red}$ next and then assign ${NT}$, $Q$, $V$,
${SA}$. We know that no assignment can work for these last four
variables, so eventually we run out of values to try at ${NT}$. Now,
the question is, where to backtrack? Backjumping cannot work, because
${NT}$ *does* have values consistent with the preceding
assigned variables—${NT}$ doesn’t have a complete conflict set of
preceding variables that caused it to fail. We know, however, that the
four variables ${NT}$, $Q$, $V$, and ${SA}$, *taken
together*, failed because of a set of preceding variables, which
must be those variables that directly conflict with the four. This leads
to a deeper notion of the conflict set for a variable such as ${NT}$:
it is that set of preceding variables that caused ${NT}$,
*together with any subsequent variables*, to have no
consistent solution. In this case, the set is ${WA}$ and ${NSW}$, so
the algorithm should backtrack to ${NSW}$ and skip over Tasmania. A
backjumping algorithm that uses conflict sets defined in this way is
called .

We must now explain how these new conflict sets are computed. The method
is in fact quite simple. The “terminal” failure of a branch of the
search always occurs because a variable’s domain becomes empty; that
variable has a standard conflict set. In our example, ${SA}$ fails,
and its conflict set is (say) $\{{WA},{NT},Q\}$. We backjump to $Q$,
and $Q$ *absorbs* the conflict set from ${SA}$ (minus $Q$
itself, of course) into its own direct conflict set, which is
$\{{NT},{NSW}\}$; the new conflict set is
$\{{WA},{NT},{NSW}\}$. That is, there is no solution from $Q$
onward, given the preceding assignment to $\{{WA},{NT},{NSW}\}$.
Therefore, we backtrack to ${NT}$, the most recent of these. ${NT}$
absorbs $\{{WA},{NT},{NSW}\}-\{{NT}\}$ into its own direct
conflict set $\{{WA}\}$, giving $\{{WA},{NSW}\}$ (as stated in the
previous paragraph). Now the algorithm backjumps to ${NSW}$, as we
would hope. To summarize: let $X_j$ be the current variable, and let
${conf}(X_j)$ be its conflict set. If every possible value for $X_j$
fails, backjump to the most recent variable $X_i$ in ${conf}(X_j)$,
and set
$${conf}(X_i) \leftarrow {conf}(X_i) \cup {conf}(X_j) - \{X_i\}\ .$$
When we reach a contradiction, backjumping can tell us how far to back
up, so we don’t waste time changing variables that won’t fix the
problem. But we would also like to avoid running into the same problem
again. When the search arrives at a contradiction, we know that some
subset of the conflict set is responsible for the problem. is the idea
of finding a minimum set of variables from the conflict set that causes
the problem. This set of variables, along with their corresponding
values, is called a [no-good-page]. We then record the no-good, either
by adding a new constraint to the CSP or by keeping a separate cache of
no-goods.

For example, consider the state $\{{WA}={red}, {NT}={green},
Q={blue}\}$ in the bottom row of . Forward checking can tell us this
state is a no-good because there is no valid assignment to ${SA}$. In
this particular case, recording the no-good would not help, because once
we prune this branch from the search tree, we will never encounter this
combination again. But suppose that the search tree in were actually
part of a larger search tree that started by first assigning values for
$V$ and $T$. Then it would be worthwhile to record
$\{{WA}={red}, {NT}={green}, Q={blue}\}$ as a no-good because
we are going to run into the same problem again for each possible set of
assignments to $V$ and $T$.

No-goods can be effectively used by forward checking or by backjumping.
Constraint learning is one of the most important techniques used by
modern CSP solvers to achieve efficiency on complex problems.

Local Search for CSPs {#csp-local-section}
---------------------

Local search algorithms (see ) turn out to be effective in solving many
CSPs. They use a complete-state formulation: the initial state assigns a
value to every variable, and the search changes the value of one
variable at a time. For example, in the 8-queens problem (see ), the
initial state might be a random configuration of 8 queens in 8 columns,
and each step moves a single queen to a new position in its column.
Typically, the initial guess violates several constraints. The point of
local search is to eliminate the violated constraints.[^2]

In choosing a new value for a variable, the most obvious heuristic is to
select the value that results in the minimum number of conflicts with
other variables—the heuristic. The algorithm is shown in and its
application to an 8-queens problem is diagrammed in .

[min-conflicts-algorithm]

[8queens-min-conflicts-figure]

Min-conflicts is surprisingly effective for many CSPs. Amazingly, on the
$n$-queens problem, if you don’t count the initial placement of queens,
the run time of min-conflicts is roughly *independent of problem
size*. It solves even the *million*-queens problem
in an average of 50 steps (after the initial assignment). This
remarkable observation was the stimulus leading to a great deal of
research in the 1990s on local search and the distinction between easy
and hard problems, which we take up in . Roughly speaking, $n$-queens is
easy for local search because solutions are densely distributed
throughout the state space. Min-conflicts also works well for hard
problems. For example, it has been used to schedule observations for the
Hubble Space Telescope, reducing the time taken to schedule a week of
observations from three weeks (!) to around 10 minutes.

All the local search techniques from are candidates for application to
CSPs, and some of those have proved especially effective. The landscape
of a CSP under the min-conflicts heuristic usually has a series of
plateaux. There may be millions of variable assignments that are only
one conflict away from a solution. Plateau search—allowing sideways
moves to another state with the same score—can help local search find
its way off this plateau. This wandering on the plateau can be directed
with : keeping a small list of recently visited states and forbidding
the algorithm to return to those states. Simulated annealing can also be
used to escape from plateaux.

Another technique, called , can help concentrate the search on the
important constraints. Each constraint is given a numeric weight, $W_i$,
initially all 1. At each step of the search, the algorithm chooses a
variable/value pair to change that will result in the lowest total
weight of all violated constraints. The weights are then adjusted by
incrementing the weight of each constraint that is violated by the
current assignment. This has two benefits: it adds topography to
plateaux, making sure that it is possible to improve from the current
state, and it also, over time, adds weight to the constraints that are
proving difficult to solve.

Another advantage of local search is that it can be used in an online
setting when the problem changes. This is particularly important in
scheduling problems. A week’s airline schedule may involve thousands of
flights and tens of thousands of personnel assignments, but bad weather
at one airport can render the schedule infeasible. We would like to
repair the schedule with a minimum number of changes. This can be easily
done with a local search algorithm starting from the current schedule. A
backtracking search with the new set of constraints usually requires
much more time and might find a solution with many changes from the
current schedule.

The Structure of Problems {#csp-structure-section}
-------------------------

In this section, we examine ways in which the *structure*
of the problem, as represented by the constraint graph, can be used to
find solutions quickly. Most of the approaches here also apply to other
problems besides CSPs, such as probabilistic reasoning. After all, the
only way we can possibly hope to deal with the real world is to
decompose it into many subproblems. Looking again at the constraint
graph for Australia ((b), repeated as (a)), one fact stands out:
Tasmania is not connected to the mainland.[^3] Intuitively, it is
obvious that coloring Tasmania and coloring the mainland are —any
solution for the mainland combined with any solution for Tasmania yields
a solution for the whole map. Independence can be ascertained simply by
finding of the constraint graph. Each component corresponds to a
subproblem ${CSP}{}_i$. If assignment $S_i$ is a solution of
${CSP}{}_i$, then $\bigcup_i S_i$ is a solution of $\bigcup_i
{CSP}{}_i$. Why is this important? Consider the following: suppose
each ${CSP}{}_i$ has $c$ variables from the total of $n$ variables,
where $c$ is a constant. Then there are $n/c$ subproblems, each of which
takes at most $d^c$ work to solve, where $d$ is the size of the domain.
Hence, the total work is $O(d^c
n/c)$, which is *linear* in $n$; without the decomposition,
the total work is $O(d^n)$, which is exponential in $n$. Let’s make this
more concrete: dividing a Boolean CSP with 80 variables into four
subproblems reduces the worst-case solution time from the lifetime of
the universe down to less than a second.

Completely independent subproblems are delicious, then, but rare.
Fortunately, some other graph structures are also easy to solve. For
example, a constraint graph is a when any two variables are connected by
only one path. We show that

any tree-structured CSP can be solved in time linear in the number of
variables.[^4]

The key is a new notion of consistency, called or DAC. A CSP is defined
to be directed arc-consistent under an ordering of variables
$X_1, X_2, \ldots, X_n$ if and only if every $X_i$ is arc-consistent
with each $X_j$ for $j > i$.

To solve a tree-structured CSP, first pick any variable to be the root
of the tree, and choose an ordering of the variables such that each
variable appears after its parent in the tree. Such an ordering is
called a . (a) shows a sample tree and (b) shows one possible ordering.
Any tree with $n$ nodes has $n-1$ arcs, so we can make this graph
directed arc-consistent in $O(n)$ steps, each of which must compare up
to $d$ possible domain values for two variables, for a total time of
$O(nd^2)$. Once we have a directed arc-consistent graph, we can just
march down the list of variables and choose any remaining value. Since
each link from a parent to its child is arc consistent, we know that for
any value we choose for the parent, there will be a valid value left to
choose for the child. That means we won’t have to backtrack; we can move
linearly through the variables. The complete algorithm is shown in .

[tree-csp-figure]

[tree-csp-algorithm]

Now that we have an efficient algorithm for trees, we can consider
whether more general constraint graphs can be *reduced* to
trees somehow. There are two primary ways to do this, one based on
removing nodes and one based on collapsing nodes together.

The first approach involves assigning values to some variables so that
the remaining variables form a tree. Consider the constraint graph for
Australia, shown again in (a). If we could delete South Australia, the
graph would become a tree, as in (b). Fortunately, we can do this (in
the graph, not the continent) by fixing a value for ${SA}$ and
deleting from the domains of the other variables any values that are
inconsistent with the value chosen for ${SA}$.

[australia-tree-figure]

Now, any solution for the CSP after ${SA}$ and its constraints are
removed will be consistent with the value chosen for ${SA}$. (This
works for binary CSPs; the situation is more complicated with
higher-order constraints.) Therefore, we can solve the remaining tree
with the algorithm given above and thus solve the whole problem. Of
course, in the general case (as opposed to map coloring), the value
chosen for ${SA}$ could be the wrong one, so we would need to try each
possible value. The general algorithm is as follows:

1.  Choose a subset $S$ of the CSP’s variables such that the constraint
    graph becomes a tree after removal of $S$. $S$ is called a .

2.  For each possible assignment to the variables in $S$ that satisfies
    all constraints on $S$,

    1.  remove from the domains of the remaining variables any values
        that are inconsistent with the assignment for $S$, and

    2.  If the remaining CSP has a solution, return it together with the
        assignment for $S$.

If the cycle cutset has size $c$, then the total run time is
$O(d^c\cdot (n-c)d^2)$: we have to try each of the $d^c$ combinations of
values for the variables in $S$, and for each combination we must solve
a tree problem of size $n-c$. If the graph is “nearly a tree,” then $c$
will be small and the savings over straight backtracking will be huge.
In the worst case, however, $c$ can be as large as $(n-2)$. Finding the
*smallest* cycle cutset is NP-hard, but several efficient
approximation algorithms are known. The overall algorithmic approach is
called ; it comes up again in , where it is used for reasoning about
probabilities.

The second approach is based on constructing a of the constraint graph
into a set of connected subproblems. Each subproblem is solved
independently, and the resulting solutions are then combined. Like most
divide-and-conquer algorithms, this works well if no subproblem is too
large. shows a tree decomposition of the map-coloring problem into five
subproblems. A tree decomposition must satisfy the following three
requirements:

-   Every variable in the original problem appears in at least one of
    the subproblems.

-   If two variables are connected by a constraint in the original
    problem, they must appear together (along with the constraint) in at
    least one of the subproblems.

-   If a variable appears in two subproblems in the tree, it must appear
    in every subproblem along the path connecting those subproblems.

The first two conditions ensure that all the variables and constraints
are represented in the decomposition. The third condition seems rather
technical, but simply reflects the constraint that any given variable
must have the same value in every subproblem in which it appears; the
links joining subproblems in the tree enforce this constraint. For
example, ${SA}$ appears in all four of the connected subproblems in .
You can verify from that this decomposition makes sense.

[australia-decomposition-figure]

We solve each subproblem independently; if any one has no solution, we
know the entire problem has no solution. If we can solve all the
subproblems, then we attempt to construct a global solution as follows.
First, we view each subproblem as a “mega-variable” whose domain is the
set of all solutions for the subproblem. For example, the leftmost
subproblem in is a map-coloring problem with three variables and hence
has six solutions—one is $\{{WA}={red}, {SA}={blue},
{NT}={green}\}$. Then, we solve the constraints connecting the
subproblems, using the efficient algorithm for trees given earlier. The
constraints between subproblems simply insist that the subproblem
solutions agree on their shared variables. For example, given the
solution $\{{WA}={red}, {SA}={blue}, {NT}={green}\}$ for the
first subproblem, the only consistent solution for the next subproblem
is $\{{SA}={blue}, {NT}={green}, Q={red}\}$.

A given constraint graph admits many tree decompositions; in choosing a
decomposition, the aim is to make the subproblems as small as possible.
The [tree-width-page] of a tree decomposition of a graph is one less
than the size of the largest subproblem; the tree width of the graph
itself is defined to be the minimum tree width among all its tree
decompositions. If a graph has tree width $w$ and we are given the
corresponding tree decomposition, then the problem can be solved in
$O(nd^{w+1})$ time. Hence,

CSPs with constraint graphs of bounded tree width are solvable in
polynomial time.

Unfortunately, finding the decomposition with minimal tree width is
NP-hard, but there are heuristic methods that work well in practice.

So far, we have looked at the structure of the constraint graph. There
can be important structure in the *values* of variables as
well. Consider the map-coloring problem with $n$ colors. For every
consistent solution, there is actually a set of $n!$ solutions formed by
permuting the color names. For example, on the Australia map we know
that ${WA}, {NT}$, and ${SA}$ must all have different colors, but
there are $3! = 6$ ways to assign the three colors to these three
regions. This is called . We would like to reduce the search space by a
factor of $n!$ by breaking the symmetry. We do this by introducing a .
For our example, we might impose an arbitrary ordering constraint,
${NT} < {SA} < {WA}$, that requires the three values to be in
alphabetical order. This constraint ensures that only one of the $n!$
solutions is possible:
$\{{NT}=\v{blue}, {SA}=\v{green}, {WA}=\v{red}\}$.

For map coloring, it was easy to find a constraint that eliminates the
symmetry, and in general it is possible to find constraints that
eliminate all but one symmetric solution in polynomial time, but it is
NP-hard to eliminate all symmetry among intermediate sets of values
during search. In practice, breaking value symmetry has proved to be
important and effective on a wide range of problems.

-   (CSPs) represent a state with a set of variable/value pairs and
    represent the conditions for a solution by a set of constraints on
    the variables. Many important real-world problems can be described
    as CSPs.

-   A number of inference techniques use the constraints to infer which
    variable/value pairs are consistent and which are not. These include
    node, arc, path, and $k$-consistency.

-   , a form of depth-first search, is commonly used for solving CSPs.
    Inference can be interwoven with search.

-   The and heuristics are domain-independent methods for deciding which
    variable to choose next in a backtracking search. The heuristic
    helps in deciding which value to try first for a given variable.
    Backtracking occurs when no legal assignment can be found for a
    variable. backtracks directly to the source of the problem.

-   Local search using the heuristic has also been applied to constraint
    satisfaction problems with great success.

-   The complexity of solving a CSP is strongly related to the structure
    of its constraint graph. Tree-structured problems can be solved in
    linear time. can reduce a general CSP to a tree-structured one and
    is quite efficient if a small cutset can be found. techniques
    transform the CSP into a tree of subproblems and are efficient if
    the of the constraint graph is small.

The earliest work related to constraint satisfaction dealt largely with
numerical constraints. Equational constraints with integer domains were
studied by the Indian mathematician in the seventh century; they are
often called , after the Greek mathematician (c. 200–284), who actually
considered the domain of positive rationals. Systematic methods for
solving linear equations by variable elimination were studied by ; the
solution of linear inequality constraints goes back to .

Finite-domain constraint satisfaction problems also have a long history.
For example, (of which map coloring is a special case) is an old problem
in mathematics. The four-color conjecture (that every planar graph can
be colored with four or fewer colors) was first made by Francis Guthrie,
a student of De Morgan, in 1852. It resisted solution—despite several
published claims to the contrary—until a proof was devised by (see the
book *Four Colors Suffice* @Wilson:2004). Purists were
disappointed that part of the proof relied on a computer, so Georges
Gonthier [-@Gonthier:2008], using the theorem prover, derived a formal
proof that Appel and Haken’s proof was correct.

Specific classes of constraint satisfaction problems occur throughout
the history of computer science. One of the most influential early
examples was the system @Sutherland:1963, which solved geometric
constraints in diagrams and was the forerunner of modern drawing
programs and CAD tools. The identification of CSPs as a
*general* class is due to Ugo . The reduction of
higher-order CSPs to purely binary CSPs with auxiliary variables (see )
is due originally to the 19th-century logician Charles Sanders Peirce.
It was introduced into the CSP literature by and was elaborated by .
CSPs with preferences among solutions are studied widely in the
optimization literature; see for a generalization of the CSP framework
to allow for preferences. The bucket-elimination algorithm @Dechter:1999
can also be applied to optimization problems.

Constraint propagation methods were popularized by Waltz’s
[-@Waltz:1975] success on polyhedral line-labeling problems for computer
vision. Waltz showed that, in many problems, propagation completely
eliminates the need for backtracking. introduced the notion of
constraint networks and propagation by path consistency. Alan proposed
the algorithm for enforcing arc consistency as well as the general idea
of combining backtracking with some degree of consistency enforcement. ,
a more efficient arc-consistency algorithm, was developed by . Soon
after Mackworth’s paper appeared, researchers began experimenting with
the tradeoff between the cost of consistency enforcement and the
benefits in terms of search reduction. favored the minimal
forward-checking algorithm described by , whereas suggested full
arc-consistency checking after each variable assignment—an algorithm
later called by . The latter paper provides somewhat convincing evidence
that, on harder CSPs, full arc-consistency checking pays off.
investigated the notion of $k$-consistency and its relationship to the
complexity of solving CSPs. describes a generic algorithmic framework
within which consistency propagation algorithms can be analyzed, and
presents a current survey.

Special methods for handling higher-order or global constraints were
developed first within the context of . provide excellent coverage of
research in this area. The ${Alldiff}$ constraint was studied by , ,
and . Bounds constraints were incorporated into constraint logic
programming by . A survey of global constraints is provided by .

Sudoku has become the most widely known CSP and was described as such by
. describe some of the strategies and show that Sudoku on an
$n^2\times n^2$ board is in the class of *NP*-hard
problems. show an interactive solver based on CSP techniques.

The idea of backtracking search goes back to , and its application to
constraint satisfaction is due to , although they trace the basic
algorithm back to the 19th century. Bitner and Reingold also introduced
the MRV heuristic, which they called the
*most-constrained-variable* heuristic. used the degree
heuristic as a tiebreaker after applying the MRV heuristic. The
resulting algorithm, despite its simplicity, is still the best method
for $k$-coloring arbitrary graphs. proposed the least-constraining-value
heuristic.

The basic method is due to John . showed that this algorithm is
essentially subsumed by forward checking. Conflict-directed backjumping
was devised by . The most general and powerful form of intelligent
backtracking was actually developed very early on by . Their technique
of led to the development of  @Doyle:1979, which we discuss in . The
connection between the two areas is analyzed by .

The work of Stallman and Sussman also introduced the idea of , in which
partial results obtained by search can be saved and reused later in the
search. The idea was formalized .  @Gaschnig:1979 is a particularly
simple method in which consistent and inconsistent pairwise assignments
are saved and used to avoid rechecking constraints. Backmarking can be
combined with conflict-directed backjumping; present a hybrid algorithm
that provably subsumes either method taken separately. The method of
 @Ginsberg:1993 retains successful partial assignments from later
subsets of variables when backtracking over an earlier choice that does
not invalidate the later success.

Empirical studies of several randomized backtracking methods were done
by and . Van Beek [-@vanBeek:2006] surveys backtracking.

Local search in constraint satisfaction problems was popularized by the
work of on simulated annealing (see ), which is widely used for
scheduling problems. The min-conflicts heuristic was first proposed by
and was developed independently by . showed how it could be applied to
solve the 3,000,000 queens problem in less than a minute. The astounding
success of local search using min-conflicts on the $n$-queens problem
led to a reappraisal of the nature and prevalence of “easy” and “hard”
problems. Peter explored the difficulty of randomly generated CSPs and
discovered that almost all such problems either are trivially easy or
have no solutions. Only if the parameters of the problem generator are
set in a certain narrow range, within which roughly half of the problems
are solvable, do we find “hard” problem instances. We discuss this
phenomenon further in . showed that local search is inferior to
backtracking search on problems with a certain degree of local
structure; this led to work that combined local search and inference,
such as that by . survey local search techniques.

Work relating the structure and complexity of CSPs originates with , who
showed that search on arc consistent trees works without any
backtracking. A similar result, with extensions to acyclic hypergraphs,
was developed in the database community @Beeri+al:1983. present an
algorithm for tree-structured CSPs that runs in linear time without any
preprocessing.

Since those papers were published, there has been a great deal of
progress in developing more general results relating the complexity of
solving a CSP to the structure of its constraint graph. The notion of
was introduced by the graph theorists . , building on the work of
Freuder, applied a related notion (which they called ) to constraint
satisfaction problems and developed the tree decomposition approach
sketched in . Drawing on this work and on results from database theory,
developed a notion, , that is based on the characterization of the CSP
as a hypergraph. In addition to showing that any CSP with hypertree
width $w$ can be solved in time $O(n^{w+1}\log n)$, they also showed
that hypertree width subsumes all previously defined measures of “width”
in the sense that there are cases where the hypertree width is bounded
and the other measures are unbounded.

Interest in look-back approaches to backtracking was rekindled by the
work of , whose algorithm combined constraint learning and backjumping
and was shown to outperform many other algorithms of the time. This led
to AND/OR search algorithms applicable to both CSPs and probabilistic
reasoning @Dechter+Mateescu:2007. introduce the idea of symmetry
breaking in CSPs, and give a recent survey.

The field of looks at solving CSPs when there is a collection of agents,
each of which controls a subset of the constraint variables. There have
been annual workshops on this problem since 2000, and good coverage
elsewhere @Collin+al:1999 [@Pearce+al:2008; @Shoham+Leyton-Brown:2009].

Comparing CSP algorithms is mostly an empirical science: few theoretical
results show that one algorithm dominates another on all problems;
instead, we need to run experiments to see which algorithms perform
better on typical instances of problems. As points out, we need to be
careful to distinguish between competitive testing—as occurs in
competitions among algorithms based on run time—and scientific testing,
whose goal is to identify the properties of an algorithm that determine
its efficacy on a class of problems.

The recent textbooks by and , and the collection by are excellent
resources on constraint processing. There are several good earlier
surveys, including those by , , and ; and the encyclopedia articles by
and . survey tractable classes of CSPs, covering both structural
decomposition methods and methods that rely on properties of the domains
or constraints themselves. give an analytical survey of backtracking
search algorithms, and give a more empirical survey. Constraint
programming is covered in the books by and . Several interesting
applications are described in the collection edited by . Papers on
constraint satisfaction appear regularly in *Artificial
Intelligence* and in the specialist journal
*Constraints*. The primary conference venue is the
International Conference on Principles and Practice of Constraint
Programming, often called *CP*.

How many solutions are there for the map-coloring problem in ? How many
solutions if four colors are allowed? Two colors?

Consider the problem of placing $k$ knights on an $n\stimes n$
chessboard such that no two knights are attacking each other, where $k$
is given and $k\leq n^2$.

1.  Choose a CSP formulation. In your formulation, what are the
    variables?

2.  What are the possible values of each variable?

3.  What sets of variables are constrained, and how?

4.  Now consider the problem of putting *as many knights as
    possible* on the board without any attacks. Explain how to
    solve this with local search by defining appropriate and functions
    and a sensible objective function.

[crossword-exercise]Consider the problem of constructing (not solving)
crossword puzzles:[^5] fitting words into a rectangular grid. The grid,
which is given as part of the problem, specifies which squares are blank
and which are shaded. Assume that a list of words (i.e., a dictionary)
is provided and that the task is to fill in the blank squares by using
any subset of the list. Formulate this problem precisely in two ways:

1.  As a general search problem. Choose an appropriate search algorithm
    and specify a heuristic function. Is it better to fill in blanks one
    letter at a time or one word at a time?

2.  As a constraint satisfaction problem. Should the variables be words
    or letters?

Which formulation do you think will be better? Why?

[csp-definition-exercise]Give precise formulations for each of the
following as constraint satisfaction problems:

1.  Rectilinear floor-planning: find non-overlapping places in a large
    rectangle for a number of smaller rectangles.

2.  Class scheduling: There is a fixed number of professors and
    classrooms, a list of classes to be offered, and a list of possible
    time slots for classes. Each professor has a set of classes that he
    or she can teach.

3.  Hamiltonian tour: given a network of cities connected by roads,
    choose an order to visit all cities in a country without repeating
    any.

Solve the cryptarithmetic problem in by hand, using the strategy of
backtracking with forward checking and the MRV and
least-constraining-value heuristics.

[nary-csp-exercise] Show how a single ternary constraint such as
“$A + B = C$” can be turned into three binary constraints by using an
auxiliary variable. You may assume finite domains. (*Hint:*
Consider a new variable that takes on values that are pairs of other
values, and consider constraints such as “$X$ is the first element of
the pair $Y$.”) Next, show how constraints with more than three
variables can be treated similarly. Finally, show how unary constraints
can be eliminated by altering the domains of variables. This completes
the demonstration that any CSP can be transformed into a CSP with only
binary constraints.

[zebra-exercise] Consider the following logic puzzle: In five houses,
each with a different color, live five persons of different
nationalities, each of whom prefers a different brand of candy, a
different drink, and a different pet. Given the following facts, the
questions to answer are “Where does the zebra live, and in which house
do they drink water?”

The Englishman lives in the red house.

The Spaniard owns the dog.

The Norwegian lives in the first house on the left.

The green house is immediately to the right of the ivory house.

The man who eats Hershey bars lives in the house next to the man with
the fox.

Kit Kats are eaten in the yellow house.

The Norwegian lives next to the blue house.

The Smarties eater owns snails.

The Snickers eater drinks orange juice.

The Ukrainian drinks tea.

The Japanese eats Milky Ways.

Kit Kats are eaten in a house next to the house where the horse is kept.

Coffee is drunk in the green house.

Milk is drunk in the middle house.

Discuss different representations of this problem as a CSP. Why would
one prefer one representation over another?

Consider the graph with 8 nodes $A_1$, $A_2$, $A_3$, $A_4$, $H$, $T$,
$F_1$, $F_2$. $A_i$ is connected to $A_{i+1}$ for all $i$, each $A_i$ is
connected to $H$, $H$ is connected to $T$, and $T$ is connected to each
$F_i$. Find a 3-coloring of this graph by hand using the following
strategy: backtracking with conflict-directed backjumping, the variable
order $A_1$, $H$, $A_4$, $F_1$, $A_2$, $F_2$, $A_3$, $T$, and the value
order $R$, $G$, $B$.

Explain why it is a good heuristic to choose the variable that is
*most* constrained but the value that is
*least* constraining in a CSP search.

Generate random instances of map-coloring problems as follows: scatter
$n$ points on the unit square; select a point $X$ at random, connect $X$
by a straight line to the nearest point $Y$ such that $X$ is not already
connected to $Y$ and the line crosses no other line; repeat the previous
step until no more connections are possible. The points represent
regions on the map and the lines connect neighbors. Now try to find
$k$-colorings of each map, for both $k\eq 3$ and $k \eq 4$, using
min-conflicts, backtracking, backtracking with forward checking, and
backtracking with MAC. Construct a table of average run times for each
algorithm for values of $n$ up to the largest you can manage. Comment on
your results.

Use the AC-3 algorithm to show that arc consistency can detect the
inconsistency of the partial assignment
$\{{WA}\eq {green},V\eq {red}\}$ for the problem shown in .

Use the AC-3 algorithm to show that arc consistency can detect the
inconsistency of the partial assignment
$\{{WA}\eq {red},V\eq {blue}\}$ for the problem shown in .

What is the worst-case complexity of running AC-3 on a tree-structured
CSP?

[ac4-exercise] AC-3 puts back on the queue *every* arc
($X_{k}, X_{i}$) whenever *any* value is deleted from the
domain of $X_{i}$, even if each value of $X_{k}$ is consistent with
several remaining values of $X_{i}$. Suppose that, for every arc
($X_{k}, X_{i}$), we keep track of the number of remaining values of
$X_{i}$ that are consistent with each value of $X_{k}$. Explain how to
update these numbers efficiently and hence show that arc consistency can
be enforced in total time $O(n^2d^2)$.

The () makes arcs consistent starting at the leaves and working
backwards towards the root. Why does it do that? What would happen if it
went in the opposite direction?

We introduced Sudoku as a CSP to be solved by search over partial
assignments because that is the way people generally undertake solving
Sudoku problems. It is also possible, of course, to attack these
problems with local search over complete assignments. How well would a
local solver using the min-conflicts heuristic do on Sudoku problems?

Define in your own words the terms constraint, backtracking search, arc
consistency, backjumping, min-conflicts, and cycle cutset.

Define in your own words the terms constraint, commutativity, arc
consistency, backjumping, min-conflicts, and cycle cutset.

Suppose that a graph is known to have a cycle cutset of no more than $k$
nodes. Describe a simple algorithm for finding a minimal cycle cutset
whose run time is not much more than $O(n^k)$ for a CSP with $n$
variables. Search the literature for methods for finding approximately
minimal cycle cutsets in time that is polynomial in the size of the
cutset. Does the existence of such algorithms make the cycle cutset
method practical?

Consider the problem of tiling a surface (completely and exactly
covering it) with $n$ dominoes ($2\times
1$ rectangles). The surface is an arbitrary edge-connected (i.e.,
adjacent along an edge, not just a corner) collection of $2n$
$1\times 1$ squares (e.g., a checkerboard, a checkerboard with some
squares missing, a $10\times 1$ row of squares, etc.).

1.  Formulate this problem precisely as a CSP where the dominoes are the
    variables.

2.  Formulate this problem precisely as a CSP where the squares are the
    variables, keeping the state space as small as possible.
    (*Hint:* does it matter which particular domino goes on
    a given pair of squares?)

3.  Construct a surface consisting of 6 squares such that your CSP
    formulation from part (b) has a *tree-structured*
    constraint graph.

4.  Describe exactly the set of solvable instances that have a
    tree-structured constraint graph.

[^1]: The AC-4 algorithm @Mohr+Henderson:1986 runs in $O(cd^2)$
    worst-case time but can be slower than AC-3 on average cases. See .

[^2]: Local search can easily be extended to constraint optimization
    problems (COPs). In that case, all the techniques for hill climbing
    and simulated annealing can be applied to optimize the objective
    function.

[^3]: A careful cartographer or patriotic Tasmanian might object that
    Tasmania should not be colored the same as its nearest mainland
    neighbor, to avoid the impression that it *might* be
    part of that state.

[^4]: Sadly, very few regions of the world have tree-structured maps,
    although comes close.

[^5]: discuss several methods for constructing crossword puzzles. tackle
    the harder problem of solving them.
Probabilistic Reasoning over Time {#dbn-chapter}
=================================

Agents in partially observable environments must be able to keep track
of the current state, to the extent that their sensors allow. In we
showed a methodology for doing that: an agent maintains a that
represents which states of the world are currently possible. From the
belief state and a , the agent can predict how the world might evolve in
the next time step. From the percepts observed and a , the agent can
update the belief state. This is a pervasive idea: in belief states were
represented by explicitly enumerated sets of states, whereas in they
were represented by logical formulas. Those approaches defined belief
states in terms of which world states were *possible*, but
could say nothing about which states were *likely* or
*unlikely*. In this chapter, we use probability theory to
quantify the degree of belief in elements of the belief state.

As we show in , time itself is handled in the same way as in : a
changing world is modeled using a variable for each aspect of the world
state *at each point in time*. The transition and sensor
models may be uncertain: the transition model describes the probability
distribution of the variables at time $t$, given the state of the world
at past times, while the sensor model describes the probability of each
percept at time $t$, given the current state of the world. defines the
basic inference tasks and describes the general structure of inference
algorithms for temporal models. Then we describe three specific kinds of
models: , , and (which include hidden Markov models and Kalman filters
as special cases). Finally, examines the problems faced when keeping
track of more than one thing.

Time and Uncertainty {#time+uncertainty-section}
--------------------

We have developed our techniques for probabilistic reasoning in the
context of *static* worlds, in which each random variable
has a single fixed value. For example, when repairing a car, we assume
that whatever is broken remains broken during the process of diagnosis;
our job is to infer the state of the car from observed evidence, which
also remains fixed.

Now consider a slightly different problem: treating a diabetic patient.
As in the case of car repair, we have evidence such as recent insulin
doses, food intake, blood sugar measurements, and other physical signs.
The task is to assess the current state of the patient, including the
actual blood sugar level and insulin level. Given this information, we
can make a decision about the patient’s food intake and insulin dose.
Unlike the case of car repair, here the *dynamic* aspects
of the problem are essential. Blood sugar levels and measurements
thereof can change rapidly over time, depending on recent food intake
and insulin doses, metabolic activity, the time of day, and so on. To
assess the current state from the history of evidence and to predict the
outcomes of treatment actions, we must model these changes.

The same considerations arise in many other contexts, such as tracking
the location of a robot, tracking the economic activity of a nation, and
making sense of a spoken or written sequence of words. How can dynamic
situations like these be modeled?

### States and observations

We view the world as a series of snapshots, or , each of which contains
a set of random variables, some observable and some not.[^1] For
simplicity, we will assume that the same subset of variables is
observable in each time slice (although this is not strictly necessary
in anything that follows). We will use $\X_t$ to denote the set of state
variables at time $t$, which are assumed to be unobservable, and $\E_t$
to denote the set of observable evidence variables. The observation at
time $t$ is $\E_t\eq \e_t$ for some set of values $\e_t$.

Consider the following example: You are the security guard stationed at
a secret underground installation. You want to know whether it’s raining
today, but your only access to the outside world occurs each morning
when you see the director coming in with, or without, an umbrella. For
each day $t$, the set $\E_t$ thus contains a single evidence variable
${Umbrella}_t$ or $U_t$ for short (whether the umbrella appears), and
the set $\X_t$ contains a single state variable ${Rain}_t$ or $R_t$
for short (whether it is raining). Other problems can involve larger
sets of variables. In the diabetes example, we might have evidence
variables, such as ${MeasuredBloodSugar}{}_t$ and ${PulseRate}{}_t$,
and state variables, such as ${BloodSugar}{}_t$ and
${StomachContents}{}_t$. (Notice that ${BloodSugar}{}_t$ and
${MeasuredBloodSugar}{}_t$ are not the same variable; this is how we
deal with noisy measurements of actual quantities.)

The interval between time slices also depends on the problem. For
diabetes monitoring, a suitable interval might be an hour rather than a
day. In this chapter we assume the interval between slices is fixed, so
we can label times by integers. We will assume that the state sequence
starts at $t\eq 0$; for various uninteresting reasons, we will assume
that evidence starts arriving at $t\eq 1$ rather than $t\eq 0$. Hence,
our umbrella world is represented by state variables
$R_0,\,R_1,\,R_2,\ldots$ and evidence variables $U_1,\,U_2,\ldots$. We
will use the notation $a{:}b$ to denote the sequence of integers from
$a$ to $b$ (inclusive), and the notation $\X_{a:b}$ to denote the set of
variables from $\X_a$ to $\X_b$. For example, $U_{1:3}$ corresponds to
the variables $U_1$, $U_2$, $U_3$.

### Transition and sensor models

With the set of state and evidence variables for a given problem decided
on, the next step is to specify how the world evolves (the transition
model) and how the evidence variables get their values (the sensor
model).

The transition model specifies the probability distribution over the
latest state variables, given the previous values, that is,
$\pv(\X_t\given \X_{0:t-1})$. Now we face a problem: the set
$\X_{0:t-1}$ is unbounded in size as $t$ increases. We solve the problem
by making a —that the current state depends on only a *finite
fixed number* of previous states. Processes satisfying this
assumption were first studied in depth by the Russian statistician
Andrei Markov (1856–1922) and are called or [markov-chain-page]. They
come in various flavors; the simplest is the , in which the current
state depends only on the previous state and not on any earlier states.
In other words, a state provides enough information to make the future
conditionally independent of the past, and we have

$$\pv(\X_t\given \X_{0:t-1}) = \pv(\X_t\given \X_{t-1})\ .
\label{markov-equation}$$

Hence, in a first-order Markov process, the transition model is the
conditional distribution $\pv(\X_t\given \X_{t-1})$. The transition
model for a second-order Markov process is the conditional distribution
$\pv(\X_t\given \X_{t-2},\X_{t-1})$. shows the Bayesian network
structures corresponding to first-order and second-order Markov
processes.

Even with the Markov assumption there is still a problem: there are
infinitely many possible values of $t$. Do we need to specify a
different distribution for each time step? We avoid this problem by
assuming that changes in the world state are caused by a
[stationarity-page]—that is, a process of change that is governed by
laws that do not themselves change over time. (Don’t confuse
*stationary* with *static*: in a
*static* process, the state itself does not change.) In the
umbrella world, then, the conditional probability of rain,
$\pv(R_t\given R_{t-1})$, is the same for all $t$, and we only have to
specify one conditional probability table.

[markov-processes-figure]

Now for the sensor model. The evidence variables $\E_t$
*could* depend on previous variables as well as the current
state variables, but any state that’s worth its salt should suffice to
generate the current sensor values. Thus, we make a as follows:

$$\pv(\E_t\given \X_{0:t},\E_{0:t-1}) =  \pv(\E_t\given \X_t)\ .
\label{sensory-markov-equation}$$

Thus, $\pv(\E_t\given \X_t)$ is our sensor model (sometimes called the
). shows both the transition model and the sensor model for the umbrella
example. Notice the direction of the dependence between state and
sensors: the arrows go from the actual state of the world to sensor
values because the state of the world *causes* the sensors
to take on particular values: the rain *causes* the
umbrella to appear. (The inference process, of course, goes in the other
direction; the distinction between the direction of modeled dependencies
and the direction of inference is one of the principal advantages of
Bayesian networks.)

[umbrella-dbn-figure]

In addition to specifying the transition and sensor models, we need to
say how everything gets started—the prior probability distribution at
time 0, $\pv(\X_0)$. With that, we have a specification of the complete
joint distribution over all the variables, using . For any $t$,

$$\pv(\X_{0:t}, \E_{1:t}) = 
   \pv(\X_0) \prod_{i\eq 1}^t \pv(\X_i\given \X_{i-1}) \, \pv(\E_i\given \X_i)\ .
\label{temporal-joint-equation}$$

The three terms on the right-hand side are the initial state model
$\pv(\X_0)$, the transition model $\pv(\X_i\given \X_{i-1})$, and the
sensor model $\pv(\E_i\given \X_i)$.

The structure in is a first-order Markov process—the probability of rain
is assumed to depend only on whether it rained the previous day. Whether
such an assumption is reasonable depends on the domain itself. The
first-order Markov assumption says that the state variables contain
*all* the information needed to characterize the
probability distribution for the next time slice. Sometimes the
assumption is exactly true—for example, if a particle is executing a
random walk along the $x$-axis, changing its position by $\pm 1$ at each
time step, then using the $x$-coordinate as the state gives a
first-order Markov process. Sometimes the assumption is only
approximate, as in the case of predicting rain only on the basis of
whether it rained the previous day. There are two ways to improve the
accuracy of the approximation:

1.  Increasing the order of the Markov process model. For example, we
    could make a second-order model by adding ${Rain}{}_{t-2}$ as a
    parent of ${Rain}{}_t$, which might give slightly more accurate
    predictions. For example, in Palo Alto, California, it very rarely
    rains more than two days in a row.

2.  Increasing the set of state variables. For example, we could add
    ${Season}{}_t$ to allow us to incorporate historical records of
    rainy seasons, or we could add ${Temperature}{}_t$,
    ${Humidity}{}_t$ and ${Pressure}{}_t$ (perhaps at a range of
    locations) to allow us to use a physical model of rainy conditions.

asks you to show that the first solution—increasing the order—can always
be reformulated as an increase in the set of state variables, keeping
the order fixed. Notice that adding state variables might improve the
system’s predictive power but also increases the prediction
*requirements*: we now have to predict the new variables as
well. Thus, we are looking for a “self-sufficient” set of variables,
which really means that we have to understand the “physics” of the
process being modeled. The requirement for accurate modeling of the
process is obviously lessened if we can add new sensors (e.g.,
measurements of temperature and pressure) that provide information
directly about the new state variables.

Consider, for example, the problem of tracking a robot wandering
randomly on the X–Y plane. One might propose that the position and
velocity are a sufficient set of state variables: one can simply use
Newton’s laws to calculate the new position, and the velocity may change
unpredictably. If the robot is battery-powered, however, then battery
exhaustion would tend to have a systematic effect on the change in
velocity. Because this in turn depends on how much power was used by all
previous maneuvers, the Markov property is violated. We can restore the
Markov property by including the charge level ${Battery}{}_t$ as one
of the state variables that make up $\X_t$. This helps in predicting the
motion of the robot, but in turn requires a model for predicting
${Battery}{}_t$ from ${Battery}{}_{t-1}$ and the velocity. In some
cases, that can be done reliably, but more often we find that error
accumulates over time. In that case, accuracy can be improved by
*adding a new sensor* for the battery level.

Inference in Temporal Models {#markov-inference-section}
----------------------------

Having set up the structure of a generic temporal model, we can
formulate the basic inference tasks that must be solved:

: This is the task of computing the —the posterior distribution over the
most recent state—given all evidence to date. Filtering[^2] is also
called . In our example, we wish to compute $\pv(\X_t\given \e_{1:t})$.
In the umbrella example, this would mean computing the probability of
rain today, given all the observations of the umbrella carrier made so
far. Filtering is what a rational agent does to keep track of the
current state so that rational decisions can be made. It turns out that
an almost identical calculation provides the likelihood of the evidence
sequence, $P(\e_{1:t})$.

This is the task of computing the posterior distribution over the
*future* state, given all evidence to date. That is, we
wish to compute $\pv(\X_{t+k}\given \e_{1:t})$ for some $k>0$. In the
umbrella example, this might mean computing the probability of rain
three days from now, given all the observations to date. Prediction is
useful for evaluating possible courses of action based on their expected
outcomes.

: This is the task of computing the posterior distribution over a
*past* state, given all evidence up to the present. That
is, we wish to compute $\pv(\X_k\given \e_{1:t})$ for some $k$ such that
$0\leq k < t$. In the umbrella example, it might mean computing the
probability that it rained last Wednesday, given all the observations of
the umbrella carrier made up to today. Smoothing provides a better
estimate of the state than was available at the time, because it
incorporates more evidence.[^3]

Given a sequence of observations, we might wish to find the sequence of
states that is most likely to have generated those observations. That
is, we wish to compute $\argmax_{\sx_{1:t}} P(\x_{1:t}\given \e_{1:t})$.
For example, if the umbrella appears on each of the first three days and
is absent on the fourth, then the most likely explanation is that it
rained on the first three days and did not rain on the fourth.
Algorithms for this task are useful in many applications, including
speech recognition—where the aim is to find the most likely sequence of
words, given a series of sounds—and the reconstruction of bit strings
transmitted over a noisy channel.

In addition to these inference tasks, we also have

The transition and sensor models, if not yet known, can be learned from
observations. Just as with static Bayesian networks, dynamic Bayes net
learning can be done as a by-product of inference. Inference provides an
estimate of what transitions actually occurred and of what states
generated the sensor readings, and these estimates can be used to update
the models. The updated models provide new estimates, and the process
iterates to convergence. The overall process is an instance of the
expectation-maximization or . (See .)

Note that learning requires smoothing, rather than filtering, because
smoothing provides better estimates of the states of the process.
Learning with filtering can fail to converge correctly; consider, for
example, the problem of learning to solve murders: unless you are an
eyewitness, smoothing is *always* required to infer what
happened at the murder scene from the observable variables.

The remainder of this section describes generic algorithms for the four
inference tasks, independent of the particular kind of model employed.
Improvements specific to each model are described in subsequent
sections.

### Filtering and prediction {#general-filtering-section}

As we pointed out in , a useful filtering algorithm needs to maintain a
current state estimate and update it, rather than going back over the
entire history of percepts for each update. (Otherwise, the cost of each
update increases as time goes by.) In other words, given the result of
filtering up to time $t$, the agent needs to compute the result for
$t+1$ from the new evidence $\e_{t+1}$,
$$\pv(\X_{t+1}\given \e_{1:t+1}) = f(\e_{t+1},\pv(\X_t\given \e_{1:t})) \ ,$$
for some function $f$. This process is called . We can view the
calculation as being composed of two parts: first, the current state
distribution is projected forward from $t$ to $t+1$; then it is updated
using the new evidence $\e_{t+1}$. This two-part process emerges quite
simply when the formula is rearranged:

$$\begin{aligned}
\lefteqn{\pv(\X_{t+1}\given \e_{1:t+1}) = \pv(\X_{t+1}\given \e_{1:t},\e_{t+1}) 
                          \quad\mbox{(dividing up the evidence)}}\nonumber \\
&=& \alpha\, \pv(\e_{t+1}\given \X_{t+1},\e_{1:t})\,\pv(\X_{t+1}\given \e_{1:t})
                          \quad\mbox{(using Bayes' rule)}\nonumber \\
&=& \alpha\, \pv(\e_{t+1}\given \X_{t+1})\,\pv(\X_{t+1}\given \e_{1:t})
                          \quad\mbox{(by the sensor Markov assumption).}
\label{prediction-update-filtering-equation}\end{aligned}$$

Here and throughout this chapter, $\alpha$ is a normalizing constant
used to make probabilities sum up to 1. The second term,
$\pv(\X_{t+1}\given \e_{1:t})$ represents a one-step prediction of the
next state, and the first term updates this with the new evidence;
notice that $\pv(\e_{t+1}\given \X_{t+1})$ is obtainable directly from
the sensor model. Now we obtain the one-step prediction for the next
state by conditioning on the current state $\X_t$:

$$\begin{aligned}
\lefteqn{\pv(\X_{t+1}\given \e_{1:t+1}) = 
   \alpha\, \pv(\e_{t+1}\given \X_{t+1})
          \sum_{\sx_t}\pv(\X_{t+1}\given \x_t,\e_{1:t})P(\x_t\given \e_{1:t})}\nonumber\\
&=& \alpha\, \pv(\e_{t+1}\given \X_{t+1})
                \sum_{\sx_t}\pv(\X_{t+1}\given \x_t)P(\x_t\given \e_{1:t})
                          \quad\mbox{(Markov assumption).}
\label{filtering-equation}\end{aligned}$$

Within the summation, the first factor comes from the transition model
and the second comes from the current state distribution. Hence, we have
the desired recursive formulation. We can think of the filtered estimate
$\pv(\X_t\given \e_{1:t})$ as a “message” $\f_{1:t}$ that is propagated
forward along the sequence, modified by each transition and updated by
each new observation. The process is given by
$$\f_{1:t+1} = \alpha\,\noprog{Forward}(\f_{1:t},\e_{t+1}) \ ,$$ where
implements the update described in and the process begins with
$\f_{1:0} = \pv(\X_0)$[forward-operator-page]. When all the state
variables are discrete, the time for each update is constant (i.e.,
independent of $t$), and the space required is also constant. (The
constants depend, of course, on the size of the state space and the
specific type of the temporal model in question.)

The time and space requirements for updating must be constant if an
agent with limited memory is to keep track of the current state
distribution over an unbounded sequence of observations.

Let us illustrate the filtering process for two steps in the basic
umbrella example (.) That is, we will compute $\pv(R_2\given u_{1:2})$
as follows:

-   On day 0, we have no observations, only the security guard’s prior
    beliefs; let’s assume that consists of $\pv(R_{0})=\<{0.5},{0.5}\>$.

-   On day 1, the umbrella appears, so $U_1\eq {true}$. The prediction
    from $t\eq 0$ to $t\eq 1$ is

    $$\begin{aligned}
    \pv(R_1) & = & \sum_{r_{0}}\pv(R_{1}\given r_{0})P(r_{0})\\
             & = & \<{0.7},{0.3}\>\stimes {0.5} + \<{0.3},{0.7}\>\stimes {0.5} = \<{0.5},{0.5}\>\ .\end{aligned}$$

    Then the update step simply multiplies by the probability of the
    evidence for $t\eq 1$ and normalizes, as shown in :

    $$\begin{aligned}
    \pv(R_1\given u_1) & = &\alpha\, \pv(u_1\given R_1)\pv(R_1) = \alpha\, \<{0.9},{0.2}\>\<{0.5},{0.5}\>\\
                 & = &\alpha\, \<{0.45},{0.1}\> \approx \<{0.818},{0.182}\>\ .\end{aligned}$$

-   On day 2, the umbrella appears, so $U_2\eq {true}$. The prediction
    from $t\eq 1$ to $t\eq 2$ is

    $$\begin{aligned}
    \pv(R_2\given u_1) & = & \sum_{r_{1}}\pv(R_{2}\given r_{1})P(r_{1}\given u_1)\\
             & = & \<{0.7},{0.3}\>\stimes {0.818} + \<{0.3},{0.7}\>\stimes {0.182} 
               \approx \<{0.627},{0.373}\>\ ,\end{aligned}$$

    and updating it with the evidence for $t\eq 2$ gives

    $$\begin{aligned}
    \pv(R_2\given u_1,u_2) & = &\alpha\, \pv(u_2\given R_2) \pv(R_2\given u_1) 
                       = \alpha\, \<{0.9},{0.2}\>\<{0.627},{0.373}\>\\
                 & = &\alpha\, \<{0.565},{0.075}\> \approx \<{0.883},{0.117}\>\ .\end{aligned}$$

Intuitively, the probability of rain increases from day 1 to day 2
because rain persists. (a) asks you to investigate this tendency
further.

The task of can be seen simply as without the addition of new evidence.
In fact, the filtering process already incorporates a one-step
prediction, and it is easy to derive the following recursive computation
for predicting the state at $t+k+1$ from a prediction for $t+k$:

$$\pv(\X_{t+k+1}\given \e_{1:t}) = \sum_{\sx_{t+k}}\pv(\X_{t+k+1}\given \x_{t+k})P(\x_{t+k}\given \e_{1:t})\ .
\label{prediction-equation}$$

Naturally, this computation involves only the transition model and not
the sensor model.

It is interesting to consider what happens as we try to predict further
and further into the future. As (b) shows, the predicted distribution
for rain converges to a fixed point $\<{0.5},{0.5}\>$, after which it
remains constant for all time. This is the of the Markov process defined
by the transition model. (See also .) A great deal is known about the
properties of such distributions and about the —roughly, the time taken
to reach the fixed point. In practical terms, this dooms to failure any
attempt to predict the *actual* state for a number of steps
that is more than a small fraction of the mixing time, unless the
stationary distribution itself is strongly peaked in a small area of the
state space. The more uncertainty there is in the transition model, the
shorter will be the mixing time and the more the future is obscured.

In addition to filtering and prediction, we can use a forward recursion
to compute the of the evidence sequence, $P(\e_{1:t})$. This is a useful
quantity if we want to compare different temporal models that might have
produced the same evidence sequence (e.g., two different models for the
persistence of rain). For this recursion, we use a likelihood message
$\bell_{1:t}(\X_t)\eq
\pv(\X_t,\e_{1:t})$. It is a simple exercise to show that the message
calculation is identical to that for filtering:
$$\bell_{1:t+1} = \noprog{Forward}(\bell_{1:t},\e_{t+1})\ .$$ Having
computed $\bell_{1:t}$, we obtain the actual likelihood by summing out
$\X_t$:

$$L_{1:t} = P(\e_{1:t}) = \sum_{\sx_t} \bell_{1:t}(\x_t)\ .
\label{forward-likelihood-equation}$$

Notice that the likelihood message represents the probabilities of
longer and longer evidence sequences as time goes by and so becomes
numerically smaller and smaller, leading to underflow problems with
floating-point arithmetic. This is an important problem in practice, but
we shall not go into solutions here.

### Smoothing

[smoothing-dbn-figure]

As we said earlier, smoothing is the process of computing the
distribution over past states given evidence up to the present; that is,
$\pv(\X_k\given \e_{1:t})$ for $0\leq k < t$. (See .) In anticipation of
another recursive message-passing approach, we can split the computation
into two parts—the evidence up to $k$ and the evidence from $k+1$ to
$t$,

$$\begin{aligned}
\pv(\X_k\given \e_{1:t}) & = & \pv(\X_k\given \e_{1:k},\e_{k+1:t})\nonumber\\
   & = & \alpha\, \pv(\X_k\given \e_{1:k}) \pv(\e_{k+1:t}\given \X_k,\e_{1:k}) 
                     \quad\mbox{(using Bayes' rule)}\nonumber\\
   & = & \alpha\, \pv(\X_k\given \e_{1:k}) \pv(\e_{k+1:t}\given \X_k) 
                     \quad\mbox{(using conditional independence)}\nonumber\\
   & = & \alpha\, \f_{1:k} \stimes \b_{k+1:t} \ .
\label{smoothing-equation}\end{aligned}$$

where “${\times}$” represents pointwise multiplication of vectors. Here
we have defined a “backward” message $\b_{k+1:t}\eq
\pv(\e_{k+1:t}\given \X_k)$, analogous to the forward message
$\f_{1:k}$. The forward message $\f_{1:k}$ can be computed by filtering
forward from 1 to $k$, as given by . It turns out that the backward
message $\b_{k+1:t}$ can be computed by a recursive process that runs
*backward* from $t$:

$$\begin{aligned}
\pv(\e_{k+1:t}\given \X_k) 
   & = & \sum_{\sx_{k+1}}\pv(\e_{k+1:t}\given \X_k,\x_{k+1})\pv(\x_{k+1}\given \X_k)  
                     \quad\mbox{(conditioning on \(\X_{k+1}\))}\nonumber\\
   & = & \sum_{\sx_{k+1}}P(\e_{k+1:t}\given \x_{k+1})\pv(\x_{k+1}\given \X_k)  
                     \quad\mbox{(by conditional independence)}\nonumber\\
   & = & \sum_{\sx_{k+1}}P(\e_{k+1},\e_{k+2:t}\given \x_{k+1})\pv(\x_{k+1}\given \X_k) 
                     \nonumber\\
   & = & \sum_{\sx_{k+1}}
           P(\e_{k+1}\given \x_{k+1})P(\e_{k+2:t}\given \x_{k+1})\pv(\x_{k+1}\given \X_k)\ ,
\label{backward-equation}\end{aligned}$$

where the last step follows by the of $\e_{k+1}$ and $\e_{k+2:t}$, given
$\X_{k+1}$. Of the three factors in this summation, the first and third
are obtained directly from the model, and the second is the “recursive
call.” Using the message notation, we have
$$\b_{k+1:t} = \noprog{Backward}(\b_{k+2:t},\e_{k+1}) \ ,$$ where
implements the update described in . As with the forward recursion, the
time and space needed for each update are constant and thus independent
of $t$.

We can now see that the two terms in can both be computed by recursions
through time, one running forward from $1$ to $k$ and using the
filtering equation ([filtering-equation]) and the other running backward
from $t$ to $k+1$ and using . Note that the backward phase is
initialized with
$\b_{t+1:t}\eq \pv(\e_{t+1:t}\given \X_t)\eq \pv(\ \given \X_t)\ones$,
where $\ones$ is a vector of 1s. (Because $\e_{t+1:t}$ is an empty
sequence, the probability of observing it is 1.)

Let us now apply this algorithm to the umbrella example, computing the
smoothed estimate for the probability of rain at time $k\eq 1$, given
the umbrella observations on days 1 and 2. From , this is given by

$$\pv(R_1\given u_1,u_2) = 
           \alpha \, \pv(R_1\given u_1) \, \pv(u_2\given R_1) \ .
\label{umbrella-smoothing-equation}$$

The first term we already know to be $\<.{818},.{182}\>$, from the
forward filtering process described earlier. The second term can be
computed by applying the backward recursion in :

$$\begin{aligned}
\pv(u_2\given R_1) 
  &=& \sum_{r_2} P(u_2\given r_2) P(\ \given r_2) \pv(r_2\given R_1) \\
  &=& ({0.9}\stimes 1 \stimes \<{0.7},{0.3}\>) + ({0.2}\stimes 1 \stimes \<{0.3},{0.7}\>)
      = \<{0.69},{0.41}\>\ .\end{aligned}$$

Plugging this into , we find that the smoothed estimate for rain on day
1 is $$\pv(R_1\given u_1,u_2) 
    = \alpha \, \<{0.818},{0.182}\> \stimes  \<{0.69},{0.41}\> 
    \approx \, \<{0.883},{0.117}\>\ .$$ Thus, the smoothed estimate for
rain on day 1 is *higher* than the filtered estimate
(0.818) in this case. This is because the umbrella on day 2 makes it
more likely to have rained on day 2; in turn, because rain tends to
persist, that makes it more likely to have rained on day 1.

Both the forward and backward recursions take a constant amount of time
per step; hence, the time complexity of smoothing with respect to
evidence $\e_{1:t}$ is $O(t)$. This is the complexity for smoothing at a
particular time step $k$. If we want to smooth the whole sequence, one
obvious method is simply to run the whole smoothing process once for
each time step to be smoothed. This results in a time complexity of
$O(t^2)$. A better approach uses a simple application of to reduce the
complexity to $O(t)$. A clue appears in the preceding analysis of the
umbrella example, where we were able to reuse the results of the
forward-filtering phase. The key to the linear-time algorithm is to
*record the results* of forward filtering over the whole
sequence. Then we run the backward recursion from $t$ down to 1,
computing the smoothed estimate at each step $k$ from the computed
backward message $\b_{k+1:t}$ and the stored forward message $\f_{1:k}$.
The algorithm, aptly called the , is shown in .

[forward-backward-algorithm]

The alert reader will have spotted that the Bayesian network structure
shown in is a *polytree* as defined on . This means that a
straightforward application of the clustering algorithm also yields a
linear-time algorithm that computes smoothed estimates for the entire
sequence. It is now understood that the forward–backward algorithm is in
fact a special case of the polytree propagation algorithm used with
clustering methods (although the two were developed independently).

The forward–backward algorithm forms the computational backbone for many
applications that deal with sequences of noisy observations. As
described so far, it has two practical drawbacks. The first is that its
space complexity can be too high when the state space is large and the
sequences are long. It uses $O(|\f|t)$ space where $|\f|$ is the size of
the representation of the forward message. The space requirement can be
reduced to $O(|\f|\log t)$ with a concomitant increase in the time
complexity by a factor of $\log t$, as shown in . In some cases (see ),
a constant-space algorithm can be used.

The second drawback of the basic algorithm is that it needs to be
modified to work in an *online* setting where smoothed
estimates must be computed for earlier time slices as new observations
are continuously added to the end of the sequence. The most common
requirement is for , which requires computing the smoothed estimate
$\pv(\X_{t-d}\given \e_{1:t})$ for fixed $d$. That is, smoothing is done
for the time slice $d$ steps behind the current time $t$; as $t$
increases, the smoothing has to keep up. Obviously, we can run the
forward–backward algorithm over the $d$-step “window” as each new
observation is added, but this seems inefficient. In , we will see that
fixed-lag smoothing can, in some cases, be done in constant time per
update, independent of the lag $d$.

### Finding the most likely sequence {#viterbi-algorithm-section}

Suppose that $[{true},{true},{false},{true},{true}]$ is the
umbrella sequence for the security guard’s first five days on the job.
What is the weather sequence most likely to explain this? Does the
absence of the umbrella on day 3 mean that it wasn’t raining, or did the
director forget to bring it? If it didn’t rain on day 3, perhaps
(because weather tends to persist) it didn’t rain on day 4 either, but
the director brought the umbrella just in case. In all, there are $2^5$
possible weather sequences we could pick. Is there a way to find the
most likely one, short of enumerating all of them?

We could try this linear-time procedure: use smoothing to find the
posterior distribution for the weather at each time step; then construct
the sequence, using at each step the weather that is most likely
according to the posterior. Such an approach should set off alarm bells
in the reader’s head, because the posterior distributions computed by
smoothing are distributions over *single* time steps,
whereas to find the most likely *sequence* we must consider
*joint* probabilities over all the time steps. The results
can in fact be quite different. ([flawed-viterbi-page]See .)

There *is* a linear-time algorithm for finding the most
likely sequence, but it requires a little more thought. It relies on the
same Markov property that yielded efficient algorithms for filtering and
smoothing. The easiest way to think about the problem is to view each
sequence as a *path* through a graph whose nodes are the
possible *states* at each time step. Such a graph is shown
for the umbrella world in (a). Now consider the task of finding the most
likely path through this graph, where the likelihood of any path is the
product of the transition probabilities along the path and the
probabilities of the given observations at each state. Let’s focus in
particular on paths that reach the state ${Rain}{}_5\eq {true}$.
Because of the Markov property, it follows that the most likely path to
the state ${Rain}{}_5\eq {true}$ consists of the most likely path to
*some* state at time 4 followed by a transition to
${Rain}{}_5\eq
{true}$; and the state at time 4 that will become part of the path to
${Rain}{}_5\eq {true}$ is whichever maximizes the likelihood of that
path. In other words,

there is a recursive relationship between most likely paths to each
state $\x_{t+1}$ and most likely paths to each state $\x_t$.

[umbrella-paths-figure]

We can write this relationship as an equation connecting the
probabilities of the paths:

$$\begin{aligned}
\lefteqn{\max_{\sx_1\ldots\sx_t} \pv(\x_1,\ldots,\x_t,\X_{t+1} \given  \e_{1:t+1})}
                                                   \nonumber\\
   & = &  \alpha\, \pv(\e_{t+1} \given  \X_{t+1})
        \max_{\sx_t}\left( 
            \pv(\X_{t+1} \given  \x_t) 
            \max_{\sx_1\ldots\sx_{t-1}} P(\x_1,\ldots,\x_{t-1},\x_t \given  \e_{1:t})
        \right)\ .
\label{viterbi-equation}\end{aligned}$$

is *identical* to the filtering
equation ([filtering-equation]) except that

1.  The forward message $\f_{1:t}\eq \pv(\X_t\given \e_{1:t})$ is
    replaced by the message
    $$\m_{1:t} = \max_{\sx_1\ldots\sx_{t-1}} \pv(\x_1,\ldots,\x_{t-1},\X_t \given  \e_{1:t})\ ,$$
    that is, the probabilities of the most likely path to each state
    $\x_t$; and

2.  the summation over $\x_t$ in is replaced by the maximization over
    $\x_t$ in .

Thus, the algorithm for computing the most likely sequence is similar to
filtering: it runs forward along the sequence, computing the $\m$
message at each time step, using . The progress of this computation is
shown in (b). At the end, it will have the probability for the most
likely sequence reaching *each* of the final states. One
can thus easily select the most likely sequence overall (the states
outlined in bold). In order to identify the actual sequence, as opposed
to just computing its probability, the algorithm will also need to
record, for each state, the best state that leads to it; these are
indicated by the bold arrows in (b). The optimal sequence is identified
by following these bold arrows backwards from the best final state.

The algorithm we have just described is called the , after its inventor.
Like the filtering algorithm, its time complexity is linear in $t$, the
length of the sequence. Unlike filtering, which uses constant space, its
space requirement is also linear in $t$. This is because the Viterbi
algorithm needs to keep the pointers that identify the best sequence
leading to each state.

Hidden Markov Models {#hmm-section}
--------------------

The preceding section developed algorithms for temporal probabilistic
reasoning using a general framework that was independent of the specific
form of the transition and sensor models. In this and the next two
sections, we discuss more concrete models and applications that
illustrate the power of the basic algorithms and in some cases allow
further improvements.

We begin with the , or . An HMM is a temporal probabilistic model in
which the state of the process is described by a *single*
*discrete* random variable. The possible values of the
variable are the possible states of the world. The umbrella example
described in the preceding section is therefore an HMM, since it has
just one state variable: ${Rain}{}_t$. What happens if you have a
model with two or more state variables? You can still fit it into the
HMM framework by combining the variables into a single “” whose values
are all possible tuples of values of the individual state variables. We
will see that the restricted structure of HMMs allows for a simple and
elegant matrix implementation of all the basic algorithms.[^4]

### Simplified matrix algorithms

With a single, discrete state variable $X_t$, we can give concrete form
to the representations of the transition model, the sensor model, and
the forward and backward messages. Let the state variable $X_t$ have
values denoted by integers $1,\ldots,S$, where $S$ is the number of
possible states. The transition model $\pv(X_t\given X_{t-1})$ becomes
an $S\stimes S$ matrix $\T$, where
$$\T_{ij} = P(X_t\eq j\given X_{t-1}\eq i)\ .$$ That is, $\T_{ij}$ is
the probability of a transition from state $i$ to state $j$. For
example, the transition matrix for the umbrella world is
$$\T = \pv(X_t\given X_{t-1}) = \left(\begin{array}{cc}{0.7} & {0.3} \\ 
                                                 {0.3} & {0.7} \end{array}\right)\ .$$
We also put the sensor model in matrix form. In this case, because the
value of the evidence variable $E_t$ is known at time $t$ (call it
$e_t$), we need only specify, for each state, how likely it is that the
state causes $e_t$ to appear: we need $P(e_t \given X_t\eq i)$ for each
state $i$. For mathematical convenience we place these values into an
$S \times S$ diagonal matrix, $\O_t$ whose $i$th diagonal entry is
$P(e_t\given X_t\eq i)$ and whose other entries are 0. For example, on
day 1 in the umbrella world of , $U_1\eq {true}$, and on day 3,
$U_3\eq {false}$, so, from , we have
$$\O_1 = \left(\begin{array}{cc}{0.9} & 0 \\ 0 & {0.2}\end{array}\right)
; \qquad
   \O_3 = \left(\begin{array}{cc}{0.1} & 0 \\ 0 & {0.8}\end{array}\right)
 \ .$$ Now, if we use column vectors to represent the forward and
backward messages, all the computations become simple matrix–vector
operations. The forward equation ([filtering-equation]) becomes

$$\f_{1:t+1} = \alpha\, \O_{t+1} \T\transpose \f_{1:t}
\label{matrix-filtering-equation}$$

and the backward equation ([backward-equation]) becomes

$$\b_{k+1:t} = \T \O_{k+1} \b_{k+2:t}\ .
\label{matrix-backward-equation}$$

From these equations, we can see that the time complexity of the
forward–backward algorithm () applied to a sequence of length $t$ is
$O(S^2 t)$, because each step requires multiplying an $S$-element vector
by an $S\stimes S$ matrix. The space requirement is $O(St)$, because the
forward pass stores $t$ vectors of size $S$.

Besides providing an elegant description of the filtering and smoothing
algorithms for HMMs, the matrix formulation reveals opportunities for
improved algorithms. The first is a simple variation on the
forward–backward algorithm that allows smoothing to be carried out in
*constant* space, independently of the length of the
sequence. The idea is that smoothing for any particular time slice $k$
requires the simultaneous presence of both the forward and backward
messages, $\f_{1:k}$ and $\b_{k+1:t}$, according to . The
forward–backward algorithm achieves this by storing the $\f$s computed
on the forward pass so that they are available during the backward pass.
Another way to achieve this is with a single pass that propagates both
$\f$ and $\b$ in the same direction. For example, the “forward” message
$\f$ can be propagated backward if we manipulate to work in the other
direction:
$$\f_{1:t} = \alpha' (\T\transpose)^{-1} \O_{t+1}^{-1} \f_{1:t+1}\ .$$
The modified smoothing algorithm works by first running the standard
forward pass to compute $\f_{t:t}$ (forgetting all the intermediate
results) and then running the backward pass for both $\b$ and $\f$
together, using them to compute the smoothed estimate at each step.
Since only one copy of each message is needed, the storage requirements
are constant (i.e., independent of $t$, the length of the sequence).
There are two significant restrictions on this algorithm: it requires
that the transition matrix be invertible and that the sensor model have
no zeroes—that is, that every observation be possible in every state.

[fixed-lag-algorithm]

A second area in which the matrix formulation reveals an improvement is
in *online* smoothing with a fixed lag. The fact that
smoothing can be done in constant space suggests that there should exist
an efficient recursive algorithm for online smoothing—that is, an
algorithm whose time complexity is independent of the length of the lag.
Let us suppose that the lag is $d$; that is, we are smoothing at time
slice $t\sminus{d}$, where the current time is $t$. By , we need to
compute $$\alpha\, \f_{1:t-d}\stimes \b_{t-d+1:t}$$ for slice $t-d$.
Then, when a new observation arrives, we need to compute
$$\alpha\, \f_{1:t-d+1}\stimes \b_{t-d+2:t+1}$$ for slice $t-d+1$. How
can this be done incrementally? First, we can compute $\f_{1:t-d+1}$
from $\f_{1:t-d}$, using the standard filtering process, .

Computing the backward message incrementally is trickier, because there
is no simple relationship between the old backward message
$\b_{t-d+1:t}$ and the new backward message $\b_{t-d+2:t+1}$. Instead,
we will examine the relationship between the old backward message
$\b_{t-d+1:t}$ and the backward message at the front of the sequence,
$\b_{t+1:t}$. To do this, we apply $d$ times to get

$$\b_{t-d+1:t} = \left(\prod_{i\eq t-d+1}^t \T \O_i\right) \b_{t+1:t} 
             = \B_{t-d+1:t} \ones\ ,
\label{backward-transform1-equation}$$

where the matrix $\B_{t-d+1:t}$ is the product of the sequence of $\T$
and $\O$ matrices. $\B$ can be thought of as a “transformation operator”
that transforms a later backward message into an earlier one. A similar
equation holds for the new backward messages *after* the
next observation arrives:

$$\b_{t-d+2:t+1} = \left(\prod_{i\eq t-d+2}^{t+1} \T \O_i\right) \b_{t+2:t+1} 
             = \B_{t-d+2:t+1} \ones\ .
\label{backward-transform2-equation}$$

Examining the product expressions in
Equations ([backward-transform1-equation])
and ([backward-transform2-equation]), we see that they have a simple
relationship: to get the second product, “divide” the first product by
the first element $\T\O_{t-d+1}$, and multiply by the new last element
$\T\O_{t+1}$. In matrix language, then, there is a simple relationship
between the old and new $\B$ matrices:

$$\B_{t-d+2:t+1} = \O_{t-d+1}^{-1} \T^{-1} \B_{t-d+1:t} \T\O_{t+1}\ .
\label{backward-transform-equation}$$

This equation provides an incremental update for the $\B$ matrix, which
in turn (through ) allows us to compute the new backward message
$\b_{t-d+2:t+1}$. The complete algorithm, which requires storing and
updating $\f$ and $\B$, is shown in .

### Hidden Markov model example: Localization {#hmm-localization-section}

On , we introduced a simple form of the problem for the vacuum world. In
that version, the robot had a single nondeterministic action and its
sensors reported perfectly whether or not obstacles lay immediately to
the north, south, east, and west; the robot’s belief state was the set
of possible locations it could be in.

Here we make the problem slightly more realistic by including a simple
probability model for the robot’s motion and by allowing for noise in
the sensors. The state variable $X_t$ represents the location of the
robot on the discrete grid; the domain of this variable is the set of
empty squares $\{s_1, \ldots, s_n\}$. Let $(s)$ be the set of empty
squares that are adjacent to $s$ and let $N(s)$ be the size of that set.
Then the transition model for action says that the robot is equally
likely to end up at any neighboring square:
$$P(X_{t+1}\eq j \given X_t\eq i) = \T_{ij} = (1/N(i) \mbox{~if~} j \in \noprog{Neighbors}(i) \mbox{~else~} 0) \ .$$
We don’t know where the robot starts, so we will assume a uniform
distribution over all the squares; that is, $P(X_0 \eq i) \eq 1/n$. For
the particular environment we consider (), $n\eq
42$ and the transition matrix $\T$ has $42\stimes 42\eq 1764$ entries.

The sensor variable $E_t$ has 16 possible values, each a four-bit
sequence giving the presence or absence of an obstacle in a particular
compass direction. We will use the notation $NS$, for example, to mean
that the north and south sensors report an obstacle and the east and
west do not. Suppose that each sensor’s error rate is $\epsilon$ and
that errors occur independently for the four sensor directions. In that
case, the probability of getting all four bits right is $(1-\epsilon)^4$
and the probability of getting them all wrong is $\epsilon^4$.
Furthermore, if $d_{it}$ is the discrepancy—the number of bits that are
different—between the true values for square $i$ and the actual reading
$e_t$, then the probability that a robot in square $i$ would receive a
sensor reading $e_t$ is
$$P(E_t\eq e_t \given X_t\eq i) = \O_{t_{ii}} = (1-\epsilon)^{4-d_{it}}
\epsilon^{d_{it}}\ .$$ For example, the probability that a square with
obstacles to the north and south would produce a sensor reading
${NSE}$ is $(1-\epsilon)^3 \epsilon^1$.

[vacuum-maze-hmm2-figure]

Given the matrices $\T$ and $\O_t$, the robot can use to compute the
posterior distribution over locations—that is, to work out where it is.
shows the distributions $\pv(X_1\given E_1\eq NSW)$ and
$\pv(X_2\given E_1\eq NSW, E_2\eq
NS)$. This is the same maze we saw before in (), but there we used
logical filtering to find the locations that were
*possible*, assuming perfect sensing. Those same locations
are still the most *likely* with noisy sensing, but now
*every* location has some nonzero probability.

[hmm-localization-figure]

In addition to filtering to estimate its current location, the robot can
use smoothing () to work out where it was at any given past time—for
example, where it began at time 0—and it can use the Viterbi algorithm
to work out the most likely path it has taken to get where it is now.
shows the localization error and Viterbi path accuracy for various
values of the per-bit sensor error rate $\epsilon$. Even when $\epsilon$
is 20%—which means that the overall sensor reading is wrong 59% of the
time—the robot is usually able to work out its location within two
squares after 25 observations. This is because of the algorithm’s
ability to integrate evidence over time and to take into account the
probabilistic constraints imposed on the location sequence by the
transition model. When $\epsilon$ is 10%, the performance after a
half-dozen observations is hard to distinguish from the performance with
perfect sensing. asks you to explore how robust the HMM localization
algorithm is to errors in the prior distribution $\pv(X_0)$ and in the
transition model itself. Broadly speaking, high levels of localization
and path accuracy are maintained even in the face of substantial errors
in the models used.

The state variable for the example we have considered in this section is
a physical location in the world. Other problems can, of course, include
other aspects of the world. asks you to consider a version of the vacuum
robot that has the policy of going straight for as long as it can; only
when it encounters an obstacle does it change to a new (randomly
selected) heading. To model this robot, each state in the model consists
of a *(location, heading)* pair. For the environment in ,
which has 42 empty squares, this leads to 168 states and a transition
matrix with $168^2\eq 28,224$ entries—still a manageable number. If we
add the possibility of dirt in the squares, the number of states is
multiplied by $2^{42}$ and the transition matrix ends up with more than
$10^{29}$ entries—no longer a manageable number; shows how to use
dynamic Bayesian networks to model domains with many state variables. If
we allow the robot to move continuously rather than in a discrete grid,
the number of states becomes infinite; the next section shows how to
handle this case.

Kalman Filters {#kalman-filter-section}
--------------

Imagine watching a small bird flying through dense jungle foliage at
dusk: you glimpse brief, intermittent flashes of motion; you try hard to
guess where the bird is and where it will appear next so that you don’t
lose it. Or imagine that you are a World War II radar operator peering
at a faint, wandering blip that appears once every 10 seconds on the
screen. Or, going back further still, imagine you are Kepler trying to
reconstruct the motions of the planets from a collection of highly
inaccurate angular observations taken at irregular and imprecisely
measured intervals. In all these cases, you are doing filtering:
estimating state variables (here, position and velocity) from noisy
observations over time. If the variables were discrete, we could model
the system with a hidden Markov model. This section examines methods for
handling continuous variables, using an algorithm called , after one of
its inventors, Rudolf E. Kalman.

The bird’s flight might be specified by six continuous variables at each
time point; three for position $(X_t,Y_t,Z_t)$ and three for velocity
$(\dot X_t,\dot Y_t,\dot Z_t)$. We will need suitable conditional
densities to represent the transition and sensor models; as in , we will
use distributions. This means that the next state $\X_{t+1}$ must be a
linear function of the current state $\X_t$, plus some Gaussian noise, a
condition that turns out to be quite reasonable in practice. Consider,
for example, the $X$-coordinate of the bird, ignoring the other
coordinates for now. Let the time interval between observations be
$\Delta$, and assume constant velocity during the interval; then the
position update is given by $ X_{t+\Delta} = X_t +  \dot X \, \Delta$.
Adding Gaussian noise (to account for wind variation, etc.), we obtain a
linear Gaussian transition model:
$$P(X_{t+\Delta}\eq x_{t+\Delta} \given  X_t\eq x_t, \dot X_t\eq \dot x_t) = N(x_t +  \dot x_t \, \Delta,\sigma^2)(x_{t+\Delta})\ .$$
The Bayesian network structure for a system with position vector $\X_t$
and velocity ${\dot{\X}}_t$ is shown in . Note that this is a very
specific form of linear Gaussian model; the general form will be
described later in this section and covers a vast array of applications
beyond the simple motion examples of the first paragraph. The reader
might wish to consult for some of the mathematical properties of
Gaussian distributions; for our immediate purposes, the most important
is that a distribution for $d$ variables is specified by a $d$-element
mean $\mean$ and a $d\stimes d$ covariance matrix $\covariance$.

[kalman-network-figure]

### Updating Gaussian distributions

In on , we alluded to a key property of the linear Gaussian family of
distributions: it remains closed under the standard Bayesian network
operations. Here, we make this claim precise in the context of filtering
in a temporal probability model. The required properties correspond to
the two-step filtering calculation in :

1.  If the current distribution $\pv(\X_t\given \e_{1:t})$ is Gaussian
    and the transition model $\pv(\X_{t+1}\given \x_t)$ is linear
    Gaussian, then the one-step predicted distribution given by

    $$\pv(\X_{t+1}\given \e_{1:t}) =
      \int_{\sx_t}\pv(\X_{t+1}\given \x_t)P(\x_t\given \e_{1:t}) \,d\x_t
    \label{kalman-prediction-equation}$$

    is also a Gaussian distribution.

2.  If the prediction $\pv(\X_{t+1}\given \e_{1:t})$ is Gaussian and the
    sensor model $\pv(\e_{t+1}\given \X_{t+1})$ is linear Gaussian,
    then, after conditioning on the new evidence, the updated
    distribution

    $$\pv(\X_{t+1}\given \e_{1:t+1}) =
      \alpha\, \pv(\e_{t+1}\given \X_{t+1}) \pv(\X_{t+1}\given \e_{1:t})
    \label{kalman-estimation-equation}$$

    is also a Gaussian distribution.

Thus, the $\prog{Forward}$ operator for Kalman filtering takes a
Gaussian forward message $\f_{1:t}$, specified by a mean $\mean_t$ and
covariance matrix $\covariance_t$, and produces a new multivariate
Gaussian forward message $\f_{1:t+1}$, specified by a mean $\mean_{t+1}$
and covariance matrix $\covariance_{t+1}$. So, if we start with a
Gaussian prior $\f_{1:0}
\eq \pv(\X_0) \eq N(\mean_0,\covariance_0)$, filtering with a linear
Gaussian model produces a Gaussian state distribution for all time.

This seems to be a nice, elegant result, but why is it so important? The
reason is that, except for a few special cases such as this,

filtering with continuous or hybrid (discrete and continuous) networks
generates state distributions whose representation grows without bound
over time.

This statement is not easy to prove in general, but shows what happens
for a simple example.

### A simple one-dimensional example

We have said that the operator for the Kalman filter maps a Gaussian
into a new Gaussian. This translates into computing a new mean and
covariance matrix from the previous mean and covariance matrix. Deriving
the update rule in the general (multivariate) case requires rather a lot
of linear algebra, so we will stick to a very simple univariate case for
now; and later give the results for the general case. Even for the
univariate case, the calculations are somewhat tedious, but we feel that
they are worth seeing because the usefulness of the Kalman filter is
tied so intimately to the mathematical properties of Gaussian
distributions.

The temporal model we consider describes a of a single continuous state
variable $X_t$ with a noisy observation $Z_t$. An example might be the
“consumer confidence” index, which can be modeled as undergoing a random
Gaussian-distributed change each month and is measured by a random
consumer survey that also introduces Gaussian sampling noise. The prior
distribution is assumed to be Gaussian with variance $\sigma_0^2$:
$$P(x_0) = \alpha\, e^{-\frac{1}{2}\left(\frac{(x_0-\mu_0)^2}{\sigma_0^2}\right)}\ .$$
(For simplicity, we use the same symbol $\alpha$ for all normalizing
constants in this section.) The transition model adds a Gaussian
perturbation of constant variance $\sigma_x^2$ to the current state:
$$P(x_{t+1}\given x_t) = 
    \alpha\, e^{-\frac{1}{2}\left(\frac{(x_{t+1}-x_t)^2}{\sigma_x^2}\right)}\ .$$
The sensor model assumes Gaussian noise with variance $\sigma_z^2$:
$$P(z_t\given x_t) = 
    \alpha\, e^{-\frac{1}{2}\left(\frac{(z_t-x_t)^2}{\sigma_z^2}\right)}\ .$$
Now, given the prior $\pv(X_0)$, the one-step predicted distribution
comes from :

$$\begin{aligned}
  P(x_1) &=& \int_{-\infty}^{\infty} P(x_1\given x_0)P(x_0) \,d x_0
         = \alpha\, \int_{-\infty}^{\infty} 
              e^{-\frac{1}{2}\left(\frac{(x_{1}-x_0)^2}{\sigma_x^2}\right)}
              e^{-\frac{1}{2}\left(\frac{(x_0-\mu_0)^2}{\sigma_0^2}\right)}\,d x_0\\
         &=& \alpha\, \int_{-\infty}^{\infty} 
              e^{-\frac{1}{2}\left(
                 \frac{\sigma_0^2(x_{1}-x_0)^2 + \sigma_x^2(x_0-\mu_0)^2}
                      {\sigma_0^2\sigma_x^2}\right)}\,d x_0\ .\end{aligned}$$

This integral looks rather complicated. The key to progress is to notice
that the exponent is the sum of two expressions that are
*quadratic* in $x_0$ and hence is itself a quadratic in
$x_0$. A simple trick known as allows the rewriting of any quadratic
$ax{}_0^2 + bx{}_0 + c$ as the sum of a squared term
$a(x_0-\frac{-b}{2a})^2$ and a residual term $c-\frac{b^2}{4a}$ that is
independent of $x_0$. The residual term can be taken outside the
integral, giving us
$$P(x_1) = \alpha\, e^{-\frac{1}{2}\left(c-\frac{b^2}{4a}\right)}
             \int_{-\infty}^{\infty}
                  e^{-\frac{1}{2}\left(a(x_0-\frac{-b}{2a})^2\right)}\,d x_0\ .$$
Now the integral is just the integral of a Gaussian over its full range,
which is simply 1. Thus, we are left with only the residual term from
the quadratic. Then, we notice that the residual term is a quadratic in
$x_1$; in fact, after simplification, we obtain $$P(x_1) = \alpha\,
    e^{-\frac{1}{2}\left(\frac{(x_1-\mu_0)^2}{\sigma_0^2+\sigma_x^2}\right)}\ .$$
That is, the one-step predicted distribution is a Gaussian with the same
mean $\mu_0$ and a variance equal to the sum of the original variance
$\sigma_0^2$ and the transition variance $\sigma_x^2$.

To complete the update step, we need to condition on the observation at
the first time step, namely, $z_1$. From , this is given by

$$\begin{aligned}
  P(x_1\given z_1) &=& \alpha\, P(z_1\given x_1)P(x_1)\\
             &=& \alpha\, e^{-\frac{1}{2}\left(\frac{(z_1-x_1)^2}{\sigma_z^2}\right)}
                  e^{-\frac{1}{2}\left(\frac{(x_1-\mu_0)^2}{\sigma_0^2+\sigma_x^2}\right)}\ .\end{aligned}$$

Once again, we combine the exponents and complete the square (),
obtaining

$$P(x_1\given z_1) = 
    \alpha\, e^{-\frac{1}{2}\left(
      \frac{(x_1-\frac{(\sigma_0^2+\sigma_x^2)z_1+
                       \sigma_z^2\mu_0}
                      {\sigma_0^2+\sigma_x^2+\sigma_z^2})^2}
           {(\sigma_0^2+\sigma_x^2)\sigma_z^2/
            (\sigma_0^2+\sigma_x^2+\sigma_z^2)}\right)}\ .
\label{kalman-one-step-equation}$$

Thus, after one update cycle, we have a new Gaussian distribution for
the state variable.

[kalman-one-step-figure]

From the Gaussian formula in , we see that the new mean and standard
deviation can be calculated from the old mean and standard deviation as
follows:

$$\mu_{t+1} = \frac{(\sigma_t^2+\sigma_x^2)z_{t+1} + \sigma_z^2\mu_t}
                   {\sigma_t^2+\sigma_x^2+\sigma_z^2} \qquad \mbox{and} \qquad 
\sigma_{t+1}^2 = \frac{(\sigma_t^2+\sigma_x^2)\sigma_z^2}
                        {\sigma_t^2+\sigma_x^2+\sigma_z^2}\ .
\label{kalman-univariate-equation}$$

shows one update cycle for particular values of the transition and
sensor models.

plays exactly the same role as the general filtering
equation ([filtering-equation]) or the HMM filtering
equation ([matrix-filtering-equation]). Because of the special nature of
Gaussian distributions, however, the equations have some interesting
additional properties. First, we can interpret the calculation for the
new mean $\mu_{t+1}$ as simply a *weighted mean* of the new
observation $z_{t+1}$ and the old mean $\mu_t$. If the observation is
unreliable, then $\sigma_z^2$ is large and we pay more attention to the
old mean; if the old mean is unreliable ($\sigma_t^2$ is large) or the
process is highly unpredictable ($\sigma_x^2$ is large), then we pay
more attention to the observation. Second, notice that the update for
the variance $\sigma_{t+1}^2$ is *independent of the
observation*. We can therefore compute in advance what the
sequence of variance values will be. Third, the sequence of variance
values converges quickly to a fixed value that depends only on
$\sigma_x^2$ and $\sigma_z^2$, thereby substantially simplifying the
subsequent calculations. (See .)

### The general case

The preceding derivation illustrates the key property of Gaussian
distributions that allows Kalman filtering to work: the fact that the
exponent is a quadratic form. This is true not just for the univariate
case; the full multivariate Gaussian distribution has the form
$$N(\mean,\covariance)(\x) = \alpha\, e^{-\frac{1}{2}\left(
          (\x-\mean)\transpose \covariance^{-1} (\x-\mean)\right)}\ .$$
Multiplying out the terms in the exponent makes it clear that the
exponent is also a quadratic function of the values $x_i$ in $\x$. As in
the univariate case, the filtering update preserves the Gaussian nature
of the state distribution.

Let us first define the general temporal model used with Kalman
filtering. Both the transition model and the sensor model allow for a
*linear* transformation with additive Gaussian noise. Thus,
we have

$$\begin{array}{rcl}
  P(\x_{t+1}\given \x_t) & = & N(\kftm \x_t,\kftv)(\x_{t+1}) \\
  P(\z_t\given \x_t) & = & N(\kfsm \x_t,\kfsv)(\z_t) \ ,
\end{array}
\label{kalman-linear-system-equation}$$

where $\kftm$ and $\kftv$ are matrices describing the linear transition
model and transition noise covariance, and $\kfsm$ and $\kfsv$ are the
corresponding matrices for the sensor model. Now the update equations
for the mean and covariance, in their full, hairy horribleness, are

$$\begin{array}{rcl}
\mean_{t+1} &=& \kftm\mean_t + \kfgm_{t+1}(\z_{t+1} - \kfsm\kftm\mean_t)\\
\covariance_{t+1} &=& (\I-\kfgm_{t+1}\kfsm)(\kftm\covariance_t\kftm\transpose+\kftv)\ ,
\end{array}
\label{kalman-update-equation}$$

where $\kfgm_{t+1}\eq (\kftm\covariance_t\kftm\transpose+\kftv)
\kfsm\transpose(\kfsm(\kftm\covariance_t\kftm\transpose+\kftv)\kfsm\transpose +\kfsv)^{-1}$
is called the . Believe it or not, these equations make some intuitive
sense. For example, consider the update for the mean state estimate
$\mean$. The term $\kftm\mean_t$ is the *predicted* state
at $t+1$, so $\kfsm\kftm\mean_t$ is the *predicted*
observation. Therefore, the term $\z_{t+1} -
\kfsm\kftm\mean_t$ represents the error in the predicted observation.
This is multiplied by $\kfgm_{t+1}$ to correct the predicted state;
hence, $\kfgm_{t+1}$ is a measure of *how seriously to take the
new observation* relative to the prediction. As in , we also have
the property that the variance update is independent of the
observations. The sequence of values for $\covariance_t$ and $\kfgm_t$
can therefore be computed offline, and the actual calculations required
during online tracking are quite modest.

To illustrate these equations at work, we have applied them to the
problem of tracking an object moving on the $X$–$Y$ plane. The state
variables are $\X = (X,Y,\dot X,\dot Y)\transpose$, so $\kftm$, $\kftv$,
$\kfsm$, and $\kfsv$ are $4\stimes 4$ matrices. (a) shows the true
trajectory, a series of noisy observations, and the trajectory estimated
by Kalman filtering, along with the covariances indicated by the
one-standard-deviation contours. The filtering process does a good job
of tracking the actual motion, and, as expected, the variance quickly
reaches a fixed point.

We can also derive equations for *smoothing* as well as
filtering with linear Gaussian models. The smoothing results are shown
in (b). Notice how the variance in the position estimate is sharply
reduced, except at the ends of the trajectory (why?), and that the
estimated trajectory is much smoother.

[kalman-2D-figure]

### Applicability of Kalman filtering

The Kalman filter and its elaborations are used in a vast array of
applications. The “classical” application is in radar tracking of
aircraft and missiles. Related applications include acoustic tracking of
submarines and ground vehicles and visual tracking of vehicles and
people. In a slightly more esoteric vein, Kalman filters are used to
reconstruct particle trajectories from bubble-chamber photographs and
ocean currents from satellite surface measurements. The range of
application is much larger than just the tracking of motion: any system
characterized by continuous state variables and noisy measurements will
do. Such systems include pulp mills, chemical plants, nuclear reactors,
plant ecosystems, and national economies.

The fact that Kalman filtering can be applied to a system does not mean
that the results will be valid or useful. The assumptions made—a linear
Gaussian transition and sensor models—are very strong. The attempts to
overcome nonlinearities in the system being modeled. A system is if the
transition model cannot be described as a matrix multiplication of the
state vector, as in . The EKF works by modeling the system as
*locally* linear in $\x_t$ in the region of
$\x_t\eq \mean_t$, the mean of the current state distribution. This
works well for smooth, well-behaved systems and allows the tracker to
maintain and update a Gaussian state distribution that is a reasonable
approximation to the true posterior. A detailed example is given in .

What does it mean for a system to be “unsmooth” or “poorly behaved”?
Technically, it means that there is significant nonlinearity in system
response within the region that is “close” (according to the covariance
$\covariance_t$) to the current mean $\mean_t$. To understand this idea
in nontechnical terms, consider the example of trying to track a bird as
it flies through the jungle. The bird appears to be heading at high
speed straight for a tree trunk. The Kalman filter, whether regular or
extended, can make only a Gaussian prediction of the location of the
bird, and the mean of this Gaussian will be centered on the trunk, as
shown in (a). A reasonable model of the bird, on the other hand, would
predict evasive action to one side or the other, as shown in (b). Such a
model is highly nonlinear, because the bird’s decision varies sharply
depending on its precise location relative to the trunk.

[kalman-bird-figure]

To handle examples like these, we clearly need a more expressive
language for representing the behavior of the system being modeled.
Within the control theory community, for which problems such as evasive
maneuvering by aircraft raise the same kinds of difficulties, the
standard solution is the [switching-kf-page]. In this approach, multiple
Kalman filters run in parallel, each using a different model of the
system—for example, one for straight flight, one for sharp left turns,
and one for sharp right turns. A weighted sum of predictions is used,
where the weight depends on how well each filter fits the current data.
We will see in the next section that this is simply a special case of
the general dynamic Bayesian network model, obtained by adding a
discrete “maneuver” state variable to the network shown in . Switching
Kalman filters are discussed further in .

Dynamic Bayesian Networks {#dbn-section}
-------------------------

A , or , is a Bayesian network that represents a temporal probability
model of the kind described in . We have already seen examples of DBNs:
the umbrella network in and the Kalman filter network in . In general,
each slice of a DBN can have any number of state variables $\X_t$ and
evidence variables $\E_t$. For simplicity, we assume that the variables
and their links are exactly replicated from slice to slice and that the
DBN represents a first-order Markov process, so that each variable can
have parents only in its own slice or the immediately preceding slice.

It should be clear that every hidden Markov model can be represented as
a DBN with a single state variable and a single evidence variable. It is
also the case that every discrete-variable DBN can be represented as an
HMM; as explained in , we can combine all the state variables in the DBN
into a single state variable whose values are all possible tuples of
values of the individual state variables. Now, if every HMM is a DBN and
every DBN can be translated into an HMM, what’s the difference? The
difference is that,

by decomposing the state of a complex system into its constituent
variables, the can take advantage of *sparseness* in the
temporal probability model.

Suppose, for example, that a DBN has 20 Boolean state variables, each of
which has three parents in the preceding slice. Then the DBN transition
model has ${20}\stimes 2^3 \eq {160}$ probabilities, whereas the
corresponding HMM has $2^{{20}}$ states and therefore $2^{{40}}$, or
roughly a trillion, probabilities in the transition matrix. This is bad
for at least three reasons: first, the HMM itself requires much more
space; second, the huge transition matrix makes HMM inference much more
expensive; and third, the problem of learning such a huge number of
parameters makes the pure HMM model unsuitable for large problems. The
relationship between DBNs and HMMs is roughly analogous to the
relationship between ordinary Bayesian networks and full tabulated joint
distributions.

We have already explained that every model can be represented in a DBN
with continuous variables and linear Gaussian conditional distributions
(). It should be clear from the discussion at the end of the preceding
section that *not* every DBN can be represented by a Kalman
filter model. In a Kalman filter, the current state distribution is
always a single multivariate Gaussian distribution—that is, a single
“bump” in a particular location. DBNs, on the other hand, can model
arbitrary distributions. For many real-world applications, this
flexibility is essential. Consider, for example, the current location of
my keys. They might be in my pocket, on the bedside table, on the
kitchen counter, dangling from the front door, or locked in the car. A
single Gaussian bump that included all these places would have to
allocate significant probability to the keys being in mid-air in the
front hall. Aspects of the real world such as purposive agents,
obstacles, and pockets introduce “nonlinearities” that require
combinations of discrete and continuous variables in order to get
reasonable models.

### Constructing DBNs

To construct a DBN, one must specify three kinds of information: the
prior distribution over the state variables, $\pv(\X_0)$; the transition
model $\pv(\X_{t+1}\given \X_t)$; and the sensor model
$\pv(\E_t\given \X_t)$. To specify the transition and sensor models, one
must also specify the topology of the connections between successive
slices and between the state and evidence variables. Because the
transition and sensor models are assumed to be stationary—the same for
all $t$—it is most convenient simply to specify them for the first
slice. For example, the complete DBN specification for the umbrella
world is given by the three-node network shown in (a). From this
specification, the complete DBN with an unbounded number of time slices
can be constructed as needed by copying the first slice.

[dbn-examples-figure]

Let us now consider a more interesting example: monitoring a
battery-powered robot moving in the X–Y plane, as introduced at the end
of . First, we need state variables, which will include both
$\X_t\eq (X_t,Y_t)$ for position and
${\dot{\X}}_t\eq ({\dot X}_t,{\dot Y}_t)$ for velocity. We assume some
method of measuring position—perhaps a fixed camera or onboard GPS
(Global Positioning System)—yielding measurements $\Z_t$. The position
at the next time step depends on the current position and velocity, as
in the standard Kalman filter model. The velocity at the next step
depends on the current velocity and the state of the battery. We add
${Battery}{}_t$ to represent the actual battery charge level, which
has as parents the previous battery level and the velocity, and we add
${BMeter}{}_t$, which measures the battery charge level. This gives us
the basic model shown in (b).

It is worth looking in more depth at the nature of the sensor model for
${BMeter}{}_t$. Let us suppose, for simplicity, that both
${Battery}{}_t$ and ${BMeter}{}_t$ can take on discrete values 0
through 5. If the meter is always accurate, then the CPT
$\pv({BMeter}{}_t\given {Battery}{}_t)$ should have probabilities of
1.0 “along the diagonal” and probabilities of 0.0 elsewhere. In reality,
noise always creeps into measurements. For continuous measurements, a
Gaussian distribution with a small variance might be used.[^5] For our
discrete variables, we can approximate a Gaussian using a distribution
in which the probability of error drops off in the appropriate way, so
that the probability of a large error is very small. We use the term to
cover both the continuous and discrete versions.

Anyone with hands-on experience of robotics, computerized process
control, or other forms of automatic sensing will readily testify to the
fact that small amounts of measurement noise are often the least of
one’s problems. Real sensors *fail*. When a sensor fails,
it does not necessarily send a signal saying, “Oh, by the way, the data
I’m about to send you is a load of nonsense.” Instead, it simply sends
the nonsense. The simplest kind of failure is called a , where the
sensor occasionally decides to send some nonsense. For example, the
battery level sensor might have a habit of sending a zero when someone
bumps the robot, even if the battery is fully charged.

Let’s see what happens when a transient failure occurs with a Gaussian
error model that doesn’t accommodate such failures. Suppose, for
example, that the robot is sitting quietly and observes 20 consecutive
battery readings of 5. Then the battery meter has a temporary seizure
and the next reading is ${BMeter}{}_{{21}}\eq 0$. What will the simple
Gaussian error model lead us to believe about ${Battery}{}_{{21}}$?
According to Bayes’ rule, the answer depends on both the sensor model
$\pv({BMeter}{}_{{21}}\eq 0\given {Battery}{}_{{21}})$ and the
prediction $\pv({Battery}{}_{{21}}\given {BMeter}{}_{1:{20}})$. If
the probability of a large sensor error is significantly less likely
than the probability of a transition to ${Battery}{}_{{21}}\eq 0$,
even if the latter is very unlikely, then the posterior distribution
will assign a high probability to the battery’s being empty. A second
reading of 0 at $t\eq {22}$ will make this conclusion almost certain. If
the transient failure then disappears and the reading returns to 5 from
$t\eq {23}$ onwards, the estimate for the battery level will quickly
return to 5, as if by magic. This course of events is illustrated in the
upper curve of (a), which shows the expected value of ${Battery}{}_t$
over time, using a discrete Gaussian error model.

Despite the recovery, there is a time ($t\eq {22}$) when the robot is
convinced that its battery is empty; presumably, then, it should send
out a mayday signal and shut down. Alas, its oversimplified sensor model
has led it astray. How can this be fixed? Consider a familiar example
from everyday human driving: on sharp curves or steep hills, one’s “fuel
tank empty” warning light sometimes turns on. Rather than looking for
the emergency phone, one simply recalls that the fuel gauge sometimes
gives a very large error when the fuel is sloshing around in the tank.
The moral of the story is the following:

for the system to handle sensor failure properly, the sensor model must
include the possibility of failure.

The simplest kind of failure model for a sensor allows a certain
probability that the sensor will return some completely incorrect value,
regardless of the true state of the world. For example, if the battery
meter fails by returning 0, we might say that
$$P({BMeter}{}_t\eq 0\given {Battery}{}_t\eq 5)\eq {0.03}\ ,$$ which
is presumably much larger than the probability assigned by the simple
Gaussian error model. Let’s call this the . How does it help when we are
faced with a reading of 0? Provided that the *predicted*
probability of an empty battery, according to the readings so far, is
much less than 0.03, then the best explanation of the observation
${BMeter}{}_{{21}}\eq 0$ is that the sensor has temporarily failed.
Intuitively, we can think of the belief about the battery level as
having a certain amount of “inertia” that helps to overcome temporary
blips in the meter reading. The upper curve in (b) shows that the
transient failure model can handle transient failures without a
catastrophic change in beliefs.

So much for temporary blips. What about a persistent sensor failure?
Sadly, failures of this kind are all too common. If the sensor returns
20 readings of 5 followed by 20 readings of 0, then the transient sensor
failure model described in the preceding paragraph will result in the
robot gradually coming to believe that its battery is empty when in fact
it may be that the meter has failed. The lower curve in (b) shows the
belief “trajectory” for this case. By $t\eq {25}$—five readings of 0—the
robot is convinced that its battery is empty. Obviously, we would prefer
the robot to believe that its battery meter is broken—if indeed this is
the more likely event.

[battery-sequence-figure]

[battery-persistence-figure]

Unsurprisingly, to handle persistent failure, we need a that describes
how the sensor behaves under normal conditions and after failure. To do
this, we need to augment the state of the system with an additional
variable, say, ${BMBroken}$, that describes the status of the battery
meter. The persistence of failure must be modeled by an arc linking
${BMBroken}{}_0$ to ${BMBroken}{}_1$. This has a CPT that gives a
small probability of failure in any given time step, say, 0.001, but
specifies that the sensor stays broken once it breaks. When the sensor
is OK, the sensor model for ${BMeter}$ is identical to the transient
failure model; when the sensor is broken, it says ${BMeter}$ is always
0, regardless of the actual battery charge.

The persistent failure model for the battery sensor is shown in (a). Its
performance on the two data sequences (temporary blip and persistent
failure) is shown in (b). There are several things to notice about these
curves. First, in the case of the temporary blip, the probability that
the sensor is broken rises significantly after the second 0 reading, but
immediately drops back to zero once a 5 is observed. Second, in the case
of persistent failure, the probability that the sensor is broken rises
quickly to almost 1 and stays there. Finally, once the sensor is known
to be broken, the robot can only assume that its battery discharges at
the “normal” rate, as shown by the gradually descending level of
$E({Battery}{}_t\given $…$)$.

So far, we have merely scratched the surface of the problem of
representing complex processes. The variety of transition models is
huge, encompassing topics as disparate as modeling the human endocrine
system and modeling multiple vehicles driving on a freeway. Sensor
modeling is also a vast subfield in itself, but even subtle phenomena,
such as sensor drift, sudden decalibration, and the effects of exogenous
conditions (such as weather) on sensor readings, can be handled by
explicit representation within dynamic Bayesian networks.

### Exact inference in DBNs

Having sketched some ideas for representing complex processes as DBNs,
we now turn to the question of inference. In a sense, this question has
already been answered: dynamic Bayesian networks *are*
Bayesian networks, and we already have algorithms for inference in
Bayesian networks. Given a sequence of observations, one can construct
the full Bayesian network representation of a DBN by replicating slices
until the network is large enough to accommodate the observations, as in
. This technique, mentioned in in the context of relational probability
models, is called . (Technically, the DBN is equivalent to the
semi-infinite network obtained by unrolling forever. Slices added beyond
the last observation have no effect on inferences within the observation
period and can be omitted.) Once the DBN is unrolled, one can use any of
the inference algorithms—variable elimination, clustering methods, and
so on—described in .

[dbn-unrolling-figure]

Unfortunately, a naive application of unrolling would not be
particularly efficient. If we want to perform filtering or smoothing
with a long sequence of observations $\e_{1:t}$, the unrolled network
would require $O(t)$ space and would thus grow without bound as more
observations were added. Moreover, if we simply run the inference
algorithm anew each time an observation is added, the inference time per
update will also increase as $O(t)$.

Looking back to , we see that constant time and space per filtering
update can be achieved if the computation can be done recursively.
Essentially, the filtering update in works by *summing out*
the state variables of the previous time step to get the distribution
for the new time step. Summing out variables is exactly what the ()
algorithm does, and it turns out that running variable elimination with
the variables in temporal order exactly mimics the operation of the
recursive filtering update in . The modified algorithm keeps at most two
slices in memory at any one time: starting with slice 0, we add slice 1,
then sum out slice 0, then add slice 2, then sum out slice 1, and so on.
In this way, we can achieve constant space and time per filtering
update. (The same performance can be achieved by suitable modifications
to the clustering algorithm.) asks you to verify this fact for the
umbrella network.

So much for the good news; now for the bad news: It turns out that the
“constant” for the per-update time and space complexity is, in almost
all cases, exponential in the number of state variables. What happens is
that, as the variable elimination proceeds, the factors grow to include
all the state variables (or, more precisely, all those state variables
that have parents in the previous time slice). The maximum factor size
is $O(d^{n+k})$ and the total update cost per step is $O(nd^{n+k})$,
where $d$ is the domain size of the variables and $k$ is the maximum
number of parents of any state variable.

Of course, this is much less than the cost of HMM updating, which is
$O(d^{2n})$, but it is still infeasible for large numbers of variables.
This grim fact is somewhat hard to accept. What it means is that

even though we can use DBNs to *represent* very complex
temporal processes with many sparsely connected variables, we cannot
*reason* efficiently and exactly about those processes.

The DBN model itself, which represents the prior joint distribution over
all the variables, is factorable into its constituent CPTs, but the
posterior joint distribution conditioned on an observation sequence—that
is, the forward message—is generally *not* factorable. So
far, no one has found a way around this problem, despite the fact that
many important areas of science and engineering would benefit enormously
from its solution. Thus, we must fall back on approximate methods.

### Approximate inference in DBNs

described two approximation algorithms: likelihood weighting () and
Markov chain Monte Carlo (MCMC, ). Of the two, the former is most easily
adapted to the DBN context. (An MCMC filtering algorithm is described
briefly in the notes at the end of the chapter.) We will see, however,
that several improvements are required over the standard likelihood
weighting algorithm before a practical method emerges.

Recall that works by sampling the nonevidence nodes of the network in
topological order, weighting each sample by the likelihood it accords to
the observed evidence variables. As with the exact algorithms, we could
apply likelihood weighting directly to an unrolled DBN, but this would
suffer from the same problems of increasing time and space requirements
per update as the observation sequence grows. The problem is that the
standard algorithm runs each sample in turn, all the way through the
network. Instead, we can simply run all $N$ samples together through the
DBN, one slice at a time. The modified algorithm fits the general
pattern of filtering algorithms, with the set of $N$ samples as the
forward message. The first key innovation, then, is to

use the samples themselves as an approximate representation of the
current state distribution.

This meets the requirement of a “constant” time per update, although the
constant depends on the number of samples required to maintain an
accurate approximation. There is also no need to unroll the DBN, because
we need to have in memory only the current slice and the next slice.

In our discussion of likelihood weighting in , we pointed out that the
algorithm’s accuracy suffers if the evidence variables are “downstream”
from the variables being sampled, because in that case the samples are
generated without any influence from the evidence. Looking at the
typical structure of a DBN—say, the umbrella DBN in —we see that indeed
the early state variables will be sampled without the benefit of the
later evidence. In fact, looking more carefully, we see that
*none* of the state variables has *any*
evidence variables among its ancestors! Hence, although the weight of
each sample will depend on the evidence, the actual set of samples
generated will be *completely independent* of the evidence.
For example, even if the boss brings in the umbrella every day, the
sampling process could still hallucinate endless days of sunshine. What
this means in practice is that the fraction of samples that remain
reasonably close to the actual series of events (and therefore have
nonnegligible weights) drops exponentially with $t$, the length of the
observation sequence. In other words, to maintain a given level of
accuracy, we need to increase the number of samples exponentially with
$t$. Given that a filtering algorithm that works in real time can use
only a fixed number of samples, what happens in practice is that the
error blows up after a very small number of update steps.

Clearly, we need a better solution. The second key innovation is to

focus the set of samples on the high-probability regions of the state
space.

This can be done by throwing away samples that have very low weight,
according to the observations, while replicating those that have high
weight. In that way, the population of samples will stay reasonably
close to reality. If we think of samples as a resource for modeling the
posterior distribution, then it makes sense to use more samples in
regions of the state space where the posterior is higher.

A family of algorithms called is designed to do just that. Particle
filtering works as follows: First, a population of $N$ initial-state
samples is created by sampling from the prior distribution $\pv(\X_0)$.
Then the update cycle is repeated for each time step:

1.  Each sample is propagated forward by sampling the next state value
    $\x_{t+1}$ given the current value $\x_t$ for the sample, based on
    the $\pv(\X_{t+1}\given \x_t)$.

2.  Each sample is weighted by the likelihood it assigns to the new
    evidence, $P(\e_{t+1}\given \x_{t+1})$.

3.  The population is *resampled* to generate a new
    population of $N$ samples. Each new sample is selected from the
    current population; the probability that a particular sample is
    selected is proportional to its weight. The new samples are
    unweighted.

The algorithm is shown in detail in , and its operation for the umbrella
DBN is illustrated in .

[particle-filtering-algorithm]

[pf-umbrella-figure]

We can show that this algorithm is consistent—gives the correct
probabilities as $N$ tends to infinity—by considering what happens
during one update cycle. We assume that the sample population starts
with a correct representation of the forward message
$\f_{1:t}\eq \pv(\X_t\given \e_{1:t})$ at time $t$. Writing
$N(\x_t\given \e_{1:t})$ for the number of samples occupying state
$\x_t$ after observations $\e_{1:t}$ have been processed, we therefore
have

$$N(\x_t\given \e_{1:t})/N = P(\x_t\given \e_{1:t})
\label{particle-initial-equation}$$

for large $N$. Now we propagate each sample forward by sampling the
state variables at $t+1$, given the values for the sample at $t$. The
number of samples reaching state $\x_{t+1}$ from each $\x_t$ is the
transition probability times the population of $\x_t$; hence, the total
number of samples reaching $\x_{t+1}$ is
$$N(\x_{t+1}\given \e_{1:t}) = \sum_{\sx_t} P(\x_{t+1}\given \x_t) N(\x_t\given \e_{1:t})   \ .$$
Now we weight each sample by its likelihood for the evidence at $t+1$. A
sample in state $\x_{t+1}$ receives weight $P(\e_{t+1}\given \x_{t+1})$.
The total weight of the samples in $\x_{t+1}$ after seeing $\e_{t+1}$ is
therefore
$$W(\x_{t+1}\given \e_{1:t+1}) = P(\e_{t+1}\given \x_{t+1}) N(\x_{t+1}\given \e_{1:t}) \ .$$
Now for the resampling step. Since each sample is replicated with
probability proportional to its weight, the number of samples in state
$\x_{t+1}$ after resampling is proportional to the total weight in
$\x_{t+1}$ before resampling:

$$\begin{aligned}
  N(\x_{t+1}\given \e_{1:t+1})/N 
    &=&  \alpha\, W(\x_{t+1}\given \e_{1:t+1}) \\
    &=&  \alpha\, P(\e_{t+1}\given \x_{t+1}) N(\x_{t+1}\given \e_{1:t}) \\
    &=&  \alpha\, P(\e_{t+1}\given \x_{t+1}) 
            \sum_{\sx_t} P(\x_{t+1}\given \x_t) N(\x_t\given \e_{1:t})\\
    &=&  \alpha\, N P(\e_{t+1}\given \x_{t+1}) 
            \sum_{\sx_t} P(\x_{t+1}\given \x_t) P(\x_t\given \e_{1:t})
\quad\mbox{(by \ref{particle-initial-equation})}\\
    &=&  \alpha' P(\e_{t+1}\given \x_{t+1}) 
            \sum_{\sx_t} P(\x_{t+1}\given \x_t) P(\x_t\given \e_{1:t})\\
    &=&  P(\x_{t+1}\given \e_{1:t+1})
\quad\mbox{(by \ref{filtering-equation}).}\end{aligned}$$

Therefore the sample population after one update cycle correctly
represents the forward message at time $t+1$.

Particle filtering is *consistent*, therefore, but is it
*efficient*? In practice, it seems that the answer is yes:
particle filtering seems to maintain a good approximation to the true
posterior using a constant number of samples. Under certain
assumptions—in particular, that the probabilities in the transition and
sensor models are strictly greater than 0 and less than 1—it is possible
to prove that the approximation maintains bounded error with high
probability. On the practical side, the range of applications has grown
to include many fields of science and engineering; some references are
given at the end of the chapter.

Keeping Track of Many Objects {#data-association-section}
-----------------------------

[classical-DA-figure]

The preceding sections have considered—without mentioning it—state
estimation problems involving a single object. In this section, we see
what happens when two or more objects generate the observations. What
makes this case different from plain old state estimation is that there
is now the possibility of *uncertainty* about which object
generated which observation. This is the problem of (), now viewed in a
temporal context. In the control theory literature, this is the
problem—that is, the problem of associating observation data with the
objects that generated them.

The data association problem was studied originally in the context of
radar tracking, where reflected pulses are detected at fixed time
intervals by a rotating radar antenna. At each time step, multiple blips
may appear on the screen, but there is no direct observation of which
blips at time $t$ belong to which blips at time $t-1$. (a) shows a
simple example with two blips per time step for five steps. Let the two
blip locations at time $t$ be $e_t^1$ and $e_t^2$. (The labeling of
blips within a time step as “1” and “2” is completely arbitrary and
carries no information.) Let us assume, for the time being, that exactly
two aircraft, $A$ and $B$, generated the blips; their true positions are
$X_t^A$ and $X_t^B$. Just to keep things simple, we’ll also assume that
the each aircraft moves independently according to a known transition
model—e.g., a linear Gaussian model as used in the Kalman filter ().

Suppose we try to write down the overall probability model for this
scenario, just as we did for general temporal processes in on . As
usual, the joint distribution factors into contributions for each time
step as follows:

$$\begin{aligned}
  \lefteqn{P(x^A_{0:t},x^B_{0:t},e^1_{1:t},e^2_{1:t}) =}\nonumber\\
   &&   P(x^A_{0})P(x^B_{0})  \prod_{i\eq 1}^t P(x^A_i \given x^A_{i-1})P(x^B_i \given x^B_{i-1}) \, P(e^1_i, e^2_i\given x^A_i, x^B_i)\ .
\label{DA-joint-equation}\end{aligned}$$

We would like to factor the observation term $P(e^1_i, e^2_i\given
x^A_i, x^B_i)$ into a product of two terms, one for each object, but
this would require knowing which observation was generated by which
object. Instead, we have to sum over all possible ways of associating
the observations with the objects. Some of those ways are shown in
(b–c); in general, for $n$ objects and $T$ time steps, there are
$(n!)^T$ ways of doing it—an awfully large number.

Mathematically speaking, the “way of associating the observations with
the objects” is a collection of unobserved random variable that identify
the source of each observation. We’ll write $\omega_t$ to denote the
one-to-one mapping from objects to observations at time $t$, with
$\omega_t(A)$ and $\omega_t(B)$ denoting the specific observations (1 or
2) that $\omega_t$ assigns to $A$ and $B$. (For $n$ objects, $\omega_t$
will have $n!$ possible values; here, $n!\eq 2$.) Because the labels “1”
ad “2” on the observations are assigned arbitrarily, the prior on
$\omega_t$ is uniform and $\omega_t$ is independent of the states of the
objects, $x^A_t$ and $ x^B_t)$. So we can condition the observation term
$P(e^1_i, e^2_i\given
x^A_i, x^B_i)$ on $\omega_t$ and then simplify:

$$\begin{aligned}
  P(e^1_i, e^2_i\given x^A_i, x^B_i) &=& \sum_{\omega_i} P(e^1_i, e^2_i\given x^A_i, x^B_i, \omega_i) P(\omega_i \given x^A_i, x^B_i)\\
                                     &=& \sum_{\omega_i} P(e_i^{\omega_i(A)} \given x^A_i) P(e_i^{\omega_i(B)} \given x^B_i) P(\omega_i \given x^A_i, x^B_i)\\
                                     &=& \frac{1}{2}\sum_{\omega_i} P(e_i^{\omega_i(A)} \given x^A_i) P(e_i^{\omega_i(B)} \given x^B_i)\ .\end{aligned}$$

Plugging this into , we get an expression that is only in terms of
transition and sensor models for individual objects and observations.

As for all probability models, inference means summing out the variables
other than the query and the evidence. For filtering in HMMs and DBNs,
we were able to sum out the state variables from 1 to $t-1$ by a simple
dynamic programming trick; for Kalman filters, we took advantage of
special properties of Gaussians. For data association, we are less
fortunate. There is no (known) efficient exact algorithm, for the same
reason that there is none for the switching Kalman filter (): the
filtering distribution $P(x^A_t\given e^1_{1:t},e^2_{1:t})$ for object
$A$ ends up as a mixture of exponentially many distributions, one for
each way of picking a sequence of observations to assign to $A$.

As a result of the complexity of exact inference, many different
approximate methods have been used. The simplest approach is to choose a
single “best” assignment at each time step, given the predicted
positions of the objects at the current time step. This assignment
associates observations with objects and enables the track of each
object to be updated and a prediction made for the next time step. For
choosing the “best” assignment, it is common to use the so-called ,
which repeatedly chooses the closest pairing of predicted position and
observation and adds that pairing to the assignment. The
nearest-neighbor filter works well when the objects are well separated
in state space and the prediction uncertainty and observation error are
small—in other words, when there is no possibility of confusion. When
there is more uncertainty as to the correct assignment, a better
approach is to choose the assignment that maximizes the joint
probability of the current observations given the predicted positions.
This can be done very efficiently using the  @Kuhn:1955, even though
there are $n!$ assignments to choose from.

Any method that commits to a single best assignment at each time step
fails miserably under more difficult conditions. In particular, if the
algorithm commits to an incorrect assignment, the prediction at the next
time step may be significantly wrong, leading to more incorrect
assignments, and so on. Two modern approaches turn out to be much more
effective. A algorithm (see ) for data association works by maintaining
a large collection of possible current assignments. An algorithm
explores the space of assignment histories—for example, (b–c) might be
states in the MCMC state space—and can change its mind about previous
assignment decisions. Current MCMC data association methods can handle
many hundreds of objects in real time while giving a good approximation
to the true posterior distributions.

[traffic-DA-figure]

The scenario described so far involved $n$ known objects generating $n$
observations at each time step. Real application of data association are
typically much more complicated. Often, the reported observations
include (also known as ), which are not caused by real objects. can
occur, meaning that no observation is reported for a real object.
Finally, new objects arrive and old ones disappear. These phenomena,
which create even more possible worlds to worry about, are illustrated
in (d).

shows two images from widely separated cameras on a California freeway.
In this application, we are interested in two goals: estimating the time
it takes, under current traffic conditions, to go from one place to
another in the freeway system; and measuring *demand*,
i.e., how many vehicles travel between any two points in the system at
particular times of the day and on particular days of the week. Both
goals require solving the data association problem over a wide area with
many cameras and tens of thousands of vehicles per hour. With visual
surveillance, false alarms are caused by moving shadows, articulated
vehicles, reflections in puddles, etc.; detection failures are caused by
occlusion, fog, darkness, and lack of visual contrast; and vehicles are
constantly entering and leaving the freeway system. Furthermore, the
appearance of any given vehicle can change dramatically between cameras
depending on lighting conditions and vehicle pose in the image, and the
transition model changes as traffic jams come and go. Despite these
problems, modern data association algorithms have been successful in
estimating traffic parameters in real-world settings.

Data association is an essential foundation for keeping track of a
complex world, because without it there is no way to combine multiple
observations of any given object. When objects in the world interact
with each other in complex activities, understanding the world requires
combining data association with the relational and open-universe
probability models of . This is currently an active area of research.

This chapter has addressed the general problem of representing and
reasoning about probabilistic temporal processes. The main points are as
follows:

-   The changing state of the world is handled by using a set of random
    variables to represent the state at each point in time.

-   Representations can be designed to satisfy the , so that the future
    is independent of the past given the present. Combined with the
    assumption that the process is —that is, the dynamics do not change
    over time—this greatly simplifies the representation.

-   A temporal probability model can be thought of as containing a
    describing the state evolution and a describing the observation
    process.

-   The principal inference tasks in temporal models are , , , and
    computing the . Each of these can be achieved using simple,
    recursive algorithms whose run time is linear in the length of the
    sequence.

-   Three families of temporal models were studied in more depth: , ,
    and (which include the other two as special cases).

-   Unless special assumptions are made, as in Kalman filters, exact
    inference with many state variables is intractable. In practice, the
    algorithm seems to be an effective approximation algorithm.

-   When trying to keep track of many objects, uncertainty arises as to
    which observations belong to which objects—the problem. The number
    of association hypotheses is typically intractably large, but MCMC
    and particle filtering algorithms for data association work well in
    practice.

Many of the basic ideas for estimating the state of dynamical systems
came from the mathematician C. F. Gauss [-@Gauss:1809], who formulated a
deterministic least-squares algorithm for the problem of estimating
orbits from astronomical observations. A. A. Markov [-@Markov:1913]
developed what was later called the in his analysis of stochastic
processes; he estimated a first-order Markov chain on letters from the
text of *Eugene Onegin*. The general theory of Markov
chains and their mixing times is covered by .

Significant classified work on filtering was done during by
Wiener [-@Wiener:1942] for continuous-time processes and by
Kolmogorov [-@Kolmogorov:1941] for discrete-time processes. Although
this work led to important technological developments over the next 20
years, its use of a frequency-domain representation made many
calculations quite cumbersome. Direct state-space modeling of the
stochastic process turned out to be simpler, as shown by Peter and
Rudolf . The latter paper described what is now known as the Kalman
filter for forward inference in linear systems with Gaussian noise;
Kalman’s results had, however, been obtained previously by the Danish
statistician Thorvold and by the Russian mathematician Ruslan , whom
Kalman met in Moscow in 1960. After a visit to NASA Ames Research Center
in 1960, Kalman saw the applicability of the method to the tracking of
rocket trajectories, and the filter was later implemented for the Apollo
missions. Important results on smoothing were derived by Rauch *et
al.* [-@Rauch+al:1965], and the impressively named
Rauch–Tung–Striebel smoother is still a standard technique today. Many
early results are gathered in Gelb [-@Gelb:1974]. Bar-Shalom and
Fortmann [-@Bar-Shalom+Fortmann:1988] give a more modern treatment with
a Bayesian flavor, as well as many references to the vast literature on
the subject. and cover the control theory approach to time series
analysis.

The hidden Markov model and associated algorithms for inference and
learning, including the forward–backward algorithm, were developed by
Baum and Petrie [-@Baum+Petrie:1966]. The Viterbi algorithm first
appeared in @Viterbi:1967. Similar ideas also appeared independently in
the Kalman filtering community @Rauch+al:1965. The forward–backward
algorithm was one of the main precursors of the general formulation of
the EM algorithm @Dempster+al:1977; see also . Constant-space smoothing
appears in Binder *et al.* [-@Binder+al:1997b], as does the
divide-and-conquer algorithm developed in . Constant-time fixed-lag
smoothing for HMMs first appeared in . HMMs have found many applications
in language processing @Charniak:1993, speech recognition
@Rabiner+Juang:1993, machine translation @Och+Ney:2003, computational
biology @Krogh+al:1994b [@Baldi+al:1994], financial economics and other
fields. There have been several extensions to the basic HMM model, for
example the Hierarchical HMM @Fine+al:1998 and Layered HMM
@Oliver+al:2004 introduce structure back into the model, replacing the
single state variable of HMMs.

Dynamic Bayesian networks (DBNs) can be viewed as a sparse encoding of a
Markov process and were first used in AI by , , and . The last work
extends the Bayes net system to accommodate dynamic Bayesian networks.
The book by helped popularize DBNs and the probabilistic approach to
planning and control within AI. provides a thorough analysis of DBNs.

Dynamic Bayesian networks have become popular for modeling a variety of
complex motion processes in computer vision @Huang+al:1994
[@Intille+Bobick:1999]. Like HMMs, they have found applications in
speech recognition @Zweig+Russell:1998a
[@Richardson+al:2000; @Stephenson+al:2000; @Nefian+al:2002; @Livescu+al:2003],
genomics @Murphy+Mian:1999 [@Perrin+al:2003; @Husmeier:2003] and robot
localization @Theocharous+al:2004. The link between HMMs and DBNs, and
between the forward–backward algorithm and Bayesian network propagation,
was made explicitly by Smyth *et al.* [-@Smyth+al:1997]. A
further unification with Kalman filters (and other statistical models)
appears in . Procedures exist for learning the
parameters @Binder+al:1997a [@Ghahramani:1998] and
structures @Friedman+al:1998 of DBNs.

The algorithm described in has a particularly interesting history. The
first sampling algorithms for particle filtering (also called sequential
Monte Carlo methods) were developed in the control theory community by
Handschin and Mayne [-@Handschin+Mayne:1969], and the resampling idea
that is the core of particle filtering appeared in a Russian control
journal @Zaritskii+al:1975. It was later reinvented in statistics as ,
or  @Rubin:1988 [@Liu+Chen:1998], in control theory as particle
filtering @Gordon+al:1993 [@Gordon:1994], in AI as  @Kanazawa+al:1995,
and in computer vision as  @Isard+Blake:1996. The paper by Kanazawa
*et al.* [-@Kanazawa+al:1995] includes an improvement
called whereby the state at time $t+1$ is sampled conditional on both
the state at time $t$ *and the evidence at time* $t+1$.
This allows the evidence to influence sample generation directly and was
proved by and to reduce the approximation error. Particle filtering has
been applied in many areas, including tracking complex motion patterns
in video @Isard+Blake:1996, predicting the stock
market @deFreitas+al:1999, and diagnosing faults on planetary
rovers @Verma+al:2004. A variant called the or RBPF @Doucet+al:2000
[@Murphy+Russell:2001] applies particle filtering to a subset of state
variables and, for each particle, performs exact inference on the
remaining variables conditioned on the value sequence in the particle.
In some cases RBPF works well with thousands of state variables. An
application of RBPF to localization and mapping in robotics is described
in . The book by collects many important papers on (SMC) algorithms, of
which particle filtering is the most important instance. Pierre Del
Moral and colleagues have performed extensive theoretical analyses of
SMC algorithms @DelMoral:2004 [@DelMoral+al:2006].

MCMC methods (see ) can be applied to the filtering problem; for
example, Gibbs sampling can be applied directly to an unrolled DBN. To
avoid the problem of increasing update times as the unrolled network
grows, the filter @Marthi+al:2002 prefers to sample more recent state
variables, with a probability that decays as $1/k^2$ for a variable $k$
steps into the past. Decayed MCMC is a provably nondivergent filter.
Nondivergence theorems can also be obtained for certain types of . An
assumed-density filter assumes that the posterior distribution over
states at time $t$ belongs to a particular finitely parameterized
family; if the projection and update steps take it outside this family,
the distribution is projected back to give the best approximation within
the family. For DBNs, the Boyen–Koller algorithm @Boyen+al:1999 and the
algorithm @Murphy+Weiss:2001 assume that the posterior distribution can
be approximated well by a product of small factors. Variational
techniques (see ) have also been developed for temporal models.
Ghahramani and Jordan [-@Ghahramani+Jordan:1997] discuss an
approximation algorithm for the , a DBN in which two or more
independently evolving Markov chains are linked by a shared observation
stream. Jordan *et al.* [-@Jordan+al:1998] cover a number
of other applications.

Data association for multitarget tracking was first described in a
probabilistic setting by . The first practical algorithm for large-scale
problems was the “multiple hypothesis tracker” or MHT
algorithm @Reid:1979. Many important papers are collected by and . The
development of an MCMC algorithm for data association is due to , who
applied it to traffic surveillance problems. provide a formal analysis
and extensive experimental comparisons to other methods. describe a data
association method based on particle filtering. Ingemar Cox analyzed the
complexity of data association @Cox:1993 [@Cox+Hingorani:1994] and
brought the topic to the attention of the vision community. He also
noted the applicability of the polynomial-time Hungarian algorithm to
the problem of finding most-likely assignments, which had long been
considered an intractable problem in the tracking community. The
algorithm itself was published by , based on translations of papers
published in 1931 by two Hungarian mathematicians, Dénes
König and Jenö Egerváry. The
basic theorem had been derived previously, however, in an unpublished
Latin manuscript by the famous Prussian mathematician Carl Gustav Jacobi
(1804–1851).

[state-augmentation-exercise] Show that any second-order Markov process
can be rewritten as a first-order Markov process with an augmented set
of state variables. Can this always be done
*parsimoniously*, i.e., without increasing the number of
parameters needed to specify the transition model?

[markov-convergence-exercise] In this exercise, we examine what happens
to the probabilities in the umbrella world in the limit of long time
sequences.

1.  Suppose we observe an unending sequence of days on which the
    umbrella appears. Show that, as the days go by, the probability of
    rain on the current day increases monotonically toward a fixed
    point. Calculate this fixed point.

2.  Now consider *forecasting* further and further into the
    future, given just the first two umbrella observations. First,
    compute the probability $P(r_{2+k}|u_1,u_2)$ for $k\eq
    1\ldots {20}$ and plot the results. You should see that the
    probability converges towards a fixed point. Prove that the exact
    value of this fixed point is 0.5.

[island-exercise] This exercise develops a space-efficient variant of
the forward–backward algorithm described in (). We wish to compute
$\pv(\X_k|\e_{1:t})$ for $k\eq 1,\ldots ,t$. This will be done with a
divide-and-conquer approach.

1.  Suppose, for simplicity, that $t$ is odd, and let the halfway point
    be $h\eq (t+1)/2$. Show that $\pv(\X_k|\e_{1:t})$ can be computed
    for $k\eq 1,\ldots ,h$ given just the initial forward message
    $\f_{1:0}$, the backward message $\b_{h+1:t}$, and the evidence
    $\e_{1:h}$.

2.  Show a similar result for the second half of the sequence.

3.  Given the results of (a) and (b), a recursive divide-and-conquer
    algorithm can be constructed by first running forward along the
    sequence and then backward from the end, storing just the required
    messages at the middle and the ends. Then the algorithm is called on
    each half. Write out the algorithm in detail.

4.  Compute the time and space complexity of the algorithm as a function
    of $t$, the length of the sequence. How does this change if we
    divide the input into more than two pieces?

[flawed-viterbi-exercise] On , we outlined a flawed procedure for
finding the most likely state sequence, given an observation sequence.
The procedure involves finding the most likely state at each time step,
using smoothing, and returning the sequence composed of these states.
Show that, for some temporal probability models and observation
sequences, this procedure returns an impossible state sequence (i.e.,
the posterior probability of the sequence is zero).

[hmm-likelihood-exercise] describes the filtering process for the matrix
formulation of HMMs. Give a similar equation for the calculation of
likelihoods, which was described generically in .

Consider the vacuum worlds of (perfect sensing) and (noisy sensing).
Suppose that the robot receives an observation sequence such that, with
perfect sensing, there is exactly one possible location it could be in.
Is this location necessarily the most probable location under noisy
sensing for sufficiently small noise probability $\epsilon$? Prove your
claim or find a counterexample.

[hmm-robust-exercise] In , the prior distribution over locations is
uniform and the transition model assumes an equal probability of moving
to any neighboring square. What if those assumptions are wrong? Suppose
that the initial location is actually chosen uniformly from the
northwest quadrant of the room and the action actually tends to move
southeast[hmm-robot-southeast-page]. Keeping the HMM model fixed,
explore the effect on localization and path accuracy as the
southeasterly tendency increases, for different values of $\epsilon$.

[roomba-viterbi-exercise] Consider a version of the vacuum robot () that
has the policy of going straight for as long as it can; only when it
encounters an obstacle does it change to a new (randomly selected)
heading. To model this robot, each state in the model consists of a
*(location, heading)* pair. Implement this model and see
how well the Viterbi algorithm can track a robot with this model. The
robot’s policy is more constrained than the random-walk robot; does that
mean that predictions of the most likely path are more accurate?

have described three policies for the vacuum robot: (1) a uniform random
walk, (2) a bias for wandering southeast, as described in , and (3) the
policy described in . Suppose an observer is given the observation
sequence from a vacuum robot, but is not sure which of the three
policies the robot is following. What approach should the observer use
to find the most likely path, given the observations? Implement the
approach and test it. How much does the localization accuracy suffer,
compared to the case in which the observer knows which policy the robot
is following?

This exercise is concerned with filtering in an environment with no
landmarks. Consider a vacuum robot in an empty room, represented by an
$n \times m$ rectangular grid. The robot’s location is hidden; the only
evidence available to the observer is a noisy location sensor that gives
an approximation to the robot’s location. If the robot is at location
$(x, y)$ then with probability .1 the sensor gives the correct location,
with probability .05 each it reports one of the 8 locations immediately
surrounding $(x, y)$, with probability .025 each it reports one of the
16 locations that surround those 8, and with the remaining probability
of .1 it reports “no reading.” The robot’s policy is to pick a direction
and follow it with probability .8 on each step; the robot switches to a
randomly selected new heading with probability .2 (or with probability 1
if it encounters a wall). Implement this as an HMM and do filtering to
track the robot. How accurately can we track the robot’s path?

This exercise is concerned with filtering in an environment with no
landmarks. Consider a vacuum robot in an empty room, represented by an
$n \times m$ rectangular grid. The robot’s location is hidden; the only
evidence available to the observer is a noisy location sensor that gives
an approximation to the robot’s location. If the robot is at location
$(x, y)$ then with probability .1 the sensor gives the correct location,
with probability .05 each it reports one of the 8 locations immediately
surrounding $(x, y)$, with probability .025 each it reports one of the
16 locations that surround those 8, and with the remaining probability
of .1 it reports “no reading.” The robot’s policy is to pick a direction
and follow it with probability .7 on each step; the robot switches to a
randomly selected new heading with probability .3 (or with probability 1
if it encounters a wall). Implement this as an HMM and do filtering to
track the robot. How accurately can we track the robot’s path?

[switching-kf-figure]

[switching-kf-exercise] Often, we wish to monitor a continuous-state
system whose behavior switches unpredictably among a set of $k$ distinct
“modes.” For example, an aircraft trying to evade a missile can execute
a series of distinct maneuvers that the missile may attempt to track. A
Bayesian network representation of such a model is shown in .

1.  Suppose that the discrete state $S_t$ has $k$ possible values and
    that the prior continuous state estimate $\pv(\X_0)$ is a
    multivariate Gaussian distribution. Show that the prediction
    $\pv(\X_1)$ is a —that is, a weighted sum of Gaussians such that the
    weights sum to 1.

2.  Show that if the current continuous state estimate
    $\pv(\X_t|\e_{1:t})$ is a mixture of $m$ Gaussians, then in the
    general case the updated state estimate $\pv(\X_{t+1}|\e_{1:t+1})$
    will be a mixture of $km$ Gaussians.

3.  What aspect of the temporal process do the weights in the Gaussian
    mixture represent?

The results in (a) and (b) show that the representation of the posterior
grows without limit even for switching Kalman filters, which are among
the simplest hybrid dynamic models.

[kalman-update-exercise] Complete the missing step in the derivation of
on , the first update step for the one-dimensional Kalman filter.

[kalman-variance-exercise] Let us examine the behavior of the variance
update in ().

1.  Plot the value of $\sigma_t^2$ as a function of $t$, given various
    values for $\sigma_x^2$ and $\sigma_z^2$.

2.  Show that the update has a fixed point $\sigma^2$ such that
    $\sigma_t^2 \rightarrow \sigma^2$ as $t \rightarrow \infty$, and
    calculate the value of $\sigma^2$.

3.  Give a qualitative explanation for what happens as
    $\sigma_x^2\rightarrow 0$ and as $\sigma_z^2\rightarrow 0$.

[sleep1-exercise] A professor wants to know if students are getting
enough sleep. Each day, the professor observes whether the students
sleep in class, and whether they have red eyes. The professor has the
following domain theory:

-   The prior probability of getting enough sleep, with no observations,
    is 0.7.

-   The probability of getting enough sleep on night $t$ is 0.8 given
    that the student got enough sleep the previous night, and 0.3 if
    not.

-   The probability of having red eyes is 0.2 if the student got enough
    sleep, and 0.7 if not.

-   The probability of sleeping in class is 0.1 if the student got
    enough sleep, and 0.3 if not.

Formulate this information as a dynamic Bayesian network that the
professor could use to filter or predict from a sequence of
observations. Then reformulate it as a hidden Markov model that has only
a single observation variable. Give the complete probability tables for
the model.

[sleep1-exercise] A professor wants to know if students are getting
enough sleep. Each day, the professor observes whether the students
sleep in class, and whether they have red eyes. The professor has the
following domain theory:

-   The prior probability of getting enough sleep, with no observations,
    is 0.6.

-   The probability of getting enough sleep on night $t$ is 0.8 given
    that the student got enough sleep the previous night, and 0.2 if
    not.

-   The probability of having red eyes is 0.2 if the student got enough
    sleep, and 0.7 if not.

-   The probability of sleeping in class is 0.1 if the student got
    enough sleep, and 0.3 if not.

Formulate this information as a dynamic Bayesian network that the
professor could use to filter or predict from a sequence of
observations. Then reformulate it as a hidden Markov model that has only
a single observation variable. Give the complete probability tables for
the model.

For the DBN specified in and for the evidence values

~1~ =\
~2~ =\
~3~ =

perform the following computations:

1.  State estimation: Compute $P({EnoughSleep}_t | \e_{1:t})$ for each
    of $t = 1,2,3$.

2.  Smoothing: Compute $P({EnoughSleep}_t | \e_{1:3})$ for each of
    $t = 1,2,3$.

3.  Compare the filtered and smoothed probabilities for $t=1$ and $t=2$.

Suppose that a particular student shows up with red eyes and sleeps in
class every day. Given the model described in , explain why the
probability that the student had enough sleep the previous night
converges to a fixed point rather than continuing to go down as we
gather more days of evidence. What is the fixed point? Answer this both
numerically (by computation) and analytically.

[battery-sequence-exercise] This exercise analyzes in more detail the
persistent-failure model for the battery sensor in (a) ().

1.  ​(b) stops at $t\eq
    {32}$. Describe qualitatively what should happen as $t\to\infty$ if
    the sensor continues to read 0.

2.  Suppose that the external temperature affects the battery sensor in
    such a way that transient failures become more likely as temperature
    increases. Show how to augment the DBN structure in (a), and explain
    any required changes to the CPTs.

3.  Given the new network structure, can battery readings be used by the
    robot to infer the current temperature?

[dbn-elimination-exercise] Consider applying the variable elimination
algorithm to the umbrella DBN unrolled for three slices, where the query
is $\pv(R_3|u_1,u_2,u_3)$. Show that the space complexity of the
algorithm—the size of the largest factor—is the same, regardless of
whether the rain variables are eliminated in forward or backward order.

[^1]: Uncertainty over *continuous* time can be modeled by
    (SDEs). The models studied in this chapter can be viewed as
    discrete-time approximations to SDEs.

[^2]: The term “filtering” refers to the roots of this problem in early
    work on signal processing, where the problem is to filter out the
    noise in a signal by estimating its underlying properties.

[^3]: In particular, when tracking a moving object with inaccurate
    position observations, smoothing gives a smoother estimated
    trajectory than filtering—hence the name.

[^4]: The reader unfamiliar with basic operations on vectors and
    matrices might wish to consult before proceeding with this section.

[^5]: Strictly speaking, a Gaussian distribution is problematic because
    it assigns nonzero probability to large negative charge levels. The
    is sometimes a better choice for a variable whose range is
    restricted.
Making Simple Decisions {#decision-theory-chapter}
=======================

In this chapter, we fill in the details of how utility theory combines
with probability theory to yield a decision-theoretic agent—an agent
that can make rational decisions based on what it believes and what it
wants. Such an agent can make decisions in contexts in which uncertainty
and conflicting goals leave a logical agent with no way to decide: a
goal-based agent has a binary distinction between good (goal) and bad
(non-goal) states, while a decision-theoretic agent has a continuous
measure of outcome quality.

introduces the basic principle of decision theory: the maximization of
expected utility. shows that the behavior of any rational agent can be
captured by supposing a utility function that is being maximized.
discusses the nature of utility functions in more detail, and in
particular their relation to individual quantities such as money. shows
how to handle utility functions that depend on several quantities. In ,
we describe the implementation of decision-making systems. In
particular, we introduce a formalism called a (also known as an ) that
extends Bayesian networks by incorporating actions and utilities. The
remainder of the chapter discusses issues that arise in applications of
decision theory to expert systems.

Combining Beliefs and Desires under Uncertainty {#rational-decisions-section}
-----------------------------------------------

Decision theory, in its simplest form, deals with choosing among actions
based on the desirability of their *immediate* outcomes;
that is, the environment is assumed to be episodic in the sense defined
on . (This assumption is relaxed in .) In we used the notation for the
state that is the deterministic outcome of taking action $a$ in state
$s_0$. In this chapter we deal with nondeterministic partially
observable environments. Since the agent may not know the current state,
we omit it and define $\Result(a)$ as a *random variable*
whose values are the possible outcome states. The probability of outcome
$s'$, given evidence observations $\mbf{e}$, is written
$$P(\Result(a)\eq s' \given a,\mbf{e}) \ ,$$ where the $a$ on the
right-hand side of the conditioning bar stands for the event that action
$a$ is executed.[^1]

The agent’s preferences are captured by a , $U(s)$, which assigns a
single number to express the desirability of a state. The of an action
given the evidence, ${EU}(a|\mbf{e})$, is just the average utility
value of the outcomes, weighted by the probability that the outcome
occurs:

$${EU}(a|\mbf{e}) = \sum\limits_{s'}  P(\Result(a)\eq s' \given a,\mbf{e}) \, U(s')\ .
\label{meu-equation}$$

The principle of (MEU) says that a rational agent should choose the
action that maximizes the agent’s expected utility:
$${action} = \argmax_a {EU}(a|\mbf{e})$$ In a sense, the MEU
principle could be seen as defining all of AI. All an intelligent agent
has to do is calculate the various quantities, maximize utility over its
actions, and away it goes. But this does not mean that the AI problem is
*solved* by the definition!

The MEU principle *formalizes* the general notion that the
agent should “do the right thing,” but goes only a small distance toward
a full *operationalization* of that advice. Estimating the
state of the world requires perception, learning, knowledge
representation, and inference. Computing $P(\Result(a)\given a,\mbf{e})$
requires a complete causal model of the world and, as we saw in ,
NP-hard inference in (very large) Bayesian networks. Computing the
outcome utilities $U(s')$ often requires searching or planning, because
an agent may not know how good a state is until it knows where it can
get to from that state. So, decision theory is not a panacea that solves
the AI problem—but it does provide a useful framework.

The MEU principle has a clear relation to the idea of performance
measures introduced in . The basic idea is simple. Consider the
environments that could lead to an agent having a given percept history,
and consider the different agents that we could design.

If an agent acts so as to maximize a utility function that correctly
reflects the performance measure, then the agent will achieve the
highest possible performance score (averaged over all the possible
environments).

This is the central justification for the MEU principle itself. While
the claim may seem tautological, it does in fact embody a very important
transition from a global, external criterion of rationality—the
performance measure over environment histories—to a local, internal
criterion involving the maximization of a utility function applied to
the next state.

The Basis of Utility Theory {#utility-theory-section}
---------------------------

Intuitively, the principle of Maximum Expected Utility (MEU) seems like
a reasonable way to make decisions, but it is by no means obvious that
it is the *only* rational way. After all, why should
maximizing the *average* utility be so special? What’s
wrong with an agent that maximizes the weighted sum of the cubes of the
possible utilities, or tries to minimize the worst possible loss? Could
an agent act rationally just by expressing preferences between states,
without giving them numeric values? Finally, why should a utility
function with the required properties exist at all? We shall see.

### Constraints on rational preferences

These questions can be answered by writing down some constraints on the
preferences that a rational agent should have and then showing that the
MEU principle can be derived from the constraints. We use the following
notation to describe an agent’s preferences:

$$\begin{aligned}
A \pref B       && \mbox{the agent prefers \(A\) over \(B\).}\index{1pref@$\pref$ (preferred)} \\
A \indiff B     && \mbox{the agent is indifferent between \(A\) and \(B\).}
\index{1indif@$\indiff$ (indifferent)} \\
A \prefeq B     && \mbox{the agent prefers \(A\) over \(B\) or is
indifferent between them.}\end{aligned}$$

Now the obvious question is, what sorts of things are $A$ and $B$? They
could be states of the world, but more often than not there is
uncertainty about what is really being offered. For example, an airline
passenger who is offered “the pasta dish or the chicken” does not know
what lurks beneath the tinfoil cover.[^2] The pasta could be delicious
or congealed, the chicken juicy or overcooked beyond recognition. We can
think of the set of outcomes for each action as a —think of each action
as a ticket. A lottery $L$ with possible outcomes $S_1,\ldots,S_n$ that
occur with probabilities $p_1,\ldots,p_n$ is written
$$L = [p_1,S_1;\ p_2,S_2;\ \ldots\ p_n,S_n] \ .$$ In general, each
outcome $S_i$ of a lottery can be either an atomic state or another
lottery. The primary issue for utility theory is to understand how
preferences between complex lotteries are related to preferences between
the underlying states in those lotteries. To address this issue we list
six constraints that we require any reasonable preference relation to
obey:

Given any two lotteries, a rational agent must either prefer one to the
other or else rate the two as equally preferable. That is, the agent
cannot avoid deciding. As we said on , refusing to bet is like refusing
to allow time to pass.
$$\mbox{Exactly one of~} (A \pref B),\, (B \pref A),\, \mbox{~or~} (A \indiff B)  \mbox{~holds.}$$

Given any three lotteries, if an agent prefers $A$ to $B$ and prefers
$B$ to $C$, then the agent must prefer $A$ to $C$.
$$(A \pref B) \land (B \pref C) \implies (A \pref C) \ .$$

If some lottery $B$ is between $A$ and $C$ in preference, then there is
some probability $p$ for which the rational agent will be indifferent
between getting $B$ for sure and the lottery that yields $A$ with
probability $p$ and $C$ with probability $1-p$.
$$A \pref B \pref C \implies \Exi{p} [p,A;\ 1-p,C] \indiff B \ .$$

If an agent is indifferent between two lotteries $A$ and $B$, then the
agent is indifferent between two more complex lotteries that are the
same except that $B$ is substituted for $A$ in one of them. This holds
regardless of the probabilities and the other outcome(s) in the
lotteries.
$$A \indiff B \implies [p,A;\ 1-p,C] \indiff [p,B; 1-p,C] \ .$$ This
also holds if we substitute $\pref$ for $\indiff$ in this axiom.

Suppose two lotteries have the same two possible outcomes, $A$ and $B$.
If an agent prefers $A$ to $B$, then the agent must prefer the lottery
that has a higher probability for $A$ (and vice versa).
$$A \pref B \implies (p > q \lequiv [p,A;\ 1-p,B] \pref [q,A;\ 1-q,B])\ .$$

Compound lotteries can be reduced to simpler ones using the laws of
probability. This has been called the “no fun in gambling” rule because
it says that two consecutive lotteries can be compressed into a single
equivalent lottery, as shown in (b).[^3]
$$[p,A;\ 1-p,[q,B;\ 1-q,C]] \indiff [p,A;\ (1-p)q,B;\ (1-p)(1-q),C] \ .$$

These constraints are known as the axioms of utility theory. Each axiom
can be motivated by showing that an agent that violates it will exhibit
patently irrational behavior in some situations. For example, we can
motivate transitivity by making an agent with nontransitive preferences
give us all its money. Suppose that the agent has the nontransitive
preferences $A \pref B \pref C \pref A$, where $A$, $B$, and $C$ are
goods that can be freely exchanged. If the agent currently has $A$, then
we could offer to trade $C$ for $A$ plus one cent. The agent prefers
$C$, and so would be willing to make this trade. We could then offer to
trade $B$ for $C$, extracting another cent, and finally trade $A$ for
$B$. This brings us back where we started from, except that the agent
has given us three cents ((a)). We can keep going around the cycle until
the agent has no money at all. Clearly, the agent has acted irrationally
in this case.

[preference-figure]

### Preferences lead to utility

Notice that the axioms of utility theory are really axioms about
preferences—they say nothing about a utility function. But in fact from
the axioms of utility we can derive the following consequences \<for the
proof, see\>VonNeumann+Morgenstern:1944:

-   **Existence of Utility Function**: If an agent’s
    preferences obey the axioms of utility, then there exists a function
    $U$ such that $U(A) > U(B)$ if and only if $A$ is preferred to $B$,
    and $U(A) =
    U(B)$ if and only if the agent is indifferent between $A$ and $B$.

    U(A) \> U(B) A B\
    U(A) = U(B) A B

-   **Expected Utility of a Lottery**: The utility of a
    lottery is the sum of the probability of each outcome times the
    utility of that outcome.
    $$U([p_1,S_1;\ldots;p_n,S_n]) = \sum\limits_i p_i U(S_i)\ .$$

In other words, once the probabilities and utilities of the possible
outcome states are specified, the utility of a compound lottery
involving those states is completely determined. Because the outcome of
a nondeterministic action is a lottery, it follows that an agent can act
rationally—that is, consistently with its preferences—only by choosing
an action that maximizes expected utility according to .

The preceding theorems establish that a utility function
*exists* for any rational agent, but they do not establish
that it is *unique*. It is easy to see, in fact, that an
agent’s behavior would not change if its utility function $U(S)$ were
transformed according to

$$U'(S) = a U(S) + b\ ,
\label{affine-utility-equation}$$

where $a$ and $b$ are constants and $a > 0$; an affine
transformation.[^4] This fact was noted in for two-player games of
chance; here, we see that it is completely general.

As in game-playing, in a deterministic environment an agent just needs a
preference ranking on states—the numbers don’t matter. This is called a
or .

It is important to remember that the existence of a utility function
that describes an agent’s preference behavior does not necessarily mean
that the agent is *explicitly* maximizing that utility
function in its own deliberations. As we showed in , rational behavior
can be generated in any number of ways. By observing a rational agent’s
preferences, however, an observer can construct the utility function
that represents what the agent is actually trying to achieve (even if
the agent doesn’t know it).

Utility Functions {#utility-function-section}
-----------------

Utility is a function that maps from lotteries to real numbers. We know
there are some axioms on utilities that all rational agents must obey.
Is that all we can say about utility functions? Strictly speaking, that
is it: an agent can have any preferences it likes. For example, an agent
might prefer to have a prime number of dollars in its bank account; in
which case, if it had 16 it would give away 3. This might be unusual,
but we can’t call it irrational. An agent might prefer a dented 1973
Ford Pinto to a shiny new Mercedes. Preferences can also interact: for
example, the agent might prefer prime numbers of dollars only when it
owns the Pinto, but when it owns the Mercedes, it might prefer more
dollars to fewer. Fortunately, the preferences of real agents are
usually more systematic, and thus easier to deal with.

### Utility assessment and utility scales {#utility-assessment-section}

If we want to build a decision-theoretic system that helps the agent
make decisions or acts on his or her behalf, we must first work out what
the agent’s utility function is. This process, often called , involves
presenting choices to the agent and using the observed preferences to
pin down the underlying utility function.

says that there is no absolute scale for utilities, but it is helpful,
nonetheless, to establish *some* scale on which utilities
can be recorded and compared for any particular problem. A scale can be
established by fixing the utilities of any two particular outcomes, just
as we fix a temperature scale by fixing the freezing point and boiling
point of water. Typically, we fix the utility of a “best possible prize”
at $U(S) = \ubest$ and a “worst possible catastrophe” at
$U(S) = \uworst$. use a scale with $\uworst=0$ and $\ubest=1$.

Given a utility scale between $\ubest$ and $\uworst$, we can assess the
utility of any particular prize $S$ by asking the agent to choose
between $S$ and a $[p,\ubest;\, (1-p),\uworst]$. The probability $p$ is
adjusted until the agent is indifferent between $S$ and the standard
lottery. Assuming normalized utilities, the utility of $S$ is given by
$p$. Once this is done for each prize, the utilities for all lotteries
involving those prizes are determined.

In medical, transportation, and environmental decision problems, among
others, people’s lives are at stake. In such cases, $\uworst$ is the
value assigned to immediate death (or perhaps many deaths).

Although nobody feels comfortable with putting a value on human life, it
is a fact that tradeoffs are made all the time.

Aircraft are given a complete overhaul at intervals determined by trips
and miles flown, rather than after every trip. Cars are manufactured in
a way that trades off costs against accident survival rates.
Paradoxically, a refusal to “put a monetary value on life” means that
life is often *undervalued*. Ross Shachter relates an
experience with a government agency that commissioned a study on
removing asbestos from schools. The decision analysts performing the
study assumed a particular dollar value for the life of a school-age
child, and argued that the rational choice under that assumption was to
remove the asbestos. The agency, morally outraged at the idea of setting
the value of a life, rejected the report out of hand. It then decided
against asbestos removal—implicitly asserting a lower value for the life
of a child than that assigned by the analysts.

Some attempts have been made to find out the value that people place on
their own lives. One common “currency” used in medical and safety
analysis is the , a one in a million chance of death. If you ask people
how much they would pay to avoid a risk—for example, to avoid playing
Russian roulette with a million-barreled revolver—they will respond with
very large numbers, perhaps tens of thousands of dollars, but their
actual behavior reflects a much lower monetary value for a micromort.
For example, driving in a car for 230 miles incurs a risk of one
micromort; over the life of your car—say, 92,000 miles—that’s 400
micromorts. People appear to be willing to pay about 10,000 (at 2009
prices) more for a safer car that halves the risk of death, or about 50
per micromort. A number of studies have confirmed a figure in this range
across many individuals and risk types. Of course, this argument holds
only for small risks. Most people won’t agree to kill themselves for 50
million.

Another measure is the , or quality-adjusted life year. Patients with a
disability are willing to accept a shorter life expectancy to be
restored to full health. For example, kidney patients on average are
indifferent between living two years on a dialysis machine and one year
at full health.

### The utility of money

Utility theory has its roots in economics, and economics provides one
obvious candidate for a utility measure: money (or more specifically, an
agent’s total net assets). The almost universal exchangeability of money
for all kinds of goods and services suggests that money plays a
significant role in human utility functions.

It will usually be the case that an agent prefers more money to less,
all other things being equal. We say that the agent exhibits a for more
money. This does not mean that money behaves as a utility function,
because it says nothing about preferences between
*lotteries* involving money.

Suppose you have triumphed over the other competitors in a television
game show. The host now offers you a choice: either you can take the
1,000,000 prize or you can gamble it on the flip of a coin. If the coin
comes up heads, you end up with nothing, but if it comes up tails, you
get 2,500,000. If you’re like most people, you would decline the gamble
and pocket the million. Are you being irrational?

Assuming the coin is fair, the (EMV) of the gamble is $\frac{1}{2}$(0) +
$\frac{1}{2}$(2,500,000) $=$ 1,250,000, which is more than the original
1,000,000. But that does not necessarily mean that accepting the gamble
is a better decision. Suppose we use $S_n$ to denote the state of
possessing total wealth $n$, and that your current wealth is $k$. Then
the expected utilities of the two actions of accepting and declining the
gamble are

$$\begin{aligned}
{EU}({Accept}) &=& {\textstyle\frac{1}{2}}U(S_k) + {\textstyle\frac{1}{2}}U(S_{k+2,{500},{000}})\ , \\
{EU}({Decline}) &=& U(S_{k+1,{000},{000}})\ .\end{aligned}$$

To determine what to do, we need to assign utilities to the outcome
states. Utility is not directly proportional to monetary value, because
the utility for your first million is very high (or so they say),
whereas the utility for an additional million is smaller. Suppose you
assign a utility of 5 to your current financial status ($S_k$), a 9 to
the state $S_{k+2,{500},{000}}$, and an 8 to the state
$S_{k+1,{000},{000}}$. Then the rational action would be to decline,
because the expected utility of accepting is only 7 (less than the 8 for
declining). On the other hand, a billionaire would most likely have a
utility function that is locally linear over the range of a few million
more, and thus would accept the gamble.

In a pioneering study of actual utility functions,
Grayson [-@Grayson:1960] found that the utility of money was almost
exactly proportional to the *logarithm* of the amount.
(This idea was first suggested by ; see .) One particular utility curve,
for a certain Mr. Beard, is shown in (a). The data obtained for
Mr. Beard’s preferences are consistent with a utility function
$$U(S_{k+n}) = -{263.31} + {22.09} \log(n+{150},{000})$$ for the range
between $n=-{\DollarSign}{150},{000}$ and $n={\DollarSign}{800},{000}$.

[utility-curve-figure]

We should not assume that this is the definitive utility function for
monetary value, but it is likely that most people have a utility
function that is concave for positive wealth. Going into debt is bad,
but preferences between different levels of debt can display a reversal
of the concavity associated with positive wealth. For example, someone
already 10,000,000 in debt might well accept a gamble on a fair coin
with a gain of 10,000,000 for heads and a loss of 20,000,000 for
tails.[^5] This yields the S-shaped curve shown in (b).

If we restrict our attention to the positive part of the curves, where
the slope is decreasing, then for any lottery $L$, the utility of being
faced with that lottery is less than the utility of being handed the
expected monetary value of the lottery as a sure thing:
$$U(L) < U(S_{{EMV}(L)}) \ .$$ That is, agents with curves of this
shape are : they prefer a sure thing with a payoff that is less than the
expected monetary value of a gamble. On the other hand, in the
“desperate” region at large negative wealth in (b), the behavior is .
The value an agent will accept in lieu of a lottery is called the of the
lottery. Studies have shown that most people will accept about 400 in
lieu of a gamble that gives 1000 half the time and 0 the other half—that
is, the certainty equivalent of the lottery is 400, while the EMV is
500. The difference between the EMV of a lottery and its certainty
equivalent is called the . Risk aversion is the basis for the insurance
industry, because it means that insurance premiums are positive. People
would rather pay a small insurance premium than gamble the price of
their house against the chance of a fire. From the insurance company’s
point of view, the price of the house is very small compared with the
firm’s total reserves. This means that the insurer’s utility curve is
approximately linear over such a small region, and the gamble costs the
company almost nothing.

Notice that for *small* changes in wealth relative to the
current wealth, almost any curve will be approximately linear. An agent
that has a linear curve is said to be . For gambles with small sums,
therefore, we expect risk neutrality. In a sense, this justifies the
simplified procedure that proposed small gambles to assess probabilities
and to justify the axioms of probability in .

### Expected utility and post-decision disappointment

The rational way to choose the best action, $a^*$, is to maximize
expected utility: $$a^* = \argmax_a {EU}(a|\mbf{e}) \ .$$ If we have
calculated the expected utility correctly according to our probability
model, and if the probability model correctly reflects the underlying
stochastic processes that generate the outcomes, then, on average, we
will get the utility we expect if the whole process is repeated many
times.

In reality, however, our model usually oversimplifies the real
situation, either because we don’t know enough (e.g., when making a
complex investment decision) or because the computation of the true
expected utility is too difficult (e.g., when estimating the utility of
successor states of the root node in backgammon). In that case, we are
really working with *estimates*
$\widehat{{EU}}(a|\mbf{e})$ of the true expected utility. We will
assume, kindly perhaps, that the estimates are , that is, the expected
value of the error, $E(\widehat{{EU}}(a|\mbf{e}) -
{EU}(a|\mbf{e})))$, is zero. In that case, it still seems reasonable
to choose the action with the highest estimated utility and to expect to
receive that utility, on average, when the action is executed.

[curse-gaussians-figure]

Unfortunately, the real outcome will usually be significantly
*worse* than we estimated, even though the estimate was
unbiased! To see why, consider a decision problem in which there are $k$
choices, each of which has true estimated utility of 0. Suppose that the
error in each utility estimate has zero mean and standard deviation of
1, shown as the bold curve in . Now, as we actually start to generate
the estimates, some of the errors will be negative (pessimistic) and
some will be positive (optimistic). Because we select the action with
the *highest* utility estimate, we are obviously favoring
the overly optimistic estimates, and that is the source of the bias. It
is a straightforward matter to calculate the distribution of the maximum
of the $k$ estimates (see ) and hence quantify the extent of our
disappointment. The curve in for $k\eq 3$ has a mean around 0.85, so the
average disappointment will be about 85% of the standard deviation in
the utility estimates. With more choices, extremely optimistic estimates
are more likely to arise: for $k\eq 30$, the disappointment will be
around twice the standard deviation in the estimates.

This tendency for the estimated expected utility of the best choice to
be too high is called the [optimizers-curse-page] @Smith+Winkler:2006.
It afflicts even the most seasoned decision analysts and statisticians.
Serious manifestations include believing that an exciting new drug that
has cured 80% patients in a trial will cure 80% of patients (it’s been
chosen from $k\eq$ thousands of candidate drugs) or that a mutual fund
advertised as having above-average returns will continue to have them
(it’s been chosen to appear in the advertisement out of $k\eq{}$ dozens
of funds in the company’s overall portfolio). It can even be the case
that what appears to be the best choice may not be, if the variance in
the utility estimate is high: a drug, selected from thousands tried,
that has cured 9 of 10 patients is probably *worse* than
one that has cured 800 of 1000.

The optimizer’s curse crops up everywhere because of the ubiquity of
utility-maximizing selection processes, so taking the utility estimates
at face value is a bad idea. We can avoid the curse by using an explicit
probability model $\pv(\widehat{{EU}}\given {EU})$ of the error in
the utility estimates. Given this model and a prior $\pv({EU})$ on
what we might reasonably expect the utilities to be, we treat the
utility estimate, once obtained, as evidence and compute the posterior
distribution for the true utility using Bayes’ rule.

### Human judgment and irrationality

Decision theory is a : it describes how a rational agent
*should* act. A , on the other hand, describes how actual
agents—for example, humans—really do act. The application of economic
theory would be greatly enhanced if the two coincided, but there appears
to be some experimental evidence to the contrary. The evidence suggests
that humans are “predictably irrational” @Ariely:2009.

The best-known problem is the Allais paradox @Allais:1953. People are
given a choice between lotteries $A$ and $B$ and then between $C$ and
$D$, which have the following prizes:
$$\begin{array}{rrrr}\label{allais-page}
A:&\mbox{{80}\% chance of {\DollarSign}{4000}} & \qquad C:&\mbox{{20}\% chance of {\DollarSign}{4000}} \\
B:&\mbox{{100}\% chance of {\DollarSign}{3000}} & \qquad D:&\mbox{{25}\% chance of {\DollarSign}{3000}} 
  \end{array}$$ Most people consistently prefer $B$ over $A$ (taking the
sure thing), and $C$ over $D$ (taking the higher EMV). The normative
analysis disagrees! We can see this most easily if we use the freedom
implied by to set $U({\DollarSign}0) = 0$. In that case, then
$B \pref A$ implies that
$U({\DollarSign}{3000}) > {0.8}\,U({\DollarSign}{4000}) $, whereas
$C \pref D$ implies exactly the reverse. In other words, there is no
utility function that is consistent with these choices. One explanation
for the apparently irrational preferences is the @Kahneman+Tversky:1979:
people are strongly attracted to gains that are certain. There are
several reasons why this may be so. First, people may prefer to reduce
their computational burden; by choosing certain outcomes, they don’t
have to compute with probabilities. But the effect persists even when
the computations involved are very easy ones. Second, people may
distrust the legitimacy of the stated probabilities. I trust that a coin
flip is roughly 50/50 if I have control over the coin and the flip, but
I may distrust the result if the flip is done by someone with a vested
interest in the outcome.[^6] In the presence of distrust, it might be
better to go for the sure thing.[^7] Third, people may be accounting for
their emotional state as well as their financial state. People know they
would experience if they gave up a certain reward ($B$) for an 80%
chance at a higher reward and then lost. In other words, if $A$ is
chosen, there is a 20% chance of getting no money *and feeling
like a complete idiot*, which is worse than just getting no
money. So perhaps people who choose $B$ over $A$ and $C$ over $D$ are
not being irrational; they are just saying that they are willing to give
up 200 of EMV to avoid a 20% chance of feeling like an idiot.

A related problem is the Ellsberg paradox. Here the prizes are fixed,
but the probabilities are underconstrained. Your payoff will depend on
the color of a ball chosen from an urn. You are told that the urn
contains 1/3 red balls, and 2/3 either black or yellow balls, but you
don’t know how many black and how many yellow. Again, you are asked
whether you prefer lottery $A$ or $B$; and then $C$ or $D$:
$$\begin{array}{rlrl}
A:&\mbox{{\DollarSign}{100} for a red ball} & \qquad C:&\mbox{{\DollarSign}{100} for a red or yellow ball} \\
B:&\mbox{{\DollarSign}{100} for a black ball} & \qquad D:&\mbox{{\DollarSign}{100} for a black or yellow ball} \ .
  \end{array}$$ It should be clear that if you think there are more red
than black balls then you should prefer $A$ over $B$ and $C$ over $D$;
if you think there are fewer red than black you should prefer the
opposite. But it turns out that most people prefer $A$ over $B$ and also
prefer $D$ over $C$, even though there is no state of the world for
which this is rational. It seems that people have : $A$ gives you a 1/3
chance of winning, while $B$ could be anywhere between 0 and 2/3.
Similarly, $D$ gives you a 2/3 chance, while $C$ could be anywhere
between 1/3 and 3/3. Most people elect the known probability rather than
the unknown unknowns.

Yet another problem is that the exact wording of a decision problem can
have a big impact on the agent’s choices; this is called the .
Experiments show that people like a medical procedure that it is
described as having a “90% survival rate” about twice as much as one
described as having a “10% death rate,” even though these two statements
mean exactly the same thing. This discrepancy in judgment has been found
in multiple experiments and is about the same whether the subjects were
patients in a clinic, statistically sophisticated business school
students, or experienced doctors.

People feel more comfortable making *relative* utility
judgments rather than absolute ones. I may have little idea how much I
might enjoy the various wines offered by a restaurant. The restaurant
takes advantage of this by offering a 200 bottle that it knows nobody
will buy, but which serves to skew upward the customer’s estimate of the
value of all wines and make the 55 bottle seem like a bargain. This is
called the .

If human informants insist on contradictory preference judgments, there
is nothing that automated agents can do to be consistent with them.
Fortunately, preference judgments made by humans are often open to
revision in the light of further consideration. Paradoxes like the
Allais paradox are greatly reduced (but not eliminated) if the choices
are explained better. In work at the Harvard Business School on
assessing the utility of money, Keeney and Raiffa [-@Keeney+Raiffa:1976
p. 210] found the following:

Subjects tend to be too risk-averse in the small and therefore $\ldots$
the fitted utility functions exhibit unacceptably large risk premiums
for lotteries with a large spread. $\ldots$ Most of the subjects,
however, can reconcile their inconsistencies and feel that they have
learned an important lesson about how they want to behave. As a
consequence, some subjects cancel their automobile collision insurance
and take out more term insurance on their lives.

The evidence for human irrationality is also questioned by researchers
in the field of , who point to the fact that our brain’s decision-making
mechanisms did not evolve to solve word problems with probabilities and
prizes stated as decimal numbers. Let us grant, for the sake of
argument, that the brain has built-in neural mechanism for computing
with probabilities and utilities, or something functionally equivalent;
if so, the required inputs would be obtained through accumulated
experience of outcomes and rewards rather than through linguistic
presentations of numerical values. It is far from obvious that we can
directly access the brain’s built-in neural mechanisms by presenting
decision problems in linguistic/numerical form. The very fact that
different wordings of the *same decision problem* elicit
different choices suggests that the decision problem itself is not
getting through. Spurred by this observation, psychologists have tried
presenting problems in uncertain reasoning and decision making in
“evolutionarily appropriate” forms; for example, instead of saying “90%
survival rate,” the experimenter might show 100 stick-figure animations
of the operation, where the patient dies in 10 of them and survives in
90. (Boredom is a complicating factor in these experiments!) With
decision problems posed in this way, people seem to be much closer to
rational behavior than previously suspected.

Multiattribute Utility Functions {#MAUT-section}
--------------------------------

Decision making in the field of public policy involves high stakes, in
both money and lives. For example, in deciding what levels of harmful
emissions to allow from a power plant, policy makers must weigh the
prevention of death and disability against the benefit of the power and
the economic burden of mitigating the emissions. Siting a new airport
requires consideration of the disruption caused by construction; the
cost of land; the distance from centers of population; the noise of
flight operations; safety issues arising from local topography and
weather conditions; and so on. Problems like these, in which outcomes
are characterized by two or more attributes, are handled by .

We will call the attributes $\X \eq X_1,\ldots,X_n$; a complete vector
of assignments will be $\x\eq \<x_1,\ldots,x_n\>$, where each $x_i$ is
either a numeric value or a discrete value with an assumed ordering on
values. We will assume that higher values of an attribute correspond to
higher utilities, all other things being equal. For example, if we
choose ${AbsenceOfNoise}$ as an attribute in the airport problem, then
the greater its value, the better the solution.[^8] We begin by
examining cases in which decisions can be made *without*
combining the attribute values into a single utility value. Then we look
at cases in which the utilities of attribute combinations can be
specified very concisely.

### Dominance

Suppose that airport site $S_1$ costs less, generates less noise
pollution, and is safer than site $S_2$. One would not hesitate to
reject $S_2$. We then say that there is of $S_1$ over $S_2$. In general,
if an option is of lower value on all attributes than some other option,
it need not be considered further. Strict dominance is often very useful
in narrowing down the field of choices to the real contenders, although
it seldom yields a unique choice. (a) shows a schematic diagram for the
two-attribute case.

[strict-dominance-figure]

That is fine for the deterministic case, in which the attribute values
are known for sure. What about the general case, where the outcomes are
uncertain? A direct analog of strict dominance can be constructed,
where, despite the uncertainty, all possible concrete outcomes for $S_1$
strictly dominate all possible outcomes for $S_2$. (See (b).) Of course,
this will probably occur even less often than in the deterministic case.

Fortunately, there is a more useful generalization called , which occurs
very frequently in real problems. Stochastic dominance is easiest to
understand in the context of a single attribute. Suppose we believe that
the cost of siting the airport at $S_1$ is uniformly distributed between
2.8 billion and 4.8 billion and that the cost at $S_2$ is uniformly
distributed between 3 billion and 5.2 billion. (a) shows these
distributions, with cost plotted as a negative value. Then, given only
the information that utility decreases with cost, we can say that $S_1$
stochastically dominates $S_2$ (i.e., $S_2$ can be discarded). It is
important to note that this does *not* follow from
comparing the expected costs. For example, if we knew the cost of $S_1$
to be *exactly* 3.8 billion, then we would be
*unable* to make a decision without additional information
on the utility of money. (It might seem odd that *more*
information on the cost of $S_1$ could make the agent
*less* able to decide. The paradox is resolved by noting
that in the absence of exact cost information, the decision is easier to
make but is more likely to be wrong.)

[stochastic-dominance-figure]

The exact relationship between the attribute distributions needed to
establish stochastic dominance is best seen by examining the , shown in
(b). (See also .) The cumulative distribution measures the probability
that the cost is less than or equal to any given amount—that is, it
integrates the original distribution. If the cumulative distribution for
$S_1$ is always to the right of the cumulative distribution for $S_2$,
then, stochastically speaking, $S_1$ is cheaper than $S_2$. Formally, if
two actions $A_1$ and $A_2$ lead to probability distributions $p_1(x)$
and $p_2(x)$ on attribute $X$, then $A_1$ stochastically dominates $A_2$
on $X$ if $$\All{x} \int\limits_{-\infty}^{x} p_1(x')\;dx' \leq 
           \int\limits_{-\infty}^{x} p_2(x')\;dx'\ .$$ The relevance of
this definition to the selection of optimal decisions comes from the
following property:

if $A_1$ stochastically dominates $A_2$, then for any monotonically
nondecreasing utility function $U(x)$, the expected utility of $A_1$ is
at least as high as the expected utility of $A_2$.

Hence, if an action is stochastically dominated by another action on all
attributes, then it can be discarded.

The stochastic dominance condition might seem rather technical and
perhaps not so easy to evaluate without extensive probability
calculations. In fact, it can be decided very easily in many cases.
Suppose, for example, that the construction transportation cost depends
on the distance to the supplier. The cost itself is uncertain, but the
greater the distance, the greater the cost. If $S_1$ is closer than
$S_2$, then $S_1$ will dominate $S_2$ on cost. Although we will not
present them here, there exist algorithms for propagating this kind of
qualitative information among uncertain variables in , enabling a system
to make rational decisions based on stochastic dominance, without using
any numeric values.

### Preference structure and multiattribute utility

Suppose we have $n$ attributes, each of which has $d$ distinct possible
values. To specify the complete utility function $U(x_1,\ldots,x_n)$, we
need $d^n$ values in the worst case. Now, the worst case corresponds to
a situation in which the agent’s preferences have no regularity at all.
Multiattribute utility theory is based on the supposition that the
preferences of typical agents have much more structure than that. The
basic approach is to identify regularities in the preference behavior we
would expect to see and to use what are called to show that an agent
with a certain kind of preference structure has a utility function
$$U(x_1,\ldots,x_n) = F[f_1(x_1),\ldots, f_n(x_n)]\ ,$$ where $F$ is, we
hope, a simple function such as addition. Notice the similarity to the
use of Bayesian networks to decompose the joint probability of several
random variables.

#### Preferences without uncertainty

Let us begin with the deterministic case. Remember that for
deterministic environments the agent has a value function
$V(x_1,\ldots,x_n)$; the aim is to represent this function concisely.
The basic regularity that arises in deterministic preference structures
is called . Two attributes $X_1$ and $X_2$ are preferentially
independent of a third attribute $X_3$ if the preference between
outcomes $\langle
x_1,x_2,x_3 \rangle$ and $\langle x_1',x_2',x_3 \rangle$ does not depend
on the particular value $x_3$ for attribute $X_3$.

Going back to the airport example, where we have (among other
attributes) ${Noise}$, ${Cost}$, and ${Deaths}$ to consider, one
may propose that ${Noise}$ and ${Cost}$ are preferentially
independent of ${Deaths}$. For example, if we prefer a state with
20,000 people residing in the flight path and a construction cost of 4
billion over a state with 70,000 people residing in the flight path and
a cost of 3.7 billion when the safety level is 0.06 deaths per million
passenger miles in both cases, then we would have the same preference
when the safety level is 0.12 or 0.03; and the same independence would
hold for preferences between any other pair of values for ${Noise}$
and ${Cost}$. It is also apparent that ${Cost}$ and ${Deaths}$ are
preferentially independent of ${Noise}$ and that ${Noise}$ and
${Deaths}$ are preferentially independent of ${Cost}$. We say that
the set of attributes $\{{Noise},{Cost},{Deaths}\}$ exhibits
(MPI). MPI says that, whereas each attribute may be important, it does
not affect the way in which one trades off the other attributes against
each other.

Mutual preferential independence is something of a mouthful, but thanks
to a remarkable theorem due to the economist Gérard
Debreu [-@Debreu:1960], we can derive from it a very simple form for the
agent’s value function:

If attributes $X_1,\, \ldots,\, X_n$ are mutually preferentially
independent, then the agent’s preference behavior can be described as
maximizing the function
$$V(x_1,\ldots,x_n) = \sum\limits_i V_i(x_i)\ ,$$ where each $V_i$ is a
value function referring only to the attribute $X_i$.

For example, it might well be the case that the airport decision can be
made using a value function
$$V({noise},{cost},{deaths}) = -{noise}\times {10}^4 - {cost} - {deaths} \times {10}^{{12}}\ .$$
A value function of this type is called an . Additive functions are an
extremely natural way to describe an agent’s preferences and are valid
in many real-world situations. For $n$ attributes, assessing an additive
value function requires assessing $n$ separate one-dimensional value
functions rather than one $n$-dimensional function; typically, this
represents an exponential reduction in the number of preference
experiments that are needed. Even when MPI does not strictly hold, as
might be the case at extreme values of the attributes, an additive value
function might still provide a good approximation to the agent’s
preferences. This is especially true when the violations of MPI occur in
portions of the attribute ranges that are unlikely to occur in practice.

To understand MPI better, it helps to look at cases where it
*doesn’t* hold. Suppose you are at a medieval market,
considering the purchase of some hunting dogs, some chickens, and some
wicker cages for the chickens. The hunting dogs are very valuable, but
if you don’t have enough cages for the chickens, the dogs will eat the
chickens; hence, the tradeoff between dogs and chickens depends strongly
on the number of cages, and MPI is violated. The existence of these
kinds of interactions among various attributes makes it much harder to
assess the overall value function.

#### Preferences with uncertainty

When uncertainty is present in the domain, we also need to consider the
structure of preferences between lotteries and to understand the
resulting properties of utility functions, rather than just value
functions. The mathematics of this problem can become quite complicated,
so we present just one of the main results to give a flavor of what can
be done. The reader is referred to Keeney and
Raiffa [-@Keeney+Raiffa:1976] for a thorough survey of the field.

The basic notion of extends preference independence to cover lotteries:
a set of attributes $\mbf{X}$ is utility independent of a set of
attributes $\mbf{Y}$ if preferences between lotteries on the attributes
in $\mbf{X}$ are independent of the particular values of the attributes
in $\mbf{Y}$. A set of attributes is (MUI) if each of its subsets is
utility-independent of the remaining attributes. Again, it seems
reasonable to propose that the airport attributes are MUI.

MUI implies that the agent’s behavior can be described using a
 @Keeney:1974. The general form of a multiplicative utility function is
best seen by looking at the case for three attributes. For conciseness,
we use $U_i$ to mean $U_i(x_i)$:

U = k~1~U~1~ + k~2~U~2~ + k~3~U~3~ + k~1~k~2~U~1~U~2~ + k~2~k~3~U~2~U~3~
+ k~3~k~1~U~3~U~1~\
 + k~1~k~2~k~3~U~1~U~2~U~3~ .

Although this does not look very simple, it contains just three
single-attribute utility functions and three constants. In general, an
$n$-attribute problem exhibiting MUI can be modeled using $n$
single-attribute utilities and $n$ constants. Each of the
single-attribute utility functions can be developed independently of the
other attributes, and this combination will be guaranteed to generate
the correct overall preferences. Additional assumptions are required to
obtain a purely additive utility function.

Decision Networks {#influence-diagram-section}
-----------------

In this section, we look at a general mechanism for making rational
decisions. The notation is often called an  @Howard+Matheson:1981, but
we will use the more descriptive term . Decision networks combine
Bayesian networks with additional node types for actions and utilities.
We use airport siting as an example.

### Representing a decision problem with a decision network

In its most general form, a decision network represents information
about the agent’s current state, its possible actions, the state that
will result from the agent’s action, and the utility of that state. It
therefore provides a substrate for implementing utility-based agents of
the type first introduced in . shows a decision network for the airport
siting problem. It illustrates the three types of nodes used:

(ovals) represent random variables, just as they do in Bayesian
networks. The agent could be uncertain about the construction cost, the
level of air traffic and the potential for litigation, and the
${Deaths}$, ${Noise}$, and total ${Cost}$ variables, each of which
also depends on the site chosen. Each chance node has associated with it
a conditional distribution that is indexed by the state of the parent
nodes. In decision networks, the parent nodes can include decision nodes
as well as chance nodes. Note that each of the current-state chance
nodes could be part of a large Bayesian network for assessing
construction costs, air traffic levels, or litigation potentials.
(rectangles) represent points where the decision maker has a choice of
actions. In this case, the ${AirportSite}$ action can take on a
different value for each site under consideration. The choice influences
the cost, safety, and noise that will result. In this chapter, we assume
that we are dealing with a single decision node. deals with cases in
which more than one decision must be made. (diamonds) represent the
agent’s utility function.[^9] The utility node has as parents all
variables describing the outcome that directly affect utility.
Associated with the utility node is a description of the agent’s utility
as a function of the parent attributes. The description could be just a
tabulation of the function, or it might be a parameterized additive or
linear function of the attribute values.

[airport-id-figure]

A simplified form is also used in many cases. The notation remains
identical, but the chance nodes describing the outcome state are
omitted. Instead, the utility node is connected directly to the
current-state nodes and the decision node. In this case, rather than
representing a utility function on outcome states, the utility node
represents the *expected* utility associated with each
action, as defined in on ; that is, the node is associated with an (also
known as a in reinforcement learning, as described in ). shows the
action-utility representation of the airport siting problem.

[airport-au-id-figure]

Notice that, because the ${Noise}$, ${Deaths}$, and ${Cost}$
chance nodes in refer to future states, they can never have their values
set as evidence variables. Thus, the simplified version that omits these
nodes can be used whenever the more general form can be used. Although
the simplified form contains fewer nodes, the omission of an explicit
description of the outcome of the siting decision means that it is less
flexible with respect to changes in circumstances. For example, in , a
change in aircraft noise levels can be reflected by a change in the
conditional probability table associated with the ${Noise}$ node,
whereas a change in the weight accorded to noise pollution in the
utility function can be reflected by a change in the utility table. In
the action-utility diagram, , on the other hand, all such changes have
to be reflected by changes to the action-utility table. Essentially, the
action-utility formulation is a *compiled* version of the
original formulation.

### Evaluating decision networks

Actions are selected by evaluating the decision network for each
possible setting of the decision node. Once the decision node is set, it
behaves exactly like a chance node that has been set as an evidence
variable. The algorithm for evaluating decision networks is the
following:

1.  Set the evidence variables for the current state.

2.  For each possible value of the decision node:

    1.  Set the decision node to that value.

    2.  Calculate the posterior probabilities for the parent nodes of
        the utility node, using a standard probabilistic inference
        algorithm.

    3.  Calculate the resulting utility for the action.

3.  Return the action with the highest utility.

This is a straightforward extension of the Bayesian network algorithm
and can be incorporated directly into the agent design given in on . We
will see in that the possibility of executing several actions in
sequence makes the problem much more interesting.

The Value of Information {#VPI-section}
------------------------

In the preceding analysis, we have assumed that all relevant
information, or at least all available information, is provided to the
agent before it makes its decision. In practice, this is hardly ever the
case.

One of the most important parts of decision making is knowing what
questions to ask.

For example, a doctor cannot expect to be provided with the results of
*all possible* diagnostic tests and questions at the time a
patient first enters the consulting room.[^10] Tests are often expensive
and sometimes hazardous (both directly and because of associated
delays). Their importance depends on two factors: whether the test
results would lead to a significantly better treatment plan, and how
likely the various test results are.

This section describes , which enables an agent to choose what
information to acquire. We assume that, prior to selecting a “real”
action represented by the decision node, the agent can acquire the value
of any of the potentially observable chance variables in the model.
Thus, information value theory involves a simplified form of sequential
decision making—simplified because the observation actions affect only
the agent’s , not the external physical state. The value of any
particular observation must derive from the potential to affect the
agent’s eventual physical action; and this potential can be estimated
directly from the decision model itself.

### A simple example

Suppose an oil company is hoping to buy one of $n$ indistinguishable
blocks of ocean-drilling rights. Let us assume further that exactly one
of the blocks contains oil worth $C$ dollars, while the others are
worthless. The asking price of each block is $C/n$ dollars. If the
company is risk-neutral, then it will be indifferent between buying a
block and not buying one.

Now suppose that a seismologist offers the company the results of a
survey of block number 3, which indicates definitively whether the block
contains oil. How much should the company be willing to pay for the
information? The way to answer this question is to examine what the
company would do if it had the information:

-   With probability $1/n$, the survey will indicate oil in block 3. In
    this case, the company will buy block 3 for $C/n$ dollars and make a
    profit of $C - C/n = (n-1)C/n$ dollars.

-   With probability $(n-1)/n$, the survey will show that the block
    contains no oil, in which case the company will buy a different
    block. Now the probability of finding oil in one of the other blocks
    changes from $1/n$ to $1/(n-1)$, so the company makes an expected
    profit of $C/(n-1) - C/n = C/n(n-1)$ dollars.

Now we can calculate the expected profit, given the survey information:
$$\frac{1}{n}\times \frac{(n-1)C}{n} + \frac{n-1}{n}\times
\frac{C}{n(n-1)} = C/n\ .$$ Therefore, the company should be willing to
pay the seismologist up to $C/n$ dollars for the information: the
information is worth as much as the block itself.

The value of information derives from the fact that *with*
the information, one’s course of action can be changed to suit the
*actual* situation. One can discriminate according to the
situation, whereas without the information, one has to do what’s best on
average over the possible situations. In general, the value of a given
piece of information is defined to be the difference in expected value
between best actions before and after information is obtained.

### A general formula for perfect information

It is simple to derive a general mathematical formula for the value of
information. We assume that exact evidence can be obtained about the
value of some random variable $E_j$ (that is, we learn $E_j =
e_{j}$), so the phrase (VPI) is used.[^11]

Let the agent’s initial evidence be $\mbf{e}$. Then the value of the
current best action $\alpha$ is defined by
$${EU}(\alpha|\mbf{e}) = \max_{a} \sum\limits_{s'}  P(\Result(a)\eq s' \given a,\mbf{e}) \, U(s') \ ,$$
and the value of the new best action (after the new evidence $E_j = e_j$
is obtained) will be
$${EU}(\alpha_{e_j}|\mbf{e},e_j) = \max_{a} \sum\limits_{s'}  P(\Result(a)\eq s' \given a,\mbf{e},e_j) \, U(s')\ .$$
But $E_j$ is a random variable whose value is *currently*
unknown, so to determine the value of discovering $E_j$, given current
information $\mbf{e}$ we must average over all possible values $e_{jk}$
that we might discover for $E_j$, using our *current*
beliefs about its value: $${VPI}{}_{\smbf{e}}(E_j) = 
\left(\sum\limits_k P(E_j \eq e_{jk}|\mbf{e})\;
{EU}(\alpha_{e_{jk}}|\mbf{e},E_j \eq e_{jk})\right) - {EU}(\alpha|\mbf{e}) \ .$$
To get some intuition for this formula, consider the simple case where
there are only two actions, $a_1$ and $a_2$, from which to choose. Their
current expected utilities are $U_1$ and $U_2$. The information
$E_j = e_{jk}$ will yield some new expected utilities $U_1'$ and $U_2'$
for the actions, but before we obtain $E_j$, we will have some
probability distributions over the possible values of $U_1'$ and $U_2'$
(which we assume are independent).

Suppose that $a_1$ and $a_2$ represent two different routes through a
mountain range in winter. $a_1$ is a nice, straight highway through a
low pass, and $a_2$ is a winding dirt road over the top. Just given this
information, $a_1$ is clearly preferable, because it is quite possible
that $a_2$ is blocked by avalanches, whereas it is unlikely that
anything blocks $a_1$. $U_1$ is therefore clearly higher than $U_2$. It
is possible to obtain satellite reports $E_j$ on the actual state of
each road that would give new expectations, $U_1'$ and $U_2'$, for the
two crossings. The distributions for these expectations are shown in
(a). Obviously, in this case, it is not worth the expense of obtaining
satellite reports, because it is unlikely that the information derived
from them will change the plan. With no change, information has no
value.

Now suppose that we are choosing between two different winding dirt
roads of slightly different lengths and we are carrying a seriously
injured passenger. Then, even when $U_1$ and $U_2$ are quite close, the
distributions of $U_1'$ and $U_2'$ are very broad. There is a
significant possibility that the second route will turn out to be clear
while the first is blocked, and in this case the difference in utilities
will be very high. The VPI formula indicates that it might be worthwhile
getting the satellite reports. Such a situation is shown in (b).

Finally, suppose that we are choosing between the two dirt roads in
summertime, when blockage by avalanches is unlikely. In this case,
satellite reports might show one route to be more scenic than the other
because of flowering alpine meadows, or perhaps wetter because of errant
streams. It is therefore quite likely that we would change our plan if
we had the information. In this case, however, the difference in value
between the two routes is still likely to be very small, so we will not
bother to obtain the reports. This situation is shown in (c).

In sum,

information has value to the extent that it is *likely* to
cause a change of plan and to the extent that the new plan will be
*significantly better* than the old plan.

[3cases-figure]

### Properties of the value of information

One might ask whether it is possible for information to be deleterious:
can it actually have negative expected value? Intuitively, one should
expect this to be impossible. After all, one could in the worst case
just ignore the information and pretend that one has never received it.
This is confirmed by the following theorem, which applies to any
decision-theoretic agent:

The expected value of information is nonnegative:

$$\All{\mbf{e},E_j} {VPI}{}_{\smbf{e}}(E_j)\geq 0\ .$$ The theorem
follows directly from the definition of VPI, and we leave the proof as
an exercise (). It is, of course, a theorem about
*expected* value, not *actual* value.
Additional information can easily lead to a plan that *turns out
to* be worse than the original plan if the information happens to
be misleading. For example, a medical test that gives a false positive
result may lead to unnecessary surgery; but that does not mean that the
test shouldn’t be done.

It is important to remember that VPI depends on the current state of
information, which is why it is subscripted. It can change as more
information is acquired. For any given piece of evidence $E_j$, the
value of acquiring it can go down (e.g., if another variable strongly
constrains the posterior for $E_j$) or up (e.g., if another variable
provides a clue on which $E_j$ builds, enabling a new and better plan to
be devised). Thus, VPI is not additive. That is,
$${VPI}{}_{\smbf{e}}(E_j,E_k) \not= {VPI}{}_{\smbf{e}}(E_j) + {VPI}{}_{\smbf{e}}(E_k) \qquad \mbox{(in general)}\ .$$
VPI is, however, order independent. That is,
$${VPI}{}_{\smbf{e}}(E_j,E_k) = {VPI}{}_{\smbf{e}}(E_j) + {VPI}{}_{\smbf{e},e_j}(E_k)
                   = {VPI}{}_{\smbf{e}}(E_k) + {VPI}{}_{\smbf{e},e_k}(E_j)\ .$$
Order independence distinguishes sensing actions from ordinary actions
and simplifies the problem of calculating the value of a sequence of
sensing actions.

### Implementation of an information-gathering agent

A sensible agent should ask questions in a reasonable order, should
avoid asking questions that are irrelevant, should take into account the
importance of each piece of information in relation to its cost, and
should stop asking questions when that is appropriate. All of these
capabilities can be achieved by using the value of information as a
guide.

shows the overall design of an agent that can gather information
intelligently before acting. For now, we assume that with each
observable evidence variable $E_j$, there is an associated cost,
${Cost}(E_j)$, which reflects the cost of obtaining the evidence
through tests, consultants, questions, or whatever. The agent requests
what appears to be the most efficient observation in terms of utility
gain per unit cost. We assume that the result of the action
${Request}(E_j)$ is that the next percept provides the value of $E_j$.
If no observation is worth its cost, the agent selects a “real” action.

[information-gathering-algorithm]

The agent algorithm we have described implements a form of information
gathering that is called . This is because it uses the VPI formula
shortsightedly, calculating the value of information as if only a single
evidence variable will be acquired. Myopic control is based on the same
heuristic idea as greedy search and often works well in practice. (For
example, it has been shown to outperform expert physicians in selecting
diagnostic tests.) However, if there is no single evidence variable that
will help a lot, a myopic agent might hastily take an action when it
would have been better to request two or more variables first and then
take action. A better approach in this situation would be to construct a
*conditional plan* (as described in ) that asks for
variable values and takes different next steps depending on the answer.

One final consideration is the effect a series of questions will have on
a human respondent. People may respond better to a series of questions
if they “make sense,” so some expert systems are built to take this into
account, asking questions in an order that maximizes the total utility
of the system and human rather than an order that maximizes value of
information.

Decision-Theoretic Expert Systems
---------------------------------

The field of , which evolved in the 1950s and 1960s, studies the
application of decision theory to actual decision problems. It is used
to help make rational decisions in important domains where the stakes
are high, such as business, government, law, military strategy, medical
diagnosis and public health, engineering design, and resource
management. The process involves a careful study of the possible actions
and outcomes, as well as the preferences placed on each outcome. It is
traditional in decision analysis to talk about two roles: the states
preferences between outcomes, and the enumerates the possible actions
and outcomes and elicits preferences from the decision maker to
determine the best course of action. Until the early 1980s, the main
purpose of decision analysis was to help humans make decisions that
actually reflect their own preferences. As more and more decision
processes become automated, decision analysis is increasingly used to
ensure that the automated processes are behaving as desired.

Early expert system research concentrated on answering questions, rather
than on making decisions. Those systems that did recommend actions
rather than providing opinions on matters of fact generally did so using
condition-action rules, rather than with explicit representations of
outcomes and preferences. The emergence of Bayesian networks in the late
1980s made it possible to build large-scale systems that generated sound
probabilistic inferences from evidence. The addition of decision
networks means that expert systems can be developed that recommend
optimal decisions, reflecting the preferences of the agent as well as
the available evidence.

A system that incorporates utilities can avoid one of the most common
pitfalls associated with the consultation process: confusing likelihood
and importance. A common strategy in early medical expert systems, for
example, was to rank possible diagnoses in order of likelihood and
report the most likely. Unfortunately, this can be disastrous! For the
majority of patients in general practice, the two most
*likely* diagnoses are usually “There’s nothing wrong with
you” and “You have a bad cold,” but if the third most likely diagnosis
for a given patient is lung cancer, that’s a serious matter. Obviously,
a testing or treatment plan should depend both on probabilities and
utilities. Current medical expert systems can take into account the
value of information to recommend tests, and then describe a
differential diagnosis.

We now describe the knowledge engineering process for decision-theoretic
expert systems. As an example we consider the problem of selecting a
medical treatment for a kind of congenital heart disease in
children \<see\>Lucas:1996.

About 0.8% of children are born with a heart anomaly, the most common
being (a constriction of the aorta). It can be treated with surgery,
angioplasty (expanding the aorta with a balloon placed inside the
artery), or medication. The problem is to decide what treatment to use
and when to do it: the younger the infant, the greater the risks of
certain treatments, but one mustn’t wait too long. A decision-theoretic
expert system for this problem can be created by a team consisting of at
least one domain expert (a pediatric cardiologist) and one knowledge
engineer. The process can be broken down into the following steps:

**Create a causal model**. Determine the possible symptoms,
disorders, treatments, and outcomes. Then draw arcs between them,
indicating what disorders cause what symptoms, and what treatments
alleviate what disorders. Some of this will be well known to the domain
expert, and some will come from the literature. Often the model will
match well with the informal graphical descriptions given in medical
textbooks.

**Simplify to a qualitative decision model**. Since we are
using the model to make treatment decisions and not for other purposes
(such as determining the joint probability of certain symptom/disorder
combinations), we can often simplify by removing variables that are not
involved in treatment decisions. Sometimes variables will have to be
split or joined to match the expert’s intuitions. For example, the
original aortic coarctation model had a *Treatment*
variable with values *surgery*, *angioplasty*,
and *medication*, and a separate variable for
*Timing* of the treatment. But the expert had a hard time
thinking of these separately, so they were combined, with
*Treatment* taking on values such as *surgery in 1
month*. This gives us the model of .

[heart-infl-diagram-figure]

**Assign probabilities**. Probabilities can come from
patient databases, literature studies, or the expert’s subjective
assessments. Note that a diagnostic system will reason from symptoms and
other observations to the disease or other cause of the problems. Thus,
in the early years of building these systems, experts were asked for the
probability of a cause given an effect. In general they found this
difficult to do, and were better able to assess the probability of an
effect given a cause. So modern systems usually assess causal knowledge
and encode it directly in the Bayesian network structure of the model,
leaving the diagnostic reasoning to the Bayesian network inference
algorithms @Shachter+Heckerman:1987.

**Assign utilities**. When there are a small number of
possible outcomes, they can be enumerated and evaluated individually
using the methods of . We would create a scale from best to worst
outcome and give each a numeric value, for example 0 for death and 1 for
complete recovery. We would then place the other outcomes on this scale.
This can be done by the expert, but it is better if the patient (or in
the case of infants, the patient’s parents) can be involved, because
different people have different preferences. If there are exponentially
many outcomes, we need some way to combine them using multiattribute
utility functions. For example, we may say that the costs of various
complications are additive.

**Verify and refine the model**. To evaluate the system we
need a set of correct (input, output) pairs; a so-called to compare
against. For medical expert systems this usually means assembling the
best available doctors, presenting them with a few cases, and asking
them for their diagnosis and recommended treatment plan. We then see how
well the system matches their recommendations. If it does poorly, we try
to isolate the parts that are going wrong and fix them. It can be useful
to run the system “backward.” Instead of presenting the system with
symptoms and asking for a diagnosis, we can present it with a diagnosis
such as “heart failure,” examine the predicted probability of symptoms
such as tachycardia, and compare with the medical literature.

**Perform** . This important step checks whether the best
decision is sensitive to small changes in the assigned probabilities and
utilities by systematically varying those parameters and running the
evaluation again. If small changes lead to significantly different
decisions, then it could be worthwhile to spend more resources to
collect better data. If all variations lead to the same decision, then
the agent will have more confidence that it is the right decision.
Sensitivity analysis is particularly important, because one of the main
criticisms of probabilistic approaches to expert systems is that it is
too difficult to assess the numerical probabilities required.
Sensitivity analysis often reveals that many of the numbers need be
specified only very approximately. For example, we might be uncertain
about the conditional probability
$P({tachycardia} \given {dyspnea})$, but if the optimal decision is
reasonably robust to small variations in the probability, then our
ignorance is less of a concern.

This chapter shows how to combine utility theory with probability to
enable an agent to select actions that will maximize its expected
performance.

-   describes what an agent should believe on the basis of evidence,
    describes what an agent wants, and puts the two together to describe
    what an agent should do.

-   We can use decision theory to build a system that makes decisions by
    considering all possible actions and choosing the one that leads to
    the best expected outcome. Such a system is known as a .

-   Utility theory shows that an agent whose preferences between
    lotteries are consistent with a set of simple axioms can be
    described as possessing a utility function; furthermore, the agent
    selects actions as if maximizing its expected utility.

-   deals with utilities that depend on several distinct attributes of
    states. is a particularly useful technique for making unambiguous
    decisions, even without precise utility values for attributes.

-   provide a simple formalism for expressing and solving decision
    problems. They are a natural extension of Bayesian networks,
    containing decision and utility nodes in addition to chance nodes.

-   Sometimes, solving a problem involves finding more information
    before making a decision. The is defined as the expected improvement
    in utility compared with making a decision without the information.

-   that incorporate utility information have additional capabilities
    compared with pure inference systems. In addition to being able to
    make decisions, they can use the value of information to decide
    which questions to ask, if any; they can recommend contingency
    plans; and they can calculate the sensitivity of their decisions to
    small changes in probability and utility assessments.

The book *L’art de Penser*, also known as the
*Port-Royal Logic* @Arnauld:1662 states:

To judge what one must do to obtain a good or avoid an evil, it is
necessary to consider not only the good and the evil in itself, but also
the probability that it happens or does not happen; and to view
geometrically the proportion that all these things have together.

Modern texts talk of *utility* rather than good and evil,
but this statement correctly notes that one should multiply utility by
probability (“view geometrically”) to give expected utility, and
maximize that over all outcomes (“all these things”) to “judge what one
must do.” It is remarkable how much this got right, 350 years ago, and
only 8 years after Pascal and Fermat showed how to use probability
correctly. The Port-Royal Logic also marked the first publication of
Pascal’s wager.

Daniel Bernoulli [-@Bernoulli:1738], investigating the St. Petersburg
paradox (see ), was the first to realize the importance of preference
measurement for lotteries, writing “the *value* of an item
must not be based on its *price*, but rather on the
*utility* that it yields” (italics his). Utilitarian
philosopher Jeremy proposed the for weighing “pleasures” and “pains,”
arguing that all decisions (not just monetary ones) could be reduced to
utility comparisons.

The derivation of numerical utilities from preferences was first carried
out by Ramsey [-@Ramsey:1931]; the axioms for preference in the present
text are closer in form to those rediscovered in *Theory of Games
and Economic Behavior* @VonNeumann+Morgenstern:1944. A good
presentation of these axioms, in the course of a discussion on risk
preference, is given by Howard [-@Howard:1977]. Ramsey had derived
subjective probabilities (not just utilities) from an agent’s
preferences; Savage [-@Savage:1954] and Jeffrey [-@Jeffrey:1983] carry
out more recent constructions of this kind. Von Winterfeldt and
Edwards [-@VonWinterfeldt+Edwards:1986] provide a modern perspective on
decision analysis and its relationship to human preference structures.
The micromort utility measure is discussed by Howard [-@Howard:1989]. A
1994 survey by the *Economist* set the value of a life at
between 750,000 and 2.6 million. However, Richard Thaler [-@Thaler:1992]
found irrational framing effects on the price one is willing to pay to
avoid a risk of death versus the price one is willing to be paid to
accept a risk. For a 1/1000 chance, a respondent wouldn’t pay more than
200 to remove the risk, but wouldn’t accept 50,000 to take on the risk.
How much are people willing to pay for a QALY? When it comes down to a
specific case of saving oneself or a family member, the number is
approximately “whatever I’ve got.” But we can ask at a societal level:
suppose there is a vaccine that would yield $X$ QALYs but costs $Y$
dollars; is it worth it? In this case people report a wide range of
values from around 10,000 to 150,000 per QALY @Prades+al:2008. QALYs are
much more widely used in medical and social policy decision making than
are micromorts; see @Russell:1990 for a typical example of an argument
for a major change in public health policy on grounds of increased
expected utility measured in QALYs.

The was brought to the attention of decision analysts in a forceful way
by , who pointed out that the financial benefits to the client projected
by analysts for their proposed course of action almost never
materialized. They trace this directly to the bias introduced by
selecting an optimal action and show that a more complete Bayesian
analysis eliminates the problem. The same underlying concept has been
called by and was noted in the context of analyzing capital investment
projects by . The optimizer’s curse is also closely related to the
@Capen+al:1971 [@Thaler:1992], which applies to competitive bidding in
auctions: whoever wins the auction is very likely to have overestimated
the value of the object in question. Capen *et al*. quote a
petroleum engineer on the topic of bidding for oil-drilling rights: “If
one wins a tract against two or three others he may feel fine about his
good fortune. But how should he feel if he won against 50 others? Ill.”
Finally, behind both curses is the general phenomenon of , whereby
individuals selected on the basis of exceptional characteristics
previously exhibited will, with high probability, become less
exceptional in future.

The Allais paradox, due to Nobel Prize-winning economist Maurice Allais
[-@Allais:1953] was tested experimentally @Tversky+Kahneman:1982
[@Conlisk:1989] to show that people are consistently inconsistent in
their judgments. The Ellsberg paradox on ambiguity aversion was
introduced in the Ph.D. thesis of Daniel Ellsberg @Ellsberg:1962, who
went on to become a military analyst at the RAND Corporation and to leak
documents known as The Pentagon Papers, which contributed to the end of
the Vietnam war and the resignation of President Nixon. describe a
further study of ambiguity aversion. Mark gives an overview of choice
under uncertainty and how it can vary from expected utility theory.

There has been a recent outpouring of more-or-less popular books on
human irrationality. The best known is *Predictably
Irrational* @Ariely:2009; others include *Sway*
@Brafman:2009, *Nudge* @Thaler+Sunstein:2009,
*Kluge* @Marcus:2009, *How We Decide*
@Lehrer:2009 and *On Being Certain* @Burton:2009. They
complement the classic @Kahneman+al:1982 and the article that started it
all @Kahneman+Tversky:1979. The field of evolutionary
psychology @Buss:2005, on the other hand, has run counter to this
literature, arguing that humans are quite rational in evolutionarily
appropriate contexts. Its adherents point out that irrationality is
penalized by definition in an evolutionary context and show that in some
cases it is an artifact of the experimental setup @Cummins+Allen:1998.
There has been a recent resurgence of interest in Bayesian models of
cognition, overturning decades of pessimism @Oaksford+Chater:1998
[@Elio:2002; @Chater+Oaksford:2008].

give a thorough introduction to multiattribute utility theory. They
describe early computer implementations of methods for eliciting the
necessary parameters for a multiattribute utility function and include
extensive accounts of real applications of the theory. In AI, the
principal reference for MAUT is Wellman’s [-@Wellman:1985] paper, which
includes a system called (Utility Reasoning Package) that can use a
collection of statements about preference independence and conditional
independence to analyze the structure of decision problems. The use of
stochastic dominance together with qualitative probability models was
investigated extensively by Wellman [-@Wellman:1988; -@Wellman:1990].
Wellman and Doyle [-@Wellman+Doyle:1992] provide a preliminary sketch of
how a complex set of utility-independence relationships might be used to
provide a structured model of a utility function, in much the same way
that Bayesian networks provide a structured model of joint probability
distributions. and give further results along these lines.

Decision theory has been a standard tool in economics, finance, and
management science since the 1950s. Until the 1980s, decision trees were
the main tool used for representing simple decision problems. gives an
overview of the methodology of decision analysis. Influence diagrams
were introduced by Howard and Matheson [-@Howard+Matheson:1981], based
on earlier work at SRI @Miller+al:1976. Howard and Matheson’s method
involved the derivation of a decision tree from a decision network, but
in general the tree is of exponential size. Shachter [-@Shachter:1986]
developed a method for making decisions based directly on a decision
network, without the creation of an intermediate decision tree. This
algorithm was also one of the first to provide complete inference for
multiply connected Bayesian networks. showed how to take advantage of
conditional independence of information to reduce the size of trees in
practice; they use the term *decision network* for networks
that use this approach (although others use it as a synonym for
influence diagram). link algorithms for decision networks to ongoing
developments in clustering algorithms for Bayesian networks. show how
influence diagrams can be used to solve games that involve gathering
information by opposing players, and show how influence diagrams can be
used as an aid to decision making for a team that shares goals but is
unable to share all information perfectly. The collection by Oliver and
Smith [-@Oliver+Smith:1990] has a number of useful articles on decision
networks, as does the 1990 special issue of the journal
*Networks*. Papers on decision networks and utility
modeling also appear regularly in the journals *Management
Science* and *Decision Analysis*.

The theory of information value was explored first in the context of
statistical experiments, where a quasi-utility (entropy reduction) was
used @Lindley:1956. The Russian control theorist Ruslan developed the
more general theory presented here, in which information has value by
virtue of its ability to affect decisions. Stratonovich’s work was not
known in the West, where Ron Howard [-@Howard:1966] pioneered the same
idea. His paper ends with the remark “If information value theory and
associated decision theoretic structures do not in the future occupy a
large part of the education of engineers, then the engineering
profession will find that its traditional role of managing scientific
and economic resources for the benefit of man has been forfeited to
another profession.” To date, the implied revolution in managerial
methods has not occurred.

Recent work by shows that computing the exact non-myopic value of
information is intractable even in polytree networks. There are other
cases—more restricted than general value of information—in which the
myopic algorithm does provide a provably good approximation to the
optimal sequence of observations @Krause+al:2008. In some cases—for
example, looking for treasure buried in one of $n$ places—ranking
experiments in order of success probability divided by cost gives an
optimal solution @Kadane+Simon:1977.

Surprisingly few early AI researchers adopted decision-theoretic tools
after the early applications in medical decision making described in .
One of the few exceptions was Jerry Feldman, who applied decision theory
to problems in vision @Feldman+Yakimovsky:1974 and
planning @Feldman+Sproull:1977. After the resurgence of interest in
probabilistic methods in AI in the 1980s, decision-theoretic expert
systems gained widespread acceptance @Horvitz+al:1988 [@Cowell+al:2002].
In fact, from 1991 onward, the cover design of the journal
*Artificial Intelligence* has depicted a decision network,
although some artistic license appears to have been taken with the
direction of the arrows.

[almanac-game](Adapted from David Heckerman.) This exercise concerns the
, which is used by decision analysts to calibrate numeric estimation.
For each of the questions that follow, give your best guess of the
answer, that is, a number that you think is as likely to be too high as
it is to be too low. Also give your guess at a 25th percentile estimate,
that is, a number that you think has a 25% chance of being too high, and
a 75% chance of being too low. Do the same for the 75th percentile.
(Thus, you should give three estimates in all—low, median, and high—for
each question.)

1.  Number of passengers who flew between New York and Los Angeles in
    1989.

2.  Population of Warsaw in 1992.

3.  Year in which Coronado discovered the Mississippi River.

4.  Number of votes received by Jimmy Carter in the 1976 presidential
    election.

5.  Age of the oldest living tree, as of 2002.

6.  Height of the Hoover Dam in feet.

7.  Number of eggs produced in Oregon in 1985.

8.  Number of Buddhists in the world in 1992.

9.  Number of deaths due to AIDS in the in 1981.

10. Number of U.S. patents granted in 1901.

The correct answers appear after the last exercise of this chapter. From
the point of view of decision analysis, the interesting thing is not how
close your median guesses came to the real answers, but rather how often
the real answer came within your 25% and 75% bounds. If it was about
half the time, then your bounds are accurate. But if you’re like most
people, you will be more sure of yourself than you should be, and fewer
than half the answers will fall within the bounds. With practice, you
can calibrate yourself to give realistic bounds, and thus be more useful
in supplying information for decision making. Try this second set of
questions and see if there is any improvement:

1.  Year of birth of Zsa Zsa Gabor.

2.  Maximum distance from Mars to the sun in miles.

3.  Value in dollars of exports of wheat from the United States in 1992.

4.  Tons handled by the port of Honolulu in 1991.

5.  Annual salary in dollars of the governor of California in 1993.

6.  Population of San Diego in 1990.

7.  Year in which Roger Williams founded Providence, Rhode Island.

8.  Height of Mt. Kilimanjaro in feet.

9.  Length of the Brooklyn Bridge in feet.

10. Number of deaths due to automobile accidents in the United States in
    1992.

Chris considers four used cars before buying the one with maximum
expected utility. Pat considers ten cars and does the same. All other
things being equal, which one is more likely to have the better car?
Which is more likely to be disappointed with their car’s quality? By how
much (in terms of standard deviations of expected quality)?

Chris considers five used cars before buying the one with maximum
expected utility. Pat considers eleven cars and does the same. All other
things being equal, which one is more likely to have the better car?
Which is more likely to be disappointed with their car’s quality? By how
much (in terms of standard deviations of expected quality)?

[St-Petersburg-exercise] In 1713, Nicolas Bernoulli stated a puzzle, now
called the St. Petersburg paradox, which works as follows. You have the
opportunity to play a game in which a fair coin is tossed repeatedly
until it comes up heads. If the first heads appears on the $n$th toss,
you win $2^n$ dollars.

1.  Show that the expected monetary value of this game is infinite.

2.  How much would you, personally, pay to play the game?

3.  Nicolas’s cousin Daniel Bernoulli resolved the apparent paradox in
    1738 by suggesting that the utility of money is measured on a
    logarithmic scale (i.e., $U(S_{n}) = a\log_2 n +b$, where $S_n$ is
    the state of having $n$). What is the expected utility of the game
    under this assumption?

4.  What is the maximum amount that it would be rational to pay to play
    the game, assuming that one’s initial wealth is $k\,$?

Write a computer program to automate the process in . Try your program
out on several people of different net worth and political outlook.
Comment on the consistency of your results, both for an individual and
across individuals.

[surprise-candy-exercise] The Surprise Candy Company makes candy in two
flavors: 75% are strawberry flavor and 25% are anchovy flavor. Each new
piece of candy starts out with a round shape; as it moves along the
production line, a machine randomly selects a certain percentage to be
trimmed into a square; then, each piece is wrapped in a wrapper whose
color is chosen randomly to be red or brown. 70% of the strawberry
candies are round and 70% have a red wrapper, while 90% of the anchovy
candies are square and 90% have a brown wrapper. All candies are sold
individually in sealed, identical, black boxes.

Now you, the customer, have just bought a Surprise candy at the store
but have not yet opened the box. Consider the three Bayes nets in .

[3candy-figure]

1.  Which network(s) can correctly represent
    $\pv(Flavor,Wrapper,Shape)$?

2.  Which network is the best representation for this problem?

3.  Does network (i) assert that $\pv(Wrapper|Shape)\eq \pv(Wrapper)$?

4.  What is the probability that your candy has a red wrapper?

5.  In the box is a round candy with a red wrapper. What is the
    probability that its flavor is strawberry?

6.  A unwrapped strawberry candy is worth $s$ on the open market and an
    unwrapped anchovy candy is worth $a$. Write an expression for the
    value of an unopened candy box.

7.  A new law prohibits trading of unwrapped candies, but it is still
    legal to trade wrapped candies (out of the box). Is an unopened
    candy box now worth more than less than, or the same as before?

[surprise-candy-exercise] The Surprise Candy Company makes candy in two
flavors: 70% are strawberry flavor and 30% are anchovy flavor. Each new
piece of candy starts out with a round shape; as it moves along the
production line, a machine randomly selects a certain percentage to be
trimmed into a square; then, each piece is wrapped in a wrapper whose
color is chosen randomly to be red or brown. 80% of the strawberry
candies are round and 80% have a red wrapper, while 90% of the anchovy
candies are square and 90% have a brown wrapper. All candies are sold
individually in sealed, identical, black boxes.

Now you, the customer, have just bought a Surprise candy at the store
but have not yet opened the box. Consider the three Bayes nets in .

[3candy-figure]

1.  Which network(s) can correctly represent
    $\pv(Flavor,Wrapper,Shape)$?

2.  Which network is the best representation for this problem?

3.  Does network (i) assert that $\pv(Wrapper|Shape)\eq \pv(Wrapper)$?

4.  What is the probability that your candy has a red wrapper?

5.  In the box is a round candy with a red wrapper. What is the
    probability that its flavor is strawberry?

6.  A unwrapped strawberry candy is worth $s$ on the open market and an
    unwrapped anchovy candy is worth $a$. Write an expression for the
    value of an unopened candy box.

7.  A new law prohibits trading of unwrapped candies, but it is still
    legal to trade wrapped candies (out of the box). Is an unopened
    candy box now worth more than less than, or the same as before?

Prove that the judgments $B \pref A$ and $C \pref D$ in the Allais
paradox () violate the axiom of substitutability.

Consider the Allais paradox described on : an agent who prefers $B$ over
$A$ (taking the sure thing), and $C$ over $D$ (taking the higher EMV) is
not acting rationally, according to utility theory. Do you think this
indicates a problem for the agent, a problem for the theory, or no
problem at all? Explain.

Tickets to a cost 1. There are two possible prizes: a 10 payoff with
probability 1/50, and a 1,000,000 payoff with probability 1/2,000,000.
What is the expected monetary value of a lottery ticket? When (if ever)
is it rational to buy a ticket? Be precise—show an equation involving
utilities. You may assume current wealth of $k$ and that $U(S_k)=0$. You
may also assume that $U(S_{k+{10}}) = {10}\times U(S_{k+1})$, but you
may not make any assumptions about $U(S_{k+1,{000},{000}})$.
Sociological studies show that people with lower income buy a
disproportionate number of lottery tickets. Do you think this is because
they are worse decision makers or because they have a different utility
function? Consider the value of contemplating the possibility of winning
the lottery versus the value of contemplating becoming an action hero
while watching an adventure movie.

[assessment-exercise]Assess your own utility for different incremental
amounts of money by running a series of preference tests between some
definite amount $M_1$ and a lottery $[p,M_2; (1-p), 0]$. Choose
different values of $M_1$ and $M_2$, and vary $p$ until you are
indifferent between the two choices. Plot the resulting utility
function.

How much is a micromort worth to you? Devise a protocol to determine
this. Ask questions based both on paying to avoid risk and being paid to
accept risk.

[kmax-exercise] Let continuous variables $X_1,\ldots,X_k$ be
independently distributed according to the same probability density
function $f(x)$. Prove that the density function for
$\max\{X_1,\ldots,X_k\}$ is given by $kf(x)(F(x))^{k-1}$, where $F$ is
the cumulative distribution for $f$.

Economists often make use of an exponential utility function for money:
$U(x) = -e^{x/R}$, where $R$ is a positive constant representing an
individual’s risk tolerance. Risk tolerance reflects how likely an
individual is to accept a lottery with a particular expected monetary
value (EMV) versus some certain payoff. As $R$ (which is measured in the
same units as $x$) becomes larger, the individual becomes less
risk-averse.

1.  Assume Mary has an exponential utility function with $R = \$500$.
    Mary is given the choice between receiving \$500 with certainty
    (probability 1) or participating in a lottery which has a 60%
    probability of winning \$5000 and a 40% probability of winning
    nothing. Assuming Marry acts rationally, which option would she
    choose? Show how you derived your answer.

2.  Consider the choice between receiving \$100 with certainty
    (probability 1) or participating in a lottery which has a 50%
    probability of winning \$500 and a 50% probability of winning
    nothing. Approximate the value of R (to 3 significant digits) in an
    exponential utility function that would cause an individual to be
    indifferent to these two alternatives. (You might find it helpful to
    write a short program to help you solve this problem.)

Economists often make use of an exponential utility function for money:
$U(x) = -e^{x/R}$, where $R$ is a positive constant representing an
individual’s risk tolerance. Risk tolerance reflects how likely an
individual is to accept a lottery with a particular expected monetary
value (EMV) versus some certain payoff. As $R$ (which is measured in the
same units as $x$) becomes larger, the individual becomes less
risk-averse.

1.  Assume Mary has an exponential utility function with $R = \$400$.
    Mary is given the choice between receiving \$400 with certainty
    (probability 1) or participating in a lottery which has a 60%
    probability of winning \$5000 and a 40% probability of winning
    nothing. Assuming Marry acts rationally, which option would she
    choose? Show how you derived your answer.

2.  Consider the choice between receiving \$100 with certainty
    (probability 1) or participating in a lottery which has a 50%
    probability of winning \$500 and a 50% probability of winning
    nothing. Approximate the value of R (to 3 significant digits) in an
    exponential utility function that would cause an individual to be
    indifferent to these two alternatives. (You might find it helpful to
    write a short program to help you solve this problem.)

Alex is given the choice between two games. In Game 1, a fair coin is
flipped and if it comes up heads, Alex receives \$100. If the coin comes
up tails, Alex receives nothing. In Game 2, a fair coin is flipped
twice. Each time the coin comes up heads, Alex receives \$50, and Alex
receives nothing for each coin flip that comes up tails. Assuming that
Alex has a monotonically increasing utility function for money in the
range [\$0, \$100], show mathematically that if Alex prefers Game 2 to
Game 1, then Alex is risk averse (at least with respect to this range of
monetary amounts).

Show that if $X_1$ and $X_2$ are preferentially independent of $X_3$,
and $X_2$ and $X_3$ are preferentially independent of $X_1$, then $X_3$
and $X_1$ are preferentially independent of $X_2$.

[airport-au-id-exercise]Repeat , using the action-utility representation
shown in .

For either of the airport-siting diagrams from Exercises
[airport-id-exercise] and [airport-au-id-exercise], to which conditional
probability table entry is the utility most sensitive, given the
available evidence?

Modify and extend the Bayesian network code in the code repository to
provide for creation and evaluation of decision networks and the
calculation of information value.

Consider a student who has the choice to buy or not buy a textbook for a
course. We’ll model this as a decision problem with one Boolean decision
node, $B$, indicating whether the agent chooses to buy the book, and two
Boolean chance nodes, $M$, indicating whether the student has mastered
the material in the book, and $P$, indicating whether the student passes
the course. Of course, there is also a utility node, $U$. A certain
student, Sam, has an additive utility function: 0 for not buying the
book and -\$100 for buying it; and \$2000 for passing the course and 0
for not passing. Sam’s conditional probability estimates are as follows:
$$\begin{array}{ll}
P(p|b,m) = 0.9              & P(m|b) = 0.9       \\
P(p|b, \lnot m) = 0.5       & P(m|\lnot b) = 0.7 \\
P(p|\lnot b, m) = 0.8       & \\
P(p|\lnot b, \lnot m) = 0.3 & \\
\end{array}$$ You might think that $P$ would be independent of $B$ given
$M$, But this course has an open-book final—so having the book helps.

1.  Draw the decision network for this problem.

2.  Compute the expected utility of buying the book and of not buying
    it.

3.  What should Sam do?

[airport-id-exercise]This exercise completes the analysis of the
airport-siting problem in .

1.  Provide reasonable variable domains, probabilities, and utilities
    for the network, assuming that there are three possible sites.

2.  Solve the decision problem.

3.  What happens if changes in technology mean that each aircraft
    generates half the noise?

4.  What if noise avoidance becomes three times more important?

5.  Calculate the VPI for ${AirTraffic}$, ${Litigation}$, and
    ${Construction}$ in your model.

[car-vpi-exercise] (Adapted from Pearl [-@Pearl:1988].) A used-car buyer
can decide to carry out various tests with various costs (e.g., kick the
tires, take the car to a qualified mechanic) and then, depending on the
outcome of the tests, decide which car to buy. We will assume that the
buyer is deciding whether to buy car $c_1$, that there is time to carry
out at most one test, and that $t_1$ is the test of $c_1$ and costs 50.

A car can be in good shape (quality $q^+$) or bad shape (quality $q^-$),
and the tests might help indicate what shape the car is in. Car $c_1$
costs 1,500, and its market value is 2,000 if it is in good shape; if
not, 700 in repairs will be needed to make it in good shape. The buyer’s
estimate is that $c_1$ has a 70% chance of being in good shape.

1.  Draw the decision network that represents this problem.

2.  Calculate the expected net gain from buying $c_1$, given no test.

3.  Tests can be described by the probability that the car will pass or
    fail the test given that the car is in good or bad shape. We have
    the following information:\
    $P({pass}(c_1,t_1) | q^+(c_1)) = {0.8}$\
    $P({pass}(c_1,t_1) | q^-(c_1)) = {0.35}$\
    Use Bayes’ theorem to calculate the probability that the car will
    pass (or fail) its test and hence the probability that it is in good
    (or bad) shape given each possible test outcome.

4.  Calculate the optimal decisions given either a pass or a fail, and
    their expected utilities.

5.  Calculate the value of information of the test, and derive an
    optimal conditional plan for the buyer.

[nonnegative-VPI-exercise]Recall the definition of *value of
information* in .

1.  Prove that the value of information is nonnegative and order
    independent.

2.  Explain why it is that some people would prefer not to get some
    information—for example, not wanting to know the sex of their baby
    when an ultrasound is done.

3.  A function $f$ on sets is if, for any element $x$ and any sets $A$
    and $B$ such that $A\subseteq B$, adding $x$ to $A$ gives a greater
    increase in $f$ than adding $x$ to $B$:
    $$A\subseteq B \implies (f(A\union \{x\}) - f(A)) \geq (f(B\union \{x\}) - f(B))\ .$$
    Submodularity captures the intuitive notion of *diminishing
    returns*. Is the value of information, viewed as a function
    $f$ on sets of possible observations, submodular? Prove this or find
    a counterexample.

The answers to (where M stands for million): First set: 3M, 1.6M, 1541,
41M, 4768, 221, 649M, 295M, 132, 25,546. Second set: 1917, 155M, 4,500M,
11M, 120,000, 1.1M, 1636, 19,340, 1,595, 41,710.

[^1]: Classical decision theory leaves the current state $S_0$ implicit,
    but we could make it explicit by writing\
    $ P(\Result(a)\eq s' \given a,\mbf{e}) \eq \sum_s P(\Result(s,a)\eq s' \given a) P(S_0\eq s \given \mbf{e})$.

[^2]: We apologize to readers whose local airlines no longer offer food
    on long flights.

[^3]: We can account for the enjoyment of gambling by encoding gambling
    events into the state description; for example, “Have 10 and
    gambled” could be preferred to “Have 10 and didn’t gamble.”

[^4]: In this sense, utilities resemble temperatures: a temperature in
    Fahrenheit is 1.8 times the Celsius temperature plus 32. You get the
    same results in either measurement system.

[^5]: Such behavior might be called desperate, but it is rational if one
    is already in a desperate situation.

[^6]: For example, the mathematician/magician Persi Diaconis can make a
    coin flip come out the way he wants every time @Landhuis:2004.

[^7]: Even the sure thing may not be certain. Despite cast-iron
    promises, we have not yet received that 27,000,000 from the Nigerian
    bank account of a previously unknown deceased relative.

[^8]: In some cases, it may be necessary to subdivide the range of
    values so that utility varies monotonically within each range. For
    example, if the ${RoomTemperature}$ attribute has a utility peak
    at $70^\circ$F, we would split it into two attributes measuring the
    difference from the ideal, one colder and one hotter. Utility would
    then be monotonically increasing in each attribute.

[^9]: These nodes are also called in the literature.

[^10]: In the , the only question that is always asked beforehand is
    whether the patient has insurance.

[^11]: There is no loss of expressiveness in requiring perfect
    information. Suppose we wanted to model the case in which we become
    somewhat more certain about a variable. We can do that by
    introducing *another* variable about which we learn
    perfect information. For example, suppose we initially have broad
    uncertainty about the variable ${Temperature}$. Then we gain the
    perfect knowledge ${Thermometer} = 37$; this gives us imperfect
    information about the true ${Temperature}$, and the uncertainty
    due to measurement error is encoded in the sensor model
    $\pv({Thermometer}\given
    {Temperature})$. See for another example.
First-Order Logic {#fol-chapter}
=================

In , we showed how a knowledge-based agent could represent the world in
which it operates and deduce what actions to take. We used propositional
logic as our representation language because it sufficed to illustrate
the basic concepts of logic and knowledge-based agents. Unfortunately,
propositional logic is too puny a language to represent knowledge of
complex environments in a concise way. In this chapter, we examine ,[^1]
which is sufficiently expressive to represent a good deal of our
commonsense knowledge. It also either subsumes or forms the foundation
of many other representation languages and has been studied intensively
for many decades. We begin in with a discussion of representation
languages in general; covers the syntax and semantics of first-order
logic; Sections [fol-use-section] and [circuits-section] illustrate the
use of first-order logic for simple representations.

Representation Revisited {#fol-repn-section}
------------------------

In this section, we discuss the nature of representation languages. Our
discussion motivates the development of first-order logic, a much more
expressive language than the propositional logic introduced in . We look
at propositional logic and at other kinds of languages to understand
what works and what fails. Our discussion will be cursory, compressing
centuries of thought, trial, and error into a few paragraphs.

Programming languages (such as C++ or Java or Lisp) are by far the
largest class of formal languages in common use. Programs themselves
represent, in a direct sense, only computational processes. Data
structures within programs can represent facts; for example, a program
could use a $4\times 4$ array to represent the contents of the wumpus
world. Thus, the programming language statement is a fairly natural way
to assert that there is a pit in square [2,2]. (Such representations
might be considered *ad hoc*; database systems were
developed precisely to provide a more general, domain-independent way to
store and retrieve facts.) What programming languages lack is any
general mechanism for deriving facts from other facts; each update to a
data structure is done by a domain-specific procedure whose details are
derived by the programmer from his or her own knowledge of the domain.
This can be contrasted with the nature of propositional logic, in which
knowledge and inference are separate, and inference is entirely domain
independent.

A second drawback of data structures in programs (and of databases, for
that matter) is the lack of any easy way to say, for example, “There is
a pit in [2,2] or [3,1]” or “If the wumpus is in [1,1] then he is not in
[2,2].” Programs can store a single value for each variable, and some
systems allow the value to be “unknown,” but they lack the
expressiveness required to handle partial information.

Propositional logic is a declarative language because its semantics is
based on a truth relation between sentences and possible worlds. It also
has sufficient expressive power to deal with partial information, using
disjunction and negation. Propositional logic has a third property that
is desirable in representation languages, namely, . In a compositional
language, the meaning of a sentence is a function of the meaning of its
parts. For example, the meaning of “$S_{1,4} \land S_{1,2}$” is related
to the meanings of “$S_{1,4}$” and “$S_{1,2}$.” It would be very strange
if “$S_{1,4}$” meant that there is a stench in square [1,4] and
“$S_{1,2}$” meant that there is a stench in square [1,2], but
“$S_{1,4} \land S_{1,2}$” meant that France and Poland drew 1–1 in last
week’s ice hockey qualifying match. Clearly, noncompositionality makes
life much more difficult for the reasoning system.

As we saw in , however, propositional logic lacks the expressive power
to *concisely* describe an environment with many objects.
For example, we were forced to write a separate rule about breezes and
pits for each square, such as
$$B_{1,1} \lequiv (P_{1,2} \lor P_{2,1})\ .$$ In English, on the other
hand, it seems easy enough to say, once and for all, “Squares adjacent
to pits are breezy.” The syntax and semantics of English somehow make it
possible to describe the environment concisely.

### The language of thought

Natural languages (such as English or Spanish) are very expressive
indeed. We managed to write almost this whole book in natural language,
with only occasional lapses into other languages (including logic,
mathematics, and the language of diagrams). There is a long tradition in
linguistics and the philosophy of language that views natural language
as a declarative knowledge representation language. If we could uncover
the rules for natural language, we could use it in representation and
reasoning systems and gain the benefit of the billions of pages that
have been written in natural language.

The modern view of natural language is that it serves a as a medium for
rather than pure representation. When a speaker points and says, “Look!”
the listener comes to know that, say, Superman has finally appeared over
the rooftops. Yet we would not want to say that the sentence “Look!”
represents that fact. Rather, the meaning of the sentence depends both
on the sentence itself and on the in which the sentence was spoken.
Clearly, one could not store a sentence such as “Look!” in a knowledge
base and expect to recover its meaning without also storing a
representation of the context—which raises the question of how the
context itself can be represented. Natural languages also suffer from ,
a problem for a representation language. As puts it: “When people think
about *spring*, surely they are not confused as to whether
they are thinking about a season or something that goes
*boing*—and if one word can correspond to two thoughts,
thoughts can’t be words.”

The famous claims that our understanding of the world *is*
strongly influenced by the language we speak. wrote “We cut nature up,
organize it into concepts, and ascribe significances as we do, largely
because we are parties to an agreement to organize it this way—an
agreement that holds throughout our speech community and is codified in
the patterns of our language.” It is certainly true that different
speech communities divide up the world differently. The French have two
words “chaise” and “fauteuil,” for a concept that English speakers cover
with one: “chair.” But English speakers can easily recognize the
category fauteuil and give it a name—roughly “open-arm chair”—so does
language really make a difference? Whorf relied mainly on intuition and
speculation, but in the intervening years we actually have real data
from anthropological, psychological and neurological studies.

For example, can you remember which of the following two phrases formed
the opening of ?
$$\mbox{``In this section, we discuss the nature of representation
languages \(\ldots\)''}$$
$$\mbox{``This section covers the topic of knowledge representation languages \(\ldots\)''}$$
did a similar experiment and found that subjects made the right choice
at chance level—about 50% of the time—but remembered the content of what
they read with better than 90% accuracy. This suggests that people
process the words to form some kind of *nonverbal*
representation.

More interesting is the case in which a concept is completely absent in
a language. Speakers of the Australian aboriginal language have no words
for relative directions, such as front, back, right, or left. Instead
they use absolute directions, saying, for example, the equivalent of “I
have a pain in my north arm.” This difference in language makes a
difference in behavior: Guugu Yimithirr speakers are better at
navigating in open terrain, while English speakers are better at placing
the fork to the right of the plate.

Language also seems to influence thought through seemingly arbitrary
grammatical features such as the gender of nouns. For example, “bridge”
is masculine in Spanish and feminine in German. asked subjects to choose
English adjectives to describe a photograph of a particular bridge.
Spanish speakers chose *big*, *dangerous*,
*strong*, and *towering*, whereas German
speakers chose *beautiful*, *elegant*,
*fragile*, and *slender*. Words can serve as
anchor points that affect how we perceive the world. showed experimental
subjects a movie of an auto accident. Subjects who were asked “How fast
were the cars going when they contacted each other?” reported an average
of 32 mph, while subjects who were asked the question with the word
“smashed” instead of “contacted” reported 41mph for the same cars in the
same movie.

In a first-order logic reasoning system that uses CNF, we can see that
the linguistic form “$\lnot(A \lor B)$” and “$\lnot A \land
\lnot B$” are the same because we can look inside the system and see
that the two sentences are stored as the same canonical CNF form. Can we
do that with the human brain? Until recently the answer was “no,” but
now it is “maybe.” put subjects in an (functional magnetic resonance
imaging) machine, showed them words such as “celery,” and imaged their
brains. The researchers were then able to train a computer program to
predict, from a brain image, what word the subject had been presented
with. Given two choices (e.g., “celery” or “airplane”), the system
predicts correctly 77% of the time. The system can even predict at
above-chance levels for words it has never seen an fMRI image of before
(by considering the images of related words) and for people it has never
seen before (proving that fMRI reveals some level of common
representation across people). This type of work is still in its
infancy, but fMRI (and other imaging technology such as intracranial
electrophysiology @Sahin+al:2009) promises to give us much more concrete
ideas of what human knowledge representations are like.

From the viewpoint of formal logic, representing the same knowledge in
two different ways makes absolutely no difference; the same facts will
be derivable from either representation. In practice, however, one
representation might require fewer steps to derive a conclusion, meaning
that a reasoner with limited resources could get to the conclusion using
one representation but not the other. For *nondeductive*
tasks such as learning from experience, outcomes are
*necessarily* dependent on the form of the representations
used. We show in that when a learning program considers two possible
theories of the world, both of which are consistent with all the data,
the most common way of breaking the tie is to choose the most succinct
theory—and that depends on the language used to represent theories.
Thus, the influence of language on thought is unavoidable for any agent
that does learning.

### Combining the best of formal and natural languages

We can adopt the foundation of propositional logic—a declarative,
compositional semantics that is context-independent and unambiguous—and
build a more expressive logic on that foundation, borrowing
representational ideas from natural language while avoiding its
drawbacks. When we look at the syntax of natural language, the most
obvious elements are nouns and noun phrases that refer to (squares,
pits, wumpuses) and verbs and verb phrases that refer to among objects
(is breezy, is adjacent to, shoots). Some of these relations are
—relations in which there is only one “value” for a given “input.” It is
easy to start listing examples of objects, relations, and functions:

-   Objects: people, houses, numbers, theories, Ronald McDonald, colors,
    baseball games, wars, centuries $\ldots$

-   Relations: these can be unary relations or such as red, round,
    bogus, prime, multistoried $\ldots$, or more general $n$-ary
    relations such as brother of, bigger than, inside, part of, has
    color, occurred after, owns, comes between, $\ldots$

-   Functions: father of, best friend, third inning of, one more than,
    beginning of $\ldots$

Indeed, almost any assertion can be thought of as referring to objects
and properties or relations. Some examples follow:

-   “One plus two equals three.”\
    Objects: one, two, three, one plus two; Relation: equals; Function:
    plus. (“One plus two” is a name for the object that is obtained by
    applying the function “plus” to the objects “one” and “two.” “Three”
    is another name for this object.)

-   “Squares neighboring the wumpus are smelly.”\
    Objects: wumpus, squares; Property: smelly; Relation: neighboring.

-   “Evil King John ruled England in 1200.”\
    Objects: John, England, 1200; Relation: ruled; Properties: evil,
    king.

The language of , whose syntax and semantics we define in the next
section, is built around objects and relations. It has been so important
to mathematics, philosophy, and artificial intelligence precisely
because those fields—and indeed, much of everyday human existence—can be
usefully thought of as dealing with objects and the relations among
them. First-order logic can also express facts about *some*
or *all* of the objects in the universe. This enables one
to represent general laws or rules, such as the statement “Squares
neighboring the wumpus are smelly.”

The primary difference between propositional and first-order logic lies
in the made by each language—that is, what it assumes about the nature
of *reality*. Mathematically, this commitment is expressed
through the nature of the formal with respect to which the truth of
sentences is defined. For example, propositional logic assumes that
there are facts that either hold or do not hold in the world. Each fact
can be in one of two states: true or false, and each model assigns
${true}$ or ${false}$ to each proposition symbol (see ).[^2]
First-order logic assumes more; namely, that the world consists of
objects with certain relations among them that do or do not hold. The
formal models are correspondingly more complicated than those for
propositional logic. Special-purpose logics make still further
ontological commitments; for example, assumes that facts hold at
particular *times* and that those times (which may be
points or intervals) are ordered. Thus, special-purpose logics give
certain kinds of objects (and the axioms about them) “first class”
status within the logic, rather than simply defining them within the
knowledge base. views the relations and functions referred to by
first-order logic as objects in themselves. This allows one to make
assertions about *all* relations—for example, one could
wish to define what it means for a relation to be transitive. Unlike
most special-purpose logics, higher-order logic is strictly more
expressive than first-order logic, in the sense that some sentences of
higher-order logic cannot be expressed by any finite number of
first-order logic sentences.

A logic can also be characterized by its —the possible states of
knowledge that it allows with respect to each fact. In both
propositional and first-order logic, a sentence represents a fact and
the agent either believes the sentence to be true, believes it to be
false, or has no opinion. These logics therefore have three possible
states of knowledge regarding any sentence. Systems using , on the other
hand, can have any *degree of belief*, ranging from 0
(total disbelief) to 1 (total belief).[^3] For example, a probabilistic
wumpus-world agent might believe that the wumpus is in [1,3] with
probability 0.75. The ontological and epistemological commitments of
five different logics are summarized in .

[ht] [ontological-epistemological-table]

In the next section, we will launch into the details of first-order
logic. Just as a student of physics requires some familiarity with
mathematics, a student of AI must develop a talent for working with
logical notation. On the other hand, it is also important
*not* to get too concerned with the
*specifics* of logical notation—after all, there are dozens
of different versions. The main things to keep hold of are how the
language facilitates concise representations and how its semantics leads
to sound reasoning procedures.

Syntax and Semantics of First-Order Logic {#fol-syntax-section}
-----------------------------------------

We begin this section by specifying more precisely the way in which the
possible worlds of first-order logic reflect the ontological commitment
to objects and relations. Then we introduce the various elements of the
language, explaining their semantics as we go along.

### Models for first-order logic {#fol-model-page}

Recall from that the models of a logical language are the formal
structures that constitute the possible worlds under consideration. Each
model links the vocabulary of the logical sentences to elements of the
possible world, so that the truth of any sentence can be determined.
Thus, models for propositional logic link proposition symbols to
predefined truth values. Models for first-order logic are much more
interesting. First, they have objects in them! The of a model is the set
of objects or it contains. The domain is required to be
*nonempty*—every possible world must contain at least one
object. (See for a discussion of empty worlds.) Mathematically speaking,
it doesn’t matter *what* these objects are—all that matters
is *how many* there are in each particular model—but for
pedagogical purposes we’ll use a concrete example. shows a model with
five objects: Richard the Lionheart, King of England from 1189 to 1199;
his younger brother, the evil King John, who ruled from 1199 to 1215;
the left legs of Richard and John; and a crown.

[fol-model-figure]

The objects in the model may be *related* in various ways.
In the figure, Richard and John are brothers. Formally speaking, a
relation is just the set of of objects that are related. (A tuple is a
collection of objects arranged in a fixed order and is written with
angle brackets surrounding the objects.) Thus, the brotherhood relation
in this model is the set

$$\{\,\<\mbox{Richard the Lionheart},\ \mbox{King John}\>,\
  \<\mbox{King John},\ \mbox{Richard the Lionheart}\>\,\}\ .
\label{brother-relation-equation}$$

(Here we have named the objects in English, but you may, if you wish,
mentally substitute the pictures for the names.) The crown is on King
John’s head, so the “on head” relation contains just one tuple,
$\<\mbox{the crown},\ \mbox{King John}\>$. The “brother” and “on head”
relations are binary relations—that is, they relate pairs of objects.
The model also contains unary relations, or properties: the “person”
property is true of both Richard and John; the “king” property is true
only of John (presumably because Richard is dead at this point); and the
“crown” property is true only of the crown.

Certain kinds of relationships are best considered as functions, in that
a given object must be related to exactly one object in this way. For
example, each person has one left leg, so the model has a unary “left
leg” function that includes the following mappings:

$$\begin{array}{l}
  \<\mbox{Richard the Lionheart}\> \rightarrow \mbox{Richard's left leg}\\
  \<\mbox{King John}\> \rightarrow \mbox{John's left leg}\ .
\end{array}
\label{leftleg-function-equation}$$

Strictly speaking, models in first-order logic require , that is, there
must be a value for every input tuple. Thus, the crown must have a left
leg and so must each of the left legs. There is a technical solution to
this awkward problem involving an additional “invisible” object that is
the left leg of everything that has no left leg, including itself.
Fortunately, as long as one makes no assertions about the left legs of
things that have no left legs, these technicalities are of no import.

So far, we have described the elements that populate models for
first-order logic. The other essential part of a model is the link
between those elements and the vocabulary of the logical sentences,
which we explain next.

### Symbols and interpretations

We turn now to the syntax of first-order logic. The impatient reader can
obtain a complete description from the formal grammar in .

The basic syntactic elements of first-order logic are the symbols that
stand for objects, relations, and functions. The symbols, therefore,
come in three kinds: , which stand for objects; , which stand for
relations; and , which stand for functions. We adopt the convention that
these symbols will begin with uppercase letters. For example, we might
use the constant symbols ${Richard}$ and ${John}$; the predicate
symbols ${Brother}$, ${OnHead}$, ${Person}$, ${King}$, and
${Crown}$; and the function symbol ${LeftLeg}$. As with proposition
symbols, the choice of names is entirely up to the user. Each predicate
and function symbol comes with an that fixes the number of arguments.

[fol-bnf-figure]

As in propositional logic, every model must provide the information
required to determine if any given sentence is true or false. Thus, in
addition to its objects, relations, and functions, each model includes
an that specifies exactly which objects, relations and functions are
referred to by the constant, predicate, and function symbols. One
possible interpretation for our example—which a logician would call the
—is as follows:

-   ${Richard}$ refers to Richard the Lionheart and ${John}$ refers
    to the evil King John.

-   ${Brother}$ refers to the brotherhood relation, that is, the set
    of tuples of objects given in ; ${OnHead}$ refers to the “on head”
    relation that holds between the crown and King John; ${Person}$,
    ${King}$, and ${Crown}$ refer to the sets of objects that are
    persons, kings, and crowns.

-   ${LeftLeg}$ refers to the “left leg” function, that is, the
    mapping given in .

There are many other possible interpretations, of course. For example,
one interpretation maps ${Richard}$ to the crown and ${John}$ to
King John’s left leg. There are five objects in the model, so there are
25 possible interpretations just for the constant symbols ${Richard}$
and ${John}$. Notice that not all the objects need have a name—for
example, the intended interpretation does not name the crown or the
legs. It is also possible for an object to have several names; there is
an interpretation under which both ${Richard}$ and ${John}$ refer to
the crown.[^4] If you find this possibility confusing, remember that, in
propositional logic, it is perfectly possible to have a model in which
${Cloudy}$ and ${Sunny}$ are both true; it is the job of the
knowledge base to rule out models that are inconsistent with our
knowledge.

[all-models-standard-figure]

In summary, a model in first-order logic consists of a set of objects
and an interpretation that maps constant symbols to objects, predicate
symbols to relations on those objects, and function symbols to functions
on those objects. Just as with propositional logic, entailment,
validity, and so on are defined in terms of *all possible
models*. To get an idea of what the set of all possible models
looks like, see . It shows that models vary in how many objects they
contain—from one up to infinity—and in the way the constant symbols map
to objects. If there are two constant symbols and one object, then both
symbols must refer to the same object; but this can still happen even
with more objects. When there are more objects than constant symbols,
some of the objects will have no names. Because the number of possible
models is unbounded, checking entailment by the enumeration of all
possible models is not feasible for first-order logic (unlike
propositional logic). Even if the number of objects is restricted, the
number of combinations can be very large. (See .) For the example in ,
there are 137,506,194,466 models with six or fewer objects.

### Terms

A is a logical expression that refers to an object. Constant symbols are
therefore terms, but it is not always convenient to have a distinct
symbol to name every object. For example, in English we might use the
expression “King John’s left leg” rather than giving a name to his leg.
This is what function symbols are for: instead of using a constant
symbol, we use ${LeftLeg}({John})$. In the general case, a complex
term is formed by a function symbol followed by a parenthesized list of
terms as arguments to the function symbol. It is important to remember
that a complex term is just a complicated kind of name. It is not a
“subroutine call” that “returns a value.” There is no ${LeftLeg}$
subroutine that takes a person as input and returns a leg. We can reason
about left legs (e.g., stating the general rule that everyone has one
and then deducing that John must have one) without ever providing a
definition of ${LeftLeg}$. This is something that cannot be done with
subroutines in programming languages.[^5]

The formal semantics of terms is straightforward. Consider a term
$f(t_1,\ldots,t_n)$. The function symbol $f$ refers to some function in
the model (call it $F$); the argument terms refer to objects in the
domain (call them $d_1,\ldots,d_n$); and the term as a whole refers to
the object that is the value of the function $F$ applied to
$d_1,\ldots,d_n$. For example, suppose the ${LeftLeg}$ function symbol
refers to the function shown in and ${John}$ refers to King John, then
${LeftLeg}({John})$ refers to King John’s left leg. In this way, the
interpretation fixes the referent of every term.

### Atomic sentences

Now that we have both terms for referring to objects and predicate
symbols for referring to relations, we can put them together to make
that state facts. An (or for short) is formed from a predicate symbol
optionally followed by a parenthesized list of terms, such as
$${Brother}({Richard},{John}).$$ This states, under the intended
interpretation given earlier, that Richard the Lionheart is the brother
of King John.[^6] Atomic sentences can have complex terms as arguments.
Thus, $${Married}({Father}({Richard}),{Mother}({John}))$$
states that Richard the Lionheart’s father is married to King John’s
mother (again, under a suitable interpretation).

An atomic sentence is in a given model if the relation referred to by
the predicate symbol holds among the objects referred to by the
arguments.

### Complex sentences

We can use to construct more complex sentences, with the same syntax and
semantics as in propositional calculus. Here are four sentences that are
true in the model of under our intended interpretation:

((),)\
(,) (,)\
() ()\
() () .

### Quantifiers

Once we have a logic that allows objects, it is only natural to want to
express properties of entire collections of objects, instead of
enumerating the objects by name. let us do this. First-order logic
contains two standard quantifiers, called *universal* and
*existential*.

#### Universal quantification ($\forall$)

Recall the difficulty we had in with the expression of general rules in
propositional logic. Rules such as “Squares neighboring the wumpus are
smelly” and “All kings are persons” are the bread and butter of
first-order logic. We deal with the first of these in . The second rule,
“All kings are persons,” is written in first-order logic as
$$\All{x} {King}(x) \implies {Person}(x)\ .$$ $\forall$ is usually
pronounced “For all $\ldots$”. (Remember that the upside-down A stands
for “all.”) Thus, the sentence says, “For all $x$, if $x$ is a king,
then $x$ is a person.” The symbol $x$ is called a . By convention,
variables are lowercase letters. A variable is a term all by itself, and
as such can also serve as the argument of a function—for example,
${LeftLeg}(x)$. A term with no variables is called a .

Intuitively, the sentence $\forall\,x\;P$, where $P$ is any logical
expression, says that $P$ is true for every object $x$. More precisely,
$\forall\,x\;P$ is true in a given model if $P$ is true in all possible
constructed from the interpretation given in the model, where each
extended interpretation specifies a domain element to which $x$ refers.

This sounds complicated, but it is really just a careful way of stating
the intuitive meaning of universal quantification. Consider the model
shown in and the intended interpretation that goes with it. We can
extend the interpretation in five ways:

x\
x\
x\
x\
x

The universally quantified sentence $\All{x} {King}(x) \textimplies
{Person}(x)$ is true in the original model if the sentence
${King}(x) \textimplies {Person}(x)$ is true under each of the five
extended interpretations. That is, the universally quantified sentence
is equivalent to asserting the following five sentences:

\
\
\
\

Let us look carefully at this set of assertions. Since, in our model,
King John is the only king, the second sentence asserts that he is a
person, as we would hope. But what about the other four sentences, which
appear to make claims about legs and crowns? Is that part of the meaning
of “All kings are persons”? In fact, the other four assertions are true
in the model, but make no claim whatsoever about the personhood
qualifications of legs, crowns, or indeed Richard. This is because none
of these objects is a king. Looking at the truth table for
$\impliessymbol$ ( on ), we see that the implication is true whenever
its premise is false—*regardless* of the truth of the
conclusion. Thus, by asserting the universally quantified sentence,
which is equivalent to asserting a whole list of individual
implications, we end up asserting the conclusion of the rule just for
those objects for whom the premise is true and saying nothing at all
about those individuals for whom the premise is false. Thus, the
truth-table definition of $\impliessymbol$ turns out to be perfect for
writing general rules with universal quantifiers.

A common mistake, made frequently even by diligent readers who have read
this paragraph several times, is to use conjunction instead of
implication. The sentence $$\All{x} {King}(x) \land {Person}(x)$$
would be equivalent to asserting

\
\
\

and so on. Obviously, this does not capture what we want.

#### Existential quantification ($\exists$)

Universal quantification makes statements about every object. Similarly,
we can make a statement about *some* object in the universe
without naming it, by using an existential quantifier. To say, for
example, that King John has a crown on his head, we write
$$\Exi{x} {Crown}(x) \land {OnHead}(x,{John})\ .$$ $\exists x$ is
pronounced “There exists an $x$ such that $\ldots$” or “For some
$x \ldots$”.

Intuitively, the sentence $\exists\,x\;P$ says that $P$ is true for at
least one object $x$. More precisely, $\exists\,x\;P$ is true in a given
model if $P$ is true in *at least one* extended
interpretation that assigns $x$ to a domain element. That is, at least
one of the following is true:

\
\
\
\

The fifth assertion is true in the model, so the original existentially
quantified sentence is true in the model. Notice that, by our
definition, the sentence would also be true in a model in which King
John was wearing two crowns. This is entirely consistent with the
original sentence “King John has a crown on his head.” [^7]

Just as $\impliessymbol$ appears to be the natural connective to use
with $\forall$, $\land$ is the natural connective to use with $\exists$.
Using $\land$ as the main connective with $\forall$ led to an overly
strong statement in the example in the previous section; using
$\impliessymbol$ with $\exists$ usually leads to a very weak statement,
indeed. Consider the following sentence:
$$\Exi{x} {Crown}(x) \implies {OnHead}(x,{John})\ .$$ On the
surface, this might look like a reasonable rendition of our sentence.
Applying the semantics, we see that the sentence says that at least one
of the following assertions is true:

\
\
\

and so on. Now an implication is true if both premise and conclusion are
true, *or if its premise is false*. So if Richard the
Lionheart is not a crown, then the first assertion is true and the
existential is satisfied. So, an existentially quantified implication
sentence is true whenever *any* object fails to satisfy the
premise; hence such sentences really do not say much at all.

#### Nested quantifiers

We will often want to express more complex sentences using multiple
quantifiers. The simplest case is where the quantifiers are of the same
type. For example, “Brothers are siblings” can be written as
$$\All{x} \All{y} {Brother}(x,y) \implies {Sibling}(x,y)\ .$$
Consecutive quantifiers of the same type can be written as one
quantifier with several variables. For example, to say that siblinghood
is a symmetric relationship, we can write
$$\All{x,y} {Sibling}(x,y) \lequiv {Sibling}(y,x)\ .$$ In other
cases we will have mixtures. “Everybody loves somebody” means that for
every person, there is someone that person loves:
$$\All{x} \Exi{y} {Loves}(x,y)\ .$$ On the other hand, to say “There
is someone who is loved by everyone,” we write
$$\Exi{y} \All{x} {Loves}(x,y)\ .$$ The order of quantification is
therefore very important. It becomes clearer if we insert parentheses.
$\forall\,{x}\ (\exists\,{y}\ {Loves}(x,y))$ says that
*everyone* has a particular property, namely, the property
that they love someone. On the other hand,
$\exists\,{y}\ (\forall\,{x}\ {Loves}(x,y))$ says that
*someone* in the world has a particular property, namely
the property of being loved by everybody.

Some confusion can arise when two quantifiers are used with the same
variable name. Consider the sentence
$$\All{x} ({Crown}(x) \lor (\Exi{x} {Brother}({Richard},x)))\ .$$
Here the $x$ in ${Brother}({Richard},x)$ is
*existentially* quantified. The rule is that the variable
belongs to the innermost quantifier that mentions it; then it will not
be subject to any other quantification. Another way to think of it is
this: $\exists\,{x}\ {Brother}({Richard},x)$ is a sentence about
Richard (that he has a brother), not about $x$; so putting a
$\forall\,x$ outside it has no effect. It could equally well have been
written $\exists\,{z}\ {Brother}({Richard},z)$. Because this can be
a source of confusion, we will always use different variable names with
nested quantifiers.

#### Connections between $\forall$ and $\exists$

The two quantifiers are actually intimately connected with each other,
through negation. Asserting that everyone dislikes parsnips is the same
as asserting there does not exist someone who likes them, and vice
versa:
$$\All{x} \lnot {Likes}(x,{Parsnips})\quad\mbox{is equivalent to}\quad
  \lnot\Exi{x} {Likes}(x,{Parsnips})\ .$$ We can go one step
further: “Everyone likes ice cream” means that there is no one who does
not like ice cream:
$$\All{x} {Likes}(x,{IceCream})\quad\mbox{is equivalent to}\quad
  \lnot\Exi{x} \lnot {Likes}(x,{IceCream})\ .$$ Because $\forall$ is
really a conjunction over the universe of objects and $\exists$ is a
disjunction, it should not be surprising that they obey De Morgan’s
rules. The De Morgan rules for quantified and unquantified sentences are
as follows: $$\begin{array}{lcl@{\hspace{0.6in}}lcl}
\All{x} \lnot P\ & \equiv &\lnot \Exi{x} P 
    & \lnot(P \lor Q)  &\equiv& \lnot P \land \lnot Q\\
\lnot \All{x} P\ & \equiv &\Exi{x} \lnot P
    & \lnot(P \land Q) & \equiv & \lnot P \lor \lnot Q \\
\All{x} P\ & \equiv &\lnot \Exi{x} \lnot P
    & P \land Q &\equiv& \lnot(\lnot P \lor \lnot Q) \\
\Exi{x} P\ & \equiv &\lnot \All{x} \lnot P
    & P \lor Q &\equiv& \lnot(\lnot P \land \lnot Q)\ .
\end{array}$$ Thus, we do not really need both $\forall$ and $\exists$,
just as we do not really need both $\land$ and $\lor$. Still,
readability is more important than parsimony, so we will keep both of
the quantifiers.

### Equality {#equality-section}

First-order logic includes one more way to make atomic sentences, other
than using a predicate and terms as described earlier. We can use the to
signify that two terms refer to the same object. For example,
$${Father}({John}) \eq {Henry}$$ says that the object referred to
by ${Father}({John})$ and the object referred to by ${Henry}$ are
the same. Because an interpretation fixes the referent of any term,
determining the truth of an equality sentence is simply a matter of
seeing that the referents of the two terms are the same object.

The equality symbol can be used to state facts about a given function,
as we just did for the ${Father}$ symbol. It can also be used with
negation to insist that two terms are not the same object. To say that
Richard has at least two brothers, we would write
$$\Exi{x,y} {Brother}(x,{Richard}) \land {Brother}(y,{Richard}) \land \lnot(x \eq y)\ .$$
The sentence
$$\Exi{x,y} {Brother}(x,{Richard}) \land {Brother}(y,{Richard})$$
does not have the intended meaning. In particular, it is true in the
model of , where Richard has only one brother. To see this, consider the
extended interpretation in which both $x$ and $y$ are assigned to King
John. The addition of $\lnot(x
\eq y)$ rules out such models. The notation $x \neq y$ is sometimes used
as an abbreviation for $\lnot(x\eq y)$.

### An alternative semantics? {#database-semantics-section}

Continuing the example from the previous section, suppose that we
believe that Richard has two brothers, John and Geoffrey.[^8] Can we
capture this state of affairs by asserting

$${Brother}({John},{Richard}) \land
{Brother}({Geoffrey},{Richard})\ ?
\label{john-bill-equation}$$

Not quite. First, this assertion is true in a model where Richard has
only one brother—we need to add ${John}\neq
{Geoffrey}$. Second, the sentence doesn’t rule out models in which
Richard has many more brothers besides John and Geoffrey. Thus, the
correct translation of “Richard’s brothers are John and Geoffrey” is as
follows:

(,) (,)\
      (x,) (x x) .

For many purposes, this seems much more cumbersome than the
corresponding natural-language expression. As a consequence, humans may
make mistakes in translating their knowledge into first-order logic,
resulting in unintuitive behaviors from logical reasoning systems that
use the knowledge. Can we devise a semantics that allows a more
straightforward logical expression?

One proposal that is very popular in database systems works as follows.
First, we insist that every constant symbol refer to a distinct
object—the so-called . Second, we assume that atomic sentences not known
to be true are in fact false—the . Finally, we invoke , meaning that
each model contains no more domain elements than those named by the
constant symbols. Under the resulting semantics, which we call to
distinguish it from the standard semantics of first-order logic, the
sentence does indeed state that Richard’s two brothers are John and
Geoffrey. Database semantics is also used in logic programming systems,
as explained in .

[all-models-database-figure]

It is instructive to consider the set of all possible models under
database semantics for the same case as shown in . shows some of the
models, ranging from the model with no tuples satisfying the relation to
the model with all tuples satisfying the relation. With two objects,
there are four possible two-element tuples, so there are $2^4\eq 16$
different subsets of tuples that can satisfy the relation. Thus, there
are 16 possible models in all—a lot fewer than the infinitely many
models for the standard first-order semantics. On the other hand, the
database semantics requires definite knowledge of what the world
contains.

This example brings up an important point: there is no one “correct”
semantics for logic. The usefulness of any proposed semantics depends on
how concise and intuitive it makes the expression of the kinds of
knowledge we want to write down, and on how easy and natural it is to
develop the corresponding rules of inference. Database semantics is most
useful when we are certain about the identity of all the objects
described in the knowledge base and when we have all the facts at hand;
in other cases, it is quite awkward. For the rest of this chapter, we
assume the standard semantics while noting instances in which this
choice leads to cumbersome expressions.

Using First-Order Logic {#fol-use-section}
-----------------------

Now that we have defined an expressive logical language, it is time to
learn how to use it. The best way to do this is through examples. We
have seen some simple sentences illustrating the various aspects of
logical syntax; in this section, we provide more systematic
representations of some simple . In knowledge representation, a domain
is just some part of the world about which we wish to express some
knowledge.

We begin with a brief description of the / interface for first-order
knowledge bases. Then we look at the domains of family relationships,
numbers, sets, and lists, and at the wumpus world. The next section
contains a more substantial example (electronic circuits) and covers
everything in the universe.

### Assertions and queries in first-order logic

Sentences are added to a knowledge base using , exactly as in
propositional logic. Such sentences are called . For example, we can
assert that John is a king, Richard is a person, and all kings are
persons:

(,()) .\
(,()) .\
(, (x) (x)) .

We can ask questions of the knowledge base using . For example,
$$\mbox{\noprog{Ask}}({KB},\,{King}({John}))$$ returns ťrue.
Questions asked with are called or . Generally speaking, any query that
is logically entailed by the knowledge base should be answered
affirmatively. For example, given the two preceding assertions, the
query $$\mbox{\noprog{Ask}}({KB},\,{Person}({John}))$$ should also
return ťrue. We can ask quantified queries, such as
$$\mbox{\noprog{Ask}}({KB},\;\Exi{x} {Person}(x))\ .$$ The answer is
ťrue, but this is perhaps not as helpful as we would like. It is rather
like answering “Can you tell me the time?” with “Yes.” If we want to
know what value of $x$ makes the sentence true, we will need a different
function, , which we call with
$$\mbox{\noprog{AskVars}}({KB},{Person}(x))$$ and which yields a
stream of answers. In this case there will be two answers:
$\{x/{John}\}$ and $\{x/{Richard}\}$. Such an answer is called a or
. is usually reserved for knowledge bases consisting solely of Horn
clauses, because in such knowledge bases every way of making the query
true will bind the variables to specific values. That is not the case
with first-order logic; if ${KB}$ has been told
${King}({John}) \lor {King}({Richard})$, then there is no
binding to $x$ for the query $\Exi{x} {King}(x)$, even though the
query is true.

### The kinship domain

[kinship-domain-section]

The first example we consider is the domain of family relationships, or
kinship. This domain includes facts such as “Elizabeth is the mother of
Charles” and “Charles is the father of William” and rules such as “One’s
grandmother is the mother of one’s parent.”

Clearly, the objects in our domain are people. We have two unary
predicates, ${Male}$ and ${Female}$. Kinship relations—parenthood,
brotherhood, marriage, and so on—are represented by binary predicates:
${Parent}$, ${Sibling}$, ${Brother}$, ${Sister}$, ${Child}$,
${Daughter}$, ${Son}$, ${Spouse}$, ${Wife}$, ${Husband}$,
${Grandparent}$, ${Grandchild}$, ${Cousin}$, ${Aunt}$, and
$\v{Uncle}$. We use functions for ${Mother}$ and ${Father}$, because
every person has exactly one of each of these (at least according to
nature’s design).

We can go through each function and predicate, writing down what we know
in terms of the other symbols. For example, one’s mother is one’s female
parent:
$$\All{m,c} {Mother}(c) \eq  m  \lequiv {Female}(m) \land {Parent}(m,c)\ .$$
One’s husband is one’s male spouse:
$$\All{w,h} {Husband}(h,w) \lequiv {Male}(h) \land {Spouse}(h,w)\ .$$
Male and female are disjoint categories:
$$\All{x} {Male}(x) \lequiv \lnot {Female}(x)\ .$$ Parent and child
are inverse relations:
$$\All{p,c} {Parent}(p,c) \lequiv {Child}(c,p)\ .$$ A grandparent is
a parent of one’s parent:
$$\All{g,c} {Grandparent}(g,c) \lequiv \Exi{p} {Parent}(g,p)\land {Parent}(p,c)\ .$$
A sibling is another child of one’s parents:
$$\All{x,y} {Sibling}(x,y) \lequiv x \neq y \land 
\Exi{p} {Parent}(p,x)  \land {Parent}(p,y)\ .$$ We could go on for
several more pages like this, and asks you to do just that.

Each of these sentences can be viewed as an of the kinship domain, as
explained in . Axioms are commonly associated with purely mathematical
domains—we will see some axioms for numbers shortly—but they are needed
in all domains. They provide the basic factual information from which
useful conclusions can be derived. Our kinship axioms are also ; they
have the form $\All{x,y} P(x,y) \lequiv \ldots$. The axioms define the
${Mother}$ function and the ${Husband}$, ${Male}$, ${Parent}$,
${Grandparent}$, and ${Sibling}$ predicates in terms of other
predicates. Our definitions “bottom out” at a basic set of predicates
(${Child}$, ${Spouse}$, and ${Female}$) in terms of which the
others are ultimately defined. This is a natural way in which to build
up the representation of a domain, and it is analogous to the way in
which software packages are built up by successive definitions of
subroutines from primitive library functions. Notice that there is not
necessarily a unique set of primitive predicates; we could equally well
have used ${Parent}$, ${Spouse}$, and ${Male}$. In some domains,
as we show, there is no clearly identifiable basic set.

Not all logical sentences about a domain are axioms. Some are —that is,
they are entailed by the axioms. For example, consider the assertion
that siblinghood is symmetric:
$$\All{x,y} {Sibling}(x,y) \lequiv {Sibling}(y,x)\ .$$ Is this an
axiom or a theorem? In fact, it is a theorem that follows logically from
the axiom that defines siblinghood. If we the knowledge base this
sentence, it should return ťrue.

From a purely logical point of view, a knowledge base need contain only
axioms and no theorems, because the theorems do not increase the set of
conclusions that follow from the knowledge base. From a practical point
of view, theorems are essential to reduce the computational cost of
deriving new sentences. Without them, a reasoning system has to start
from first principles every time, rather like a physicist having to
rederive the rules of calculus for every new problem.

Not all axioms are definitions. Some provide more general information
about certain predicates without constituting a definition. Indeed, some
predicates have no complete definition because we do not know enough to
characterize them fully. For example, there is no obvious definitive way
to complete the sentence $$\All{x} {Person}(x) \lequiv \ldots$$
Fortunately, first-order logic allows us to make use of the ${Person}$
predicate without completely defining it. Instead, we can write partial
specifications of properties that every person has and properties that
make something a person:

​(x) …\
 …(x) .

Axioms can also be “just plain facts,” such as ${Male}({Jim})$ and
${Spouse}({Jim},{Laura})$. Such facts form the descriptions of
specific problem instances, enabling specific questions to be answered.
The answers to these questions will then be theorems that follow from
the axioms. Often, one finds that the expected answers are not
forthcoming—for example, from ${Spouse}({Jim},{Laura})$ one
expects (under the laws of many countries) to be able to infer
$\lnot {Spouse}({George},{Laura})$; but this does not follow from
the axioms given earlier—even after we add ${Jim}\neq {George}$ as
suggested in . This is a sign that an axiom is missing. asks the reader
to supply it.

### Numbers, sets, and lists {#Peano-section}

Numbers are perhaps the most vivid example of how a large theory can be
built up from a tiny kernel of axioms. We describe here the theory of or
non-negative integers. We need a predicate ${NatNum}$ that will be
true of natural numbers; we need one constant symbol, $0$; and we need
one function symbol, $S$ (successor). The define natural numbers and
addition.[^9] Natural numbers are defined recursively:

(0) .\
 (n) (S(n)) .

That is, 0 is a natural number, and for every object $n$, if $n$ is a
natural number, then $S(n)$ is a natural number. So the natural numbers
are $0$, $S(0)$, $S(S(0))$, and so on. (After reading , you will notice
that these axioms allow for other natural numbers besides the usual
ones; see .) We also need axioms to constrain the successor function:

0S(n) .\
 mn S(m) S(n) .

Now we can define addition in terms of the successor function:

​(m) +(0,m) = m .\
 (m) (n) +(S(m),n) = S(+(m,n)) .

The first of these axioms says that adding 0 to any natural number $m$
gives $m$ itself. Notice the use of the binary function symbol “$+$” in
the term $+(m,0)$; in ordinary mathematics, the term would be written
$m+0$ using notation. (The notation we have used for first-order logic
is called .) To make our sentences about numbers easier to read, we
allow the use of infix notation. We can also write $S(n)$ as $n+1$, so
the second axiom becomes
$$\All{m,n} {NatNum}(m) \land {NatNum}(n) \implies (m+1)+n = (m+n)+1\ .$$
This axiom reduces addition to repeated application of the successor
function.

The use of infix notation is an example of , that is, an extension to or
abbreviation of the standard syntax that does not change the semantics.
Any sentence that uses sugar can be “desugared” to produce an equivalent
sentence in ordinary first-order logic.

Once we have addition, it is straightforward to define multiplication as
repeated addition, exponentiation as repeated multiplication, integer
division and remainders, prime numbers, and so on. Thus, the whole of
number theory (including cryptography) can be built up from one
constant, one function, one predicate and four axioms.

The domain of is also fundamental to mathematics as well as to
commonsense reasoning. (In fact, it is possible to define number theory
in terms of set theory.) We want to be able to represent individual
sets, including the empty set. We need a way to build up sets by adding
an element to a set or taking the union or intersection of two sets. We
will want to know whether an element is a member of a set and we will
want to distinguish sets from objects that are not sets.

We will use the normal vocabulary of set theory as syntactic sugar. The
empty set is a constant written as $\emptyset$. There is one unary
predicate, ${Set}$, which is true of sets. The binary predicates are
$x\elt s$ ($x$ is a member of set $s$) and $s_1 \subseteq s_2$ (set
$s_1$ is a subset, not necessarily proper, of set $s_2$). The binary
functions are $s_1 \intersection s_2$ (the intersection of two sets),
$s_1 \union s_2$ (the union of two sets), and $\adjoin{x}{s}$ (the set
resulting from adjoining element $x$ to set $s$). One possible set of
axioms is as follows:

1.  The only sets are the empty set and those made by adjoining
    something to a set: $$\All{s} {Set}(s) \lequiv (s \eq  \emptyset) 
            \lor (\Exi{x,s_2}  {Set}(s_2) \land s \eq  \adjoin{x}{s_2})\ .$$

2.  The empty set has no elements adjoined into it. In other words,
    there is no way to decompose $\emptyset$ into a smaller set and an
    element: $$\lnot \Exi{x,s} \adjoin{x}{s} \eq  \emptyset\ .$$

3.  Adjoining an element already in the set has no effect:
    $$\All{x,s} x\elt s \lequiv s \eq  \adjoin{x}{s}\ .$$

4.  The only members of a set are the elements that were adjoined into
    it. We express this recursively, saying that $x$ is a member of $s$
    if and only if $s$ is equal to some set $s_2$ adjoined with some
    element $y$, where either $y$ is the same as $x$ or $x$ is a member
    of $s_2$:
    $$\All{x,s} x\elt s \,\lequiv\, \Exi{y,s_2} (s \eq  \adjoin{y}{s_2} \land (x \eq  y \lor x\elt s_2))\ .$$

5.  A set is a subset of another set if and only if all of the first
    set’s members are members of the second set:
    $$\All{s_1,s_2} s_1\subseteq s_2 \lequiv (\All{x} x\elt s_1 \implies x\elt s_2)\ .$$

6.  Two sets are equal if and only if each is a subset of the other:
    $$\All{s_1,s_2} (s_1 \eq  s_2) \lequiv (s_1\subseteq s_2 \land s_2\subseteq s_1)\ .$$

7.  An object is in the intersection of two sets if and only if it is a
    member of both sets:
    $$\All{x,s_1,s_2} x\elt (s_1\intersection s_2) \lequiv (x\elt s_1 \land x\elt s_2)\ .$$

8.  An object is in the union of two sets if and only if it is a member
    of either set:
    $$\All{x,s_1,s_2} x\elt (s_1\union s_2) \lequiv (x\elt s_1 \lor x\elt s_2)\ .$$

are similar to sets. The differences are that lists are ordered and the
same element can appear more than once in a list. We can use the
vocabulary of Lisp for lists: ${Nil}$ is the constant list with no
elements; ${Cons}$, ${Append}$, ${First}$, and ${Rest}$ are
functions; and ${Find}$ is the predicate that does for lists what
${Member}$ does for sets. ${List}?$ is a predicate that is true only
of lists. As with sets, it is common to use syntactic sugar in logical
sentences involving lists. The empty list is $\emptylist$. The term
${Cons}(x,y)$, where $y$ is a nonempty list, is written $[x|y]$. The
term ${Cons}(x,{Nil})$ (i.e., the list containing the element $x$)
is written as $[x]$. A list of several elements, such as $[A,B,C]$,
corresponds to the nested term
${Cons}(A,{Cons}(B,{Cons}(C,{Nil})))$. asks you to write out the
axioms for lists.

### The wumpus world {#fol-wumpus}

Some propositional logic axioms for the wumpus world were given in . The
first-order axioms in this section are much more concise, capturing in a
natural way exactly what we want to say.

Recall that the wumpus agent receives a percept vector with five
elements. The corresponding first-order sentence stored in the knowledge
base must include both the percept and the time at which it occurred;
otherwise, the agent will get confused about when it saw what. We use
integers for time steps. A typical percept sentence would be
$${Percept}([{Stench},{Breeze},{Glitter},{None},{None}],5)\ .$$
Here, ${Percept}$ is a binary predicate, and ${Stench}$ and so on
are constants placed in a list. The actions in the wumpus world can be
represented by logical terms:
$${Turn}({Right}),\ \  {Turn}({Left}),\ \  {Forward},\ \  {Shoot},\ \  {Grab},
\ \   {Climb}\ .$$ To determine which is best, the agent program
executes the query $$\noprog{AskVars}(\Exi{a} {BestAction}(a,5))\ ,$$
which returns a binding list such as $\{a/{Grab}\}$. The agent program
can then return ${Grab}$ as the action to take. The raw percept data
implies certain facts about the current state. For example:

([s,,g,m,c],t) (t) ,\
 ([s,b,,m,c],t) (t) ,

and so on. These rules exhibit a trivial form of the reasoning process
called , which we study in depth in . Notice the quantification over
time $t$. In propositional logic, we would need copies of each sentence
for each time step.

Simple “reflex” behavior can also be implemented by quantified
implication sentences. For example, we have
$$\All{t} {Glitter}(t) \implies {BestAction}({Grab},t)\ .$$ Given
the percept and rules from the preceding paragraphs, this would yield
the desired conclusion ${BestAction}({Grab},5)$—that is, ${Grab}$
is the right thing to do.

We have represented the agent’s inputs and outputs; now it is time to
represent the environment itself. Let us begin with objects. Obvious
candidates are squares, pits, and the wumpus. We could name each
square—${Square}{}_{1,2}$ and so on—but then the fact that
${Square}{}_{1,2}$ and ${Square}{}_{1,3}$ are adjacent would have to
be an “extra” fact, and we would need one such fact for each pair of
squares. It is better to use a complex term in which the row and column
appear as integers; for example, we can simply use the list term
$[1,2]$. Adjacency of any two squares can be defined as

([x,y], [a,b])\
(x=a (y=b-1 y=b+1)) (y=b (x=a-1 x=a+1))  .

We could name each pit, but this would be inappropriate for a different
reason: there is no reason to distinguish among pits.[^10] It is simpler
to use a unary predicate ${Pit}$ that is true of squares containing
pits. Finally, since there is exactly one wumpus, a constant
${Wumpus}$ is just as good as a unary predicate (and perhaps more
dignified from the wumpus’s viewpoint).

The agent’s location changes over time, so we write
${At}({Agent},s,t)$ to mean that the agent is at square $s$ at time
$t$. We can fix the wumpus’s location with
$\forall t\,{At}({Wumpus},[2,2],t)$. We can then say that objects
can only be at one location at a time:
$$\All{x,s_1,s_2,t} {At}(x,s_1,t) \land {At}(x,s_2,t) \implies s_1 = s_2 \ .$$
Given its current location, the agent can infer properties of the square
from properties of its current percept. For example, if the agent is at
a square and perceives a breeze, then that square is breezy:
$$\All{s,t} {At}({Agent},s,t) \land {Breeze}(t) \implies {Breezy}(s)\ .$$
It is useful to know that a *square* is breezy because we
know that the pits cannot move about. Notice that ${Breezy}$ has no
time argument.

Having discovered which places are breezy (or smelly) and, very
important, *not* breezy (or *not* smelly), the
agent can deduce where the pits are (and where the wumpus is). Whereas
propositional logic necessitates a separate axiom for each square (see
$R_2$ and $R_3$ on ) and would need a different set of axioms for each
geographical layout of the world, first-order logic just needs one
axiom:

$$\All{s} {Breezy}(s) \lequiv \Exi{r} {Adjacent}(r,s) \land {Pit}(r)\ .
\label{pit-biconditional-equation}$$

Similarly, in first-order logic we can quantify over time, so we need
just one successor-state axiom for each predicate, rather than a
different copy for each time step. For example, the axiom for the arrow
( on ) becomes
$$\All{t} {HaveArrow}(t+1) \lequiv ({HaveArrow}(t) \land \lnot {Action}({Shoot},t)) \ .$$
From these two example sentences, we can see that the first-order logic
formulation is no less concise than the original English-language
description given in . The reader is invited to construct analogous
axioms for the agent’s location and orientation; in these cases, the
axioms quantify over both space and time. As in the case of
propositional state estimation, an agent can use logical inference with
axioms of this kind to keep track of aspects of the world that are not
directly observed. goes into more depth on the subject of first-order
successor-state axioms and their uses for constructing plans.

Knowledge Engineering in First-Order Logic {#circuits-section}
------------------------------------------

The preceding section illustrated the use of first-order logic to
represent knowledge in three simple domains. This section describes the
general process of knowledge-base construction—a process called . A
knowledge engineer is someone who investigates a particular domain,
learns what concepts are important in that domain, and creates a formal
representation of the objects and relations in the domain. We illustrate
the knowledge engineering process in an electronic circuit domain that
should already be fairly familiar, so that we can concentrate on the
representational issues involved. The approach we take is suitable for
developing *special-purpose* knowledge bases whose domain
is carefully circumscribed and whose range of queries is known in
advance. *General-purpose* knowledge bases, which cover a
broad range of human knowledge and are intended to support tasks such as
natural language understanding, are discussed in .

### The knowledge-engineering process

Knowledge engineering projects vary widely in content, scope, and
difficulty, but all such projects include the following steps:

1.  *Identify the task.* The knowledge engineer must
    delineate the range of questions that the knowledge base will
    support and the kinds of facts that will be available for each
    specific problem instance. For example, does the wumpus knowledge
    base need to be able to choose actions or is it required to answer
    questions only about the contents of the environment? Will the
    sensor facts include the current location? The task will determine
    what knowledge must be represented in order to connect problem
    instances to answers. This step is analogous to the PEAS process for
    designing agents in .

2.  *Assemble the relevant knowledge.* The knowledge
    engineer might already be an expert in the domain, or might need to
    work with real experts to extract what they know—a process called .
    At this stage, the knowledge is not represented formally. The idea
    is to understand the scope of the knowledge base, as determined by
    the task, and to understand how the domain actually works.

    For the wumpus world, which is defined by an artificial set of
    rules, the relevant knowledge is easy to identify. (Notice, however,
    that the definition of adjacency was not supplied explicitly in the
    wumpus-world rules.) For real domains, the issue of relevance can be
    quite difficult—for example, a system for simulating VLSI designs
    might or might not need to take into account stray capacitances and
    skin effects.

3.  *Decide on a vocabulary of predicates, functions, and
    constants.* That is, translate the important domain-level
    concepts into logic-level names. This involves many questions of
    knowledge-engineering *style*. Like programming style,
    this can have a significant impact on the eventual success of the
    project. For example, should pits be represented by objects or by a
    unary predicate on squares? Should the agent’s orientation be a
    function or a predicate? Should the wumpus’s location depend on
    time? Once the choices have been made, the result is a vocabulary
    that is known as the of the domain. The word *ontology*
    means a particular theory of the nature of being or existence. The
    ontology determines what kinds of things exist, but does not
    determine their specific properties and interrelationships.

4.  *Encode general knowledge about the domain.* The
    knowledge engineer writes down the axioms for all the vocabulary
    terms. This pins down (to the extent possible) the meaning of the
    terms, enabling the expert to check the content. Often, this step
    reveals misconceptions or gaps in the vocabulary that must be fixed
    by returning to step 3 and iterating through the process.

5.  *Encode a description of the specific problem
    instance.* If the ontology is well thought out, this step
    will be easy. It will involve writing simple atomic sentences about
    instances of concepts that are already part of the ontology. For a
    logical agent, problem instances are supplied by the sensors,
    whereas a “disembodied” knowledge base is supplied with additional
    sentences in the same way that traditional programs are supplied
    with input data.

6.  *Pose queries to the inference procedure and get
    answers.* This is where the reward is: we can let the
    inference procedure operate on the axioms and problem-specific facts
    to derive the facts we are interested in knowing. Thus, we avoid the
    need for writing an application-specific solution algorithm.

7.  *Debug the knowledge base.* Alas, the answers to
    queries will seldom be correct on the first try. More precisely, the
    answers will be correct *for the knowledge base as
    written*, assuming that the inference procedure is sound, but
    they will not be the ones that the user is expecting. For example,
    if an axiom is missing, some queries will not be answerable from the
    knowledge base. A considerable debugging process could ensue.
    Missing axioms or axioms that are too weak can be easily identified
    by noticing places where the chain of reasoning stops unexpectedly.
    For example, if the knowledge base includes a diagnostic rule (see )
    for finding the wumpus,
    $$\All{s} {Smelly}(s) \implies {Adjacent}({Home}({Wumpus}),s)\ ,$$
    instead of the biconditional, then the agent will never be able to
    prove the *absence* of wumpuses. Incorrect axioms can
    be identified because they are false statements about the world. For
    example, the sentence
    $$\All{x} {NumOfLegs}(x,4) \implies {Mammal}(x)$$ is false for
    reptiles, amphibians, and, more importantly, tables.

    The falsehood of this sentence can be determined independently of
    the rest of the knowledge base.

    In contrast, a typical error in a program looks like this:
    $$\mbox{\tt offset = position + 1}\ .$$ It is impossible to tell
    whether this statement is correct without looking at the rest of the
    program to see whether, for example, offset is used to
    refer to the current position, or to one beyond the current
    position, or whether the value of position is changed
    by another statement and so offset should also be
    changed again.

To understand this seven-step process better, we now apply it to an
extended example—the domain of electronic circuits.

### The electronic circuits domain {#electronic-circuits-domain-subsection}

We will develop an ontology and knowledge base that allow us to reason
about digital circuits of the kind shown in . We follow the seven-step
process for knowledge engineering.

[adder-figure]

#### Identify the task

There are many reasoning tasks associated with digital circuits. At the
highest level, one analyzes the circuit’s functionality. For example,
does the circuit in actually add properly? If all the inputs are high,
what is the output of gate A2? Questions about the circuit’s structure
are also interesting. For example, what are all the gates connected to
the first input terminal? Does the circuit contain feedback loops? These
will be our tasks in this section. There are more detailed levels of
analysis, including those related to timing delays, circuit area, power
consumption, production cost, and so on. Each of these levels would
require additional knowledge.

#### Assemble the relevant knowledge

What do we know about digital circuits? For our purposes, they are
composed of wires and gates. Signals flow along wires to the input
terminals of gates, and each gate produces a signal on the output
terminal that flows along another wire. To determine what these signals
will be, we need to know how the gates transform their input signals.
There are four types of gates: AND, OR, and XOR gates have two input
terminals, and NOT gates have one. All gates have one output terminal.
Circuits, like gates, have input and output terminals.

To reason about functionality and connectivity, we do not need to talk
about the wires themselves, the paths they take, or the junctions where
they come together. All that matters is the connections between
terminals—we can say that one output terminal is connected to another
input terminal without having to say what actually connects them. Other
factors such as the size, shape, color, or cost of the various
components are irrelevant to our analysis.

If our purpose were something other than verifying designs at the gate
level, the ontology would be different. For example, if we were
interested in debugging faulty circuits, then it would probably be a
good idea to include the wires in the ontology, because a faulty wire
can corrupt the signal flowing along it. For resolving timing faults, we
would need to include gate delays. If we were interested in designing a
product that would be profitable, then the cost of the circuit and its
speed relative to other products on the market would be important.

#### Decide on a vocabulary

We now know that we want to talk about circuits, terminals, signals, and
gates. The next step is to choose functions, predicates, and constants
to represent them. First, we need to be able to distinguish gates from
each other and from other objects. Each gate is represented as an object
named by a constant, about which we assert that it is a gate with, say,
${Gate}(X_1)$. The behavior of each gate is determined by its type:
one of the constants $AND, OR$, $XOR$, or $NOT$. Because a gate has
exactly one type, a function is appropriate:
${Type}(X_1) \eq {XOR}$. Circuits, like gates, are identified by a
predicate: ${Circuit}(C_1)$.

Next we consider terminals, which are identified by the predicate
${Terminal}(x)$. A gate or circuit can have one or more input
terminals and one or more output terminals. We use the function
${In}(1,X_1)$ to denote the first input terminal for gate $X_1$. A
similar function ${Out}$ is used for output terminals. The function
${Arity}(c, i, j)$ says that circuit $c$ has $i$ input and $j$ output
terminals. The connectivity between gates can be represented by a
predicate, ${Connected}$, which takes two terminals as arguments, as
in ${Connected}({Out}(1,X_1),{In}(1,X_2))$.

Finally, we need to know whether a signal is on or off. One possibility
is to use a unary predicate, ${On}(t)$, which is true when the signal
at a terminal is on. This makes it a little difficult, however, to pose
questions such as “What are all the possible values of the signals at
the output terminals of circuit $C_1$?” We therefore introduce as
objects two signal values, $1$ and $0$, and a function ${Signal}(t)$
that denotes the signal value for the terminal $t$.

#### Encode general knowledge of the domain

One sign that we have a good is that we require only a few general
rules, which can be stated clearly and concisely. These are all the
axioms we will need:

1.  If two terminals are connected, then they have the same signal:

    $\All{t_1,t_2} {Terminal}(t_1) \land {Terminal}(t_2) \land {Connected}(t_1,t_2) \implies \\
    \mbox{~~~~~~~~~~~} {Signal}(t_1) \eq
    {Signal}(t_2)$ .

2.  The signal at every terminal is either 1 or 0:

    $\All{t} {Terminal}(t) \implies {Signal}(t) \eq  1 \lor {Signal}(t) \eq  0$
     .

3.  Connected is commutative:

    $\All{t_1,t_2} {Connected}(t_1,t_2) \lequiv {Connected}(t_2,t_1)$ .

4.  There are four types of gates:

    $\All{g} {Gate}(g) \land k = {Type}(g) \implies k = {AND} \lor k = {OR} \lor k = {XOR} \lor k = {NOT}$ .

5.  An AND gate’s output is 0 if and only if any of its inputs is 0:

    $\All{g} {Gate}(g) \land {Type}(g) \eq  {AND} \implies \\
    \mbox{~~~~~~~~~~~} {Signal}({Out}(1,g)) \eq  0 \lequiv \Exi{n}
    {Signal}({In}(n,g)) \eq  0$ .

6.  An OR gate’s output is 1 if and only if any of its inputs is 1:

    $\All{g} {Gate}(g) \land {Type}(g) \eq  {OR} \implies \\
    \mbox{~~~~~~~~~~~} {Signal}({Out}(1,g)) \eq  1  \lequiv \Exi{n}
    {Signal}({In}(n,g)) \eq  1$ .

7.  An XOR gate’s output is 1 if and only if its inputs are different:

    $\All{g} {Gate}(g) \land {Type}(g) \eq  {XOR} \implies \\
    \mbox{~~~~~~~~~~~} {Signal}({Out}(1,g)) \eq  1 \lequiv
    {Signal}({In}(1,g)) \neq {Signal}({In}(2,g))$ .

8.  A NOT gate’s output is different from its input:

    $\All{g} {Gate}(g) \land ({Type}(g) \eq  {NOT}) \implies \\
    \mbox{~~~~~~~~~~~} {Signal}({Out}(1,g)) \neq {Signal}({In}(1,g))$ .

9.  The gates (except for NOT) have two inputs and one output.

    $  \All{g} {Gate}(g) \land {Type}(g) = {NOT} \implies {Arity}(g, 1, 1) $
     .

    $\All{g} {Gate}(g) \land k = {Type}(g) \land (k = {AND} \lor k = {OR} \lor k = {XOR}) \implies \\
    \mbox{~~~~~~~~~~~} {Arity}(g, 2, 1) $

10. A circuit has terminals, up to its input and output arity, and
    nothing beyond its arity:

    $\All{c,i,j} {Circuit}(c) \land {Arity}(c,i,j) \implies \\
    \mbox{~~~~~~~~~~~~} \All{n} (n \le i \implies {Terminal}({In}(c, n))) \land (n > i \implies {In}(c, n) = {Nothing}) \land {} \\
    \mbox{~~~~~~~~~~~~} \All{n} (n \le j \implies {Terminal}({Out}(c, n))) \land (n > j \implies {Out}(c, n) = {Nothing}) $

11. Gates, terminals, signals, gate types, and ${Nothing}$ are all
    distinct.

    $\All{g,t} {Gate}(g) \land {Terminal}(t) \implies \\
    \mbox{~~~~~~~~~~~~} g \neq t \neq 1 \neq 0 \neq {OR} \neq {AND} \neq {XOR} \neq {NOT} \neq {Nothing}$ .

12. Gates are circuits.

    $\All{g} {Gate}(g) \implies {Circuit}(g)$

#### Encode the specific problem instance

The circuit shown in is encoded as circuit $C_1$ with the following
description. First, we categorize the circuit and its component gates:
$$\begin{array}{l}
{Circuit}(C_1) \land {Arity}(C_1, 3, 2) \\
{Gate}(X_1) \land {Type}(X_1) \eq  {XOR} \\
{Gate}(X_2) \land {Type}(X_2) \eq  {XOR} \\
{Gate}(A_1) \land {Type}(A_1) \eq  {AND} \\
{Gate}(A_2) \land {Type}(A_2) \eq  {AND} \\
{Gate}(O_1) \land {Type}(O_1) \eq  {OR}\ . 
\end{array}$$ Then, we show the connections between them:
$$\begin{array}{ll}
 {Connected}({Out}(1,X_1),{In}(1,X_2)) & {Connected}({In}(1,C_1),{In}(1,X_1)) \\
 {Connected}({Out}(1,X_1),{In}(2,A_2)) & {Connected}({In}(1,C_1),{In}(1,A_1)) \\
 {Connected}({Out}(1,A_2),{In}(1,O_1)) & {Connected}({In}(2,C_1),{In}(2,X_1)) \\
 {Connected}({Out}(1,A_1),{In}(2,O_1)) & {Connected}({In}(2,C_1),{In}(2,A_1)) \\
 {Connected}({Out}(1,X_2),{Out}(1,C_1))& {Connected}({In}(3,C_1),{In}(2,X_2)) \\
 {Connected}({Out}(1,O_1),{Out}(2,C_1))& {Connected}({In}(3,C_1),{In}(1,A_2)) \ .
\end{array}$$

#### Pose queries to the inference procedure

What combinations of inputs would cause the first output of $C_1$ (the
sum bit) to be 0 and the second output of $C_1$ (the carry bit) to be 1?
$${\small \begin{array}{l}
\Exi{i_1,i_2,i_3} {Signal}({In}(1,C_1)) \eq  i_1 \land {Signal}({In}(2,C_1)) \eq  i_2 
      \land {Signal}({In}(3,C_1)) \eq  i_3 \\
\ \qquad\land\ {Signal}({Out}(1,C_1)) \eq  0 \land {Signal}({Out}(2,C_1)) \eq  1\ .
  \end{array}}$$ The answers are substitutions for the variables $i_1$,
$i_2$, and $i_3$ such that the resulting sentence is entailed by the
knowledge base. will give us three such substitutions:
$$\{i_1/1,\, i_2/1,\, i_3/0\}\quad \{i_1/1,\, i_2/0,\, i_3/1\}\quad \{i_1/0,\, i_2/1,\, i_3/1\}\ .$$
What are the possible sets of values of all the terminals for the adder
circuit? $${\small \begin{array}{l}
\Exi{i_1,i_2,i_3,o_1,o_2} {Signal}({In}(1,C_1)) \eq  i_1 \land {Signal}({In}(2,C_1))
\eq  i_2 \\
\ \qquad \land\ {Signal}({In}(3,C_1)) \eq  i_3 \land {Signal}({Out}(1,C_1)) \eq  o_1 \land
{Signal}({Out}(2,C_1)) \eq  o_2\ .
\end{array}}$$ This final query will return a complete input–output
table for the device, which can be used to check that it does in fact
add its inputs correctly. This is a simple example of . We can also use
the definition of the circuit to build larger digital systems, for which
the same kind of verification procedure can be carried out. (See .) Many
domains are amenable to the same kind of structured knowledge-base
development, in which more complex concepts are defined on top of
simpler concepts.

#### Debug the knowledge base

We can perturb the knowledge base in various ways to see what kinds of
erroneous behaviors emerge. For example, suppose we fail to read and
hence forget to assert that $ 1 \neq 0 $. Suddenly, the system will be
unable to prove any outputs for the circuit, except for the input cases
000 and 110. We can pinpoint the problem by asking for the outputs of
each gate. For example, we can ask $${\small 
\Exi{i_1,i_2,o} {Signal}({In}(1,C_1)) \eq  i_1 \land {Signal}({In}(2,C_1))
\eq  i_2 \land {Signal}({Out}(1,X_1)) \ ,
}$$ which reveals that no outputs are known at $X_1$ for the input cases
10 and 01. Then, we look at the axiom for XOR gates, as applied to
$X_1$:
$${Signal}({Out}(1,X_1)) \eq  1 \lequiv {Signal}({In}(1,X_1)) \neq
  {Signal}({In}(2,X_1))\ .$$ If the inputs are known to be, say, 1
and 0, then this reduces to
$${Signal}({Out}(1,X_1)) \eq  1 \lequiv 1\neq 0\ .$$ Now the problem
is apparent: the system is unable to infer that
${Signal}({Out}(1,X_1)) \eq  1$, so we need to tell it that
$1\neq 0$.

This chapter has introduced , a representation language that is far more
powerful than propositional logic. The important points are as follows:

-   Knowledge representation languages should be declarative,
    compositional, expressive, context independent, and unambiguous.

-   Logics differ in their and . While propositional logic commits only
    to the existence of facts, first-order logic commits to the
    existence of objects and relations and thereby gains expressive
    power.

-   The syntax of first-order logic builds on that of propositional
    logic. It adds terms to represent objects, and has universal and
    existential quantifiers to construct assertions about all or some of
    the possible values of the quantified variables.

-   A , or , for first-order logic includes a set of objects and an that
    maps constant symbols to objects, predicate symbols to relations
    among objects, and function symbols to functions on objects.

-   An atomic sentence is true just when the relation named by the
    predicate holds between the objects named by the terms. , which map
    quantifier variables to objects in the model, define the truth of
    quantified sentences.

-   Developing a knowledge base in first-order logic requires a careful
    process of analyzing the domain, choosing a vocabulary, and encoding
    the axioms required to support the desired inferences.

Although Aristotle’s logic deals with generalizations over objects, it
fell far short of the expressive power of first-order logic. A major
barrier to its further development was its concentration on one-place
predicates to the exclusion of many-place relational predicates. The
first systematic treatment of relations was given by Augustus De Morgan
[-@DeMorgan:1864], who cited the following example to show the sorts of
inferences that Aristotle’s logic could not handle: “All horses are
animals; therefore, the head of a horse is the head of an animal.” This
inference is inaccessible to Aristotle because any valid rule that can
support this inference must first analyze the sentence using the
two-place predicate “$x$ is the head of $y$.” The logic of relations was
studied in depth by Charles Sanders
Peirce [-@Peirce:1870; -@Misak:2004].

True first-order logic dates from the introduction of quantifiers in
Gottlob Frege’s [-@Frege:1879] *Begriffschrift* (“Concept
Writing” or “Conceptual Notation”). also developed first-order logic
independently of Frege, although slightly later. Frege’s ability to nest
quantifiers was a big step forward, but he used an awkward notation. The
present notation for first-order logic is due substantially to Giuseppe
Peano [-@Peano:1889], but the semantics is virtually identical to
Frege’s. Oddly enough, Peano’s axioms were due in large measure to and .

Leopold Löwenheim [-@Loewenheim:1915] gave a systematic
treatment of model theory for first-order logic, including the first
proper treatment of the equality symbol. Löwenheim’s
results were further extended by Thoralf Skolem [-@Skolem:1920]. Alfred
Tarski [-@Tarski:1935; -@Tarski:1956] gave an explicit definition of
truth and model-theoretic satisfaction in first-order logic, using set
theory.

McCarthy [-@McCarthy:1958] was primarily responsible for the
introduction of first-order logic as a tool for building AI systems. The
prospects for logic-based AI were advanced significantly by
Robinson’s [-@Robinson:1965] development of resolution, a complete
procedure for first-order inference described in . The logicist approach
took root at Stanford University. Cordell developed a first-order
reasoning system, , leading to the first attempts to build a logical
robot at SRI @Fikes+Nilsson:1971. First-order logic was applied by Zohar
Manna and Richard Waldinger [-@Manna+Waldinger:1971] for reasoning about
programs and later by Michael for reasoning about circuits. In Europe,
logic programming (a restricted form of first-order reasoning) was
developed for linguistic analysis @Colmerauer+al:1973 and for general
declarative systems @Kowalski:1974. Computational logic was also well
entrenched at Edinburgh through the (Logic for Computable Functions)
project @Gordon+al:1979. These developments are chronicled further in
Chapters [logical-inference-chapter] and [kr-chapter].

Practical applications built with first-order logic include a system for
evaluating the manufacturing requirements for electronic
products @Mannion:2002, a system for reasoning about policies for file
access and digital rights management @Halpern+Weissman:2008, and a
system for the automated composition of Web services
@McIlraith+Zeng:2001.

Reactions to the Whorf hypothesis @Whorf:1956 and the problem of
language and thought in general, appear in several recent books
@Gumperz+Levinson:1996
[@Bowerman+Levinson:2001; @Pinker:2003; @Gentner:2003]. The “theory”
theory @Gopnik+Glymour:2002 [@Tenenbaum+al:2007] views children’s
learning about the world as analogous to the construction of scientific
theories. Just as the predictions of a machine learning algorithm depend
strongly on the vocabulary supplied to it, so will the child’s
formulation of theories depend on the linguistic environment in which
learning occurs.

There are a number of good introductory texts on first-order logic,
including some by leading figures in the history of logic: Alfred ,
Alonzo , and W.V. Quine [-@Quine:1982] (which is one of the most
readable). Enderton [-@Enderton:1972] gives a more mathematically
oriented perspective. A highly formal treatment of first-order logic,
along with many more advanced topics in logic, is provided by Bell and
Machover [-@Bell+Machover:1977]. Manna and
Waldinger [-@Manna+Waldinger:1985] give a readable introduction to logic
from a computer science perspective, as do , who concentrate on program
verification. take an approach similar to the one used here. presents
results concisely, using the tableau format. Gallier [-@Gallier:1986]
provides an extremely rigorous mathematical exposition of first-order
logic, along with a great deal of material on its use in automated
reasoning. *Logical Foundations of Artificial Intelligence*
@Genesereth+Nilsson:1987 is both a solid introduction to logic and the
first systematic treatment of logical agents with percepts and actions,
and there are two good handbooks: and . The journal of record for the
field of pure mathematical logic is the *Journal of Symbolic
Logic*, whereas the *Journal of Applied Logic* deals
with concerns closer to those of artificial intelligence.

A logical knowledge base represents the world using a set of sentences
with no explicit structure. An representation, on the other hand, has
physical structure that corresponds directly to the structure of the
thing represented. Consider a road map of your country as an analogical
representation of facts about the country—it represents facts with a map
language. The two-dimensional structure of the map corresponds to the
two-dimensional surface of the area.

1.  Give five examples of *symbols* in the map language.

2.  An *explicit* sentence is a sentence that the creator
    of the representation actually writes down. An
    *implicit* sentence is a sentence that results from
    explicit sentences because of properties of the analogical
    representation. Give three examples each of *implicit*
    and *explicit* sentences in the map language.

3.  Give three examples of facts about the physical structure of your
    country that cannot be represented in the map language.

4.  Give two examples of facts that are much easier to express in the
    map language than in first-order logic.

5.  Give two other examples of useful analogical representations. What
    are the advantages and disadvantages of each of these languages?

Consider a knowledge base containing just two sentences: $P(a)$ and
$P(b)$. Does this knowledge base entail $\forall\,x\ P(x)$? Explain your
answer in terms of models.

Is the sentence $\Exi{x,y} x\eq y$ valid? Explain.

Write down a logical sentence such that every world in which it is true
contains exactly one object.

Write down a logical sentence such that every world in which it is true
contains exactly two objects.

[fol-model-count-exercise] Consider a symbol vocabulary that contains
$c$ constant symbols, $p_k$ predicate symbols of each arity $k$, and
$f_k$ function symbols of each arity $k$, where $1\leq k\leq A$. Let the
domain size be fixed at $D$. For any given model, each predicate or
function symbol is mapped onto a relation or function, respectively, of
the same arity. You may assume that the functions in the model allow
some input tuples to have no value for the function (i.e., the value is
the invisible object). Derive a formula for the number of possible
models for a domain with $D$ elements. Don’t worry about eliminating
redundant combinations.

Which of the following are valid (necessarily true) sentences?

1.  $(\exists x\ x\eq x) \implies (\All{y} \exists z\ y\eq z)$.

2.  $\All{x} P(x) \lor \lnot P(x)$.

3.  $\All{x} {Smart}(x) \lor (x\eq x)$.

[empty-universe-exercise] Consider a version of the semantics for
first-order logic in which models with empty domains are allowed. Give
at least two examples of sentences that are valid according to the
standard semantics but not according to the new semantics. Discuss which
outcome makes more intuitive sense for your examples.

[hillary-exercise]Does the fact $\lnot {Spouse}({George},{Laura})$
follow from the facts ${Jim}\neq {George}$ and
${Spouse}({Jim},{Laura})$? If so, give a proof; if not, supply
additional axioms as needed. What happens if we use ${Spouse}$ as a
unary function symbol instead of a binary predicate?

This exercise uses the function ${MapColor}$ and predicates
${In}(x,y)$, ${Borders}(x,y)$, and ${Country}(x)$, whose arguments
are geographical regions, along with constant symbols for various
regions. In each of the following we give an English sentence and a
number of candidate logical expressions. For each of the logical
expressions, state whether it (1) correctly expresses the English
sentence; (2) is syntactically invalid and therefore meaningless; or (3)
is syntactically valid but does not express the meaning of the English
sentence.

1.  Paris and Marseilles are both in France.

    1.  ${In}({Paris} \land {Marseilles}, {France})$.

    2.  ${In}({Paris},{France}) \land {In}({Marseilles},{France})$.

    3.  ${In}({Paris},{France}) \lor {In}({Marseilles},{France})$.

2.  There is a country that borders both Iraq and Pakistan.

    1.  $\Exi{c}$
        ${Country}(c) \land {Border}(c,{Iraq}) \land {Border}(c,{Pakistan})$.

    2.  $\Exi{c}$
        ${Country}(c) \implies [{Border}(c,{Iraq}) \land {Border}(c,{Pakistan})]$.

    3.  $[\Exi{c}$
        ${Country}(c)] \implies [{Border}(c,{Iraq}) \land {Border}(c,{Pakistan})]$.

    4.  $\Exi{c}$
        ${Border}({Country}(c),{Iraq} \land {Pakistan})$.

3.  All countries that border Ecuador are in South America.

    1.  $\All{c}  Country(c) \land {Border}(c,{Ecuador}) \implies {In}(c,{SouthAmerica})$.

    2.  $\All{c}  {Country}(c) \implies [{Border}(c,{Ecuador}) \implies {In}(c,{SouthAmerica})]$.

    3.  $\All{c}  [{Country}(c) \implies {Border}(c,{Ecuador})] \implies {In}(c,{SouthAmerica})$.

    4.  $\All{c}  Country(c) \land {Border}(c,{Ecuador}) \land {In}(c,{SouthAmerica})$.

4.  No region in South America borders any region in Europe.

    1.  $\lnot [\Exi{c,d}  {In}(c,{SouthAmerica}) \land {In}(d,{Europe}) \land {Borders}(c,d)]$.

    2.  $\All{c,d}  [{In}(c,{SouthAmerica}) \land {In}(d,{Europe})] \implies \lnot {Borders}(c,d)]$.

    3.  $\lnot \All{c}  {In}(c,{SouthAmerica}) \implies \Exi{d} {In}(d,{Europe}) \land 
        \lnot {Borders}(c,d)$.

    4.  $\All{c} {In}(c,{SouthAmerica}) \implies \All{d} {In}(d,{Europe}) \implies \lnot {Borders}(c,d)$.

5.  No two adjacent countries have the same map color.

    1.  $\All{x,y} \lnot {Country}(x) \lor \lnot {Country}(y) \lor \lnot {Borders}(x,y) \lor {}$\
        $\lnot ({MapColor}(x) = {MapColor}(y))$.

    2.  $\All{x,y} ({Country}(x) \land {Country}(y) \land {Borders}(x,y) \land \lnot(x=y)) \implies {}$\
        $\lnot ({MapColor}(x) = {MapColor}(y))$.

    3.  $\All{x,y} {Country}(x) \land {Country}(y) \land {Borders}(x,y) \land {}$\
        $\lnot ({MapColor}(x) = {MapColor}(y))$.

    4.  $\All{x,y} ({Country}(x) \land {Country}(y) \land {Borders}(x,y) ) \implies {MapColor}(x\neq y)$.

Consider a vocabulary with the following symbols:

> ${Occupation}(p,o)$: Predicate. Person $p$ has occupation $o$.\
> ${Customer}(p1,p2)$: Predicate. Person $p1$ is a customer of person
> $p2$.\
> ${Boss}(p1,p2)$: Predicate. Person $p1$ is a boss of person $p2$.\
> ${Doctor}$, $ {Surgeon}$, $ {Lawyer}$, $ {Actor}$: Constants
> denoting occupations.\
> ${Emily}$, $ {Joe}$: Constants denoting people.

Use these symbols to write the following assertions in first-order
logic:

1.  Emily is either a surgeon or a lawyer.

2.  Joe is an actor, but he also holds another job.

3.  All surgeons are doctors.

4.  Joe does not have a lawyer (i.e., is not a customer of any lawyer).

5.  Emily has a boss who is a lawyer.

6.  There exists a lawyer all of whose customers are doctors.

7.  Every surgeon has a lawyer.

In each of the following we give an English sentence and a number of
candidate logical expressions. For each of the logical expressions,
state whether it (1) correctly expresses the English sentence; (2) is
syntactically invalid and therefore meaningless; or (3) is syntactically
valid but does not express the meaning of the English sentence.

1.  Every cat loves its mother or father.

    1.  $\All{x} {Cat}(x) \implies {Loves}(x,{Mother}(x)\lor {Father}(x))$.

    2.  $\All{x} \lnot {Cat}(x) \lor {Loves}(x,{Mother}(x)) \lor {Loves}(x,{Father}(x))$.

    3.  $\All{x} {Cat}(x) \land ({Loves}(x,{Mother}(x))\lor {Loves}(x,{Father}(x)))$.

2.  Every dog who loves one of its brothers is happy.

    1.  $\All{x} {Dog}(x) \land (\exists y\ {Brother}(y,x) \land {Loves}(x,y)) \implies {Happy}(x)$.

    2.  $\All{x,y} {Dog}(x) \land {Brother}(y,x) \land {Loves}(x,y) \implies {Happy}(x)$.

    3.  $\All{x} {Dog}(x) \land [\All{y} {Brother}(y,x) \lequiv {Loves}(x,y)] \implies {Happy}(x)$.

3.  No dog bites a child of its owner.

    1.  $\All{x} {Dog}(x) \implies \lnot {Bites}(x,{Child}({Owner}(x)))$.

    2.  $\lnot \Exi{x,y} {Dog}(x) \land {Child}(y,{Owner}(x)) \land {Bites}(x,y)$.

    3.  $\All{x} {Dog}(x) \implies (\All{y} {Child}(y,{Owner}(x)) \implies \lnot {Bites}(x,y))$.

    4.  $\lnot \Exi{x} {Dog}(x) \implies (\Exi{y} {Child}(y,{Owner}(x)) \land {Bites}(x,y))$.

4.  Everyone’s zip code within a state has the same first digit.

    1.  $\All{x,s,z_1} [{State}(s) \land {LivesIn}(x,s) \land {Zip}(x)\eq z_1] \implies {}$\
        $[\All{y,z_2} {LivesIn}(y,s) \land {Zip}(y)\eq z_2 \implies {Digit}(1,z_1) \eq {Digit}(1,z_2) ]$.

    2.  $\All{x,s} [{State}(s) \land {LivesIn}(x,s) \land \Exi{z_1} {Zip}(x)\eq z_1] \implies{}$\
        $ [\All{y,z_2} {LivesIn}(y,s) \land {Zip}(y)\eq z_2 \land {Digit}(1,z_1) \eq {Digit}(1,z_2) ]$.

    3.  $\All{x,y,s} {State}(s) \land {LivesIn}(x,s) \land {LivesIn}(y,s) \implies {Digit}(1,{Zip}(x)\eq {Zip}(y))$.

    4.  $\All{x,y,s} {State}(s) \land {LivesIn}(x,s) \land {LivesIn}(y,s) \implies {}$\
        ${Digit}(1,{Zip}(x)) \eq {Digit}(1,{Zip}(y))$.

[language-determination-exercise] Complete the following exercises about
logical senntences:

1.  Translate into *good, natural* English (no $x$s or
    $y$s!):

    (x,l) (y,l)\
    (x,y) (y,x).

2.  Explain why this sentence is entailed by the sentence

    (x,l) (y,l)\
    (x,y).

3.  Translate into first-order logic the following sentences:

    1.  Understanding leads to friendship.

    2.  Friendship is transitive.

    Remember to define all predicates, functions, and constants you use.

True or false? Explain.

1.  $\Exi{x} x\eq {Rumpelstiltskin}$ is a valid (necessarily true)
    sentence of first-order logic.

2.  Every existentially quantified sentence in first-order logic is true
    in any model that contains exactly one object.

3.  $\All{x,y} x\eq y$is satisfiable.

[Peano-completion-exercise] Rewrite the first two Peano axioms in as a
single axiom that defines ${NatNum}(x)$ so as to exclude the
possibility of natural numbers except for those generated by the
successor function.

[wumpus-diagnostic-exercise] on defines the conditions under which a
square is breezy. Here we consider two other ways to describe this
aspect of the wumpus world.

1.  We can write leading from observed effects to hidden causes. For
    finding pits, the obvious diagnostic rules say that if a square is
    breezy, some adjacent square must contain a pit; and if a square is
    not breezy, then no adjacent square contains a pit. Write these two
    rules in first-order logic and show that their conjunction is
    logically equivalent to .

2.  We can write leading from cause to effect. One obvious causal rule
    is that a pit causes all adjacent squares to be breezy. Write this
    rule in first-order logic, explain why it is incomplete compared to
    , and supply the missing axiom.

[kinship-exercise]Write axioms describing the predicates
${Grandchild}$, ${Greatgrandparent}$, ${Ancestor}$, ${Brother}$,
${Sister}$, ${Daughter}$, ${Son}$, ${FirstCousin}$,
${BrotherInLaw}$, ${SisterInLaw}$, ${Aunt}$, and ${Uncle}$. Find
out the proper definition of $m$th cousin $n$ times removed, and write
the definition in first-order logic. Now write down the basic facts
depicted in the family tree in . Using a suitable logical reasoning
system, it all the sentences you have written down, and it who are
Elizabeth’s grandchildren, Diana’s brothers-in-law, Zara’s
great-grandparents, and Eugenie’s ancestors.

[family1-figure]

Write down a sentence asserting that + is a commutative function. Does
your sentence follow from the Peano axioms? If so, explain why; if not,
give a model in which the axioms are true and your sentence is false.

Explain what is wrong with the following proposed definition of the set
membership predicate $\elt$:

x {x|s}\
 x s x {y|s} .

[list-representation-exercise]Using the set axioms as examples, write
axioms for the list domain, including all the constants, functions, and
predicates mentioned in the chapter.

[adjacency-exercise]Explain what is wrong with the following proposed
definition of adjacent squares in the wumpus world:
$$\All{x,y} {Adjacent}([x,y], [x+1, y]) \land {Adjacent}([x,y], [x, y+1])\ .$$

Write out the axioms required for reasoning about the wumpus’s location,
using a constant symbol ${Wumpus}$ and a binary predicate
${At}({Wumpus}, {Location})$. Remember that there is only one
wumpus.

Assuming predicates ${Parent}(p,q)$ and ${Female}(p)$ and constants
${Joan}$ and ${Kevin}$, with the obvious meanings, express each of
the following sentences in first-order logic. (You may use the
abbreviation $\exists^{1}$ to mean “there exists exactly one.”)

1.  Joan has a daughter (possibly more than one, and possibly sons as
    well).

2.  Joan has exactly one daughter (but may have sons as well).

3.  Joan has exactly one child, a daughter.

4.  Joan and Kevin have exactly one child together.

5.  Joan has at least one child with Kevin, and no children with anyone
    else.

Arithmetic assertions can be written in first-order logic with the
predicate symbol $<$, the function symbols ${+}$ and ${\times}$, and the
constant symbols 0 and 1. Additional predicates can also be defined with
biconditionals.

1.  Represent the property “$x$ is an even number.”

2.  Represent the property “$x$ is prime.”

3.  Goldbach’s conjecture is the conjecture (unproven as yet) that every
    even number is equal to the sum of two primes. Represent this
    conjecture as a logical sentence.

In , we used equality to indicate the relation between a variable and
its value. For instance, we wrote ${WA}\eq {red}$ to mean that
Western Australia is colored red. Representing this in first-order
logic, we must write more verbosely ${ColorOf}({WA})\eq {red}$.
What incorrect inference could be drawn if we wrote sentences such as
${WA}\eq {red}$ directly as logical assertions?

Write in first-order logic the assertion that every key and at least one
of every pair of socks will eventually be lost forever, using only the
following vocabulary: ${Key}(x)$, $x$ is a key; ${Sock}(x)$, $x$ is
a sock; ${Pair}(x,y)$, $x$ and $y$ are a pair; ${Now}$, the current
time; ${Before}(t_1,t_2)$, time $t_1$ comes before time $t_2$;
${Lost}(x,t)$, object $x$ is lost at time $t$.

For each of the following sentences in English, decide if the
accompanying first-order logic sentence is a good translation. If not,
explain why not and correct it. (Some sentences may have more than one
error!)

1.  No two people have the same social security number.
    $$\lnot \Exi{x,y,n} {Person}(x) \land {Person}(y) \implies [{HasSS}\#(x,n) \land {HasSS}\#(y,n)].$$

2.  John’s social security number is the same as Mary’s.
    $$\Exi{n} {HasSS}\#({John},n) \land {HasSS}\#({Mary},n).$$

3.  Everyone’s social security number has nine digits.
    $$\All{x,n} {Person}(x) \implies [{HasSS}\#(x,n) \land {Digits}(n,9)].$$

4.  Rewrite each of the above (uncorrected) sentences using a function
    symbol ${SS}\#$ instead of the predicate ${HasSS}\#$.

Translate into first-order logic the sentence “Everyone’s DNA is unique
and is derived from their parents’ DNA.” You must specify the precise
intended meaning of your vocabulary terms. (*Hint*: Do not
use the predicate ${Unique}(x)$, since uniqueness is not really a
property of an object in itself!)

For each of the following sentences in English, decide if the
accompanying first-order logic sentence is a good translation. If not,
explain why not and correct it.

1.  Any apartment in London has lower rent than some apartments in
    Paris.

    [(x) (x,)] ([(y) (y,)]\
          ((x) \< (y)))  .

2.  There is exactly one apartment in Paris with rent below \$1000.

    ​(x) (x,)\
           [(y) (y,) ((y) \< (1000)) ](yx).

3.  If an apartment is more expensive than all apartments in London, it
    must be in Moscow.

    (x)\
           (x,).

Represent the following sentences in first-order logic, using a
consistent vocabulary (which you must define):

1.  Some students took French in spring 2001.

2.  Every student who takes French passes it.

3.  Only one student took Greek in spring 2001.

4.  The best score in Greek is always higher than the best score in
    French.

5.  Every person who buys a policy is smart.

6.  No person buys an expensive policy.

7.  There is an agent who sells policies only to people who are not
    insured.

8.  There is a barber who shaves all men in town who do not shave
    themselves.

9.  A person born in the UK, each of whose parents is a UK citizen or a
    UK resident, is a UK citizen by birth.

10. A person born outside the UK, one of whose parents is a UK citizen
    by birth, is a UK citizen by descent.

11. Politicians can fool some of the people all of the time, and they
    can fool all of the people some of the time, but they can’t fool all
    of the people all of the time.

12. All Greeks speak the same language. (Use ${Speaks}(x,l)$ to mean
    that person $x$ speaks language $l$.)

Represent the following sentences in first-order logic, using a
consistent vocabulary (which you must define):

1.  Some students took French in spring 2009.

2.  Every student who takes French passes it.

3.  Only one student took Greek in spring 2009.

4.  The best score in Greek is always lower than the best score in
    French.

5.  Every person who buys a policy is smart.

6.  There is an agent who sells policies only to people who are not
    insured.

7.  There is a barber who shaves all men in town who do not shave
    themselves.

8.  A person born in the UK, each of whose parents is a UK citizen or a
    UK resident, is a UK citizen by birth.

9.  A person born outside the UK, one of whose parents is a UK citizen
    by birth, is a UK citizen by descent.

10. Politicians can fool some of the people all of the time, and they
    can fool all of the people some of the time, but they can’t fool all
    of the people all of the time.

11. All Greeks speak the same language. (Use ${Speaks}(x,l)$ to mean
    that person $x$ speaks language $l$.)

Write a general set of facts and axioms to represent the assertion
“Wellington heard about Napoleon’s death” and to correctly answer the
question “Did Napoleon hear about Wellington’s death?”

[4bit-adder-exercise]Extend the vocabulary from to define addition for
$n$-bit binary numbers. Then encode the description of the four-bit
adder in , and pose the queries needed to verify that it is in fact
correct.

[4bit-adder-figure]

The circuit representation in the chapter is more detailed than
necessary if we care only about circuit functionality. A simpler
formulation describes any $m$-input, $n$-output gate or circuit using a
predicate with $m+n$ arguments, such that the predicate is true exactly
when the inputs and outputs are consistent. For example, NOT gates are
described by the binary predicate ${NOT}(i,o)$, for which
${NOT}(0,1)$ and ${NOT}(1,0)$ are known. Compositions of gates are
defined by conjunctions of gate predicates in which shared variables
indicate direct connections. For example, a NAND circuit can be composed
from ${AND}$s and ${NOT}$s:
$$\All{i_1,i_2,o_a,o} {AND}(i_1,i_2,o_a) \land {NOT}(o_a,o) \implies {NAND}(i_1,i_2,o)\ .$$
Using this representation, define the one-bit adder in and the four-bit
adder in , and explain what queries you would use to verify the designs.
What kinds of queries are *not* supported by this
representation that *are* supported by the representation
in ?

Obtain a passport application for your country, identify the rules
determining eligibility for a passport, and translate them into
first-order logic, following the steps outlined in .

Consider a first-order logical knowledge base that describes worlds
containing people, songs, albums (e.g., “Meet the Beatles”) and disks
(i.e., particular physical instances of CDs). The vocabulary contains
the following symbols:

> ${CopyOf}(d,a)$: Predicate. Disk $d$ is a copy of album $a$.\
> ${Owns}(p,d)$: Predicate. Person $p$ owns disk $d$.\
> ${Sings}(p,s,a)$: Album $a$ includes a recording of song $s$ sung by
> person $p$.\
> ${Wrote}(p,s)$: Person $p$ wrote song $s$.\
> ${McCartney}$, ${Gershwin}$, ${BHoliday}$, ${Joe}$,
> ${EleanorRigby}$, ${TheManILove}$, ${Revolver}$: Constants with
> the obvious meanings.

Express the following statements in first-order logic:

1.  Gershwin wrote “The Man I Love.”

2.  Gershwin did not write “Eleanor Rigby.”

3.  Either Gershwin or McCartney wrote “The Man I Love.”

4.  Joe has written at least one song.

5.  Joe owns a copy of *Revolver*.

6.  Every song that McCartney sings on *Revolver* was
    written by McCartney.

7.  Gershwin did not write any of the songs on *Revolver*.

8.  Every song that Gershwin wrote has been recorded on some album.
    (Possibly different songs are recorded on different albums.)

9.  There is a single album that contains every song that Joe has
    written.

10. Joe owns a copy of an album that has Billie Holiday singing “The Man
    I Love.”

11. Joe owns a copy of every album that has a song sung by McCartney.
    (Of course, each different album is instantiated in a different
    physical CD.)

12. Joe owns a copy of every album on which all the songs are sung by
    Billie Holiday.

[^1]: Also called , sometimes abbreviated as or .

[^2]: In contrast, facts in have a between 0 and 1. For example, the
    sentence “Vienna is a large city” might be true in our world only to
    degree 0.6 in fuzzy logic.

[^3]: It is important not to confuse the degree of belief in probability
    theory with the degree of truth in fuzzy logic. Indeed, some fuzzy
    systems allow uncertainty (degree of belief) about degrees of truth.

[^4]: Later, in , we examine a semantics in which every object has
    exactly one name.

[^5]: provide a useful notation in which new function symbols are
    constructed “on the fly.” For example, the function that squares its
    argument can be written as $(\Lam{x} x
    \stimes x)$ and can be applied to arguments just like any other
    function symbol. A $\lambda$-expression can also be defined and used
    as a predicate symbol. (See .) The lambda operator in
    Lisp plays exactly the same role.[lambda-abstraction-page] Notice
    that the use of $\lambda$ in this way does *not*
    increase the formal expressive power of first-order logic, because
    any sentence that includes a $\lambda$-expression can be rewritten
    by “plugging in” its arguments to yield an equivalent sentence.

[^6]: We usually follow the argument-ordering convention that $P(x,y)$
    is read as “$x$ is a $P$ of $y$.”

[^7]: There is a variant of the existential quantifier, usually written
    $\exists^1$ or ${\exists}!$, that means “There exists exactly one.”
    The same meaning can be expressed using equality statements.

[^8]: Actually he had four, the others being William and Henry.

[^9]: The Peano axioms also include the principle of induction, which is
    a sentence of second-order logic rather than of first-order logic.
    The importance of this distinction is explained in .

[^10]: Similarly, most of us do not name each bird that flies overhead
    as it migrates to warmer regions in winter. An ornithologist wishing
    to study migration patterns, survival rates, and so on
    *does* name each bird, by means of a ring on its leg,
    because individual birds must be tracked.
AI: The Present and Future {#future-chapter}
==========================

In , we suggested that it would be helpful to view the AI task as that
of designing rational agents—that is, agents whose actions maximize
their expected utility given their percept histories. We showed that the
design problem depends on the percepts and actions available to the
agent, the utility function that the agent’s behavior should satisfy,
and the nature of the environment. A variety of different agent designs
are possible, ranging from reflex agents to fully deliberative,
knowledge-based, decision-theoretic agents. Moreover, the components of
these designs can have a number of different instantiations—for example,
logical or probabilistic reasoning, and atomic, factored, or structured
representations of states. The intervening chapters presented the
principles by which these components operate.

For all the agent designs and components, there has been tremendous
progress both in our scientific understanding and in our technological
capabilities. In this chapter, we stand back from the details and ask,

“Will all this progress lead to a general-purpose intelligent agent that
can perform well in a wide variety of environments?”

looks at the components of an intelligent agent to assess what’s known
and what’s missing. does the same for the overall agent architecture.
asks whether designing rational agents is the right goal in the first
place. (The answer is, “Not really, but it’s OK for now.”) Finally,
examines the consequences of success in our endeavors.

Agent Components {#future-pieces-section}
----------------

[utility-based-agent-again-figure]

presented several agent designs and their components. To focus our
discussion here, we will look at the , which we show again in . When
endowed with a learning component (), this is the most general of our
agent designs. Let’s see where the state of the art stands for each of
the components.

Interaction with the environment through sensors and
actuators: For much of the history of AI, this has been a glaring
weak point. With a few honorable exceptions, AI systems were built in
such a way that humans had to supply the inputs and interpret the
outputs, while robotic systems focused on low-level tasks in which
high-level reasoning and planning were largely absent. This was due in
part to the great expense and engineering effort required to get real
robots to work at all. The situation has changed rapidly in recent years
with the availability of ready-made programmable robots. These, in turn,
have benefited from small, cheap, high-resolution CCD cameras and
compact, reliable motor drives. MEMS (micro-electromechanical systems)
technology has supplied miniaturized accelerometers, gyroscopes, and
actuators for an artificial flying insect @Floreano+al:2009. It may also
be possible to combine millions of MEMS devices to produce powerful
macroscopic actuators.

Thus, we see that AI systems are at the cusp of moving from primarily
software-only systems to embedded robotic systems. The state of robotics
today is roughly comparable to the state of personal computers in about
1980: at that time researchers and hobbyists could experiment with PCs,
but it would take another decade before they became commonplace.

Keeping track of the state of the world: This is one of the
core capabilities required for an intelligent agent. It requires both
perception and updating of internal representations. showed how to keep
track of atomic state representations; described how to do it for
factored (propositional) state representations; extended this to
first-order logic; and described algorithms for probabilistic reasoning
in uncertain environments. Current filtering and perception algorithms
can be combined to do a reasonable job of reporting low-level predicates
such as “the cup is on the table.” Detecting higher-level actions, such
as “Dr. Russell is having a cup of tea with Dr. Norvig while discussing
plans for next week,” is more difficult. Currently it can be done (see
on ) only with the help of annotated examples.

Another problem is that, although the approximate filtering algorithms
from can handle quite large environments, they are still dealing with a
factored representation—they have random variables, but do not represent
objects and relations explicitly. explained how probability and
first-order logic can be combined to solve this problem, and showed how
we can handle uncertainty about the identity of objects. We expect that
the application of these ideas for tracking complex environments will
yield huge benefits. However, we are still faced with a daunting task of
defining general, reusable representation schemes for complex domains.
As discussed in , we don’t yet know how to do that in general; only for
isolated, simple domains. It is possible that a new focus on
probabilistic rather than logical representation coupled with aggressive
machine learning (rather than hand-encoding of knowledge) will allow for
progress.

Projecting, evaluating, and selecting future courses of
action: The basic knowledge-representation requirements here are
the same as for keeping track of the world; the primary difficulty is
coping with courses of action—such as having a conversation or a cup of
tea—that consist eventually of thousands or millions of primitive steps
for a real agent. It is only by imposing on behavior that we humans cope
at all. We saw in how to use hierarchical representations to handle
problems of this scale; furthermore, work in has succeeded in combining
some of these ideas with the techniques for decision making under
uncertainty described in . As yet, algorithms for the partially
observable case (POMDPs) are using the same atomic state representation
we used for the search algorithms of . There is clearly a great deal of
work to do here, but the technical foundations are largely in place.
discusses the question of how the search for effective long-range plans
might be controlled.

Utility as an expression of preferences: In principle,
basing rational decisions on the maximization of expected utility is
completely general and avoids many of the problems of purely goal-based
approaches, such as conflicting goals and uncertain attainment. As yet,
however, there has been very little work on constructing
*realistic* utility functions—imagine, for example, the
complex web of interacting preferences that must be understood by an
agent operating as an office assistant for a human being. It has proven
very difficult to decompose preferences over complex states in the same
way that Bayes nets decompose beliefs over complex states. One reason
may be that preferences over states are really *compiled*
from preferences over state histories, which are described by (see ).
Even if the reward function is simple, the corresponding utility
function may be very complex. This suggests that we take seriously the
task of knowledge engineering for reward functions as a way of conveying
to our agents what it is that we want them to do.

Learning: Chapters [concept-learning-chapter]
to [reinforcement-learning-chapter] described how learning in an agent
can be formulated as inductive learning (supervised, unsupervised, or
reinforcement-based) of the functions that constitute the various
components of the agent. Very powerful logical and statistical
techniques have been developed that can cope with quite large problems,
reaching or exceeding human capabilities in many tasks—as long as we are
dealing with a predefined vocabulary of features and concepts. On the
other hand, machine learning has made very little progress on the
important problem of constructing new representations at levels of
abstraction higher than the input vocabulary. In computer vision, for
example, learning complex concepts such as ${Classroom}$ and
${Cafeteria}$ would be made unnecessarily difficult if the agent were
forced to work from pixels as the input representation; instead, the
agent needs to be able to form intermediate concepts first, such as
${Desk}$ and ${Tray}$, without explicit human supervision. Similar
considerations apply to learning behavior: ${HavingACupOfTea}$ is a
very important high-level step in many plans, but how does it get into
an action library that initially contains much simpler actions such as
${RaiseArm}$ and ${Swallow}$? Perhaps this will incorporate some of
the ideas of —Bayesian networks that have multiple layers of hidden
variables, as in the work of , , and .

The vast majority of machine learning research today assumes a factored
representation, learning a function $h : \mathbb{R}^n
\rightarrow \mathbb{R}$ for regression and $h : \mathbb{R}^n
\rightarrow \{0,1\}$ for classification. Learning researchers will need
to adapt their very successful techniques for factored representations
to structured representations, particularly hierarchical
representations. The work on inductive logic programming in is a first
step in this direction; the logical next step is to combine these ideas
with the probabilistic languages of .

Unless we understand such issues, we are faced with the daunting task of
constructing large commonsense knowledge bases by hand, an approach that
has not fared well to date. There is great promise in using the Web as a
source of natural language text, images, and videos to serve as a
comprehensive knowledge base, but so far machine learning algorithms are
limited in the amount of organized knowledge they can extract from these
sources.

Agent Architectures {#future-architecture-section}
-------------------

It is natural to ask, “Which of the agent architectures in should an
agent use?” The answer is, “All of them!” We have seen that reflex
responses are needed for situations in which time is of the essence,
whereas knowledge-based deliberation allows the agent to plan ahead. A
complete agent must be able to do both, using a . One important property
of hybrid architectures is that the boundaries between different
decision components are not fixed. For example, continually converts
declarative information at the deliberative level into more efficient
representations, eventually reaching the reflex level—see . (This is the
purpose of explanation-based learning, as discussed in .) Agent
architectures such as  @Laird+al:1987 and  @Mitchell:1990 have exactly
this structure. Every time they solve a problem by explicit
deliberation, they save away a generalized version of the solution for
use by the reflex component. A less studied problem is the
*reversal* of this process: when the environment changes,
learned reflexes may no longer be appropriate and the agent must return
to the deliberative level to produce new behaviors.

[compilation-figure]

Agents also need ways to control their own deliberations. They must be
able to cease deliberating when action is demanded, and they must be
able to use the time available for deliberation to execute the most
profitable computations. For example, a taxi-driving agent that sees an
accident ahead must decide in a split second either to brake or to take
evasive action. It should also spend that split second thinking about
the most important questions, such as whether the lanes to the left and
right are clear and whether there is a large truck close behind, rather
than worrying about wear and tear on the tires or where to pick up the
next passenger. These issues are usually studied under the heading of .
As AI systems move into more complex domains, all problems will become
real-time, because the agent will never have long enough to solve the
decision problem exactly.

Clearly, there is a pressing need for *general* methods of
controlling deliberation, rather than specific recipes for what to think
about in each situation. The first useful idea is to employ
 @Dean+Boddy:1988 [@Horvitz:1987a]. An anytime algorithm is an algorithm
whose output quality improves gradually over time, so that it has a
reasonable decision ready whenever it is interrupted. Such algorithms
are controlled by a decision procedure that assesses whether further
computation is worthwhile. (See for a brief description of metalevel
decision making.) Example of an anytime algorithms include iterative
deepening in game-tree search and MCMC in Bayesian networks.

The second technique for controlling deliberation is
@Russell+Wefald:1989
[@Russell+Wefald:1991; @Horvitz:1989b; @Horvitz+Breese:1996]. This
method applies the theory of information value () to the selection of
individual computations. The value of a computation depends on both its
cost (in terms of delaying action) and its benefits (in terms of
improved decision quality). Metareasoning techniques can be used to
design better search algorithms and to guarantee that the algorithms
have the anytime property. Metareasoning is expensive, of course, and
compilation methods can be applied so that the overhead is small
compared to the costs of the computations being controlled. Metalevel
reinforcement learning may provide another way to acquire effective
policies for controlling deliberation: in essence, computations that
lead to better decisions are reinforced, while those that turn out to
have no effect are penalized. This approach avoids the myopia problems
of the simple value-of-information calculation.

Metareasoning is one specific example of a —that is, an architecture
that enables deliberation about the computational entities and actions
occurring within the architecture itself. A theoretical foundation for
reflective architectures can be built by defining a joint state space
composed from the environment state and the computational state of the
agent itself. Decision-making and learning algorithms can be designed
that operate over this joint state space and thereby serve to implement
and improve the agent’s computational activities. Eventually, we expect
task-specific algorithms such as alpha–beta search and backward chaining
to disappear from AI systems, to be replaced by general methods that
direct the agent’s computations toward the efficient generation of
high-quality decisions.

Are We Going in the Right Direction? {#bo-section}
------------------------------------

The preceding section listed many advances and many opportunities for
further progress. But where is this all leading? gives the analogy of
trying to get to the moon by climbing a tree; one can report steady
progress, all the way to the top of the tree. In this section, we
consider whether AI’s current path is more like a tree climb or a rocket
trip.

In , we said that our goal was to build agents that *act
rationally*. However, we also said that

$\ldots$ achieving perfect rationality—always doing the right thing—is
not feasible in complicated environments. The computational demands are
just too high. For most of the book, however, we will adopt the working
hypothesis that perfect rationality is a good starting point for
analysis.

Now it is time to consider again what exactly the goal of AI is. We want
to build agents, but with what specification in mind? Here are four
possibilities:

. A perfectly rational agent acts at every instant in such a way as to
maximize its expected utility, given the information it has acquired
from the environment. We have seen that the calculations necessary to
achieve perfect rationality in most environments are too time consuming,
so perfect rationality is not a realistic goal.

. This is the notion of rationality that we have used implicitly in
designing logical and decision-theoretic agents, and most of theoretical
AI research has focused on this property. A calculatively rational agent
*eventually* returns what *would have been*
the rational choice at the beginning of its deliberation. This is an
interesting property for a system to exhibit, but in most environments,
the right answer at the wrong time is of no value. In practice, AI
system designers are forced to compromise on decision quality to obtain
reasonable overall performance; unfortunately, the theoretical basis of
calculative rationality does not provide a well-founded way to make such
compromises.

. Herbert Simon [-@Simon:1957] rejected the notion of perfect (or even
approximately perfect) rationality and replaced it with bounded
rationality, a descriptive theory of decision making by real agents. He
wrote,

The capacity of the human mind for formulating and solving complex
problems is very small compared with the size of the problems whose
solution is required for objectively rational behavior in the real
world—or even for a reasonable approximation to such objective
rationality.

He suggested that bounded rationality works primarily by —that is,
deliberating only long enough to come up with an answer that is “good
enough.” Simon won the Nobel Prize in economics for this work and has
written about it in depth @Simon:1982. It appears to be a useful model
of human behaviors in many cases. It is not a formal specification for
intelligent agents, however, because the definition of “good enough” is
not given by the theory. Furthermore, satisficing seems to be just one
of a large range of methods used to cope with bounded resources.

(BO). A bounded optimal agent behaves as well as possible, *given
its computational resources*. That is, the expected utility of
the agent program for a bounded optimal agent is at least as high as the
expected utility of any other agent program running on the same machine.

Of these four possibilities, bounded optimality seems to offer the best
hope for a strong theoretical foundation for AI. It has the advantage of
being possible to achieve: there is always at least one best
program—something that perfect rationality lacks. Bounded optimal agents
are actually useful in the real world, whereas calculatively rational
agents usually are not, and satisficing agents might or might not be,
depending on how ambitious they are.

The traditional approach in AI has been to start with calculative
rationality and then make compromises to meet resource constraints. If
the problems imposed by the constraints are minor, one would expect the
final design to be similar to a BO agent design. But as the resource
constraints become more critical—for example, as the environment becomes
more complex—one would expect the two designs to diverge. In the theory
of bounded optimality, these constraints can be handled in a principled
fashion.

As yet, little is known about bounded optimality. It is possible to
construct bounded optimal programs for very simple machines and for
somewhat restricted kinds of environments @Etzioni:1989
[@Russell+Subramanian:1993], but as yet we have no idea what BO programs
are like for large, general-purpose computers in complex environments.
If there is to be a constructive theory of bounded optimality, we have
to hope that the design of bounded optimal programs does not depend too
strongly on the details of the computer being used. It would make
scientific research very difficult if adding a few kilobytes of memory
to a gigabyte machine made a significant difference to the design of the
BO program. One way to make sure this cannot happen is to be slightly
more relaxed about the criteria for bounded optimality. By analogy with
the notion of asymptotic complexity (), we can define (ABO) as
follows @Russell+Subramanian:1995. Suppose a program $P$ is bounded
optimal for a machine $M$ in a class of environments $\mbf{E}$, where
the complexity of environments in $\mbf{E}$ is unbounded. Then program
$P'$ is ABO for $M$ in $\mbf{E}$ if it can outperform $P$ by running on
a machine $kM$ that is $k$ times faster (or larger) than $M$. Unless $k$
were enormous, we would be happy with a program that was ABO for a
nontrivial environment on a nontrivial architecture. There would be
little point in putting enormous effort into finding BO rather than ABO
programs, because the size and speed of available machines tends to
increase by a constant factor in a fixed amount of time anyway.

We can hazard a guess that BO or ABO programs for powerful computers in
complex environments will not necessarily have a simple, elegant
structure. We have already seen that general-purpose intelligence
requires some reflex capability and some deliberative capability; a
variety of forms of knowledge and decision making; learning and
compilation mechanisms for all of those forms; methods for controlling
reasoning; and a large store of domain-specific knowledge. A bounded
optimal agent must adapt to the environment in which it finds itself, so
that eventually its internal organization will reflect optimizations
that are specific to the particular environment. This is only to be
expected, and it is similar to the way in which racing cars restricted
by engine capacity have evolved into extremely complex designs. We
suspect that a science of artificial intelligence based on bounded
optimality will involve a good deal of study of the processes that allow
an agent program to converge to bounded optimality and perhaps less
concentration on the details of the messy programs that result.

In sum, the concept of bounded optimality is proposed as a formal task
for AI research that is both well defined and feasible. Bounded
optimality specifies optimal *programs* rather than optimal
*actions*. Actions are, after all, generated by programs,
and it is over programs that designers have control.

What If AI Does Succeed? {#future-conclusion-section}
------------------------

In David Lodge’s *Small World* [-@Lodge:1984], a novel
about the academic world of literary criticism, the protagonist causes
consternation by asking a panel of eminent but contradictory literary
theorists the following question: “*What if you were
right?*” None of the theorists seems to have considered this
question before, perhaps because debating unfalsifiable theories is an
end in itself. Similar confusion can be evoked by asking AI researchers,
“*What if you succeed?*”

As relates, there are ethical issues to consider. Intelligent computers
are more powerful than dumb ones, but will that power be used for good
or ill? Those who strive to develop AI have a responsibility to see that
the impact of their work is a positive one. The scope of the impact will
depend on the degree of success of AI. Even modest successes in AI have
already changed the ways in which computer science is taught @Stein:2002
and software development is practiced. AI has made possible new
applications such as speech recognition systems, inventory control
systems, surveillance systems, robots, and search engines.

We can expect that medium-level successes in AI would affect all kinds
of people in their daily lives. So far, computerized communication
networks, such as cell phones and the Internet, have had this kind of
pervasive effect on society, but AI has not. AI has been at work behind
the scenes—for example, in automatically approving or denying credit
card transactions for every purchase made on the Web—but has not been
visible to the average consumer. We can imagine that truly useful
personal assistants for the office or the home would have a large
positive impact on people’s lives, although they might cause some
economic dislocation in the short term. Automated assistants for driving
could prevent accidents, saving tens of thousands of lives per year. A
technological capability at this level might also be applied to the
development of autonomous weapons, which many view as undesirable. Some
of the biggest societal problems we face today—such as the harnessing of
genomic information for treating disease, the efficient management of
energy resources, and the verification of treaties concerning nuclear
weapons—are being addressed with the help of AI technologies.

Finally, it seems likely that a large-scale success in AI—the creation
of human-level intelligence and beyond—would change the lives of a
majority of humankind. The very nature of our work and play would be
altered, as would our view of intelligence, consciousness, and the
future destiny of the human race. AI systems at this level of capability
could threaten human autonomy, freedom, and even survival. For these
reasons, we cannot divorce AI research from its ethical consequences
(see ).

Which way will the future go? Science fiction authors seem to favor
dystopian futures over utopian ones, probably because they make for more
interesting plots. But so far, AI seems to fit in with other
revolutionary technologies (printing, plumbing, air travel, telephony)
whose negative repercussions are outweighed by their positive aspects.

In conclusion, we see that AI has made great progress in its short
history, but the final sentence of Alan Turing’s [-@Turing:1950] essay
on *Computing Machinery and Intelligence* is still valid
today:

*We can see only a short distance ahead,\
but we can see that much remains to be done. *
Adversarial Search {#game-playing-chapter}
==================

Games
-----

[games-intro-section]

introduced , in which each agent needs to consider the actions of other
agents and how they affect its own welfare. The unpredictability of
these other agents can introduce into the agent’s problem-solving
process, as discussed in . In this chapter we cover environments, in
which the agents’ goals are in conflict, giving rise to problems—often
known as .

Mathematical , a branch of economics, views any multiagent environment
as a game, provided that the impact of each agent on the others is
“significant,” regardless of whether the agents are cooperative or
competitive.[^1] In AI, the most common games are of a rather
specialized kind—what game theorists call deterministic, turn-taking,
two-player, of (such as chess). In our terminology, this means
deterministic, fully observable environments in which two agents act
alternately and in which the utility values at the end of the game are
always equal and opposite. For example, if one player wins a game of
chess, the other player necessarily loses. It is this opposition between
the agents’ utility functions that makes the situation adversarial.

Games have engaged the intellectual faculties of humans—sometimes to an
alarming degree—for as long as civilization has existed. For AI
researchers, the abstract nature of games makes them an appealing
subject for study. The state of a game is easy to represent, and agents
are usually restricted to a small number of actions whose outcomes are
defined by precise rules. Physical games, such as croquet and ice
hockey, have much more complicated descriptions, a much larger range of
possible actions, and rather imprecise rules defining the legality of
actions. With the exception of robot soccer, these physical games have
not attracted much interest in the AI community.

Games, unlike most of the toy problems studied in , are interesting
*because* they are too hard to solve. For example, chess
has an average branching factor of about 35, and games often go to 50
moves by each player, so the search tree has about ${35}^{{100}}$ or
${10}^{{154}}$ nodes (although the search graph has “only” about
${10}^{{40}}$ distinct nodes). Games, like the real world, therefore
require the ability to make *some* decision even when
calculating the *optimal* decision is infeasible. Games
also penalize inefficiency severely. Whereas an implementation of A
search that is half as efficient will simply take twice as long to run
to completion, a chess program that is half as efficient in using its
available time probably will be beaten into the ground, other things
being equal. Game-playing research has therefore spawned a number of
interesting ideas on how to make the best possible use of time.

We begin with a definition of the optimal move and an algorithm for
finding it. We then look at techniques for choosing a good move when
time is limited. allows us to ignore portions of the search tree that
make no difference to the final choice, and heuristic allow us to
approximate the true utility of a state without doing a complete search.
discusses games such as backgammon that include an element of chance; we
also discuss bridge, which includes elements of because not all cards
are visible to each player. Finally, we look at how state-of-the-art
game-playing programs fare against human opposition and at directions
for future developments.

We first consider games with two players, whom we call max
and min for reasons that will soon become obvious.
max moves first, and then they take turns moving until the
game is over. At the end of the game, points are awarded to the winning
player and penalties are given to the loser. A game can be formally
defined as a kind of search problem with the following elements:

-   $S_0$: The , which specifies how the game is set up at the start.

-   $(s)$: Defines which player has the move in a state.

-   $(s)$: Returns the set of legal moves in a state.

-   : The , which defines the result of a move.

-   $(s)$: A , which is true when the game is over and false otherwise.
    States where the game has ended are called .

-   $(s, p)$: A (also called an objective function or payoff function),
    defines the final numeric value for a game that ends in terminal
    state $s$ for a player $p$. In chess, the outcome is a win, loss, or
    draw, with values $+1$, 0, or $\frac{1}{2}$. Some games have a wider
    variety of possible outcomes; the payoffs in backgammon range from 0
    to $+{192}$. A [zero-sum-game-page] is (confusingly) defined as one
    where the total payoff to all players is the same for every instance
    of the game. Chess is zero-sum because every game has payoff of
    either $0+1$, $1+0$ or $\frac{1}{2}+\frac{1}{2}$. “Constant-sum”
    would have been a better term, but zero-sum is traditional and makes
    sense if you imagine each player is charged an entry fee of
    $\frac{1}{2}$.

The initial state, function, and function define the for the game—a tree
where the nodes are game states and the edges are moves. shows part of
the game tree for tic-tac-toe (noughts and crosses). From the initial
state, max has nine possible moves. Play alternates between
max’s placing an x and min’s
placing an o until we reach leaf nodes corresponding to
terminal states such that one player has three in a row or all the
squares are filled. The number on each leaf node indicates the utility
value of the terminal state from the point of view of max;
high values are assumed to be good for max and bad for
min (which is how the players get their names).

For tic-tac-toe the game tree is relatively small—fewer than $9! =
362,880$ terminal nodes. But for chess there are over $10^{40}$ nodes,
so the game tree is best thought of as a theoretical construct that we
cannot realize in the physical world. But regardless of the size of the
game tree, it is max’s job to search for a good move. We
use the term for a tree that is superimposed on the full game tree, and
examines enough nodes to allow a player to determine what move to make.

[tictactoe-figure]

[minimax-figure]

Optimal Decisions in Games {#two-player-section}
--------------------------

In a normal search problem, the optimal solution would be a sequence of
actions leading to a goal state—a terminal state that is a win. In
adversarial search, min has something to say about it.
max therefore must find a contingent , which specifies
max’s move in the initial state, then max’s
moves in the states resulting from every possible response by
min, then max’s moves in the states resulting
from every possible response by min to *those*
moves, and so on. This is exactly analogous to the and–or
search algorithm () with max playing the role of
or and min equivalent to and.
Roughly speaking, an optimal strategy leads to outcomes at least as good
as any other strategy when one is playing an infallible opponent. We
begin by showing how to find this optimal strategy.

Even a simple game like tic-tac-toe is too complex for us to draw the
entire game tree on one page, so we will switch to the trivial game in .
The possible moves for max at the root node are labeled
$a_1$, $a_2$, and $a_3$. The possible replies to $a_1$ for
min are $b_{1}$, $b_{2}$, $b_{3}$, and so on. This
particular game ends after one move each by max and
min. (In game parlance, we say that this tree is one move
deep, consisting of two half-moves, each of which is called a .) The
utilities of the terminal states in this game range from 2 to 14.

Given a game tree, the optimal strategy can be determined from the of
each node, which we write as $\noprog{Minimax}(n)$. The minimax value of
a node is the utility (for max) of being in the
corresponding state, *assuming that both players play
optimally* from there to the end of the game. Obviously, the
minimax value of a terminal state is just its utility. Furthermore,
given a choice, Max prefers to move to a state of maximum
value, whereas Min prefers a state of minimum value. So we
have the following:

$$\begin{aligned}
   \lefteqn{\prog{Minimax}(s) =}\\
   &&\left\{\begin{array}{ll}
       \noprog{Utility}(s) 
             & \mbox{ if \noprog{Terminal-Test}\((s)\)} \\
       \max_{a\in {Actions}(s)} \noprog{Minimax}(\result{s}{a}) 
             & \mbox{ if \noprog{Player}\((s) = \) {\sc max}} \\
       \min_{a\in {Actions}(s)} \noprog{Minimax}(\result{s}{a}) 
             & \mbox{ if \noprog{Player}\((s) = \) {\sc min}} \end{array}\right.\end{aligned}$$

Let us apply these definitions to the game tree in . The terminal nodes
on the bottom level get their utility values from the game’s function.
The first min node, labeled $B$, has three successor states
with values 3, 12, and 8, so its minimax value is 3. Similarly, the
other two min nodes have minimax value 2. The root node is
a max node; its successor states have minimax values 3, 2,
and 2; so it has a minimax value of 3. We can also identify the at the
root: action $a_1$ is the optimal choice for max because it
leads to the state with the highest minimax value.

This definition of optimal play for max assumes that
min also plays optimally—it maximizes the
*worst-case* outcome for max. What if
min does not play optimally? Then it is easy to show ()
that max will do even better. Other strategies against
suboptimal opponents may do better than the minimax strategy, but these
strategies necessarily do worse against optimal opponents.

### The minimax algorithm

The () computes the minimax decision from the current state. It uses a
simple recursive computation of the minimax values of each successor
state, directly implementing the defining equations. The recursion
proceeds all the way down to the leaves of the tree, and then the
minimax values are through the tree as the recursion unwinds. For
example, in , the algorithm first recurses down to the three bottom-left
nodes and uses the function on them to discover that their values are 3,
12, and 8, respectively. Then it takes the minimum of these values, 3,
and returns it as the backed-up value of node $B$. A similar process
gives the backed-up values of 2 for $C$ and 2 for $D$. Finally, we take
the maximum of 3, 2, and 2 to get the backed-up value of 3 for the root
node.

The minimax algorithm performs a complete depth-first exploration of the
game tree. If the maximum depth of the tree is $m$ and there are $b$
legal moves at each point, then the time complexity of the minimax
algorithm is $O(b\,^m)$. The space complexity is $O(bm)$ for an
algorithm that generates all actions at once, or $O(m)$ for an algorithm
that generates actions one at a time (see ). For real games, of course,
the time cost is totally impractical, but this algorithm serves as the
basis for the mathematical analysis of games and for more practical
algorithms.

[minimax-algorithm]

### Optimal decisions in multiplayer games

Many popular games allow more than two players. Let us examine how to
extend the minimax idea to multiplayer games. This is straightforward
from the technical viewpoint, but raises some interesting new conceptual
issues.

First, we need to replace the single value for each node with a
*vector* of values. For example, in a three-player game
with players $A$, $B$, and $C$, a vector $\<v_A,v_B,v_C\>$ is associated
with each node. For terminal states, this vector gives the utility of
the state from each player’s viewpoint. (In two-player, zero-sum games,
the two-element vector can be reduced to a single value because the
values are always opposite.) The simplest way to implement this is to
have the function return a vector of utilities.

Now we have to consider nonterminal states. Consider the node marked $X$
in the game tree shown in . In that state, player $C$ chooses what to
do. The two choices lead to terminal states with utility vectors
$\<v_A\eq 1,v_B\eq 2,v_C\eq 6\>$ and $\<v_A\eq 4,v_B\eq 2,v_C\eq 3\>$.
Since 6 is bigger than 3, $C$ should choose the first move. This means
that if state $X$ is reached, subsequent play will lead to a terminal
state with utilities $\<v_A\eq
1,v_B\eq 2,v_C\eq 6\>$. Hence, the backed-up value of $X$ is this
vector. The backed-up value of a node $n$ is always the utility vector
of the successor state with the highest value for the player choosing at
$n$.

[minimax3-figure]

Anyone who plays multiplayer games, such as , quickly becomes aware that
much more is going on than in two-player games. Multiplayer games
usually involve , whether formal or informal, among the players.
Alliances are made and broken as the game proceeds. How are we to
understand such behavior? Are alliances a natural consequence of optimal
strategies for each player in a multiplayer game? It turns out that they
can be. For example, suppose $A$ and $B$ are in weak positions and $C$
is in a stronger position. Then it is often optimal for both $A$ and $B$
to attack $C$ rather than each other, lest $C$ destroy each of them
individually. In this way, collaboration emerges from purely selfish
behavior. Of course, as soon as $C$ weakens under the joint onslaught,
the alliance loses its value, and either $A$ or $B$ could violate the
agreement. In some cases, explicit alliances merely make concrete what
would have happened anyway. In other cases, a social stigma attaches to
breaking an alliance, so players must balance the immediate advantage of
breaking an alliance against the long-term disadvantage of being
perceived as untrustworthy. See for more on these complications.

If the game is not zero-sum, then collaboration can also occur with just
two players. Suppose, for example, that there is a terminal state with
utilities $\<v_A\eq {1000},v_B\eq {1000}\>$ and that 1000 is the highest
possible utility for each player. Then the optimal strategy is for both
players to do everything possible to reach this state—that is, the
players will automatically cooperate to achieve a mutually desirable
goal.

Alpha–Beta Pruning
------------------

The problem with minimax search is that the number of game states it has
to examine is exponential in the depth of the tree. Unfortunately, we
can’t eliminate the exponent, but it turns out we can effectively cut it
in half. The trick is that it is possible to compute the correct minimax
decision without looking at every node in the game tree. That is, we can
borrow the idea of from to eliminate large parts of the tree from
consideration. The particular technique we examine is called . When
applied to a standard minimax tree, it returns the same move as minimax
would, but prunes away branches that cannot possibly influence the final
decision.

[alpha-beta-progress-figure]

Consider again the two-ply game tree from . Let’s go through the
calculation of the optimal decision once more, this time paying careful
attention to what we know at each point in the process. The steps are
explained in . The outcome is that we can identify the minimax decision
without ever evaluating two of the leaf nodes.

Another way to look at this is as a simplification of the formula for .
Let the two unevaluated successors of node $C$ in have values $x$ and
$y$. Then the value of the root node is given by

$$\begin{aligned}
 \noprog{Minimax}({root}) &=& \max(\min(3, {12}, 8), \min(2, x, y),\min({14}, 5, 2)) \\
                              &=& \max(3, \min(2, x, y), 2)\\ 
                  &=& \max(3, z, 2) \qquad \mbox{~where~} z = \min(2, x, y) \le 2\\
                              &=&   {3.}\end{aligned}$$

In other words, the value of the root and hence the minimax decision are
*independent* of the values of the pruned leaves $x$ and
$y$.

Alpha–beta pruning can be applied to trees of any depth, and it is often
possible to prune entire subtrees rather than just leaves. The general
principle is this: consider a node $n$ somewhere in the tree (see ),
such that Player has a choice of moving to that node. If Player has a
better choice $m$ either at the parent node of $n$ or at any choice
point further up, then

$n$ will never be reached in actual play.

So once we have found out enough about $n$ (by examining some of its
descendants) to reach this conclusion, we can prune it.

[alpha-beta-general-figure]

Remember that minimax search is depth-first, so at any one time we just
have to consider the nodes along a single path in the tree. Alpha–beta
pruning gets its name from the following two parameters that describe
bounds on the backed-up values that appear anywhere along the path:

$\alpha =$
:   the value of the best (i.e., highest-value) choice we have found so
    far at any choice point along the path for max.

$\beta =$
:   the value of the best (i.e., lowest-value) choice we have found so
    far at any choice point along the path for min.

Alpha–beta search updates the values of $\alpha$ and $\beta$ as it goes
along and prunes the remaining branches at a node (i.e., terminates the
recursive call) as soon as the value of the current node is known to be
worse than the current $\alpha$ or $\beta$ value for max or
min, respectively. The complete algorithm is given in . We
encourage you to trace its behavior when applied to the tree in .

[alpha-beta-algorithm]

### Move ordering

The effectiveness of alpha–beta pruning is highly dependent on the order
in which the states are examined. For example, in (e) and (f), we could
not prune any successors of $D$ at all because the worst successors
(from the point of view of min) were generated first. If
the third successor of $D$ had been generated first, we would have been
able to prune the other two. This suggests that it might be worthwhile
to try to examine first the successors that are likely to be best.

If this can be done,[^2] then it turns out that alpha–beta needs to
examine only $O(b^{m/2})$ nodes to pick the best move, instead of
$O(b^m)$ for minimax. This means that the effective branching factor
becomes $\sqrt{b}$ instead of $b$—for chess, about 6 instead of 35. Put
another way, alpha–beta can solve a tree roughly twice as deep as
minimax in the same amount of time. If successors are examined in random
order rather than best-first, the total number of nodes examined will be
roughly $O(b^{3m/4})$ for moderate $b$. For chess, a fairly simple
ordering function (such as trying captures first, then threats, then
forward moves, and then backward moves) gets you to within about a
factor of 2 of the best-case $O(b^{m/2})$ result.

Adding dynamic move-ordering schemes, such as trying first the moves
that were found to be best in the past, brings us quite close to the
theoretical limit. The past could be the previous move—often the same
threats remain—or it could come from previous exploration of the current
move. One way to gain information from the current move is with
iterative deepening search. First, search 1 ply deep and record the best
path of moves. Then search 1 ply deeper, but use the recorded path to
inform move ordering. As we saw in , iterative deepening on an
exponential game tree adds only a constant fraction to the total search
time, which can be more than made up from better move ordering. The best
moves are often called and to try them first is called the killer move
heuristic.

In , we noted that repeated states in the search tree can cause an
exponential increase in search cost. In many games, repeated states
occur frequently because of —different permutations of the move sequence
that end up in the same position. For example, if White has one move,
$a_1$, that can be answered by Black with $b_1$ and an unrelated move
$a_2$ on the other side of the board that can be answered by $b_2$, then
the sequences $[a_1, b_1, a_2, b_2]$ and $[a_2, b_2, a_1, b_1]$ both end
up in the same position. It is worthwhile to store the evaluation of the
resulting position in a hash table the first time it is encountered so
that we don’t have to recompute it on subsequent occurrences. The hash
table of previously seen positions is traditionally called a ; it is
essentially identical to the *explored* list in (). Using a
transposition table can have a dramatic effect, sometimes as much as
doubling the reachable search depth in chess. On the other hand, if we
are evaluating a million nodes per second, at some point it is not
practical to keep *all* of them in the transposition table.
Various strategies have been used to choose which nodes to keep and
which to discard.

Imperfect Real-Time Decisions
-----------------------------

The minimax algorithm generates the entire game search space, whereas
the alpha–beta algorithm allows us to prune large parts of it. However,
alpha–beta still has to search all the way to terminal states for at
least a portion of the search space. This depth is usually not
practical, because moves must be made in a reasonable amount of
time—typically a few minutes at most. Claude Shannon’s paper
*Programming a Computer for Playing Chess* [-@Shannon:1950]
proposed instead that programs should cut off the search earlier and
apply a heuristic to states in the search, effectively turning
nonterminal nodes into terminal leaves. In other words, the suggestion
is to alter minimax or alpha–beta in two ways: replace the utility
function by a heuristic evaluation function , which estimates the
position’s utility, and replace the terminal test by a that decides when
to apply . That gives us the following for heuristic minimax for state
$s$ and maximum depth $d$:

$$\begin{aligned}
   \lefteqn{\noprog{H-Minimax}(s, d) =}\\
   &&\left\{\begin{array}{ll}
       \noprog{Eval}(s) 
             & \mbox{ if \prog{Cutoff-Test}\((s, d)\)} \\
       \max_{a\in {Actions}(s)} \noprog{H-Minimax}(\result{s}{a}, d+1) 
             & \mbox{ if \noprog{Player}\((s) = \) {\sc max}}\\
       \min_{a\in {Actions}(s)} \noprog{H-Minimax}(\result{s}{a}, d+1) 
             & \mbox{ if \noprog{Player}\((s) = \) {\sc min}.} \end{array}\right.\end{aligned}$$

### Evaluation functions

An evaluation function returns an *estimate* of the
expected utility of the game from a given position, just as the
heuristic functions of return an estimate of the distance to the goal.
The idea of an estimator was not new when Shannon proposed it. For
centuries, chess players (and aficionados of other games) have developed
ways of judging the value of a position because humans are even more
limited in the amount of search they can do than are computer programs.
It should be clear that the performance of a game-playing program
depends strongly on the quality of its evaluation function. An
inaccurate evaluation function will guide an agent toward positions that
turn out to be lost. How exactly do we design good evaluation functions?

First, the evaluation function should order the *terminal*
states in the same way as the true utility function: states that are
wins must evaluate better than draws, which in turn must be better than
losses. Otherwise, an agent using the evaluation function might err even
if it can see ahead all the way to the end of the game. Second, the
computation must not take too long! (The whole point is to search
faster.) Third, for nonterminal states, the evaluation function should
be strongly correlated with the actual chances of winning.

One might well wonder about the phrase “chances of winning.” After all,
chess is not a game of chance: we know the current state with certainty,
and no dice are involved. But if the search must be cut off at
nonterminal states, then the algorithm will necessarily be
*uncertain* about the final outcomes of those states. This
type of uncertainty is induced by computational, rather than
informational, limitations. Given the limited amount of computation that
the evaluation function is allowed to do for a given state, the best it
can do is make a guess about the final outcome.

Let us make this idea more concrete. Most evaluation functions work by
calculating various of the state—for example, in chess, we would have
features for the number of white pawns, black pawns, white queens, black
queens, and so on. The features, taken together, define various
*categories* or *equivalence classes* of
states: the states in each category have the same values for all the
features. For example, one category contains all two-pawn vs. one-pawn
endgames. Any given category, generally speaking, will contain some
states that lead to wins, some that lead to draws, and some that lead to
losses. The evaluation function cannot know which states are which, but
it can return a single value that reflects the *proportion*
of states with each outcome. For example, suppose our experience
suggests that 72% of the states encountered in the two-pawns vs.
one-pawn category lead to a win (utility +1); 20% to a loss (0), and 8%
to a draw (1/2). Then a reasonable evaluation for states in the category
is the : $({0.72}\times +1) + ({0.20} \times 0) +
({0.08} \times 1/2) = {0.76}$. In principle, the expected value can be
determined for each category, resulting in an evaluation function that
works for any state. As with terminal states, the evaluation function
need not return actual expected values as long as the
*ordering* of the states is the same.

In practice, this kind of analysis requires too many categories and
hence too much experience to estimate all the probabilities of winning.
Instead, most evaluation functions compute separate numerical
contributions from each feature and then *combine* them to
find the total value. For example, introductory chess books give an
approximate for each piece: each pawn is worth 1, a knight or bishop is
worth 3, a rook 5, and the queen 9. Other features such as “good pawn
structure” and “king safety” might be worth half a pawn, say. These
feature values are then simply added up to obtain the evaluation of the
position.

A secure advantage equivalent to a pawn gives a substantial likelihood
of winning, and a secure advantage equivalent to three pawns should give
almost certain victory, as illustrated in (a). Mathematically, this kind
of evaluation function is called a because it can be expressed as
$$\noprog{Eval}(s) = w_1f_1(s) + w_2f_2(s) + \cdots + w_n f_n(s)
    = \sum_{i=1}^n{w_if_i(s)}\ ,$$ where each $w_i$ is a weight and each
$f_i$ is a feature of the position. For chess, the $f_i$ could be the
numbers of each kind of piece on the board, and the $w_i$ could be the
values of the pieces (1 for pawn, 3 for bishop, etc.).

[chess-evaluation-figure]

Adding up the values of features seems like a reasonable thing to do,
but in fact it involves a strong assumption: that the contribution of
each feature is *independent* of the values of the other
features. For example, assigning the value 3 to a bishop ignores the
fact that bishops are more powerful in the endgame, when they have a lot
of space to maneuver. For this reason, current programs for chess and
other games also use *nonlinear* combinations of features.
For example, a pair of bishops might be worth slightly more than twice
the value of a single bishop, and a bishop is worth more in the endgame
(that is, when the *move number* feature is high or the
*number of remaining pieces* feature is low).

The astute reader will have noticed that the features and weights are
*not* part of the rules of chess! They come from centuries
of human chess-playing experience. In games where this kind of
experience is not available, the weights of the evaluation function can
be estimated by the machine learning techniques of . Reassuringly,
applying these techniques to chess has confirmed that a bishop is indeed
worth about three pawns.

### Cutting off search

The next step is to modify so that it will call the heuristic function
when it is appropriate to cut off the search. We replace the two lines
in that mention with the following line:

(, ) ()

We also must arrange for some bookkeeping so that the current is
incremented on each recursive call. The most straightforward approach to
controlling the amount of search is to set a fixed depth limit so that
(, ) returns ťrue for all greater than some fixed depth $d$. (It must
also return for all terminal states, just as did.) The depth $d$ is
chosen so that a move is selected within the allocated time. A more
robust approach is to apply iterative deepening. (See .) When time runs
out, the program returns the move selected by the deepest completed
search. As a bonus, iterative deepening also helps with move ordering.

These simple approaches can lead to errors due to the approximate nature
of the evaluation function. Consider again the simple evaluation
function for chess based on material advantage. Suppose the program
searches to the depth limit, reaching the position in (b), where Black
is ahead by a knight and two pawns. It would report this as the
heuristic value of the state, thereby declaring that the state is a
probable win by Black. But White’s next move captures Black’s queen with
no compensation. Hence, the position is really won for White, but this
can be seen only by looking ahead one more ply.

Obviously, a more sophisticated cutoff test is needed. The evaluation
function should be applied only to positions that are —that is, unlikely
to exhibit wild swings in value in the near future. In chess, for
example, positions in which favorable captures can be made are not
quiescent for an evaluation function that just counts material.
Nonquiescent positions can be expanded further until quiescent positions
are reached. This extra search is called a ; sometimes it is restricted
to consider only certain types of moves, such as capture moves, that
will quickly resolve the uncertainties in the position.

The is more difficult to eliminate. It arises when the program is facing
an opponent’s move that causes serious damage and is ultimately
unavoidable, but can be temporarily avoided by delaying tactics.
Consider the chess game in . It is clear that there is no way for the
black bishop to escape. For example, the white rook can capture it by
moving to h1, then a1, then a2; a capture at depth 6 ply. But Black does
have a sequence of moves that pushes the capture of the bishop “over the
horizon.” Suppose Black searches to depth 8 ply. Most moves by Black
will lead to the eventual capture of the bishop, and thus will be marked
as “bad” moves. But Black will consider checking the white king with the
pawn at e4. This will lead to the king capturing the pawn. Now Black
will consider checking again, with the pawn at f5, leading to another
pawn capture. That takes up 4 ply, and from there the remaining 4 ply is
not enough to capture the bishop. Black thinks that the line of play has
saved the bishop at the price of two pawns, when actually all it has
done is push the inevitable capture of the bishop beyond the horizon
that Black can see.

One strategy to mitigate the horizon effect is the , a move that is
“clearly better” than all other moves in a given position. Once
discovered anywhere in the tree in the course of a search, this singular
move is remembered. When the search reaches the normal depth limit, the
algorithm checks to see if the singular extension is a legal move; if it
is, the algorithm allows the move to be considered. This makes the tree
deeper, but because there will be few singular extensions, it does not
add many total nodes to the tree.

[horizon-figure]

### Forward pruning

So far, we have talked about cutting off search at a certain level and
about doing alpha–beta pruning that provably has no effect on the result
(at least with respect to the heuristic evaluation values). It is also
possible to do , meaning that some moves at a given node are pruned
immediately without further consideration. Clearly, most humans playing
chess consider only a few moves from each position (at least
consciously). One approach to forward pruning is : on each ply, consider
only a “beam” of the $n$ best moves (according to the evaluation
function) rather than considering all possible moves. Unfortunately,
this approach is rather dangerous because there is no guarantee that the
best move will not be pruned away.

The , or probabilistic cut, algorithm @Buro:1995 is a forward-pruning
version of alpha–beta search that uses statistics gained from prior
experience to lessen the chance that the best move will be pruned.
Alpha–beta search prunes any node that is *provably*
outside the current $(\alpha, \beta)$ window. also prunes nodes that are
*probably* outside the window. It computes this probability
by doing a shallow search to compute the backed-up value $v$ of a node
and then using past experience to estimate how likely it is that a score
of $v$ at depth $d$ in the tree would be outside $(\alpha, \beta)$. Buro
applied this technique to his Othello program, , and found that a
version of his program with beat the regular version 64% of the time,
even when the regular version was given twice as much time.

Combining all the techniques described here results in a program that
can play creditable chess (or other games). Let us assume we have
implemented an evaluation function for chess, a reasonable cutoff test
with a quiescence search, and a large transposition table. Let us also
assume that, after months of tedious bit-bashing, we can generate and
evaluate around a million nodes per second on the latest PC, allowing us
to search roughly 200 million nodes per move under standard time
controls (three minutes per move). The branching factor for chess is
about 35, on average, and ${35}^{5}$ is about 50 million, so if we used
minimax search, we could look ahead only about five plies. Though not
incompetent, such a program can be fooled easily by an average human
chess player, who can occasionally plan six or eight plies ahead. With
alpha–beta search we get to about 10 plies, which results in an expert
level of play. describes additional pruning techniques that can extend
the effective search depth to roughly 14 plies. To reach grandmaster
status we would need an extensively tuned evaluation function and a
large database of optimal opening and endgame moves.

### Search versus lookup

Somehow it seems like overkill for a chess program to start a game by
considering a tree of a billion game states, only to conclude that it
will move its pawn to e4. Books describing good play in the opening and
endgame in chess have been available for about a century
@Tattersall:1911. It is not surprising, therefore, that many
game-playing programs use *table lookup* rather than search
for the opening and ending of games.

For the openings, the computer is mostly relying on the expertise of
humans. The best advice of human experts on how to play each opening is
copied from books and entered into tables for the computer’s use.
However, computers can also gather statistics from a database of
previously played games to see which opening sequences most often lead
to a win. In the early moves there are few choices, and thus much expert
commentary and past games on which to draw. Usually after ten moves we
end up in a rarely seen position, and the program must switch from table
lookup to search.

Near the end of the game there are again fewer possible positions, and
thus more chance to do lookup. But here it is the computer that has the
expertise: computer analysis of endgames goes far beyond anything
achieved by humans. A human can tell you the general strategy for
playing a king-and-rook-versus-king (KRK) endgame: reduce the opposing
king’s mobility by squeezing it toward one edge of the board, using your
king to prevent the opponent from escaping the squeeze. Other endings,
such as king, bishop, and knight versus king (KBNK), are difficult to
master and have no succinct strategy description. A computer, on the
other hand, can completely *solve* the endgame by producing
a , which is a mapping from every possible state to the best move in
that state. Then we can just look up the best move rather than recompute
it anew. How big will the KBNK lookup table be? It turns out there are
462 ways that two kings can be placed on the board without being
adjacent. After the kings are placed, there are 62 empty squares for the
bishop, 61 for the knight, and two possible players to move next, so
there are just $462 \times 62 \times 61
\times 2 = 3,494,568$ possible positions. Some of these are checkmates;
mark them as such in a table. Then do a minimax search: reverse the
rules of chess to do unmoves rather than moves. Any move by White that,
no matter what move Black responds with, ends up in a position marked as
a win, must also be a win. Continue this search until all 3,494,568
positions are resolved as win, loss, or draw, and you have an infallible
lookup table for all KBNK endgames.

Using this technique and a *tour de force* of optimization
tricks, Ken Thompson [-@Thompson:1986; -@Thompson:1996] and Lewis
Stiller [-@Stiller:1992; -@Stiller:1995] solved all chess endgames with
up to five pieces and some with six pieces, making them available on the
Internet. Stiller discovered one case where a forced mate existed but
required 262 moves; this caused some consternation because the rules of
chess require a capture or pawn move to occur within 50 moves. Later
work by Marc Bourzutschky and Yakov Konoval @Bourzutschky:2006 solved
all pawnless six-piece and some seven-piece endgames; there is a KQNKRBN
endgame that with best play requires 517 moves until a capture, which
then leads to a mate.

If we could extend the chess endgame tables from 6 pieces to 32, then
White would know on the opening move whether it would be a win, loss, or
draw. This has not happened so far for chess, but it has happened for
checkers, as explained in the historical notes section.

Stochastic Games {#backgammon-section}
----------------

In real life, many unpredictable external events can put us into
unforeseen situations. Many games mirror this unpredictability by
including a random element, such as the throwing of dice. We call these
. Backgammon is a typical game that combines luck and skill. Dice are
rolled at the beginning of a player’s turn to determine the legal moves.
In the backgammon position of , for example, White has rolled a 6–5 and
has four possible moves.

[backgammon-position-figure]

[backgammon-tree-figure]

Although White knows what his or her own legal moves are, White does not
know what Black is going to roll and thus does not know what Black’s
legal moves will be. That means White cannot construct a standard game
tree of the sort we saw in chess and tic-tac-toe. A game tree in
backgammon must include in addition to max and
min nodes. Chance nodes are shown as circles in . The
branches leading from each chance node denote the possible dice rolls;
each branch is labeled with the roll and its probability. There are 36
ways to roll two dice, each equally likely; but because a 6–5 is the
same as a 5–6, there are only 21 distinct rolls. The six doubles (1–1
through 6–6) each have a probability of 1/36, so we say
$P(\mbox{1--1}) =
1/36$. The other 15 distinct rolls each have a 1/18 probability.

The next step is to understand how to make correct decisions. Obviously,
we still want to pick the move that leads to the best position. However,
positions do not have definite minimax values. Instead, we can only
calculate the of a position: the average over all possible outcomes of
the chance nodes.

This leads us to generalize the for deterministic games to an for games
with chance nodes. Terminal nodes and max and
min nodes (for which the dice roll is known) work exactly
the same way as before. For chance nodes we compute the expected value,
which is the sum of the value over all outcomes, weighted by the
probability of each chance action:

$$\begin{aligned}
\lefteqn{\prog{Expectiminimax}(s) =}\\
&& \left\{\begin{array}{ll}

\noprog{Utility}(s) &
            \mbox{if \prog{Terminal-Test}\((s)\) }\\
 \max_{a} \noprog{Expectiminimax}(\result{s}{a}) &
                    \mbox{if \noprog{Player}\((s) \eq \) {\sc max}}\\
 \min_{a} \noprog{Expectiminimax}(\result{s}{a}) &
                    \mbox{if \noprog{Player}\((s) \eq \) {\sc min}}\\
 \sum_{r} P(r) \noprog{Expectiminimax}(\result{s}{r}) &
                    \mbox{if \noprog{Player}\((s) \eq \) {\sc chance}}\end{array}\right. \end{aligned}$$

where $r$ represents a possible dice roll (or other chance event) and
$\result{s}{r}$ is the same state as $s$, with the additional fact that
the result of the dice roll is $r$.

### Evaluation functions for games of chance

As with minimax, the obvious approximation to make with expectiminimax
is to cut the search off at some point and apply an evaluation function
to each leaf. One might think that evaluation functions for games such
as backgammon should be just like evaluation functions for chess—they
just need to give higher scores to better positions. But in fact, the
presence of chance nodes means that one has to be more careful about
what the evaluation values mean. shows what happens: with an evaluation
function that assigns the values [1, 2, 3, 4] to the leaves, move $a_1$
is best; with values [1, 20, 30, 400], move $a_2$ is best. Hence, the
program behaves totally differently if we make a change in the scale of
some evaluation values! It turns out that to avoid this sensitivity, the
evaluation function must be a positive linear transformation of the
probability of winning from a position (or, more generally, of the
expected utility of the position). This is an important and general
property of situations in which uncertainty is involved, and we discuss
it further in .

[chance-evaluation-figure]

If the program knew in advance all the dice rolls that would occur for
the rest of the game, solving a game with dice would be just like
solving a game without dice, which minimax does in $O(b^m)$ time, where
$b$ is the branching factor and $m$ is the maximum depth of the game
tree. Because expectiminimax is also considering all the possible
dice-roll sequences, it will take $O(b^m n^m)$, where $n$ is the number
of distinct rolls.

Even if the search depth is limited to some small depth $d$, the extra
cost compared with that of minimax makes it unrealistic to consider
looking ahead very far in most games of chance. In backgammon $n$ is 21
and $b$ is usually around 20, but in some situations can be as high as
4000 for dice rolls that are doubles. Three plies is probably all we
could manage.

Another way to think about the problem is this: the advantage of
alpha–beta is that it ignores future developments that just are not
going to happen, given best play. Thus, it concentrates on likely
occurrences. In games with dice, there are *no* likely
sequences of moves, because for those moves to take place, the dice
would first have to come out the right way to make them legal. This is a
general problem whenever uncertainty enters the picture: the
possibilities are multiplied enormously, and forming detailed plans of
action becomes pointless because the world probably will not play along.

It may have occurred to you that something like alpha–beta pruning could
be applied to game trees with chance nodes. It turns out that it can.
The analysis for min and max nodes is
unchanged, but we can also prune chance nodes, using a bit of ingenuity.
Consider the chance node $C$ in and what happens to its value as we
examine and evaluate its children. Is it possible to find an upper bound
on the value of $C$ before we have looked at all its children? (Recall
that this is what alpha–beta needs in order to prune a node and its
subtree.) At first sight, it might seem impossible because the value of
$C$ is the *average* of its children’s values, and in order
to compute the average of a set of numbers, we must look at all the
numbers. But if we put bounds on the possible values of the utility
function, then we can arrive at bounds for the average without looking
at every number. For example, say that all utility values are between
$-2$ and $+2$; then the value of leaf nodes is bounded, and in turn we
*can* place an upper bound on the value of a chance node
without looking at all its children.

An alternative is to do to evaluate a position. Start with an alpha–beta
(or other) search algorithm. From a start position, have the algorithm
play thousands of games against itself, using random dice rolls. In the
case of backgammon, the resulting win percentage has been shown to be a
good approximation of the value of the position, even if the algorithm
has an imperfect heuristic and is searching only a few plies
@Tesauro:1995. For games with dice, this type of simulation is called a
.

Partially Observable Games {#partially-observable-game-section}
--------------------------

Chess has often been described as war in miniature, but it lacks at
least one major characteristic of real wars, namely, . In the “fog of
war,” the existence and disposition of enemy units is often unknown
until revealed by direct contact. As a result, warfare includes the use
of scouts and spies to gather information and the use of concealment and
bluff to confuse the enemy. Partially observable games share these
characteristics and are thus qualitatively different from the games
described in the preceding sections.

### Kriegspiel: Partially observable chess

In *deterministic* partially observable games, uncertainty
about the state of the board arises entirely from lack of access to the
choices made by the opponent. This class includes children’s games such
as Battleships (where each player’s ships are placed in locations hidden
from the opponent but do not move) and Stratego (where piece locations
are known but piece types are hidden). We will examine the game of , a
partially observable variant of chess in which pieces can move but are
completely invisible to the opponent.

The rules of Kriegspiel are as follows: White and Black each see a board
containing only their own pieces. A referee, who can see all the pieces,
adjudicates the game and periodically makes announcements that are heard
by both players. On his turn, White proposes to the referee any move
that would be legal if there were no black pieces. If the move is in
fact not legal (because of the black pieces), the referee announces
“illegal.” In this case, White may keep proposing moves until a legal
one is found—and learns more about the location of Black’s pieces in the
process. Once a legal move is proposed, the referee announces one or
more of the following: “Capture on square *X*” if there is
a capture, and “Check by *D*” if the black king is in
check, where *D* is the direction of the check, and can be
one of “Knight,” “Rank,” “File,” “Long diagonal,” or “Short diagonal.”
(In case of discovered check, the referee may make two “Check”
announcements.) If Black is checkmated or stalemated, the referee says
so; otherwise, it is Black’s turn to move.

Kriegspiel may seem terrifyingly impossible, but humans manage it quite
well and computer programs are beginning to catch up. It helps to recall
the notion of a as defined in and illustrated in —the set of all
*logically possible* board states given the complete
history of percepts to date. Initially, White’s belief state is a
singleton because Black’s pieces haven’t moved yet. After White makes a
move and Black responds, White’s belief state contains 20 positions
because Black has 20 replies to any White move. Keeping track of the
belief state as the game progresses is exactly the problem of , for
which the update step is given in . We can map Kriegspiel state
estimation directly onto the partially observable, nondeterministic
framework of if we consider the opponent as the source of
nondeterminism; that is, the of White’s move are composed from the
(predictable) outcome of White’s own move and the unpredictable outcome
given by Black’s reply.[^3]

Given a current belief state, White may ask, “Can I win the game?” For a
partially observable game, the notion of a is altered; instead of
specifying a move to make for each possible *move* the
opponent might make, we need a move for every possible *percept
sequence* that might be received. For Kriegspiel, a winning
strategy, or , is one that, for each possible percept sequence, leads to
an actual checkmate for every possible board state in the current belief
state, regardless of how the opponent moves. With this definition, the
opponent’s belief state is irrelevant—the strategy has to work even if
the opponent can see all the pieces. This greatly simplifies the
computation. shows part of a guaranteed checkmate for the KRK (king and
rook against king) endgame. In this case, Black has just one piece (the
king), so a belief state for White can be shown in a single board by
marking each possible position of the Black king.

[kriegspiel-krk-figure]

The general and-or search algorithm can be applied to the
belief-state space to find guaranteed checkmates, just as in . The
incremental belief-state algorithm mentioned in that section often finds
midgame checkmates up to depth 9—probably well beyond the abilities of
human players.

In addition to guaranteed checkmates, Kriegspiel admits an entirely new
concept that makes no sense in fully observable games: . Such checkmates
are still required to work in every board state in the belief state;
they are probabilistic with respect to randomization of the winning
player’s moves. To get the basic idea, consider the problem of finding a
lone black king using just the white king. Simply by moving randomly,
the white king will *eventually* bump into the black king
even if the latter tries to avoid this fate, since Black cannot keep
guessing the right evasive moves indefinitely. In the terminology of
probability theory, detection occurs *with probability* 1.
The KBNK endgame—king, bishop and knight against king—is won in this
sense; White presents Black with an infinite random sequence of choices,
for one of which Black will guess incorrectly and reveal his position,
leading to checkmate. The KBBK endgame, on the other hand, is won with
probability $1-\epsilon$. White can force a win only by leaving one of
his bishops unprotected for one move. If Black happens to be in the
right place and captures the bishop (a move that would lose if the
bishops are protected), the game is drawn. White can choose to make the
risky move at some randomly chosen point in the middle of a very long
sequence, thus reducing $\epsilon$ to an arbitrarily small constant, but
cannot reduce $\epsilon$ to zero.

It is quite rare that a guaranteed or probabilistic checkmate can be
found within any reasonable depth, except in the endgame. Sometimes a
checkmate strategy works for *some* of the board states in
the current belief state but not others. Trying such a strategy may
succeed, leading to an —accidental in the sense that White could not
*know* that it would be checkmate—if Black’s pieces happen
to be in the right places. (Most checkmates in games between humans are
of this accidental nature.) This idea leads naturally to the question of
*how likely* it is that a given strategy will win, which
leads in turn to the question of *how likely* it is that
each board state in the current belief state is the true board state.

One’s first inclination might be to propose that all board states in the
current belief state are equally likely—but this can’t be right.
Consider, for example, White’s belief state after Black’s first move of
the game. By definition (assuming that Black plays optimally), Black
must have played an optimal move, so all board states resulting from
suboptimal moves ought to be assigned zero probability. This argument is
not quite right either, because

each player’s goal is not just to move pieces to the right squares but
also to minimize the information that the opponent has about their
location.

Playing any *predictable* “optimal” strategy provides the
opponent with information. Hence, optimal play in partially observable
games requires a willingness to play somewhat *randomly*.
(This is why restaurant hygiene inspectors do *random*
inspection visits.) This means occasionally selecting moves that may
seem “intrinsically” weak—but they gain strength from their very
unpredictability, because the opponent is unlikely to have prepared any
defense against them.

From these considerations, it seems that the probabilities associated
with the board states in the current belief state can only be calculated
given an optimal randomized strategy; in turn, computing that strategy
seems to require knowing the probabilities of the various states the
board might be in. This conundrum can be resolved by adopting the
game-theoretic notion of an solution, which we pursue further in . An
equilibrium specifies an optimal randomized strategy for each player.
Computing equilibria is prohibitively expensive, however, even for small
games, and is out of the question for Kriegspiel. At present, the design
of effective algorithms for general Kriegspiel play is an open research
topic. Most systems perform bounded-depth lookahead in their own
belief-state space, ignoring the opponent’s belief state. Evaluation
functions resemble those for the observable game but include a component
for the size of the belief state—smaller is better!

### Card games

Card games provide many examples of *stochastic* partial
observability, where the missing information is generated randomly. For
example, in many games, cards are dealt randomly at the beginning of the
game, with each player receiving a hand that is not visible to the other
players. Such games include bridge, whist, hearts, and some forms of
poker.

At first sight, it might seem that these card games are just like dice
games: the cards are dealt randomly and determine the moves available to
each player, but all the “dice” are rolled at the beginning! Even though
this analogy turns out to be incorrect, it suggests an effective
algorithm: consider all possible deals of the invisible cards; solve
each one as if it were a fully observable game; and then choose the move
that has the best outcome averaged over all the deals. Suppose that each
deal $s$ occurs with probability $P(s)$; then the move we want is

$$\argmax_a \sum_s P(s)\, \noprog{Minimax}(\result{s}{a})\ .
\label{exact-clairvoyance-equation}$$

Here, we run exact if computationally feasible; otherwise, we run .

Now, in most card games, the number of possible deals is rather large.
For example, in bridge play, each player sees just two of the four
hands; there are two unseen hands of 13 cards each, so the number of
deals is ${{26} \choose {13}} = 10,400,600$. Solving even one deal is
quite difficult, so solving ten million is out of the question. Instead,
we resort to a Monte Carlo approximation: instead of adding up
*all* the deals, we take a *random sample* of
$N$ deals, where the probability of deal $s$ appearing in the sample is
proportional to $P(s)$:

$$\argmax_a \frac{1}{N} \sum_{i\eq 1}^{N} \noprog{Minimax}(\result{s_i}{a})\ .
\label{monte-carlo-clairvoyance-equation}$$

(Notice that $P(s)$ does not appear explicitly in the summation, because
the samples are already drawn according to $P(s)$.) As $N$ grows large,
the sum over the random sample tends to the exact value, but even for
fairly small $N$—say, 100 to 1,000—the method gives a good
approximation. It can also be applied to deterministic games such as
Kriegspiel, given some reasonable estimate of $P(s)$.

For games like whist and hearts, where there is no bidding or betting
phase before play commences, each deal will be equally likely and so the
values of $P(s)$ are all equal. For bridge, play is preceded by a
bidding phase in which each team indicates how many tricks it expects to
win. Since players bid based on the cards they hold, the other players
learn more about the probability of each deal. Taking this into account
in deciding how to play the hand is tricky, for the reasons mentioned in
our description of Kriegspiel: players may bid in such a way as to
minimize the information conveyed to their opponents. Even so, the
approach is quite effective for bridge, as we show in .

The strategy described in Equations [exact-clairvoyance-equation]
and [monte-carlo-clairvoyance-equation] is sometimes called
*averaging over clairvoyance* because it assumes that the
game will become observable to both players immediately after the first
move. Despite its intuitive appeal, the strategy can lead one astray.
Consider the following story:

Day 1: Road *A* leads to a heap of gold; Road
*B* leads to a fork. Take the left fork and you’ll find a
bigger heap of gold, but take the right fork and you’ll be run over by a
bus.\
Day 2: Road *A* leads to a heap of gold; Road
*B* leads to a fork. Take the right fork and you’ll find a
bigger heap of gold, but take the left fork and you’ll be run over by a
bus.\
Day 3: Road *A* leads to a heap of gold; Road
*B* leads to a fork. One branch of the fork leads to a
bigger heap of gold, but take the wrong fork and you’ll be hit by a bus.
Unfortunately you don’t know which fork is which.

Averaging over clairvoyance leads to the following reasoning: on Day 1,
*B* is the right choice; on Day 2, *B* is the
right choice; on Day 3, the situation is the same as either Day 1 or Day
2, so *B* must still be the right choice.

Now we can see how averaging over clairvoyance fails: it does not
consider the *belief state* that the agent will be in after
acting. A belief state of total ignorance is not desirable, especially
when one possibility is certain death. Because it assumes that every
future state will automatically be one of perfect knowledge, the
approach never selects actions that *gather information*
(like the first move in ); nor will it choose actions that hide
information from the opponent or provide information to a partner
because it assumes that they already know the information; and it will
never in poker,[^4] because it assumes the opponent can see its cards.
In , we show how to construct algorithms that do all these things by
virtue of solving the true partially observable decision problem.

State-of-the-Art Game Programs
------------------------------

[state-art-game-playing-section]In 1965, the Russian mathematician
Alexander Kronrod called chess “the *Drosophila* of
artificial intelligence.” John McCarthy disagrees: whereas geneticists
use fruit flies to make discoveries that apply to biology more broadly,
AI has used chess to do the equivalent of breeding very fast fruit
flies. Perhaps a better analogy is that chess is to AI as Grand Prix
motor racing is to the car industry: state-of-the-art game programs are
blindingly fast, highly optimized machines that incorporate the latest
engineering advances, but they aren’t much use for doing the shopping or
driving off-road. Nonetheless, racing and game-playing generate
excitement and a steady stream of innovations that have been adopted by
the wider community. In this section we look at what it takes to come
out on top in various games.

: IBM’s chess program, now retired, is well known for defeating world
champion Garry Kasparov in a widely publicized exhibition match. Deep
Blue ran on a parallel computer with 30 IBM RS/6000 processors doing
alpha–beta search. The unique part was a configuration of 480 custom
VLSI chess processors that performed move generation and move ordering
for the last few levels of the tree, and evaluated the leaf nodes. Deep
Blue searched up to 30 billion positions per move, reaching depth 14
routinely. The key to its success seems to have been its ability to
generate singular extensions beyond the depth limit for sufficiently
interesting lines of forcing/forced moves. In some cases the search
reached a depth of 40 plies. The evaluation function had over 8000
features, many of them describing highly specific patterns of pieces. An
“opening book” of about 4000 positions was used, as well as a database
of 700,000 grandmaster games from which consensus recommendations could
be extracted. The system also used a large endgame database of solved
positions containing all positions with five pieces and many with six
pieces. This database had the effect of substantially extending the
effective search depth, allowing Deep Blue to play perfectly in some
cases even when it was many moves away from checkmate.

The success of reinforced the widely held belief that progress in
computer game-playing has come primarily from ever-more-powerful
hardware—a view encouraged by . But algorithmic improvements have
allowed programs running on standard PCs to win World Computer Chess
Championships. A variety of pruning heuristics are used to reduce the
effective branching factor to less than 3 (compared with the actual
branching factor of about 35). The most important of these is the
heuristic, which generates a good lower bound on the value of a
position, using a shallow search in which the opponent gets to move
twice at the beginning. This lower bound often allows alpha–beta pruning
without the expense of a full-depth search. Also important is , which
helps decide in advance which moves will cause a beta cutoff in the
successor nodes.

can be seen as the successor to . runs on a 64-processor cluster with 1
gigabyte per processor and with custom hardware in the form of FPGA
(Field Programmable Gate Array) chips. reaches 200 million evaluations
per second, about the same as Deep Blue, but reaches 18 plies deep
rather than just 14 because of aggressive use of the null move heuristic
and forward pruning.

, winner of the 2008 and 2009 World Computer Chess Championships, is
considered the strongest current computer player. It uses an
off-the-shelf 8-core 3.2 GHz Intel Xeon processor, but little is known
about the design of the program. ’s main advantage appears to be its
evaluation function, which has been tuned by its main developer,
International Master Vasik Rajlich, and at least three other
grandmasters.

The most recent matches suggest that the top computer chess programs
have pulled ahead of all human contenders. (See the historical notes for
details.)

: Jonathan Schaeffer and colleagues developed , which runs on regular
PCs and uses alpha–beta search. Chinook defeated the long-running human
champion in an abbreviated match in 1990, and since 2007 has been able
to play perfectly by using alpha–beta search combined with a database of
39 trillion endgame positions.

, also called , is probably more popular as a computer game than as a
board game. It has a smaller search space than chess, usually 5 to 15
legal moves, but evaluation expertise had to be developed from scratch.
In 1997, the program @Buro:2002 defeated the human world champion,
Takeshi Murakami,  by six games to none. It is generally acknowledged
that humans are no match for computers at Othello.

: explained why the inclusion of uncertainty from dice rolls makes deep
search an expensive luxury. Most work on backgammon has gone into
improving the evaluation function. Gerry Tesauro [-@Tesauro:1992]
combined reinforcement learning with neural networks to develop a
remarkably accurate evaluator that is used with a search to depth 2 or
3. After playing more than a million training games against itself,
Tesauro’s program, , is competitive with top human players. The
program’s opinions on the opening moves of the game have in some cases
radically altered the received wisdom.

is the most popular board game in Asia. Because the board is
${19} \times {19}$ and moves are allowed into (almost) every empty
square, the branching factor starts at 361, which is too daunting for
regular alpha–beta search methods. In addition, it is difficult to write
an evaluation function because control of territory is often very
unpredictable until the endgame. Therefore the top programs, such as ,
avoid alpha–beta search and instead use Monte Carlo rollouts. The trick
is to decide what moves to make in the course of the rollout. There is
no aggressive pruning; all moves are possible. The UCT (upper confidence
bounds on trees) method works by making random moves in the first few
iterations, and over time guiding the sampling process to prefer moves
that have led to wins in previous samples. Some tricks are added,
including *knowledge-based rules* that suggest particular
moves whenever a given pattern is detected and *limited local
search* to decide tactical questions. Some programs also include
special techniques from to analyze endgames. These techniques decompose
a position into sub-positions that can be analyzed separately and then
combined @Berlekamp+Wolfe:1994 [@Muller:2003]. The optimal solutions
obtained in this way have surprised many professional Go players, who
thought they had been playing optimally all along. Current Go programs
play at the master level on a reduced $9 \times 9$ board, but are still
at advanced amateur level on a full board.

is a card game of imperfect information: a player’s cards are hidden
from the other players. Bridge is also a *multiplayer* game
with four players instead of two, although the players are paired into
two teams. As in , optimal play in partially observable games like
bridge can include elements of information gathering, communication, and
careful weighing of probabilities. Many of these techniques are used in
the Bridge Baron program @Smith+al:1998, which won the 1997 computer
bridge championship. While it does not play optimally, Bridge Baron is
one of the few successful game-playing systems to use complex,
hierarchical plans (see ) involving high-level ideas, such as and , that
are familiar to bridge players.

The program @Ginsberg:1999 won the 2000 computer bridge championship
quite decisively using the Monte Carlo method. Since then, other winning
programs have followed ’s lead. ’s major innovation is using to compute
and cache general rules for optimal play in various standard classes of
situations rather than evaluating each situation individually. For
example, in a situation where one player has the cards A-K-Q-J-4-3-2 of
one suit and another player has 10-9-8-7-6-5, there are
$7 \times 6 = 42$ ways that the first player can lead from that suit and
the second player can follow. But treats these situations as just two:
the first player can lead either a high card or a low card; the exact
cards played don’t matter. With this optimization (and a few others),
can solve a 52-card, fully observable deal *exactly* in
about a second. ’s tactical accuracy makes up for its inability to
reason about information. It finished 12th in a field of 35 in the par
contest (involving just play of the hand, not bidding) at the 1998 human
world championship, far exceeding the expectations of many human
experts.

There are several reasons why plays at expert level with Monte Carlo
simulation, whereas Kriegspiel programs do not. First, ’s evaluation of
the fully observable version of the game is exact, searching the full
game tree, while Kriegspiel programs rely on inexact heuristics. But far
more important is the fact that in bridge, most of the uncertainty in
the partially observable information comes from the randomness of the
deal, not from the adversarial play of the opponent. Monte Carlo
simulation handles randomness well, but does not always handle strategy
well, especially when the strategy involves the value of information.

: Most people think the hard part about Scrabble is coming up with good
words, but given the official dictionary, it turns out to be rather easy
to program a move generator to find the highest-scoring
move @Gordon:1994. That doesn’t mean the game is solved, however: merely
taking the top-scoring move each turn results in a good but not expert
player. The problem is that Scrabble is both partially observable and
stochastic: you don’t know what letters the other player has or what
letters you will draw next. So playing Scrabble well combines the
difficulties of backgammon and bridge. Nevertheless, in 2006, the
program defeated the former world champion, David Boys, 3–2.

Alternative Approaches {#game-discussion-section}
----------------------

Because calculating optimal decisions in games is intractable in most
cases, all algorithms must make some assumptions and approximations. The
standard approach, based on minimax, evaluation functions, and
alpha–beta, is just one way to do this. Probably because it has been
worked on for so long, the standard approach dominates other methods in
tournament play. Some believe that this has caused game playing to
become divorced from the mainstream of AI research: the standard
approach no longer provides much room for new insight into general
questions of decision making. In this section, we look at the
alternatives.

First, let us consider heuristic minimax. It selects an optimal move in
a given search tree *provided that the leaf node evaluations are
exactly correct*. In reality, evaluations are usually crude
estimates of the value of a position and can be considered to have large
errors associated with them. shows a two-ply game tree for which minimax
suggests taking the right-hand branch because $100 > 99$. That is the
correct move if the evaluations are all correct. But of course the
evaluation function is only approximate. Suppose that the evaluation of
each node has an error that is independent of other nodes and is
randomly distributed with mean zero and standard deviation of $\sigma$.
Then when $\sigma = 5$, the left-hand branch is actually better 71% of
the time, and 58% of the time when $\sigma = 2$. The intuition behind
this is that the right-hand branch has four nodes that are close to 99;
if an error in the evaluation of any one of the four makes the
right-hand branch slip below 99, then the left-hand branch is better.

In reality, circumstances are actually worse than this because the error
in the evaluation function is *not* independent. If we get
one node wrong, the chances are high that nearby nodes in the tree will
also be wrong. The fact that the node labeled 99 has siblings labeled
1000 suggests that in fact it might have a higher true value. We can use
an evaluation function that returns a probability distribution over
possible values, but it is difficult to combine these distributions
properly, because we won’t have a good model of the very strong
dependencies that exist between the values of sibling nodes

[minimax-error-figure]

Next, we consider the search algorithm that generates the tree. The aim
of an algorithm designer is to specify a computation that runs quickly
and yields a good move. The alpha–beta algorithm is designed not just to
select a good move but also to calculate bounds on the values of all the
legal moves. To see why this extra information is unnecessary, consider
a position in which there is only one legal move. Alpha–beta search
still will generate and evaluate a large search tree, telling us that
the only move is the best move and assigning it a value. But since we
have to make the move anyway, knowing the move’s value is useless.
Similarly, if there is one obviously good move and several moves that
are legal but lead to a quick loss, we would not want alpha–beta to
waste time determining a precise value for the lone good move. Better to
just make the move quickly and save the time for later. This leads to
the idea of the *utility of a node expansion*. A good
search algorithm should select node expansions of high utility—that is,
ones that are likely to lead to the discovery of a significantly better
move. If there are no node expansions whose utility is higher than their
cost (in terms of time), then the algorithm should stop searching and
make a move. Notice that this works not only for clear-favorite
situations but also for the case of *symmetrical* moves,
for which no amount of search will show that one move is better than
another.

This kind of reasoning about what computations to do is called
(reasoning about reasoning). It applies not just to game playing but to
any kind of reasoning at all. All computations are done in the service
of trying to reach better decisions, all have costs, and all have some
likelihood of resulting in a certain improvement in decision quality.
Alpha–beta incorporates the simplest kind of metareasoning, namely, a
theorem to the effect that certain branches of the tree can be ignored
without loss. It is possible to do much better. In , we see how these
ideas can be made precise and implementable.

Finally, let us reexamine the nature of search itself. Algorithms for
heuristic search and for game playing generate sequences of concrete
states, starting from the initial state and then applying an evaluation
function. Clearly, this is not how humans play games. In chess, one
often has a particular goal in mind—for example, trapping the opponent’s
queen—and can use this goal to *selectively* generate
plausible plans for achieving it. This kind of goal-directed reasoning
or planning sometimes eliminates combinatorial search altogether. David
Wilkins’ [-@Wilkins:1980] is the only program to have used goal-directed
reasoning successfully in chess: it was capable of solving some chess
problems requiring an 18-move combination. As yet there is no good
understanding of how to *combine* the two kinds of
algorithms into a robust and efficient system, although might be a step
in the right direction. A fully integrated system would be a significant
achievement not just for game-playing research but also for AI research
in general, because it would be a good basis for a general intelligent
agent.

We have looked at a variety of games to understand what optimal play
means and to understand how to play well in practice. The most important
ideas are as follows:

-   A game can be defined by the (how the board is set up), the legal in
    each state, the of each action, a (which says when the game is
    over), and a that applies to terminal states.

-   In two-player zero-sum games with , the algorithm can select optimal
    moves by a depth-first enumeration of the game tree.

-   The search algorithm computes the same optimal move as minimax, but
    achieves much greater efficiency by eliminating subtrees that are
    provably irrelevant.

-   Usually, it is not feasible to consider the whole game tree (even
    with alpha–beta), so we need to cut the search off at some point and
    apply a heuristic that estimates the utility of a state.

-   Many game programs precompute tables of best moves in the opening
    and endgame so that they can look up a move rather than search.

-   Games of chance can be handled by an extension to the minimax
    algorithm that evaluates a by taking the average utility of all its
    children, weighted by the probability of each child.

-   Optimal play in games of , such as Kriegspiel and bridge, requires
    reasoning about the current and future of each player. A simple
    approximation can be obtained by averaging the value of an action
    over each possible configuration of missing information.

-   Programs have bested even champion human players at games such as
    chess, checkers, and Othello. Humans retain the edge in several
    games of imperfect information, such as poker, bridge, and
    Kriegspiel, and in games with very large branching factors and
    little good heuristic knowledge, such as Go.

The early history of mechanical game playing was marred by numerous
frauds. The most notorious of these was Baron Wolfgang von Kempelen’s
(1734–1804) “The Turk,” a supposed chess-playing automaton that defeated
Napoleon before being exposed as a magician’s trick cabinet housing a
human chess expert \<see\>Levitt:2000. It played from 1769
to 1854. In 1846, Charles Babbage (who had been fascinated by the Turk)
appears to have contributed the first serious discussion of the
feasibility of computer chess and checkers @Morrison+Morrison:1961. He
did not understand the exponential complexity of search trees, claiming
“the combinations involved in the Analytical Engine enormously surpassed
any required, even by the game of chess.” Babbage also designed, but did
not build, a special-purpose machine for playing . The first true
game-playing machine was built around 1890 by the Spanish engineer
Leonardo Torres y Quevedo. It specialized in the “KRK” (king and rook
vs. king) chess endgame, guaranteeing a win with king and rook from any
position.

The minimax algorithm is traced to a 1912 paper by Ernst Zermelo, the
developer of modern set theory. The paper unfortunately contained
several errors and did not describe minimax correctly. On the other
hand, it did lay out the ideas of retrograde analysis and proposed (but
did not prove) what became known as Zermelo’s theorem: that chess is
determined—White can force a win or Black can or it is a draw; we just
don’t know which. Zermelo says that should we eventually know, “Chess
would of course lose the character of a game at all.” A solid foundation
for game theory was developed in the seminal work *Theory of Games
and Economic Behavior* @VonNeumann+Morgenstern:1944, which
included an analysis showing that some games *require*
strategies that are randomized (or otherwise unpredictable). See for
more information.

John McCarthy conceived the idea of alpha–beta search in 1956, although
he did not publish it. The  @Newell+al:1958 used a simplified version of
alpha–beta; it was the first chess program to do so. Alpha–beta pruning
was described by and . Alpha–beta was used by the “Kotok–McCarthy” chess
program written by a student of John McCarthy @Kotok:1962. Knuth and
Moore [-@Knuth+Moore:1975] proved the correctness of alpha–beta and
analysed its time complexity. Pearl [-@Pearl:1982a] shows alpha–beta to
be asymptotically optimal among all fixed-depth game-tree search
algorithms.

Several attempts have been made to overcome the problems with the
“standard approach” that were outlined in . The first nonexhaustive
heuristic search algorithm with some theoretical grounding was probably
B$^*$ @Berliner:1979, which attempts to maintain interval bounds on the
possible value of a node in the game tree rather than giving it a single
point-valued estimate. Leaf nodes are selected for expansion in an
attempt to refine the top-level bounds until one move is “clearly best.”
Palay [-@Palay:1985] extends the B$^*$ idea using probability
distributions on values in place of intervals. David
McAllester’s [-@McAllester:1988] conspiracy number search expands leaf
nodes that, by changing their values, could cause the program to prefer
a new move at the root. MGSS$^*$ @Russell+Wefald:1989 uses the
decision-theoretic techniques of to estimate the value of expanding each
leaf in terms of the expected improvement in decision quality at the
root. It outplayed an alpha–beta algorithm at Othello despite searching
an order of magnitude fewer nodes. The MGSS$^*$ approach is, in
principle, applicable to the control of any form of deliberation.

Alpha–beta search is in many ways the two-player analog of depth-first
branch-and-bound, which is dominated by A in the single-agent case. The
SSS$^*$ algorithm @Stockman:1979 can be viewed as a two-player A and
never expands more nodes than alpha–beta to reach the same decision. The
memory requirements and computational overhead of the queue make SSS$^*$
in its original form impractical, but a linear-space version has been
developed from the RBFS algorithm @Korf+Chickering:1996. developed a new
view of SSS$^*$ as a combination of alpha–beta and transposition tables,
showing how to overcome the drawbacks of the original algorithm and
developing a new variant called MTD(*f*) that has been
adopted by a number of top programs.

D. F. Beal [-@Beal:1980] and Dana Nau [-@Nau:1980; -@Nau:1983b] studied
the weaknesses of minimax applied to approximate evaluations. They
showed that under certain assumptions about the distribution of leaf
values in the tree, minimaxing can yield values at the root that are
actually *less* reliable than the direct use of the
evaluation function itself. Pearl’s book
*Heuristics* [-@Pearl:1984] partially explains this
apparent paradox and analyzes many game-playing algorithms. propose a
probability-based replacement for minimax, showing that it results in
better choices in certain games. The expectiminimax algorithm was
proposed by Donald Michie [-@Michie:1966]. Bruce
Ballard [-@Ballard:1983] extended alpha–beta pruning to cover trees with
chance nodes and reexamines this work and provides empirical results.

describe a system for completely solving partially observable games. The
system is quite general, handling games whose optimal strategy requires
randomized moves and games that are more complex than those handled by
any previous system. Still, it can’t handle games as complex as poker,
bridge, and Kriegspiel. describe several variants of Monte Carlo search,
including one where min has complete information but
max does not. Among deterministic, partially observable
games, Kriegspiel has received the most attention. Ferguson demonstrated
hand-derived randomized strategies for winning Kriegspiel with a bishop
and knight [-@Ferguson:1992] or two bishops [-@Ferguson:1995] against a
king. The first Kriegspiel programs concentrated on finding endgame
checkmates and performed and–or search in belief-state
space @Sakuta+Iida:2002 [@Bolognesi+Ciancarini:2003]. Incremental
belief-state algorithms enabled much more complex midgame checkmates to
be found @Russell+Wolfe:2005 [@Wolfe+Russell:2007], but efficient state
estimation remains the primary obstacle to effective general
play @Parker+al:2005.

was one of the first tasks undertaken in AI, with early efforts by many
of the pioneers of computing, including Konrad Zuse in 1945, Norbert
Wiener in his book *Cybernetics* [-@Wiener:1948], and Alan
Turing in 1950 \<see\>Turing:1953. But it was Claude
Shannon’s article *Programming a Computer for Playing
Chess* [-@Shannon:1950] that had the most complete set of ideas,
describing a representation for board positions, an evaluation function,
quiescence search, and some ideas for selective (nonexhaustive)
game-tree search. and the commentators on his article also explored the
possibilities for computer chess play.

D. G. Prinz [-@Prinz:1952] completed a program that solved chess endgame
problems but did not play a full game. Stan Ulam and a group at the Los
Alamos National Lab produced a program that played chess on a
$6\stimes 6$ board with no bishops @Kister+al:1957. It could search 4
plies deep in about 12 minutes. Alex Bernstein wrote the first
documented program to play a full game of standard chess
@Bernstein+Roberts:1958.[^5]

The first computer chess match featured the Kotok–McCarthy program from
MIT @Kotok:1962 and the program written in the mid-1960s at Moscow’s
Institute of Theoretical and Experimental Physics
@Adelson-Velsky+al:1970. This intercontinental match was played by
telegraph. It ended with a 3–1 victory for the program in 1967. The
first chess program to compete successfully with humans was MIT’s
@Greenblatt+al:1967. Its Elo rating of approximately 1400 was well above
the novice level of 1000.

[mccarthy-kotok-figure]

The Fredkin Prize, established in 1980, offered awards for progressive
milestones in chess play. The 5,000 prize for the first program to
achieve a master rating went to @Condon+Thompson:1982, which achieved a
rating of 2250. The 10,000 prize for the first program to achieve a USCF
(United States Chess Federation) rating of 2500 (near the grandmaster
level) was awarded to @Hsu+al:1990 in 1989. The grand prize, 100,000,
went to @Campbell+al:2002 [@Hsu:2004] for its landmark victory over
world champion Garry Kasparov in a 1997 exhibition match. Kasparov
wrote:

The decisive game of the match was Game 2, which left a scar in my
memory $\ldots$ we saw something that went well beyond our wildest
expectations of how well a computer would be able to foresee the
long-term positional consequences of its decisions. The machine refused
to move to a position that had a decisive short-term advantage—showing a
very human sense of danger. @Kasparov:1997

Probably the most complete description of a modern chess program is
provided by Ernst , whose program was the highest-ranked noncommercial
PC program at the 1999 world championships.

In recent years, chess programs are pulling ahead of even the world’s
best humans. In 2004–2005 defeated grand master Evgeny Vladimirov
3.5–0.5, world champion Ruslan Ponomariov 2–0, and seventh-ranked
Michael Adams 5.5–0.5. In 2006, beat world champion Vladimir Kramnik
4–2, and in 2007 defeated several grand masters in games in which it
gave odds (such as a pawn) to the human players. As of 2009, the highest
Elo rating ever recorded was Kasparov’s 2851. @Donninger+Lorenz:2004 is
rated somewhere between 2850 and 3000, based mostly on its trouncing of
Michael Adams. The program is rated between 2900 and 3100, but this is
based on a small number of games and is not considered reliable. shows
how human players have learned to exploit some of the weaknesses of the
computer programs.

was the first of the classic games fully played by a computer.
Christopher Strachey [-@Strachey:1952] wrote the first working program
for checkers. Beginning in 1952, Arthur Samuel of IBM, working in his
spare time, developed a checkers program that learned its own evaluation
function by playing itself thousands of times @Samuel:1959
[@Samuel:1967]. We describe this idea in more detail in . Samuel’s
program began as a novice but after only a few days’ self-play had
improved itself beyond Samuel’s own level. In 1962 it defeated Robert
Nealy, a champion at “blind checkers,” through an error on his part.
When one considers that Samuel’s computing equipment (an IBM 704) had
10,000 words of main memory, magnetic tape for long-term storage, and a
.000001 GHz processor, the win remains a great accomplishment.

The challenge started by Samuel was taken up by Jonathan Schaeffer of
the University of Alberta. His program came in second in the 1990
U.S. Open and earned the right to challenge for the world championship.
It then ran up against a problem, in the form of Marion Tinsley.
Dr. Tinsley had been world champion for over 40 years, losing only three
games in all that time. In the first match against , Tinsley suffered
his fourth and fifth losses, but won the match 20.5–18.5. A rematch at
the 1994 world championship ended prematurely when Tinsley had to
withdraw for health reasons. became the official world champion.
Schaeffer kept on building on his database of endgames, and in 2007
“solved” checkers @Schaeffer+al:2007 [@Schaeffer:2008]. This had been
predicted by Richard . In the paper that introduced the dynamic
programming approach to retrograde analysis, he wrote, “In checkers, the
number of possible moves in any given situation is so small that we can
confidently expect a complete digital computer solution to the problem
of optimal play in this game.” Bellman did not, however, fully
appreciate the size of the checkers game tree. There are about 500
quadrillion positions. After 18 years of computation on a cluster of 50
or more machines, Jonathan Schaeffer’s team completed an endgame table
for all checkers positions with 10 or fewer pieces: over 39 trillion
entries. From there, they were able to do forward alpha–beta search to
derive a policy that proves that checkers is in fact a draw with best
play by both sides. Note that this is an application of bidirectional
search (). Building an endgame table for all of checkers would be
impractical: it would require a billion gigabytes of storage. Searching
without any table would also be impractical: the search tree has about
$8^{47}$ positions, and would take thousands of years to search with
today’s technology. Only a combination of clever search, endgame data,
and a drop in the price of processors and memory could solve checkers.
Thus, checkers joins @Patashnik:1980, @Allis:1988, and  @Gasser:1998 as
games that have been solved by computer analysis.

, a game of chance, was analyzed mathematically by Gerolamo Cardano
[-@Cardano:1663], but only taken up for computer play in the late 1970s,
first with the BKG program @Berliner:1980; it used a complex, manually
constructed evaluation function and searched only to depth 1. It was the
first program to defeat a human world champion at a major classic
game @Berliner:1980a. Berliner readily acknowledged that BKG was very
lucky with the dice. Gerry Tesauro’s [-@Tesauro:1995] played
consistently at world champion level. The program was the winner of the
2008 Computer Olympiad.

is a deterministic game, but the large branching factor makes it
challeging. The key issues and early literature in computer Go are
summarized by  and . Up to 1997 there were no competent Go programs. Now
the best programs play *most* of their moves at the master
level; the only problem is that over the course of a game they usually
make at least one serious blunder that allows a strong opponent to win.
Whereas alpha–beta search reigns in most games, many recent Go programs
have adopted Monte Carlo methods based on the UCT (upper confidence
bounds on trees) scheme @Kocsis+Szepesvari:2006. The strongest Go
program as of 2009 is Gelly and Silver’s @Wang+Gelly:2007
[@Gelly+Silver:2008]. In August 2008, scored a surprising win against
top professional Myungwan Kim, albeit with receiving a handicap of nine
stones (about the equivalent of a queen handicap in chess). Kim
estimated ’s strength at 2–3 dan, the low end of advanced amateur. For
this match, was run on an 800-processor 15 teraflop supercomputer (1000
times Deep Blue). A few weeks later, , with only a five-stone handicap,
won against a 6-dan professional. In the $9\stimes 9$ form of Go, is at
approximately the 1-dan professional level. Rapid advances are likely as
experimentation continues with new forms of Monte Carlo search. The
*Computer Go Newsletter*, published by the Computer Go
Association, describes current developments.

: report on how their planning-based program won the 1998 computer
bridge championship, and @Ginsberg:2001 describes how his program, based
on Monte Carlo simulation, won the following computer championship and
did surprisingly well against human players and standard book problem
sets. From 2001–2007, the computer bridge championship was won five
times by and twice by . Neither has had academic articles explaining
their structure, but both are rumored to use the Monte Carlo technique,
which was first proposed for bridge by .

: A good description of a top program, , is given by its creator, Brian
Sheppard [-@Sheppard:2002]. Generating the highest-scoring move is
described by , and modeling opponents is covered by .

@Kitano+al:1997 [@Visser+al:2008] and @Lam+Greenspan:2008
[@Archibald+al:2009] and other stochastic games with a continuous space
of actions are beginning to attract attention in AI, both in simulation
and with physical robot players.

Computer game competitions occur annually, and papers appear in a
variety of venues. The rather misleadingly named conference proceedings
*Heuristic Programming in Artificial Intelligence* report
on the Computer Olympiads, which include a wide variety of games. The
General Game Competition @Love+al:2006 tests programs that must learn to
play an unknown game given only a logical description of the rules of
the game. There are also several edited collections of important papers
on game-playing research @Levy:1988
[@Levy:1988a; @Marsland+Schaeffer:1990]. The International Computer
Chess Association (ICCA), founded in 1977, publishes the *ICGA
Journal* (formerly the *ICCA Journal*). Important
papers have been published in the serial anthology *Advances in
Computer Chess*, starting with . Volume 134 of the journal
*Artificial Intelligence* (2002) contains descriptions of
state-of-the-art programs for chess, Othello, Hex, shogi, Go,
backgammon, poker, Scrabble, and other games. Since 1998, a biennial
*Computers and Games* conference has been held.

Suppose you have an oracle, $OM(s)$, that correctly predicts the
opponent’s move in any state. Using this, formulate the definition of a
game as a (single-agent) search problem. Describe an algorithm for
finding the optimal move.

Consider the problem of solving two 8-puzzles.

1.  Give a complete problem formulation in the style of .

2.  How large is the reachable state space? Give an exact numerical
    expression.

3.  Suppose we make the problem adversarial as follows: the two players
    take turns moving; a coin is flipped to determine the puzzle on
    which to make a move in that turn; and the winner is the first to
    solve one puzzle. Which algorithm can be used to choose a move in
    this setting?

4.  Give an informal proof that someone will eventually win if both play
    perfectly.

[pursuit-evasion-game-figure]

Imagine that, in , one of the friends wants to avoid the other. The
problem then becomes a two-player game. We assume now that the players
take turns moving. The game ends only when the players are on the same
node; the terminal payoff to the pursuer is minus the total time taken.
(The evader “wins” by never losing.) An example is shown in .

1.  Copy the game tree and mark the values of the terminal nodes.

2.  Next to each internal node, write the strongest fact you can infer
    about its value (a number, one or more inequalities such as
    “$\geq 14$”, or a “?”).

3.  Beneath each question mark, write the name of the node reached by
    that branch.

4.  Explain how a bound on the value of the nodes in (c) can be derived
    from consideration of shortest-path lengths on the map, and derive
    such bounds for these nodes. Remember the cost to get to each leaf
    as well as the cost to solve it.

5.  Now suppose that the tree as given, with the leaf bounds from (d),
    is evaluated from left to right. Circle those “?” nodes that would
    *not* need to be expanded further, given the bounds
    from part (d), and cross out those that need not be considered at
    all.

6.  Can you prove anything in general about who wins the game on a map
    that is a tree?

[game-playing-chance-exercise]Describe and implement state descriptions,
move generators, terminal tests, utility functions, and evaluation
functions for one or more of the following stochastic games: Monopoly,
Scrabble, bridge play with a given contract, or Texas hold’em poker.

and implement a *real-time*, *multiplayer*
game-playing environment, where time is part of the environment state
and players are given fixed time allocations.

Discuss how well the standard approach to game playing would apply to
games such as tennis, pool, and croquet, which take place in a
continuous physical state space.

[minimax-optimality-exercise] Prove the following assertion: For every
game tree, the utility obtained by max using minimax
decisions against a suboptimal min will be never be lower
than the utility obtained playing against an optimal min.
Can you come up with a game tree in which max can do still
better using a *suboptimal* strategy against a suboptimal
min?

[line-game4-figure]

Consider the two-player game described in .

1.  Draw the complete game tree, using the following conventions:

    -   Write each state as $(s_A,s_B)$, where $s_A$ and $s_B$ denote
        the token locations.

    -   Put each terminal state in a square box and write its game value
        in a circle.

    -   Put *loop states* (states that already appear on
        the path to the root) in double square boxes. Since their value
        is unclear, annotate each with a “?” in a circle.

2.  Now mark each node with its backed-up minimax value (also in a
    circle). Explain how you handled the “?” values and why.

3.  Explain why the standard minimax algorithm would fail on this game
    tree and briefly sketch how you might fix it, drawing on your answer
    to (b). Does your modified algorithm give optimal decisions for all
    games with loops?

4.  This 4-square game can be generalized to $n$ squares for any
    $n > 2$. Prove that $A$ wins if $n$ is even and loses if $n$ is odd.

This problem exercises the basic concepts of game playing, using
(noughts and crosses) as an example. We define $X_n$ as the number of
rows, columns, or diagonals with exactly $n$ $X$’s and no $O$’s.
Similarly, $O_n$ is the number of rows, columns, or diagonals with just
$n$ $O$’s. The utility function assigns $+1$ to any position with
$X_3=1$ and $-1$ to any position with $O_3 = 1$. All other terminal
positions have utility 0. For nonterminal positions, we use a linear
evaluation function defined as ${Eval}(s) = 3X_2(s) + X_1(s) -
(3O_2(s) + O_1(s))$.

1.  Approximately how many possible games of tic-tac-toe are there?

2.  Show the whole game tree starting from an empty board down to depth
    2 (i.e., one $X$ and one $O$ on the board), taking symmetry into
    account.

3.  Mark on your tree the evaluations of all the positions at depth 2.

4.  Using the minimax algorithm, mark on your tree the backed-up values
    for the positions at depths 1 and 0, and use those values to choose
    the best starting move.

5.  Circle the nodes at depth 2 that would *not* be
    evaluated if alpha–beta pruning were applied, assuming the nodes are
    generated in the optimal order for alpha–beta pruning.

Consider the family of generalized tic-tac-toe games, defined as
follows. Each particular game is specified by a set $\mathcal S$ of
*squares* and a collection $\mathcal W$ of *winning
positions.* Each winning position is a subset of $\mathcal S$.
For example, in standard tic-tac-toe, $\mathcal S$ is a set of 9 squares
and $\mathcal W$ is a collection of 8 subsets of $\cal W$: the three
rows, the three columns, and the two diagonals. In other respects, the
game is identical to standard tic-tac-toe. Starting from an empty board,
players alternate placing their marks on an empty square. A player who
marks every square in a winning position wins the game. It is a tie if
all squares are marked and neither player has won.

1.  Let $N= |{\mathcal S}|$, the number of squares. Give an upper bound
    on the number of nodes in the complete game tree for generalized
    tic-tac-toe as a function of $N$.

2.  Give a lower bound on the size of the game tree for the worst case,
    where ${\mathcal W} = \emptyset$.

3.  Propose a plausible evaluation function that can be used for any
    instance of generalized tic-tac-toe. The function may depend on
    $\mathcal S$ and $\mathcal W$.

4.  Assume that it is possible to generate a new board and check whether
    it is a winning position in 100$N$ machine instructions and assume a
    2 gigahertz processor. Ignore memory limitations. Using your
    estimate in (a), roughly how large a game tree can be completely
    solved by alpha–beta in a second of CPU time? a minute? an hour?

Develop a general game-playing program, capable of playing a variety of
games.

1.  Implement move generators and evaluation functions for one or more
    of the following games: Kalah, Othello, checkers, and chess.

2.  Construct a general alpha–beta game-playing agent.

3.  Compare the effect of increasing search depth, improving move
    ordering, and improving the evaluation function. How close does your
    effective branching factor come to the ideal case of perfect move
    ordering?

4.  Implement a selective search algorithm, such as B\* @Berliner:1979,
    conspiracy number search @McAllester:1988, or MGSS\*
    @Russell+Wefald:1989 and compare its performance to A\*.

Describe how the minimax and alpha–beta algorithms change for
two-player, non-zero-sum games in which each player has a distinct
utility function and both utility functions are known to both players.
If there are no constraints on the two terminal utilities, is it
possible for any node to be pruned by alpha–beta? What if the player’s
utility functions on any state differ by at most a constant $k$, making
the game almost cooperative?

Describe how the minimax and alpha–beta algorithms change for
two-player, non-zero-sum games in which each player has a distinct
utility function and both utility functions are known to both players.
If there are no constraints on the two terminal utilities, is it
possible for any node to be pruned by alpha–beta? What if the player’s
utility functions on any state sum to a number between constants $-k$
and $k$, making the game almost zero-sum?

Develop a formal proof of correctness for alpha–beta pruning. To do
this, consider the situation shown in . The question is whether to prune
node $n_j$, which is a max-node and a descendant of node $n_1$. The
basic idea is to prune it if and only if the minimax value of $n_1$ can
be shown to be independent of the value of $n_j$.

1.  Mode $n_1$ takes on the minimum value among its children:
    $n_1 = \min(n_2,n_{{21}},\ldots,n_{2b_2})$. Find a similar
    expression for $n_2$ and hence an expression for $n_1$ in terms of
    $n_j$.

2.  Let $l_i$ be the minimum (or maximum) value of the nodes to the
    *left* of node $n_i$ at depth $i$, whose minimax value
    is already known. Similarly, let $r_i$ be the minimum (or maximum)
    value of the unexplored nodes to the right of $n_i$ at depth $i$.
    Rewrite your expression for $n_1$ in terms of the $l_i$ and $r_i$
    values.

3.  Now reformulate the expression to show that in order to affect
    $n_1$, $n_j$ must not exceed a certain bound derived from the $l_i$
    values.

4.  Repeat the process for the case where $n_j$ is a min-node.

[alpha-beta-proof-figure]

Prove that alpha–beta pruning takes time $O(2^{m/2})$ with optimal move
ordering, where $m$ is the maximum depth of the game tree.

Suppose you have a chess program that can evaluate 5 million nodes per
second. Decide on a compact representation of a game state for storage
in a transposition table. About how many entries can you fit in a
1-gigabyte in-memory table? Will that be enough for the three minutes of
search allocated for one move? How many table lookups can you do in the
time it would take to do one evaluation? Now suppose the transposition
table is stored on disk. About how many evaluations could you do in the
time it takes to do one disk seek with standard disk hardware?

Suppose you have a chess program that can evaluate 10 million nodes per
second. Decide on a compact representation of a game state for storage
in a transposition table. About how many entries can you fit in a
2-gigabyte in-memory table? Will that be enough for the three minutes of
search allocated for one move? How many table lookups can you do in the
time it would take to do one evaluation? Now suppose the transposition
table is stored on disk. About how many evaluations could you do in the
time it takes to do one disk seek with standard disk hardware?

[trivial-chance-game-figure]

This question considers pruning in games with chance nodes. shows the
complete game tree for a trivial game. Assume that the leaf nodes are to
be evaluated in left-to-right order, and that before a leaf node is
evaluated, we know nothing about its value—the range of possible values
is $-\infty$ to $\infty$.

1.  Copy the figure, mark the value of all the internal nodes, and
    indicate the best move at the root with an arrow.

2.  Given the values of the first six leaves, do we need to evaluate the
    seventh and eighth leaves? Given the values of the first seven
    leaves, do we need to evaluate the eighth leaf? Explain your
    answers.

3.  Suppose the leaf node values are known to lie between –2 and 2
    inclusive. After the first two leaves are evaluated, what is the
    value range for the left-hand chance node?

4.  Circle all the leaves that need not be evaluated under the
    assumption in (c).

the expectiminimax algorithm and the \*-alpha–beta algorithm, which is
described by , for pruning game trees with chance nodes. Try them on a
game such as backgammon and measure the pruning effectiveness of
\*-alpha–beta.

[game-linear-transform] Prove that with a positive linear transformation
of leaf values (i.e., transforming a value $x$ to $ax + b$ where
$a > 0$), the choice of move remains unchanged in a game tree, even when
there are chance nodes.

[game-playing-monte-carlo-exercise]Consider the following procedure for
choosing moves in games with chance nodes:

-   Generate some dice-roll sequences (say, 50) down to a suitable depth
    (say, 8).

-   With known dice rolls, the game tree becomes deterministic. For each
    dice-roll sequence, solve the resulting deterministic game tree
    using alpha–beta.

-   Use the results to estimate the value of each move and to choose the
    best.

Will this procedure work well? Why (or why not)?

In the following, a “max” tree consists only of max nodes, whereas an
“expectimax” tree consists of a max node at the root with alternating
layers of chance and max nodes. At chance nodes, all outcome
probabilities are nonzero. The goal is to *find the value of the
root* with a bounded-depth search. For each of (a)–(f), either
give an example or explain why this is impossible.

1.  Assuming that leaf values are finite but unbounded, is pruning (as
    in alpha–beta) ever possible in a max tree?

2.  Is pruning ever possible in an expectimax tree under the same
    conditions?

3.  If leaf values are all nonnegative, is pruning ever possible in a
    max tree? Give an example, or explain why not.

4.  If leaf values are all nonnegative, is pruning ever possible in an
    expectimax tree? Give an example, or explain why not.

5.  If leaf values are all in the range $[0,1]$, is pruning ever
    possible in a max tree? Give an example, or explain why not.

6.  If leaf values are all in the range $[0,1]$, is pruning ever
    possible in an expectimax tree?

7.  Consider the outcomes of a chance node in an expectimax tree. Which
    of the following evaluation orders is most likely to yield pruning
    opportunities?

    1.  Lowest probability first

    2.  Highest probability first

    3.  Doesn’t make any difference

In the following, a “max” tree consists only of max nodes, whereas an
“expectimax” tree consists of a max node at the root with alternating
layers of chance and max nodes. At chance nodes, all outcome
probabilities are nonzero. The goal is to *find the value of the
root* with a bounded-depth search.

1.  Assuming that leaf values are finite but unbounded, is pruning (as
    in alpha–beta) ever possible in a max tree? Give an example, or
    explain why not.

2.  Is pruning ever possible in an expectimax tree under the same
    conditions? Give an example, or explain why not.

3.  If leaf values are constrained to be in the range $[0,1]$, is
    pruning ever possible in a max tree? Give an example, or explain why
    not.

4.  If leaf values are constrained to be in the range $[0,1]$, is
    pruning ever possible in an expectimax tree? Give an example
    (qualitatively different from your example in (e), if any), or
    explain why not.

5.  If leaf values are constrained to be nonnegative, is pruning ever
    possible in a max tree? Give an example, or explain why not.

6.  If leaf values are constrained to be nonnegative, is pruning ever
    possible in an expectimax tree? Give an example, or explain why not.

7.  Consider the outcomes of a chance node in an expectimax tree. Which
    of the following evaluation orders is most likely to yield pruning
    opportunities: (i) Lowest probability first; (ii) Highest
    probability first; (iii) Doesn’t make any difference?

Which of the following are true and which are false? Give brief
explanations.

1.  In a fully observable, turn-taking, zero-sum game between two
    perfectly rational players, it does not help the first player to
    know what strategy the second player is using—that is, what move the
    second player will make, given the first player’s move.

2.  In a partially observable, turn-taking, zero-sum game between two
    perfectly rational players, it does not help the first player to
    know what move the second player will make, given the first player’s
    move.

3.  A perfectly rational backgammon agent never loses.

Consider carefully the interplay of chance events and partial
information in each of the games in .

1.  For which is the standard expectiminimax model appropriate?
    Implement the algorithm and run it in your game-playing agent, with
    appropriate modifications to the game-playing environment.

2.  For which would the scheme described in be appropriate?

3.  Discuss how you might deal with the fact that in some of the games,
    the players do not have the same knowledge of the current state.

[^1]: Environments with very many agents are often viewed as rather than
    games.

[^2]: Obviously, it cannot be done perfectly; otherwise, the ordering
    function could be used to play a perfect game!

[^3]: Sometimes, the belief state will become too large to represent
    just as a list of board states, but we will ignore this issue for
    now; Chapters [knowledge+logic-chapter] and [fol-chapter] suggest
    methods for compactly representing very large belief states.

[^4]: Bluffing—betting as if one’s hand is good, even when it’s not—is a
    core part of poker strategy.

[^5]: A Russian program, may have predated Bernstein’s program.
Knowledge in Learning {#ilp-chapter}
=====================

In all of the approaches to learning described in the previous chapter,
the idea is to construct a function that has the input–output behavior
observed in the data. In each case, the learning methods can be
understood as searching a hypothesis space to find a suitable function,
starting from only a very basic assumption about the form of the
function, such as “second-degree polynomial” or “decision tree” and
perhaps a preference for simpler hypotheses. Doing this amounts to
saying that before you can learn something new, you must first forget
(almost) everything you know. In this chapter, we study learning methods
that can take advantage of about the world. In most cases, the prior
knowledge is represented as general first-order logical theories; thus
for the first time we bring together the work on knowledge
representation and learning.

A Logical Formulation of Learning {#logical-learning-section}
---------------------------------

defined pure inductive learning as a process of finding a hypothesis
that agrees with the observed examples. Here, we specialize this
definition to the case where the hypothesis is represented by a set of
logical sentences. Example descriptions and classifications will also be
logical sentences, and a new example can be classified by inferring a
classification sentence from the hypothesis and the example description.
This approach allows for incremental construction of hypotheses, one
sentence at a time. It also allows for prior knowledge, because
sentences that are already known can assist in the classification of new
examples. The logical formulation of learning may seem like a lot of
extra work at first, but it turns out to clarify many of the issues in
learning. It enables us to go well beyond the simple learning methods of
by using the full power of logical inference in the service of learning.

### Examples and hypotheses

Recall from the restaurant learning problem: learning a rule for
deciding whether to wait for a table. Examples were described by such as
Ǎlternate, Bar, Fri/Sat, and so on. In a logical setting, an example is
described by a logical sentence; the attributes become unary predicates.
Let us generically call the $i$th example $X_i$. For instance, the first
example from () is described by the sentences
$${Alternate}(X_1) \land \lnot {Bar}(X_1) \land \lnot {Fri}/{Sat}(X_1)
\land {Hungry}(X_1) \land \ldots$$ We will use the notation $D_i(X_i)$
to refer to the description of $X_i$, where $D_i$ can be any logical
expression taking a single argument. The classification of the example
is given by a literal using the goal predicate, in this case
$${WillWait}(X_1) \qquad\mbox{~or~}\qquad \lnot {WillWait}(X_1)\ .$$
The complete training set can thus be expressed as the conjunction of
all the example descriptions and goal literals.

The aim of inductive learning in general is to find a hypothesis that
classifies the examples well and generalizes well to new examples. Here
we are concerned with hypotheses expressed in logic; each hypothesis
$h_j$ will have the form $$\All{x} {Goal}(x) \lequiv C_j(x) \ ,$$
where $C_j(x)$ is a candidate definition—some expression involving the
attribute predicates. For example, a decision tree can be interpreted as
a logical expression of this form. Thus, the tree in () expresses the
following logical definition (which we will call $h_r$ for future
reference):

$$\begin{array}{rrl}
\All{r} {WillWait}(r) 
& \Leftrightarrow & {Patrons}(r,{Some}) \\
& \lor & {Patrons}(r,{Full}) \land {Hungry}(r) \land {Type}(r,{French}) \\
& \lor & {Patrons}(r,{Full}) \land {Hungry}(r) \land {Type}(r,{Thai}) \\
&     & \qquad {} \land {Fri}/{Sat}(r)\\
& \lor & {Patrons}(r,{Full}) \land {Hungry}(r) \land {Type}(r,{Burger}) \ .
\end{array}
\label{logical-tree-equation}$$

Each hypothesis predicts that a certain set of examples—namely, those
that satisfy its candidate definition—will be examples of the goal
predicate. This set is called the of the predicate. Two hypotheses with
different extensions are therefore logically inconsistent with each
other, because they disagree on their predictions for at least one
example. If they have the same extension, they are logically equivalent.

The hypothesis space $\Hyp$ is the set of all hypotheses
$\{h_1,\ldots,h_n\}$ that the learning algorithm is designed to
entertain. For example, the algorithm can entertain any decision tree
hypothesis defined in terms of the attributes provided; its hypothesis
space therefore consists of all these decision trees. Presumably, the
learning algorithm believes that one of the hypotheses is correct; that
is, it believes the sentence

$$h_1 \lor h_2 \lor h_3 \lor \ldots \lor h_n\ .
\label{bias-sentence}$$

As the examples arrive, hypotheses that are not with the examples can be
ruled out. Let us examine this notion of consistency more carefully.
Obviously, if hypothesis $h_j$ is consistent with the entire training
set, it has to be consistent with each example in the training set. What
would it mean for it to be inconsistent with an example? There are two
possible ways that this can happen:

-   An example can be a for the hypothesis, if the hypothesis says it
    should be negative but in fact it is positive. For instance, the new
    example $X_{{13}}$ described by\
    ${Patrons}(X_{{13}},{Full}) \land 
    \lnot {Hungry}(X_{{13}}) \land \ldots \land {WillWait}(X_{{13}})$\
    would be a false negative for the hypothesis $h_r$ given earlier.
    From $h_r$ and the example description, we can deduce both
    ${WillWait}(X_{{13}})$, which is what the example says, and $\lnot
    {WillWait}(X_{{13}})$, which is what the hypothesis predicts. The
    hypothesis and the example are therefore logically inconsistent.

-   An example can be a for the hypothesis, if the hypothesis says it
    should be positive but in fact it is negative.[^1]

If an example is a false positive or false negative for a hypothesis,
then the example and the hypothesis are logically inconsistent with each
other. Assuming that the example is a correct observation of fact, then
the hypothesis can be ruled out. Logically, this is exactly analogous to
the resolution rule of inference (see ), where the disjunction of
hypotheses corresponds to a clause and the example corresponds to a
literal that resolves against one of the literals in the clause. An
ordinary logical inference system therefore could, in principle, learn
from the example by eliminating one or more hypotheses. Suppose, for
example, that the example is denoted by the sentence $I_1$, and the
hypothesis space is $h_1 \lor h_2 \lor h_3 \lor h_4$. Then if $I_1$ is
inconsistent with $h_2$ and $h_3$, the logical inference system can
deduce the new hypothesis space $h_1 \lor h_4$.

We therefore can characterize inductive learning in a logical setting as
a process of gradually eliminating hypotheses that are inconsistent with
the examples, narrowing down the possibilities. Because the hypothesis
space is usually vast (or even infinite in the case of first-order
logic), we do not recommend trying to build a learning system using
resolution-based theorem proving and a complete enumeration of the
hypothesis space. Instead, we will describe two approaches that find
logically consistent hypotheses with much less effort.

### Current-best-hypothesis search

The idea behind search is to maintain a single hypothesis, and to adjust
it as new examples arrive in order to maintain consistency. The basic
algorithm was described by John Stuart Mill [-@Mill:1843], and may well
have appeared even earlier.

Suppose we have some hypothesis such as $h_r$, of which we have grown
quite fond. As long as each new example is consistent, we need do
nothing. Then along comes a false negative example, $X_{{13}}$. What do
we do? (a) shows $h_r$ schematically as a region: everything inside the
rectangle is part of the extension of $h_r$. The examples that have
actually been seen so far are shown as “+” or “–”, and we see that $h_r$
correctly categorizes all the examples as positive or negative examples
of ${WillWait}$. In (b), a new example (circled) is a false negative:
the hypothesis says it should be negative but it is actually positive.
The extension of the hypothesis must be increased to include it. This is
called ; one possible generalization is shown in (c). Then in (d), we
see a false positive: the hypothesis says the new example (circled)
should be positive, but it actually is negative. The extension of the
hypothesis must be decreased to exclude the example. This is called ; in
(e) we see one possible specialization of the hypothesis. The “more
general than” and “more specific than” relations between hypotheses
provide the logical structure on the hypothesis space that makes
efficient search possible.

[cbh-figure]

We can now specify the algorithm, shown in . Notice that each time we
consider generalizing or specializing the hypothesis, we must check for
consistency with the other examples, because an arbitrary
increase/decrease in the extension might include/exclude previously seen
negative/positive examples.

[cbh-algorithm]

We have defined generalization and specialization as operations that
change the *extension* of a hypothesis. Now we need to
determine exactly how they can be implemented as syntactic operations
that change the candidate definition associated with the hypothesis, so
that a program can carry them out. This is done by first noting that
generalization and specialization are also *logical*
relationships between hypotheses. If hypothesis $h_1$, with definition
$C_1$, is a generalization of hypothesis $h_2$ with definition $C_2$,
then we must have $$\All{x} C_2(x) \implies C_1(x)\ .$$ Therefore in
order to construct a generalization of $h_2$, we simply need to find a
definition $C_1$ that is logically implied by $C_2$. This is easily
done. For example, if $C_2(x)$ is
${Alternate}(x) \land {Patrons}(x,{Some})$, then one possible
generalization is given by $C_1(x)
\equiv {Patrons}(x,{Some})$. This is called . Intuitively, it
generates a weaker definition and therefore allows a larger set of
positive examples. There are a number of other generalization
operations, depending on the language being operated on. Similarly, we
can specialize a hypothesis by adding extra conditions to its candidate
definition or by removing disjuncts from a disjunctive definition. Let
us see how this works on the restaurant example, using the data in .

-   The first example, $X_1$, is positive. The attribute
    ${Alternate}(X_1)$ is true, so let the initial hypothesis be
    $$h_1:\;\; \All{x} {WillWait}(x) \lequiv {Alternate}(x)\ .$$

-   The second example, $X_2$, is negative. $h_1$ predicts it to be
    positive, so it is a false positive. Therefore, we need to
    specialize $h_1$. This can be done by adding an extra condition that
    will rule out $X_2$, while continuing to classify $X_1$ as positive.
    One possibility is
    $$h_2:\;\; \All{x} {WillWait}(x) \lequiv {Alternate}(x) \land {Patrons}(x,{Some})\ .$$

-   The third example, $X_3$, is positive. $h_2$ predicts it to be
    negative, so it is a false negative. Therefore, we need to
    generalize $h_2$. We drop the ${Alternate}$ condition, yielding
    $$h_3:\;\; \All{x} {WillWait}(x) \lequiv {Patrons}(x,{Some})\ .$$

-   The fourth example, $X_4$, is positive. $h_3$ predicts it to be
    negative, so it is a false negative. We therefore need to generalize
    $h_3$. We cannot drop the ${Patrons}$ condition, because that
    would yield an all-inclusive hypothesis that would be inconsistent
    with $X_2$. One possibility is to add a disjunct:

    h~4~: (x) (x,)\
    ((x,) /(x))  .

Already, the hypothesis is starting to look reasonable. Obviously, there
are other possibilities consistent with the first four examples; here
are two of them:

h~4~’: (x) (x,)  .\
\
h~4~”: (x) (x,)\
((x,) (x,))  .

The algorithm is described nondeterministically, because at any point,
there may be several possible specializations or generalizations that
can be applied. The choices that are made will not necessarily lead to
the simplest hypothesis, and may lead to an unrecoverable situation
where no simple modification of the hypothesis is consistent with all of
the data. In such cases, the program must backtrack to a previous choice
point.

The algorithm and its variants have been used in many machine learning
systems, starting with Patrick Winston’s [-@Winston:1970]
“arch-learning” program. With a large number of examples and a large
space, however, some difficulties arise:

1.  Checking all the previous examples over again for each modification
    is very expensive.

2.  The search process may involve a great deal of backtracking. As we
    saw in , hypothesis space can be a doubly exponentially large place.

### Least-commitment search

Backtracking arises because the current-best-hypothesis approach has to
*choose* a particular hypothesis as its best guess even
though it does not have enough data yet to be sure of the choice. What
we can do instead is to keep around all and only those hypotheses that
are consistent with all the data so far. Each new example will either
have no effect or will get rid of some of the hypotheses. Recall that
the original hypothesis space can be viewed as a disjunctive sentence
$$h_1 \lor h_2 \lor h_3 \ldots \lor h_n\ .$$ As various hypotheses are
found to be inconsistent with the examples, this disjunction shrinks,
retaining only those hypotheses not ruled out. Assuming that the
original hypothesis space does in fact contain the right answer, the
reduced disjunction must still contain the right answer because only
incorrect hypotheses have been removed. The set of hypotheses remaining
is called the , and the learning algorithm (sketched in ) is called the
version space learning algorithm (also the algorithm).

[vsl-algorithm]

One important property of this approach is that it is
*incremental*: one never has to go back and reexamine the
old examples. All remaining hypotheses are guaranteed to be consistent
with them already. But there is an obvious problem. We already said that
the hypothesis space is enormous, so how can we possibly write down this
enormous disjunction?

The following simple analogy is very helpful. How do you represent all
the real numbers between 1 and 2? After all, there are an infinite
number of them! The answer is to use an interval representation that
just specifies the boundaries of the set: [1,2]. It works because we
have an *ordering* on the real numbers.

We also have an ordering on the hypothesis space, namely,
generalization/specialization. This is a partial ordering, which means
that each boundary will not be a point but rather a set of hypotheses
called a . The great thing is that we can represent the entire version
space using just two boundary sets: a most general boundary (the ) and a
most specific boundary (the ). *Everything in between is
guaranteed to be consistent with the examples*. Before we prove
this, let us recap:

-   The current version space is the set of hypotheses consistent with
    all the examples so far. It is represented by the S-set and G-set,
    each of which is a set of hypotheses.

-   Every member of the S-set is consistent with all observations so
    far, and there are no consistent hypotheses that are more specific.

-   Every member of the G-set is consistent with all observations so
    far, and there are no consistent hypotheses that are more general.

We want the initial version space (before any examples have been seen)
to represent all possible hypotheses. We do this by setting the G-set to
contain ${True}$ (the hypothesis that contains everything), and the
S-set to contain ${False}$ (the hypothesis whose extension is empty).

[version-space-figure]

shows the general structure of the boundary-set representation of the
version space. To show that the representation is sufficient, we need
the following two properties:

1.  Every consistent hypothesis (other than those in the boundary sets)
    is more specific than some member of the G-set, and more general
    than some member of the S-set. (That is, there are no “stragglers”
    left outside.) This follows directly from the definitions of $S$ and
    $G$. If there were a straggler $h$, then it would have to be no more
    specific than any member of $G$, in which case it belongs in $G$; or
    no more general than any member of $S$, in which case it belongs in
    $S$.

2.  Every hypothesis more specific than some member of the G-set and
    more general than some member of the S-set is a consistent
    hypothesis. (That is, there are no “holes” between the boundaries.)
    Any $h$ between $S$ and $G$ must reject all the negative examples
    rejected by each member of $G$ (because it is more specific), and
    must accept all the positive examples accepted by any member of $S$
    (because it is more general). Thus, $h$ must agree with all the
    examples, and therefore cannot be inconsistent. shows the situation:
    there are no known examples outside $S$ but inside $G$, so any
    hypothesis in the gap must be consistent.

We have therefore shown that *if* $S$ and $G$ are
maintained according to their definitions, then they provide a
satisfactory representation of the version space. The only remaining
problem is how to *update* $S$ and $G$ for a new example
(the job of the function). This may appear rather complicated at first,
but from the definitions and with the help of , it is not too hard to
reconstruct the algorithm.

[vs-proof-figure]

We need to worry about the members $S_i$ and $G_i$ of the S- and G-sets.
For each one, the new example may be a false positive or a false
negative.

1.  False positive for $S_i$: This means $S_i$ is too general, but there
    are no consistent specializations of $S_i$ (by definition), so we
    throw it out of the S-set.

2.  False negative for $S_i$: This means $S_i$ is too specific, so we
    replace it by all its immediate generalizations, provided they are
    more specific than some member of $G$.

3.  False positive for $G_i$: This means $G_i$ is too general, so we
    replace it by all its immediate specializations, provided they are
    more general than some member of $S$.

4.  False negative for $G_i$: This means $G_i$ is too specific, but
    there are no consistent generalizations of $G_i$ (by definition) so
    we throw it out of the G-set.

We continue these operations for each new example until one of three
things happens:

1.  We have exactly one hypothesis left in the version space, in which
    case we return it as the unique hypothesis.

2.  The version space *collapses*—either S or G becomes
    empty, indicating that there are no consistent hypotheses for the
    training set. This is the same case as the failure of the simple
    version of the decision tree algorithm.

3.  We run out of examples and have several hypotheses remaining in the
    version space. This means the version space represents a disjunction
    of hypotheses. For any new example, if all the disjuncts agree, then
    we can return their classification of the example. If they disagree,
    one possibility is to take the majority vote.

We leave as an exercise the application of the algorithm to the
restaurant data.

There are two principal drawbacks to the version-space approach:

-   If the domain contains noise or insufficient attributes for exact
    classification, the version space will always collapse.

-   If we allow unlimited disjunction in the hypothesis space, the S-set
    will always contain a single most-specific hypothesis, namely, the
    disjunction of the descriptions of the positive examples seen to
    date. Similarly, the G-set will contain just the negation of the
    disjunction of the descriptions of the negative examples.

-   For some hypothesis spaces, the number of elements in the S-set or
    G-set may grow exponentially in the number of attributes, even
    though efficient learning algorithms exist for those hypothesis
    spaces.

To date, no completely successful solution has been found for the
problem of noise. The problem of disjunction can be addressed by
allowing only limited forms of disjunction or by including a of more
general predicates. For example, instead of using the disjunction
${WaitEstimate}(x,\mbox{{\it {30}-{60}}})
\lor {WaitEstimate}(x,{>}{60})$, we might use the single literal
${LongWait}(x)$. The set of generalization and specialization
operations can be easily extended to handle this.

The pure version space algorithm was first applied in the
Meta-Dendral system, which was designed to learn rules for
predicting how molecules would break into pieces in a mass spectrometer
@Buchanan+Mitchell:1978. Meta-Dendral was able to generate
rules that were sufficiently novel to warrant publication in a journal
of analytical chemistry—the first real scientific knowledge generated by
a computer program. It was also used in the elegant system
@Mitchell+al:1983, which was able to learn to solve symbolic integration
problems by studying its own successes and failures. Although version
space methods are probably not practical in most real-world learning
problems, mainly because of noise, they provide a good deal of insight
into the logical structure of hypothesis space.

Knowledge in Learning {#knowledge-in-learning}
---------------------

The preceding section described the simplest setting for inductive
learning. To understand the role of prior knowledge, we need to talk
about the logical relationships among hypotheses, example descriptions,
and classifications. Let ${Descriptions}$ denote the conjunction of
all the example descriptions in the training set, and let
${Classifications}$ denote the conjunction of all the example
classifications. Then a ${Hypothesis}$ that “explains the
observations” must satisfy the following property (recall that $\models$
means “logically entails”):

$${Hypothesis} \land {Descriptions} \models {Classifications}\ .
\label{kfil-equation}$$

We call this kind of relationship an , in which ${Hypothesis}$ is the
“unknown.” Pure inductive learning means solving this constraint, where
${Hypothesis}$ is drawn from some predefined hypothesis space. For
example, if we consider a decision tree as a logical formula (see on ),
then a decision tree that is consistent with all the examples will
satisfy . If we place *no* restrictions on the logical form
of the hypothesis, of course, then
${Hypothesis} = {Classifications}$ also satisfies the constraint.
Ockham’s razor tells us to prefer *small*, consistent
hypotheses, so we try to do better than simply memorizing the examples.

This simple knowledge-free picture of inductive learning persisted until
the early 1980s. The modern approach is to design agents that

already know something

and are trying to learn some more. This may not sound like a
terrifically deep insight, but it makes quite a difference to the way we
design agents. It might also have some relevance to our theories about
how science itself works. The general idea is shown schematically in .

[cumulative-learning-figure]

An autonomous learning agent that uses background knowledge must somehow
obtain the background knowledge in the first place, in order for it to
be used in the new learning episodes. This method must itself be a
learning process. The agent’s life history will therefore be
characterized by *cumulative*, or
*incremental*, development. Presumably, the agent could
start out with nothing, performing inductions *in vacuo*
like a good little pure induction program. But once it has eaten from
the Tree of Knowledge, it can no longer pursue such naive speculations
and should use its background knowledge to learn more and more
effectively. The question is then how to actually do this.

### Some simple examples

Let us consider some commonsense examples of learning with background
knowledge. Many apparently rational cases of inferential behavior in the
face of observations clearly do not follow the simple principles of pure
induction.

-   Sometimes one leaps to general conclusions after only one
    observation. Gary Larson once drew a cartoon in which a bespectacled
    caveman, Zog, is roasting his lizard on the end of a pointed stick.
    He is watched by an amazed crowd of his less intellectual
    contemporaries, who have been using their bare hands to hold their
    victuals over the fire. This enlightening experience is enough to
    convince the watchers of a general principle of painless cooking.

-   Or consider the case of the traveler to Brazil meeting her first
    Brazilian. On hearing him speak Portuguese, she immediately
    concludes that Brazilians speak Portuguese, yet on discovering that
    his name is Fernando, she does not conclude that all Brazilians are
    called Fernando. Similar examples appear in science. For example,
    when a freshman physics student measures the density and conductance
    of a sample of copper at a particular temperature, she is quite
    confident in generalizing those values to all pieces of copper. Yet
    when she measures its mass, she does not even consider the
    hypothesis that all pieces of copper have that mass. On the other
    hand, it would be quite reasonable to make such a generalization
    over all pennies.

-   Finally, consider the case of a pharmacologically ignorant but
    diagnostically sophisticated medical student observing a consulting
    session between a patient and an expert internist. After a series of
    questions and answers, the expert tells the patient to take a course
    of a particular antibiotic. The medical student infers the general
    rule that that particular antibiotic is effective for a particular
    type of infection.

These are all cases in which

the use of background knowledge allows much faster learning than one
might expect from a pure induction program.

### Some general schemes

In each of the preceding examples, one can appeal to prior knowledge to
try to justify the generalizations chosen. We will now look at what
kinds of entailment constraints are operating in each case. The
constraints will involve the ${Background}$ knowledge, in addition to
the ${Hypothesis}$ and the observed ${Descriptions}$ and
${Classifications}$.

In the case of lizard toasting, the cavemen generalize by
*explaining* the success of the pointed stick: it supports
the lizard while keeping the hand away from the fire. From this
explanation, they can infer a general rule: that any long, rigid, sharp
object can be used to toast small, soft-bodied edibles. This kind of
generalization process has been called , or . Notice that the general
rule *follows logically* from the background knowledge
possessed by the cavemen. Hence, the entailment constraints satisfied by
EBL are the following: $$\begin{array}{l}
{Hypothesis} \land {Descriptions} \models {Classifications} \\
{Background} \models {Hypothesis} \ .
\end{array}$$ Because EBL uses , it was initially thought to be a way to
learn from examples. But because it requires that the background
knowledge be sufficient to explain the ${Hypothesis}$, which in turn
explains the observations,

the agent does not actually learn anything factually new from the
example.

The agent *could have* derived the example from what it
already knew, although that might have required an unreasonable amount
of computation. EBL is now viewed as a method for converting
first-principles theories into useful, special-purpose knowledge. We
describe algorithms for EBL in .

The situation of our traveler in Brazil is quite different, for she
cannot necessarily explain why Fernando speaks the way he does, unless
she knows her papal bulls. Moreover, the same generalization would be
forthcoming from a traveler entirely ignorant of colonial history. The
relevant prior knowledge in this case is that, within any given country,
most people tend to speak the same language; on the other hand, Fernando
is not assumed to be the name of all Brazilians because this kind of
regularity does not hold for names. Similarly, the freshman physics
student also would be hard put to explain the particular values that she
discovers for the conductance and density of copper. She does know,
however, that the material of which an object is composed and its
temperature together determine its conductance. In each case, the prior
knowledge ${Background}$ concerns the of a set of features to the goal
predicate. This knowledge, *together with the
observations*, allows the agent to infer a new, general rule that
explains the observations:

We call this kind of generalization , or (although the name is not
standard). Notice that whereas RBL does make use of the content of the
observations, it does not produce hypotheses that go beyond the logical
content of the background knowledge and the observations. It is a
*deductive* form of learning and cannot by itself account
for the creation of new knowledge starting from scratch.

In the case of the medical student watching the expert, we assume that
the student’s prior knowledge is sufficient to infer the patient’s
disease $D$ from the symptoms. This is not, however, enough to explain
the fact that the doctor prescribes a particular medicine $M$. The
student needs to propose another rule, namely, that $M$ generally is
effective against $D$. Given this rule and the student’s prior
knowledge, the student can now explain why the expert prescribes $M$ in
this particular case. We can generalize this example to come up with the
entailment constraint

$${Background} \land {Hypothesis} \land {Descriptions} \models {Classifications}\ .
\label{kbil-equation}$$

That is,

the background knowledge and the new hypothesis combine to explain the
examples.

As with pure inductive learning, the learning algorithm should propose
hypotheses that are as simple as possible, consistent with this
constraint. Algorithms that satisfy constraint ([kbil-equation]) are
called , or , algorithms.

KBIL algorithms, which are described in detail in , have been studied
mainly in the field of , or . In ILP systems, prior knowledge plays two
key roles in reducing the complexity of learning:

1.  Because any hypothesis generated must be consistent with the prior
    knowledge as well as with the new observations, the effective
    hypothesis space size is reduced to include only those theories that
    are consistent with what is already known.

2.  For any given set of observations, the size of the hypothesis
    required to construct an explanation for the observations can be
    much reduced, because the prior knowledge will be available to help
    out the new rules in explaining the observations. The smaller the
    hypothesis, the easier it is to find.

In addition to allowing the use of prior knowledge in induction, ILP
systems can formulate hypotheses in general first-order logic, rather
than in the restricted attribute-based language of . This means that
they can learn in environments that cannot be understood by simpler
systems.

Explanation-Based Learning {#ebl-section}
--------------------------

Explanation-based learning is a method for extracting general rules from
individual observations. As an example, consider the problem of
differentiating and simplifying algebraic expressions (). If we
differentiate an expression such as $X^2$ with respect to $X$, we obtain
$2X$. (We use a capital letter for the arithmetic unknown $X$, to
distinguish it from the logical variable $x$.) In a logical reasoning
system, the goal might be expressed as $\prog{Ask}({Derivative}(X^2,X)
\eq d,\,{KB})$, with solution $d = 2X$.

Anyone who knows differential calculus can see this solution “by
inspection” as a result of practice in solving such problems. A student
encountering such problems for the first time, or a program with no
experience, will have a much more difficult job. Application of the
standard rules of differentiation eventually yields the expression
$1\times (2 \times (X^{(2-1)}))$, and eventually this simplifies to
$2X$. In the authors’ logic programming implementation, this takes 136
proof steps, of which 99 are on dead-end branches in the proof. After
such an experience, we would like the program to solve the same problem
much more quickly the next time it arises.

The technique of has long been used in computer science to speed up
programs by saving the results of computation. The basic idea of memo
functions is to accumulate a database of input–output pairs; when the
function is called, it first checks the database to see whether it can
avoid solving the problem from scratch. Explanation-based learning takes
this a good deal further, by creating *general* rules that
cover an entire class of cases. In the case of differentiation,
memoization would remember that the derivative of $X^2$ with respect to
$X$ is $2X$, but would leave the agent to calculate the derivative of
$Z^2$ with respect to $Z$ from scratch. We would like to be able to
extract the general rule that for any arithmetic unknown $u$, the
derivative of $u^2$ with respect to $u$ is $2u$. (An even more general
rule for $u^n$ can also be produced, but the current example suffices to
make the point.) In logical terms, this is expressed by the rule
$${ArithmeticUnknown}(u) \implies {Derivative}(u^2,u)\eq 2u\ .$$ If
the knowledge base contains such a rule, then any new case that is an
instance of this rule can be solved immediately.

This is, of course, merely a trivial example of a very general
phenomenon. Once something is understood, it can be generalized and
reused in other circumstances. It becomes an “obvious” step and can then
be used as a building block in solving problems still more complex.
Alfred North Whitehead [-@Whitehead:1911], co-author with Bertrand
Russell of *Principia Mathematica*, wrote

“Civilization advances by extending the number of important operations
that we can do without thinking about them,”

perhaps himself applying EBL to his understanding of events such as
Zog’s discovery. If you have understood the basic idea of the
differentiation example, then your brain is already busily trying to
extract the general principles of explanation-based learning from it.
Notice that you hadn’t *already* invented EBL before you
saw the example. Like the cavemen watching Zog, you (and we) needed an
example before we could generate the basic principles. This is because
*explaining why* something is a good idea is much easier
than coming up with the idea in the first place.

### Extracting general rules from examples

The basic idea behind EBL is first to construct an explanation of the
observation using prior knowledge, and then to establish a definition of
the class of cases for which the same explanation structure can be used.
This definition provides the basis for a rule covering all of the cases
in the class. The “explanation” can be a logical proof, but more
generally it can be any reasoning or problem-solving process whose steps
are well defined. The key is to be able to identify the necessary
conditions for those same steps to apply to another case.

We will use for our reasoning system the simple backward-chaining
theorem prover described in . The proof tree for
${Derivative}(X^2,X)\eq 2X$ is too large to use as an example, so we
will use a simpler problem to illustrate the generalization method.
Suppose our problem is to simplify $1 \times (0 +
X)$. The knowledge base includes the following rules:

(u,v) (v,w) (u,w) .\
(u) (u,u) .\
(u) (u) .\
(u) (u) .\
(1 u,u) .\
(0+u,u) .\
 

The proof that the answer is $X$ is shown in the top half of . The EBL
method actually constructs two proof trees simultaneously. The second
proof tree uses a *variabilized* goal in which the
constants from the original goal are replaced by variables. As the
original proof proceeds, the variabilized proof proceeds in step, using
*exactly the same rule applications*. This could cause some
of the variables to become instantiated. For example, in order to use
the rule ${Rewrite}(1\times u,u)$, the variable $x$ in the subgoal
${Rewrite}(x\times (y+z),v)$ must be bound to 1. Similarly, $y$ must
be bound to 0 in the subgoal ${Rewrite}(y+z,v')$ in order to use the
rule ${Rewrite}(0+u,u)$. Once we have the generalized proof tree, we
take the leaves (with the necessary bindings) and form a general rule
for the goal predicate:

(1(0+z),0+z) (0+z,z) (z)\
 (1(0+z),z) .

[simplify-proof2-figure]

Notice that the first two conditions on the left-hand side are true
*regardless of the value of $z$*. We can therefore drop
them from the rule, yielding
$${ArithmeticUnknown}(z) \implies {Simplify}(1\times (0+z),z) \ .$$
In general, conditions can be dropped from the final rule if they impose
no constraints on the variables on the right-hand side of the rule,
because the resulting rule will still be true and will be more
efficient. Notice that we cannot drop the condition
${ArithmeticUnknown}(z)$, because not all possible values of $z$ are
arithmetic unknowns. Values other than arithmetic unknowns might require
different forms of simplification: for example, if $z$ were $2\times 3$,
then the correct simplification of $1\times (0+(2\times
3))$ would be $6$ and not $2\times 3$.

To recap, the basic EBL process works as follows:

1.  Given an example, construct a proof that the goal predicate applies
    to the example using the available background knowledge.

2.  In parallel, construct a generalized proof tree for the variabilized
    goal using the same inference steps as in the original proof.

3.  Construct a new rule whose left-hand side consists of the leaves of
    the proof tree and whose right-hand side is the variabilized goal
    (after applying the necessary bindings from the generalized proof).

4.  Drop any conditions from the left-hand side that are true regardless
    of the values of the variables in the goal.

### Improving efficiency

The generalized proof tree in actually yields more than one generalized
rule. For example, if we terminate, or , the growth of the right-hand
branch in the proof tree when it reaches the ${Primitive}$ step, we
get the rule
$${Primitive}(z) \implies {Simplify}(1\times (0+z),z) \ .$$ This
rule is as valid as, but *more general* than, the rule
using ${ArithmeticUnknown}$, because it covers cases where $z$ is a
number. We can extract a still more general rule by pruning after the
step ${Simplify}(y+z,w)$, yielding the rule
$${Simplify}(y+z,w) \implies {Simplify}(1\times (y+z),w)\ .$$ In
general, a rule can be extracted from *any partial subtree*
of the generalized proof tree. Now we have a problem: which of these
rules do we choose?

The choice of which rule to generate comes down to the question of
efficiency. There are three factors involved in the analysis of
efficiency gains from EBL:

1.  Adding large numbers of rules can slow down the reasoning process,
    because the inference mechanism must still check those rules even in
    cases where they do not yield a solution. In other words, it
    increases the in the search space.

2.  To compensate for the slowdown in reasoning, the derived rules must
    offer significant increases in speed for the cases that they do
    cover. These increases come about mainly because the derived rules
    avoid dead ends that would otherwise be taken, but also because they
    shorten the proof itself.

3.  Derived rules should be as general as possible, so that they apply
    to the largest possible set of cases.

A common approach to ensuring that derived rules are efficient is to
insist on the of each subgoal in the rule. A subgoal is operational if
it is “easy” to solve. For example, the subgoal ${Primitive}(z)$ is
easy to solve, requiring at most two steps, whereas the subgoal
${Simplify}(y+z,w)$ could lead to an arbitrary amount of inference,
depending on the values of $y$ and $z$. If a test for operationality is
carried out at each step in the construction of the generalized proof,
then we can prune the rest of a branch as soon as an operational subgoal
is found, keeping just the operational subgoal as a conjunct of the new
rule.

Unfortunately, there is usually a tradeoff between operationality and
generality. More specific subgoals are generally easier to solve but
cover fewer cases. Also, operationality is a matter of degree: one or
two steps is definitely operational, but what about 10 or 100? Finally,
the cost of solving a given subgoal depends on what other rules are
available in the knowledge base. It can go up or down as more rules are
added. Thus, EBL systems really face a very complex optimization problem
in trying to maximize the efficiency of a given initial knowledge base.
It is sometimes possible to derive a mathematical model of the effect on
overall efficiency of adding a given rule and to use this model to
select the best rule to add. The analysis can become very complicated,
however, especially when recursive rules are involved. One promising
approach is to address the problem of efficiency empirically, simply by
adding several rules and seeing which ones are useful and actually speed
things up.

Empirical analysis of efficiency is actually at the heart of EBL. What
we have been calling loosely the “efficiency of a given knowledge base”
is actually the average-case complexity on a distribution of problems.

By generalizing from past example problems, EBL makes the knowledge base
more efficient for the kind of problems that it is reasonable to expect.

This works as long as the distribution of past examples is roughly the
same as for future examples—the same assumption used for PAC-learning in
. If the EBL system is carefully engineered, it is possible to obtain
significant speedups. For example, a very large Prolog-based natural
language system designed for speech-to-speech translation between
Swedish and English was able to achieve real-time performance only by
the application of EBL to the parsing process @Samuelsson+Rayner:1991.

Learning Using Relevance Information {#rbl-section}
------------------------------------

Our traveler in Brazil seems to be able to make a confident
generalization concerning the language spoken by other Brazilians. The
inference is sanctioned by her background knowledge, namely, that people
in a given country (usually) speak the same language. We can express
this in first-order logic as follows:[^2]

$${Nationality}(x,n) \land {Nationality}(y,n)
\land {Language}(x,l) \implies {Language}(y,l)\ .
\label{det-example-equation}$$

(Literal translation: “If $x$ and $y$ have the same nationality $n$ and
$x$ speaks language $l$, then $y$ also speaks it.”) It is not difficult
to show that, from this sentence and the observation that
$${Nationality}({Fernando},{Brazil}) \land {Language}({Fernando},{Portuguese})\ ,$$
the following conclusion is entailed (see ):[dbsig-page]
$${Nationality}(x,{Brazil}) \implies {Language}(x,{Portuguese})\ .$$

Sentences such as ([det-example-equation]) express a strict form of
relevance: given nationality, language is fully determined. (Put another
way: language is a function of nationality.) These sentences are called
or . They occur so commonly in certain kinds of applications (e.g.,
defining database designs) that a special syntax is used to write them.
We adopt the notation of Davies [-@Davies:1985]:
$${Nationality}(x,n) \succ {Language}(x,l)\ .$$ As usual, this is
simply a syntactic sugaring, but it makes it clear that the
determination is really a relationship between the predicates:
nationality determines language. The relevant properties determining
conductance and density can be expressed similarly:

(x,m) (x,t) (x,)  ;\
(x,m) (x,t) (x,d)  .

The corresponding generalizations follow logically from the
determinations and observations.

### Determining the hypothesis space

Although the determinations sanction general conclusions concerning all
Brazilians, or all pieces of copper at a given temperature, they cannot,
of course, yield a general predictive theory for *all*
nationalities, or for *all* temperatures and materials,
from a single example. Their main effect can be seen as limiting the
space of hypotheses that the learning agent need consider. In predicting
conductance, for example, one need consider only material and
temperature and can ignore mass, ownership, day of the week, the current
president, and so on. Hypotheses can certainly include terms that are in
turn determined by material and temperature, such as molecular
structure, thermal energy, or free-electron density.

Determinations specify a sufficient basis vocabulary from which to
construct hypotheses concerning the target predicate.

This statement can be proven by showing that a given determination is
logically equivalent to a statement that the correct definition of the
target predicate is one of the set of all definitions expressible using
the predicates on the left-hand side of the determination.

Intuitively, it is clear that a reduction in the hypothesis space size
should make it easier to learn the target predicate. Using the basic
results of computational learning theory (), we can quantify the
possible gains. First, recall that for Boolean functions, $\log(|\Hyp|)$
examples are required to converge to a reasonable hypothesis, where
$|\Hyp|$ is the size of the hypothesis space. If the learner has $n$
Boolean features with which to construct hypotheses, then, in the
absence of further restrictions, $|\Hyp| = O(2^{2^n})$, so the number of
examples is $O(2^n)$. If the determination contains $d$ predicates in
the left-hand side, the learner will require only $O(2^d)$ examples, a
reduction of $O(2^{n-d})$.

### Learning and using relevance information

As we stated in the introduction to this chapter, prior knowledge is
useful in learning; but it too has to be learned. In order to provide a
complete story of relevance-based learning, we must therefore provide a
learning algorithm for determinations. The learning algorithm we now
present is based on a straightforward attempt to find the simplest
determination consistent with the observations. A determination $P
\succ Q$ says that if any examples match on $P$, then they must also
match on $Q$. A determination is therefore consistent with a set of
examples if every pair that matches on the predicates on the left-hand
side also matches on the goal predicate. For example, suppose we have
the following examples of conductance measurements on material samples:

|c|c|c|c|c||c| Sample & Mass & Temperature & Material &
Size & Conductance\
S1 & 12 & 26 & Copper & 3 & 0.59\
S1 & 12 & 100 & Copper & 3 & 0.57\
S2 & 24 & 26 & Copper & 6 & 0.59\
S3 & 12 & 26 & Lead & 2 & 0.05\
S3 & 12 & 100 & Lead & 2 & 0.04\
S4 & 24 & 26 & Lead & 4 & 0.05\

The minimal consistent determination is
${Material} \land {Temperature}
\succ {Conductance}$. There is a nonminimal but consistent
determination, namely,
${Mass} \land {Size} \land {Temperature} \succ
{Conductance}$. This is consistent with the examples because mass and
size determine density and, in our data set, we do not have two
different materials with the same density. As usual, we would need a
larger sample set in order to eliminate a nearly correct hypothesis.

There are several possible algorithms for finding minimal consistent
determinations. The most obvious approach is to conduct a search through
the space of determinations, checking all determinations with one
predicate, two predicates, and so on, until a consistent determination
is found. We will assume a simple attribute-based representation, like
that used for decision tree learning in . A determination will be
represented by the set of attributes on the left-hand side, because the
target predicate is assumed to be fixed. The basic algorithm is outlined
in .

[minimal-consistent-det-algorithm]

The time complexity of this algorithm depends on the size of the
smallest consistent determination. Suppose this determination has $p$
attributes out of the $n$ total attributes. Then the algorithm will not
find it until searching the subsets of of size $p$. There are
${n \choose p} = O(n^p)$ such subsets; hence the algorithm is
exponential in the size of the minimal determination. It turns out that
the problem is , so we cannot expect to do better in the general case.
In most domains, however, there will be sufficient local structure (see
for a definition of locally structured domains) that $p$ will be small.

Given an algorithm for learning determinations, a learning agent has a
way to construct a minimal hypothesis within which to learn the target
predicate. For example, we can combine with the algorithm. This yields a
relevance-based decision-tree learning algorithm that first identifies a
minimal set of relevant attributes and then passes this set to the
decision tree algorithm for learning. Unlike , simultaneously learns and
uses relevance information in order to minimize its hypothesis space. We
expect that will learn faster than , and this is in fact the case. shows
the learning performance for the two algorithms on randomly generated
data for a function that depends on only 5 of 16 attributes. Obviously,
in cases where all the available attributes are relevant, will show no
advantage.

[rbdtl-happy-graph-figure]

This section has only scratched the surface of the field of , which aims
to understand how prior knowledge can be used to identify the
appropriate hypothesis space within which to search for the correct
target definition. There are many unanswered questions:

-   How can the algorithms be extended to handle noise?

-   Can we handle continuous-valued variables?

-   How can other kinds of prior knowledge be used, besides
    determinations?

-   How can the algorithms be generalized to cover any first-order
    theory, rather than just an attribute-based representation?

Some of these questions are addressed in the next section.

Inductive Logic Programming {#ilp-section}
---------------------------

Inductive logic programming (ILP) combines inductive methods with the
power of first-order representations, concentrating in particular on the
representation of hypotheses as logic programs.[^3] It has gained
popularity for three reasons. First, ILP offers a rigorous approach to
the general knowledge-based inductive learning problem. Second, it
offers complete algorithms for inducing general, first-order theories
from examples, which can therefore learn successfully in domains where
attribute-based algorithms are hard to apply. An example is in learning
how protein structures fold (). The three-dimensional configuration of a
protein molecule cannot be represented reasonably by a set of
attributes, because the configuration inherently refers to
*relationships* between objects, not to attributes of a
single object. First-order logic is an appropriate language for
describing the relationships. Third, inductive logic programming
produces hypotheses that are (relatively) easy for humans to read. For
example, the English translation in can be scrutinized and criticized by
working biologists. This means that inductive logic programming systems
can participate in the scientific cycle of experimentation, hypothesis
generation, debate, and refutation. Such participation would not be
possible for systems that generate “black-box” classifiers, such as
neural networks.

[helix-folding-figure]

### An example

Recall from that the general knowledge-based induction problem is to
“solve” the entailment constraint
$${Background} \land {Hypothesis} \land {Descriptions}  \models {Classifications}$$
for the unknown ${Hypothesis}$, given the ${Background}$ knowledge
and examples described by ${Descriptions}$ and ${Classifications}$.
To illustrate this, we will use the problem of learning family
relationships from examples. The descriptions will consist of an
extended family tree, described in terms of ${Mother}$, ${Father}$,
and ${Married}$ relations and ${Male}$ and ${Female}$ properties.
As an example, we will use the family tree from , shown here in . The
corresponding descriptions are as follows: $$\begin{array}{lll}
{Father}({Philip},{Charles}) & {Father}({Philip},{Anne}) & \ldots \\
{Mother}({Mum},{Margaret}) & {Mother}({Mum},{Elizabeth}) & \ldots \\
{Married}({Diana},{Charles}) & {Married}({Elizabeth},{Philip}) & \ldots \\
{Male}({Philip}) & {Male}({Charles}) & \ldots \\
{Female}({Beatrice}) & {Female}({Margaret}) & \ldots
\end{array}$$ The sentences in ${Classifications}$ depend on the
target concept being learned. We might want to learn ${Grandparent}$,
${BrotherInLaw}$, or ${Ancestor}$, for example. For
${Grandparent}$, the complete set of ${Classifications}$ contains
${20}\stimes {20}\eq {400}$ conjuncts of the form $$\begin{array}{lll}
{Grandparent}({Mum},{Charles}) & {Grandparent}({Elizabeth},{Beatrice}) & \ldots \\
\lnot {Grandparent}({Mum},{Harry}) & \lnot {Grandparent}({Spencer},{Peter}) & \ldots
\end{array}$$ We could of course learn from a subset of this complete
set.

[family2-figure]

The object of an inductive learning program is to come up with a set of
sentences for the ${Hypothesis}$ such that the entailment constraint
is satisfied. Suppose, for the moment, that the agent has no background
knowledge: ${Background}$ is empty. Then one possible solution for
${Hypothesis}$ is the following: $$\begin{array}{lll}
{Grandparent}(x,y) & \lequiv & [\Exi{z} {Mother}(x,z) \land {Mother}(z,y)] \\
                 & \lor    & [\Exi{z} {Mother}(x,z) \land {Father}(z,y)] \\
                 & \lor    & [\Exi{z} {Father}(x,z) \land {Mother}(z,y)] \\
                 & \lor    & [\Exi{z} {Father}(x,z) \land {Father}(z,y)]\ .
\end{array}$$ Notice that an attribute-based learning algorithm, such as
, will get nowhere in solving this problem. In order to express
${Grandparent}$ as an attribute (i.e., a unary predicate), we would
need to make *pairs* of people into objects:
$${Grandparent}(\langle {Mum},{Charles} \rangle ) \ldots$$ Then we
get stuck in trying to represent the example descriptions. The only
possible attributes are horrible things such as
$${FirstElementIsMotherOfElizabeth}(\langle {Mum},{Charles} \rangle )\ .$$
The definition of ${Grandparent}$ in terms of these attributes simply
becomes a large disjunction of specific cases that does not generalize
to new examples at all.

Attribute-based learning algorithms are incapable of learning relational
predicates.

Thus, one of the principal advantages of ILP algorithms is their
applicability to a much wider range of problems, including relational
problems.

The reader will certainly have noticed that a little bit of background
knowledge would help in the representation of the ${Grandparent}$
definition. For example, if ${Background}$ included the sentence
$${Parent}(x,y) \lequiv [{Mother}(x,y) \lor {Father}(x,y)]\ ,$$
then the definition of ${Grandparent}$ would be reduced to
$${Grandparent}(x,y) \lequiv [\Exi{z} {Parent}(x,z) \land {Parent}(z,y)]\ .$$
This shows how background knowledge can dramatically reduce the size of
hypotheses required to explain the observations.

It is also possible for ILP algorithms to *create* new
predicates in order to facilitate the expression of explanatory
hypotheses. Given the example data shown earlier, it is entirely
reasonable for the ILP program to propose an additional predicate, which
we would call “${Parent}$,” in order to simplify the definitions of
the target predicates. Algorithms that can generate new predicates are
called algorithms. Clearly, constructive induction is a necessary part
of the picture of cumulative learning. It has been one of the hardest
problems in machine learning, but some ILP techniques provide effective
mechanisms for achieving it.

In the rest of this chapter, we will study the two principal approaches
to ILP. The first uses a generalization of decision tree methods, and
the second uses techniques based on inverting a resolution proof.

### Top-down inductive learning methods

The first approach to ILP works by starting with a very general rule and
gradually specializing it so that it fits the data. This is essentially
what happens in decision-tree learning, where a decision tree is
gradually grown until it is consistent with the observations. To do ILP
we use first-order literals instead of attributes, and the hypothesis is
a set of clauses instead of a decision tree. This section describes
 @Quinlan:1990, one of the first ILP programs.

Suppose we are trying to learn a definition of the
${Grandfather}(x,y)$ predicate, using the same family data as before.
As with decision-tree learning, we can divide the examples into positive
and negative examples. Positive examples are
$$\langle {George},{Anne} \rangle,\ 
   \langle {Philip},{Peter} \rangle,\ 
   \langle {Spencer},{Harry} \rangle,\ \ldots$$ and negative
examples are $$\langle {George},{Elizabeth} \rangle,\ 
   \langle {Harry},{Zara} \rangle,\ 
   \langle {Charles},{Philip} \rangle,\ \ldots$$ Notice that each
example is a *pair* of objects, because ${Grandfather}$
is a binary predicate. In all, there are 12 positive examples in the
family tree and 388 negative examples (all the other pairs of people).

constructs a set of clauses, each with ${Grandfather}(x,y)$ as the
head. The clauses must classify the 12 positive examples as instances of
the ${Grandfather}(x,y)$ relationship, while ruling out the 388
negative examples. The clauses are Horn clauses, with the extension that
negated literals are allowed in the body of a clause and are interpreted
using negation as failure, as in Prolog. The initial clause has an empty
body: $$\qquad \implies {Grandfather}(x,y)\ .$$ This clause classifies
every example as positive, so it needs to be specialized. We do this by
adding literals one at a time to the left-hand side. Here are three
potential additions:

(x,y) (x,y)  .\
(x,z) (x,y)  .\
(x,z) (x,y)  .

(Notice that we are assuming that a clause defining ${Parent}$ is
already part of the background knowledge.) The first of these three
clauses incorrectly classifies all of the 12 positive examples as
negative and can thus be ignored. The second and third agree with all of
the positive examples, but the second is incorrect on a larger fraction
of the negative examples—twice as many, because it allows mothers as
well as fathers. Hence, we prefer the third clause.

Now we need to specialize this clause further, to rule out the cases in
which $x$ is the father of some $z$, but $z$ is not a parent of $y$.
Adding the single literal ${Parent}(z,y)$ gives
$${Father}(x,z) \land {Parent}(z,y) \implies {Grandfather}(x,y)\ ,$$
which correctly classifies all the examples. will find and choose this
literal, thereby solving the learning task. In general, the solution is
a set of Horn clauses, each of which implies the target predicate. For
example, if we didn’t have the ${Parent}$ predicate in our vocabulary,
then the solution might be

(x,z) (z,y) (x,y)\
(x,z) (z,y) (x,y)  .

Note that each of these clauses covers some of the positive examples,
that together they cover all the positive examples, and that is designed
in such a way that no clause will incorrectly cover a negative example.
In general will have to search through many unsuccessful clauses before
finding a correct solution.

[foil-algorithm]

This example is a very simple illustration of how operates. A sketch of
the complete algorithm is shown in . Essentially, the algorithm
repeatedly constructs a clause, literal by literal, until it agrees with
some subset of the positive examples and none of the negative examples.
Then the positive examples covered by the clause are removed from the
training set, and the process continues until no positive examples
remain. The two main subroutines to be explained are , which constructs
all possible new literals to add to the clause, and , which selects a
literal to add.

takes a clause and constructs all possible “useful” literals that could
be added to the clause. Let us use as an example the clause
$${Father}(x,z) \implies {Grandfather}(x,y)\ .$$ There are three
kinds of literals that can be added:

1.  *Literals using predicates*: the literal can be negated
    or unnegated, any existing predicate (including the goal predicate)
    can be used, and the arguments must all be variables. Any variable
    can be used for any argument of the predicate, with one restriction:
    each literal must include *at least one* variable from
    an earlier literal or from the head of the clause. Literals such as
    ${Mother}(z,u)$, ${Married}(z,z)$, $\lnot {Male}(y)$, and
    ${Grandfather}(v,x)$ are allowed, whereas ${Married}(u,v)$ is
    not. Notice that the use of the predicate from the head of the
    clause allows to learn *recursive* definitions.

2.  *Equality and inequality literals*: these relate
    variables already appearing in the clause. For example, we might add
    $z \neq x$. These literals can also include user-specified
    constants. For learning arithmetic we might use 0 and 1, and for
    learning list functions we might use the empty list $[\,]$.

3.  *Arithmetic comparisons*: when dealing with functions
    of continuous variables, literals such as $x>y$ and $y\leq z$ can be
    added. As in decision-tree learning, a constant threshold value can
    be chosen to maximize the discriminatory power of the test.

The resulting branching factor in this search space is very large (see
), but can also use type information to reduce it. For example, if the
domain included numbers as well as people, type restrictions would
prevent from generating literals such as ${Parent}(x,n)$, where $x$ is
a person and $n$ is a number.

uses a heuristic somewhat similar to information gain (see ) to decide
which literal to add. The exact details are not important here, and a
number of different variations have been tried. One interesting
additional feature of is the use of Ockham’s razor to eliminate some
hypotheses. If a clause becomes longer (according to some metric) than
the total length of the positive examples that the clause explains, that
clause is not considered as a potential hypothesis. This technique
provides a way to avoid overcomplex clauses that fit noise in the data.

and its relatives have been used to learn a wide variety of definitions.
One of the most impressive demonstrations @Quinlan+Cameron-Jones:1993
involved solving a long sequence of exercises on list-processing
functions from Bratko’s [-@Bratko:1986] Prolog textbook. In each case,
the program was able to learn a correct definition of the function from
a small set of examples, using the previously learned functions as
background knowledge.

### Inductive learning with inverse deduction

The second major approach to ILP involves inverting the normal deductive
proof process. is based on the observation that if the example
${Classifications}$ follow from ${Background} \land
{Hypothesis} \land {Descriptions}$, then one must be able to prove
this fact by resolution (because resolution is complete). If we can “run
the proof backward,” then we can find a ${Hypothesis}$ such that the
proof goes through. The key, then, is to find a way to invert the
resolution process.

We will show a backward proof process for inverse resolution that
consists of individual backward steps. Recall that an ordinary
resolution step takes two clauses $C_1$ and $C_2$ and resolves them to
produce the $C$. An inverse resolution step takes a resolvent $C$ and
produces two clauses $C_1$ and $C_2$, such that $C$ is the result of
resolving $C_1$ and $C_2$. Alternatively, it may take a resolvent $C$
and clause $C_1$ and produce a clause $C_2$ such that $C$ is the result
of resolving $C_1$ and $C_2$.

The early steps in an inverse resolution process are shown in , where we
focus on the positive example ${Grandparent}({George},{Anne})$.
The process begins at the end of the proof (shown at the bottom of the
figure). We take the resolvent $C$ to be empty clause (i.e. a
contradiction) and $C_2$ to be $\lnot
{Grandparent}({George},{Anne})$, which is the negation of the goal
example. The first inverse step takes $C$ and $C_2$ and generates the
clause ${Grandparent}({George},{Anne})$ for $C_1$. The next step
takes this clause as $C$ and the clause
${Parent}({Elizabeth},{Anne})$ as $C_2$, and generates the clause
$$\lnot {Parent}({Elizabeth},y) \lor {Grandparent}({George},y)$$
as $C_1$. The final step treats this clause as the resolvent. With
${Parent}({George}, {Elizabeth})$ as $C_2$, one possible clause
$C_1$ is the hypothesis
$${Parent}(x, z) \land {Parent}(z, y) \implies {Grandparent}(x, y)\ .$$
Now we have a resolution proof that the hypothesis, descriptions, and
background knowledge entail the classification
${Grandparent}({George},{Anne})$.

[inverse-proof-figure]

Clearly, inverse resolution involves a search. Each inverse resolution
step is nondeterministic, because for any $C$, there can be many or even
an infinite number of clauses $C_1$ and $C_2$ that resolve to $C$. For
example, instead of choosing $\lnot {Parent}({Elizabeth},y) \lor
{Grandparent}({George},y)$ for $C_1$ in the last step of , the
inverse resolution step might have chosen any of the following
sentences:

(,) (,)  .\
(z,) (,)  .\
(z,y) (,y)  .\
 

(See Exercises [ir-step-exercise] and [prolog-ir-exercise].)
Furthermore, the clauses that participate in each step can be chosen
from the ${Background}$ knowledge, from the example
${Descriptions}$, from the negated ${Classifications}$, or from
hypothesized clauses that have already been generated in the inverse
resolution tree. The large number of possibilities means a large
branching factor (and therefore an inefficient search) without
additional controls. A number of approaches to taming the search have
been tried in implemented ILP systems:

1.  Redundant choices can be eliminated—for example, by generating only
    the most specific hypotheses possible and by requiring that all the
    hypothesized clauses be consistent with each other, and with the
    observations. This last criterion would rule out the clause
    $\lnot {Parent}(z,y) \lor {Grandparent}({George},y)$, listed
    before.

2.  The proof strategy can be restricted. For example, we saw in that is
    a complete, restricted strategy. Linear resolution produces proof
    trees that have a linear branching structure—the whole tree follows
    one line, with only single clauses branching off that line (as in ).

3.  The representation language can be restricted, for example by
    eliminating function symbols or by allowing only Horn clauses. For
    instance, operates with Horn clauses using . The idea is to change
    the entailment constraint
    $${Background} \land {Hypothesis} \land {Descriptions}  \models {Classifications}$$
    to the logically equivalent form
    $${Background} \land {Descriptions} \land \lnot {Classifications} \models \lnot {Hypothesis}.$$
    From this, one can use a process similar to the normal Prolog
    Horn-clause deduction, with negation-as-failure to derive
    ${Hypothesis}$. Because it is restricted to Horn clauses, this is
    an incomplete method, but it can be more efficient than full
    resolution. It is also possible to apply complete inference with
    inverse entailment @Inoue:2001.

4.  Inference can be done with model checking rather than theorem
    proving. The system @Muggleton:1995 uses a form of model checking to
    limit the search. That is, like answer set programming, it generates
    possible values for logical variables, and checks for consistency.

5.  Inference can be done with ground propositional clauses rather than
    in first-order logic. The system @Lavrac+Dzeroski:1994 works by
    translating first-order theories into propositional logic, solving
    them with a propositional learning system, and then translating
    back. Working with propositional formulas can be more efficient on
    some problems, as we saw with in .

### Making discoveries with inductive logic programming

An inverse resolution procedure that inverts a complete resolution
strategy is, in principle, a complete algorithm for learning first-order
theories. That is, if some unknown ${Hypothesis}$ generates a set of
examples, then an inverse resolution procedure can generate
${Hypothesis}$ from the examples. This observation suggests an
interesting possibility: Suppose that the available examples include a
variety of trajectories of falling bodies. Would an inverse resolution
program be theoretically capable of inferring the law of gravity? The
answer is clearly yes, because the law of gravity allows one to explain
the examples, given suitable background mathematics. Similarly, one can
imagine that electromagnetism, quantum mechanics, and the theory of
relativity are also within the scope of ILP programs. Of course, they
are also within the scope of a monkey with a typewriter; we still need
better heuristics and new ways to structure the search space.

One thing that inverse resolution systems *will* do for you
is invent new predicates. This ability is often seen as somewhat
magical, because computers are often thought of as “merely working with
what they are given.” In fact, new predicates fall directly out of the
inverse resolution step. The simplest case arises in hypothesizing two
new clauses $C_1$ and $C_2$, given a clause $C$. The resolution of $C_1$
and $C_2$ eliminates a literal that the two clauses share; hence, it is
quite possible that the eliminated literal contained a predicate that
does not appear in $C$. Thus, when working backward, one possibility is
to generate a new predicate from which to reconstruct the missing
literal.

shows an example in which the new predicate $P$ is generated in the
process of learning a definition for ${Ancestor}$. Once generated, $P$
can be used in later inverse resolution steps. For example, a later step
might hypothesize that ${Mother}(x,y) \implies P(x,y)$. Thus, the new
predicate $P$ has its meaning constrained by the generation of
hypotheses that involve it. Another example might lead to the constraint
${Father}(x,y) \implies
P(x,y)$. In other words, the predicate $P$ is what we usually think of
as the ${Parent}$ relationship. As we mentioned earlier, the invention
of new predicates can significantly reduce the size of the definition of
the goal predicate. Hence, by including the ability to invent new
predicates, inverse resolution systems can often solve learning problems
that are infeasible with other techniques.

[new-predicate-figure]

Some of the deepest revolutions in science come from the invention of
new predicates and functions—for example, Galileo’s invention of
acceleration or Joule’s invention of thermal energy. Once these terms
are available, the discovery of new laws becomes (relatively) easy. The
difficult part lies in realizing that some new entity, with a specific
relationship to existing entities, will allow an entire body of
observations to be explained with a much simpler and more elegant theory
than previously existed.

As yet, ILP systems have not made discoveries on the level of Galileo or
Joule, but their discoveries have been deemed publishable in the
scientific literature. For example, in the *Journal of Molecular
Biology*, describe the automated discovery of rules for protein
folding by the ILP program . Many of the rules discovered by could have
been derived from known principles, but most had not been previously
published as part of a standard biological database. (See for an
example.). In related work, dealt with the problem of discovering
molecular-structure-based rules for the mutagenicity of nitroaromatic
compounds. These compounds are found in automobile exhaust fumes. For
80% of the compounds in a standard database, it is possible to identify
four important features, and linear regression on these features
outperforms ILP. For the remaining 20%, the features alone are not
predictive, and ILP identifies relationships that allow it to outperform
linear regression, neural nets, and decision trees. Most impressively,
endowed a robot with the ability to perform molecular biology
experiments and extended ILP techniques to include experiment design,
thereby creating an autonomous scientist that actually discovered new
knowledge about the functional genomics of yeast. For all these examples
it appears that the ability both to represent relations and to use
background knowledge contribute to ILP’s high performance. The fact that
the rules found by ILP can be interpreted by humans contributes to the
acceptance of these techniques in biology journals rather than just
computer science journals.

ILP has made contributions to other sciences besides biology. One of the
most important is natural language processing, where ILP has been used
to extract complex relational information from text. These results are
summarized in .

This chapter has investigated various ways in which prior knowledge can
help an agent to learn from new experiences. Because much prior
knowledge is expressed in terms of relational models rather than
attribute-based models, we have also covered systems that allow learning
of relational models. The important points are:

-   The use of prior knowledge in learning leads to a picture of , in
    which learning agents improve their learning ability as they acquire
    more knowledge.

-   Prior knowledge helps learning by eliminating otherwise consistent
    hypotheses and by “filling in” the explanation of examples, thereby
    allowing for shorter hypotheses. These contributions often result in
    faster learning from fewer examples.

-   Understanding the different logical roles played by prior knowledge,
    as expressed by , helps to define a variety of learning techniques.

-   (EBL) extracts general rules from single examples by
    *explaining* the examples and generalizing the
    explanation. It provides a deductive method for turning
    first-principles knowledge into useful, efficient, special-purpose
    expertise.

-   (RBL) uses prior knowledge in the form of determinations to identify
    the relevant attributes, thereby generating a reduced hypothesis
    space and speeding up learning. RBL also allows deductive
    generalizations from single examples.

-   (KBIL) finds inductive hypotheses that explain sets of observations
    with the help of background knowledge.

-   (ILP) techniques perform KBIL on knowledge that is expressed in
    first-order logic. ILP methods can learn relational knowledge that
    is not expressible in attribute-based systems.

-   ILP can be done with a top-down approach of refining a very general
    rule or through a bottom-up approach of inverting the deductive
    process.

-   ILP methods naturally generate new predicates with which concise new
    theories can be expressed and show promise as general-purpose
    scientific theory formation systems.

Although the use of prior knowledge in learning would seem to be a
natural topic for philosophers of science, little formal work was done
until quite recently. *Fact, Fiction, and Forecast*, by the
philosopher Nelson Goodman [-@Goodman:1954], refuted the earlier
supposition that induction was simply a matter of seeing enough examples
of some universally quantified proposition and then adopting it as a
hypothesis. Consider, for example, the hypothesis “All emeralds are
grue,” where *grue* means “green if observed before time
$t$, but blue if observed thereafter.” At any time up to $t$, we might
have observed millions of instances confirming the rule that emeralds
are grue, and no disconfirming instances, and yet we are unwilling to
adopt the rule. This can be explained only by appeal to the role of
relevant prior knowledge in the induction process. Goodman proposes a
variety of different kinds of prior knowledge that might be useful,
including a version of determinations called . Unfortunately, Goodman’s
ideas were never pursued in machine learning.

The approach is an old idea in philosophy @Mill:1843. Early work in
cognitive psychology also suggested that it is a natural form of concept
learning in humans @Bruner+al:1957. In AI, the approach is most closely
associated with the work of Patrick Winston, whose
Ph.D. thesis @Winston:1970 addressed the problem of learning
descriptions of complex objects. The method @Mitchell:1977
[@Mitchell:1982] takes a different approach, maintaining the set of
*all* consistent hypotheses and eliminating those found to
be inconsistent with new examples. The approach was used in the
Meta-Dendral expert system for chemistry
@Buchanan+Mitchell:1978, and later in Mitchell’s [-@Mitchell+al:1983]
system, which learns to solve calculus problems. A third influential
thread was formed by the work of Michalski and colleagues on the AQ
series of algorithms, which learned sets of logical
rules @Michalski:1969 [@Michalski+al:1986b].

EBL had its roots in the techniques used by the planner @Fikes+al:1972.
When a plan was constructed, a generalized version of it was saved in a
plan library and used in later planning as a . Similar ideas appeared in
Anderson’s ACT\* architecture, under the heading of  @Anderson:1983, and
in the architecture, as  @Laird+al:1986. @DeJong:1981, @Mitchell:1982,
and  @Minton:1984 were immediate precursors of the rapid growth of
interest in EBL stimulated by the papers of  and . Hirsh [-@Hirsh:1987]
introduced the EBL algorithm described in the text, showing how it could
be incorporated directly into a logic programming system. Van Harmelen
and Bundy [-@VanHarmelen+Bundy:1988] explain EBL as a variant of the
method used in program analysis systems @Jones+al:1993.

Initial enthusiasm for EBL was tempered by Minton’s
finding [-@Minton:1988] that, without extensive extra work, EBL could
easily slow down a program significantly. Formal probabilistic analysis
of the expected payoff of EBL can be found in   and . An excellent
survey of early work on EBL appears in  .

Instead of using examples as foci for generalization, one can use them
directly to solve new problems, in a process known as . This form of
reasoning ranges from a form of plausible reasoning based on degree of
similarity @Gentner:1983, through a form of deductive inference based on
determinations but requiring the participation of the
example @Davies+Russell:1987, to a form of “lazy” EBL that tailors the
direction of generalization of the old example to fit the needs of the
new problem. This latter form of analogical reasoning is found most
commonly in  @Kolodner:1993 and  @Veloso+Carbonell:1993.

Relevance information in the form of functional dependencies was first
developed in the database community, where it is used to structure large
sets of attributes into manageable subsets. Functional dependencies were
used for analogical reasoning by Carbonell and
Collins [-@Carbonell+Collins:1973] and rediscovered and given a full
logical analysis by Davies and Russell @Davies:1985
[@Davies+Russell:1987]. Their role as prior knowledge in inductive
learning was explored by Russell and Grosof [-@Russell+Grosof:1987]. The
equivalence of determinations to a restricted-vocabulary hypothesis
space was proved in . Learning algorithms for determinations and the
improved performance obtained by were first shown in the algorithm, due
to . Tadepalli [-@Tadepalli:1993] describes a very ingenious algorithm
for learning with determinations that shows large improvements in
learning speed.

The idea that inductive learning can be performed by inverse deduction
can be traced to W. S. Jevons [-@Jevons:1874], who wrote, “The study
both of Formal Logic and of the Theory of Probabilities has led me to
adopt the opinion that there is no such thing as a distinct method of
induction as contrasted with deduction, but that induction is simply an
inverse employment of deduction.” Computational investigations began
with the remarkable Ph.D. thesis by Gordon Plotkin [-@Plotkin:1971] at
Edinburgh. Although Plotkin developed many of the theorems and methods
that are in current use in ILP, he was discouraged by some
undecidability results for certain subproblems in induction.
MIS @Shapiro:1981 reintroduced the problem of learning logic programs,
but was seen mainly as a contribution to the theory of automated
debugging. Work on rule induction, such as the  @Quinlan:1986 and
 @Clark+Niblett:1989 systems, led to  @Quinlan:1990, which for the first
time allowed practical induction of relational rules. The field of
relational learning was reinvigorated by Muggleton and
Buntine [-@Muggleton+Buntine:1988], whose program incorporated a
slightly incomplete version of inverse resolution and was capable of
generating new predicates. The inverse resolution method also appears
in @Russell:1986b, with a simple algorithm given in a footnote. The next
major system was  @Muggleton+Feng:1990, which uses a covering algorithm
based on Plotkin’s concept of relative least general generalization.
 @Rouveirol+Puget:1989 and  @DeRaedt:1992 were other systems of that
era. More recently,  @Muggleton:1995 has taken a hybrid (top-down and
bottom-up) approach to inverse entailment and has been applied to a
number of practical problems, particularly in biology and natural
language processing. describes an extension of to handle uncertainty in
the form of stochastic logic programs.

A formal analysis of ILP methods appears in , a large collection of
papers in , and a collection of techniques and applications in the book
by . give a more recent overview of the field’s history and challenges
for the future. Early complexity results by Haussler [-@Haussler:1989]
suggested that learning first-order sentences was intractible. However,
with better understanding of the importance of syntactic restrictions on
clauses, positive results have been obtained even for clauses with
recursion @Dzeroski+al:1992. Learnability results for ILP are surveyed
by and .

Although ILP now seems to be the dominant approach to constructive
induction, it has not been the only approach taken. So-called aim to
model the process of scientific discovery of new concepts, usually by a
direct search in the space of concept definitions. Doug Lenat’s
Automated Mathematician, or @Davis+Lenat:1982, used discovery heuristics
expressed as expert system rules to guide its search for concepts and
conjectures in elementary number theory. Unlike most systems designed
for mathematical reasoning, AM lacked a concept of proof and could only
make conjectures. It rediscovered Goldbach’s conjecture and the Unique
Prime Factorization theorem. ’s architecture was generalized in the
system @Lenat:1983 by adding a mechanism capable of rewriting the
system’s own discovery heuristics. was applied in a number of areas
other than mathematical discovery, although with less success than . The
methodology of and has been controversial @Ritchie+Hanna:1984
[@Lenat+Brown:1984].

Another class of discovery systems aims to operate with real scientific
data to find new laws. The systems , , and @Langley+al:1987 are
rule-based systems that look for quantitative relationships in
experimental data from physical systems; in each case, the system has
been able to recapitulate a well-known discovery from the history of
science. Discovery systems based on probabilistic techniques—especially
clustering algorithms that discover new categories—are discussed in .

[dbsig-exercise]Show, by translating into conjunctive normal form and
applying resolution, that the conclusion drawn on concerning Brazilians
is sound.

For each of the following determinations, write down the logical
representation and explain why the determination is true (if it is):

1.  Design and denomination determine the mass of a coin.

2.  For a given program, input determines output.

3.  Climate, food intake, exercise, and metabolism determine weight gain
    and loss.

4.  Baldness is determined by the baldness (or lack thereof) of one’s
    maternal grandfather.

For each of the following determinations, write down the logical
representation and explain why the determination is true (if it is):

1.  Zip code determines the state (U.S.).

2.  Design and denomination determine the mass of a coin.

3.  Climate, food intake, exercise, and metabolism determine weight gain
    and loss.

4.  Baldness is determined by the baldness (or lack thereof) of one’s
    maternal grandfather.

Would a probabilistic version of determinations be useful? Suggest a
definition.

[ir-step-exercise]Fill in the missing values for the clauses $C_1$ or
$C_2$ (or both) in the following sets of clauses, given that $C$ is the
resolvent of $C_1$ and $C_2$:

1.  $C = {True} \implies P(A,B)$, $C_1 = P(x,y) \implies Q(x,y)$, $C_2
    = ??$.

2.  $C = {True} \implies P(A,B)$, $C_1 = ??$, $C_2 = ??$.

3.  $C = P(x,y) \implies P(x,f(y))$, $C_1 = ??$, $C_2 = ??$.

If there is more than one possible solution, provide one example of each
different kind.

[prolog-ir-exercise]Suppose one writes a logic program that carries out
a resolution inference step. That is, let ${Resolve}(c_1,c_2,c)$
succeed if $c$ is the result of resolving $c_1$ and $c_2$. Normally,
${Resolve}$ would be used as part of a theorem prover by calling it
with $c_1$ and $c_2$ instantiated to particular clauses, thereby
generating the resolvent $c$. Now suppose instead that we call it with
$c$ instantiated and $c_1$ and $c_2$ uninstantiated. Will this succeed
in generating the appropriate results of an inverse resolution step?
Would you need any special modifications to the logic programming system
for this to work?

[foil-literals-exercise]Suppose that is considering adding a literal to
a clause using a binary predicate $P$ and that previous literals
(including the head of the clause) contain five different variables.

1.  How many functionally different literals can be generated? Two
    literals are functionally identical if they differ only in the names
    of the *new* variables that they contain.

2.  Can you find a general formula for the number of different literals
    with a predicate of arity $r$ when there are $n$ variables
    previously used?

3.  Why does not allow literals that contain no previously used
    variables?

Using the data from the family tree in , or a subset thereof, apply the
algorithm to learn a definition for the ${Ancestor}$ predicate.

[^1]: The terms “false positive” and “false negative” are used in
    medicine to describe erroneous results from lab tests. A result is a
    false positive if it indicates that the patient has the disease when
    in fact no disease is present.

[^2]: We assume for the sake of simplicity that a person speaks only one
    language. Clearly, the rule would have to be amended for countries
    such as Switzerland and India.

[^3]: It might be appropriate at this point for the reader to refer to
    for some of the underlying concepts, including Horn clauses,
    conjunctive normal form, unification, and resolution.
[intro-part]

Introduction {#intro-chapter}
============

We call ourselves *Homo sapiens*—man the wise—because our
is so important to us. For thousands of years, we have tried to
understand *how we think*; that is, how a mere handful of
matter can perceive, understand, predict, and manipulate a world far
larger and more complicated than itself. The field of , or AI, goes
further still: it attempts not just to understand but also to
*build* intelligent entities.

AI is one of the newest fields in science and engineering.
Work[translation-example-page] started in earnest soon after World War
II, and the name itself was coined in 1956. Along with molecular
biology, AI is regularly cited as the “field I would most like to be in”
by scientists in other disciplines. A student in physics might
reasonably feel that all the good ideas have already been taken by
Galileo, Newton, Einstein, and the rest. AI, on the other hand, still
has openings for several full-time Einsteins and Edisons.

AI currently encompasses a huge variety of subfields, ranging from the
general (learning and perception) to the specific, such as playing
chess, proving mathematical theorems, writing poetry, driving a car on a
crowded street, and diagnosing diseases. AI is relevant to any
intellectual task; it is truly a universal field.

What Is AI? {#ai-is-section}
-----------

We have claimed that AI is exciting, but we have not said what it
*is*. In we see eight definitions of AI, laid out along two
dimensions. The definitions on top are concerned with *thought
processes* and *reasoning*, whereas the ones on the
bottom address *behavior*. The definitions on the left
measure success in terms of fidelity to *human*
performance, whereas the ones on the right measure against an
*ideal* performance measure, called . A system is rational
if it does the “right thing,” given what it knows.

[tbp] [ai-is-table]

Historically, all four approaches to AI have been followed, each by
different people with different methods. A human-centered approach must
be in part an empirical science, involving observations and hypotheses
about human behavior. A rationalist[^1] approach involves a combination
of mathematics and engineering. The various group have both disparaged
and helped each other. Let us look at the four approaches in more
detail.

### Acting humanly: The Turing Test approach {#turing-test-section}

The , proposed by Alan Turing [-@Turing:1950], was designed to provide a
satisfactory operational definition of intelligence. A computer passes
the test if a human interrogator, after posing some written questions,
cannot tell whether the written responses come from a person or from a
computer. discusses the details of the test and whether a computer would
really be intelligent if it passed. For now, we note that programming a
computer to pass a rigorously applied test provides plenty to work on.
The computer would need to possess the following capabilities:

to enable it to communicate successfully in English; to store what it
knows or hears; to use the stored information to answer questions and to
draw new conclusions; to adapt to new circumstances and to detect and
extrapolate patterns.

Turing’s test deliberately avoided direct physical interaction between
the interrogator and the computer, because *physical*
simulation of a person is unnecessary for intelligence. However, the
so-called includes a video signal so that the interrogator can test the
subject’s perceptual abilities, as well as the opportunity for the
interrogator to pass physical objects “through the hatch.” To pass the
total Turing Test, the computer will need

to perceive objects, and to manipulate objects and move about.

These six disciplines compose most of AI, and Turing deserves credit for
designing a test that remains relevant 60 years later. Yet AI
researchers have devoted little effort to passing the Turing Test,
believing that it is more important to study the underlying principles
of intelligence than to duplicate an exemplar. The quest for “artificial
flight” succeeded when the Wright brothers and others stopped imitating
birds and started using wind tunnels and learning about aerodynamics.
Aeronautical engineering texts do not define the goal of their field as
making “machines that fly so exactly like pigeons that they can fool
even other pigeons.”

### Thinking humanly: The cognitive modeling approach

If we are going to say that a given program thinks like a human, we must
have some way of determining how humans think. We need to get
*inside* the actual workings of human minds. There are
three ways to do this: through introspection—trying to catch our own
thoughts as they go by; through psychological experiments—observing a
person in action; and through brain imaging—observing the brain in
action. Once we have a sufficiently precise theory of the mind, it
becomes possible to express the theory as a computer program. If the
program’s input–output behavior matches corresponding human behavior,
that is evidence that some of the program’s mechanisms could also be
operating in humans. For example, Allen Newell and Herbert Simon, who
developed , the “General Problem Solver” @Newell+Simon:1961, were not
content merely to have their program solve problems correctly. They were
more concerned with comparing the trace of its reasoning steps to traces
of human subjects solving the same problems. The interdisciplinary field
of brings together computer models from AI and experimental techniques
from psychology to construct precise and testable theories of the human
mind.

Cognitive science is a fascinating field in itself, worthy of several
textbooks and at least one encyclopedia @Wilson+Keil:1999. We will
occasionally comment on similarities or differences between AI
techniques and human cognition. Real cognitive science, however, is
necessarily based on experimental investigation of actual humans or
animals. We will leave that for other books, as we assume the reader has
only a computer for experimentation.

In the early days of AI there was often confusion between the
approaches: an author would argue that an algorithm performs well on a
task and that it is *therefore* a good model of human
performance, or vice versa. Modern authors separate the two kinds of
claims; this distinction has allowed both AI and cognitive science to
develop more rapidly. The two fields continue to fertilize each other,
most notably in computer vision, which incorporates neurophysiological
evidence into computational models.

### Thinking rationally: The “laws of thought” approach

The Greek philosopher Aristotle was one of the first to attempt to
codify “right thinking,” that is, irrefutable reasoning processes. His
provided patterns for argument structures that always yielded correct
conclusions when given correct premises—for example, “Socrates is a man;
all men are mortal; therefore, Socrates is mortal.” These laws of
thought were supposed to govern the operation of the mind; their study
initiated the field called .

Logicians in the 19th century developed a precise notation for
statements about all kinds of objects in the world and the relations
among them. (Contrast this with ordinary arithmetic notation, which
provides only for statements about *numbers*.) By 1965,
programs existed that could, in principle, solve *any*
solvable problem described in logical notation. (Although if no solution
exists, the program might loop forever.) The so-called tradition within
artificial intelligence hopes to build on such programs to create
intelligent systems.

There are two main obstacles to this approach. First, it is not easy to
take informal knowledge and state it in the formal terms required by
logical notation, particularly when the knowledge is less than 100%
certain. Second, there is a big difference between solving a problem “in
principle” and solving it in practice. Even problems with just a few
hundred facts can exhaust the computational resources of any computer
unless it has some guidance as to which reasoning steps to try first.
Although both of these obstacles apply to *any* attempt to
build computational reasoning systems, they appeared first in the
logicist tradition.

### Acting rationally: The rational agent approach

An is just something that acts (*agent* comes from the
Latin *agere*, to do). Of course, all computer programs do
something, but computer agents are expected to do more: operate
autonomously, perceive their environment, persist over a prolonged time
period, adapt to change, and create and pursue goals. A is one that acts
so as to achieve the best outcome or, when there is uncertainty, the
best expected outcome.

In the “laws of thought” approach to AI, the emphasis was on correct
inferences. Making correct inferences is sometimes *part*
of being a rational agent, because one way to act rationally is to
reason logically to the conclusion that a given action will achieve
one’s goals and then to act on that conclusion. On the other hand,
correct inference is not *all* of rationality; in some
situations, there is no provably correct thing to do, but something must
still be done. There are also ways of acting rationally that cannot be
said to involve inference. For example, recoiling from a hot stove is a
reflex action that is usually more successful than a slower action taken
after careful deliberation.

All the skills needed for the Turing Test also allow an agent to act
rationally. Knowledge representation and reasoning enable agents to
reach good decisions. We need to be able to generate comprehensible
sentences in natural language to get by in a complex society. We need
learning not only for erudition, but also because it improves our
ability to generate effective behavior.

The rational-agent approach has two advantages over the other
approaches. First, it is more general than the “laws of thought”
approach because correct inference is just one of several possible
mechanisms for achieving rationality. Second, it is more amenable to
scientific development than are approaches based on human behavior or
human thought. The standard of rationality is mathematically well
defined and completely general, and can be “unpacked” to generate agent
designs that provably achieve it. Human behavior, on the other hand, is
well adapted for one specific environment and is defined by, well, the
sum total of all the things that humans do.

This book therefore concentrates on general principles of rational
agents and on components for constructing them.

We will see that despite the apparent simplicity with which the problem
can be stated, an enormous variety of issues come up when we try to
solve it. outlines some of these issues in more detail.

One important point to keep in mind: We will see before too long that
achieving perfect rationality—always doing the right thing—is not
feasible in complicated environments. The computational demands are just
too high. For most of the book, however, we will adopt the working
hypothesis that perfect rationality is a good starting point for
analysis. It simplifies the problem and provides the appropriate setting
for most of the foundational material in the field. Chapters
[game-playing-chapter] and [complex-decisions-chapter] deal explicitly
with the issue of —acting appropriately when there is not enough time to
do all the computations one might like.

The Foundations of Artificial Intelligence {#prehistory-section}
------------------------------------------

In this section, we provide a brief history of the disciplines that
contributed ideas, viewpoints, and techniques to AI. Like any history,
this one is forced to concentrate on a small number of people, events,
and ideas and to ignore others that also were important. We organize the
history around a series of questions. We certainly would not wish to
give the impression that these questions are the only ones the
disciplines address or that the disciplines have all been working toward
AI as their ultimate fruition.

### Philosophy

-   Can formal rules be used to draw valid conclusions?

-   How does the mind arise from a physical brain?

-   Where does knowledge come from?

-   How does knowledge lead to action?

Aristotle (384–322 b.c.), whose bust appears on the front
cover of this book, was the first to formulate a precise set of laws
governing the rational part of the mind. He developed an informal system
of syllogisms for proper reasoning, which in principle allowed one to
generate conclusions mechanically, given initial premises. Much later,
Ramon Lull (d. 1315) had the idea that useful reasoning could actually
be carried out by a mechanical artifact. Thomas Hobbes (1588–1679)
proposed that reasoning was like numerical computation, that “we add and
subtract in our silent thoughts.” The automation of computation itself
was already well under way. Around 1500, Leonardo da Vinci (1452–1519)
designed but did not build a mechanical calculator; recent
reconstructions have shown the design to be functional. The first known
calculating machine was constructed around 1623 by the German scientist
Wilhelm Schickard (1592–1635), although the Pascaline, built in 1642 by
Blaise Pascal (1623–1662), is more famous. Pascal wrote that “the
arithmetical machine produces effects which appear nearer to thought
than all the actions of animals.” Gottfried Wilhelm Leibniz (1646–1716)
built a mechanical device intended to carry out operations on concepts
rather than numbers, but its scope was rather limited. Leibniz did
surpass Pascal by building a calculator that could add, subtract,
multiply, and take roots, whereas the Pascaline could only add and
subtract.[calculator-page] Some speculated that machines might not just
do calculations but actually be able to think and act on their own. In
his 1651 book *Leviathan*, Thomas Hobbes suggested the idea
of an “artificial animal,” arguing “For what is the heart but a spring;
and the nerves, but so many strings; and the joints, but so many
wheels.”

It’s one thing to say that the mind operates, at least in part,
according to logical rules, and to build physical systems that emulate
some of those rules; it’s another to say that the mind itself
*is* such a physical system. René Descartes (1596–1650)
gave the first clear discussion of the distinction between mind and
matter and of the problems that arise. One problem with a purely
physical conception of the mind is that it seems to leave little room
for free will: if the mind is governed entirely by physical laws, then
it has no more free will than a rock “deciding” to fall toward the
center of the earth. Descartes was a strong advocate of the power of
reasoning in understanding the world, a philosophy now called , and one
that counts Aristotle and Leibnitz as members. But Descartes was also a
proponent of . He held that there is a part of the human mind (or soul
or spirit) that is outside of nature, exempt from physical laws.
Animals, on the other hand, did not possess this dual quality; they
could be treated as machines. An alternative to dualism is , which holds
that the brain’s operation according to the laws of physics
*constitutes* the mind. Free will is simply the way that
the perception of available choices appears to the choosing entity.

Given a physical mind that manipulates knowledge, the next problem is to
establish the source of knowledge. The movement, starting with Francis
Bacon’s (1561–1626) *Novum Organum*,[^2] is characterized
by a dictum of John Locke (1632–1704): “Nothing is in the understanding,
which was not first in the senses.” David Hume’s (1711–1776) *A
Treatise of Human Nature* @Hume:1739 proposed what is now known
as the principle of : that general rules are acquired by exposure to
repeated associations between their elements. Building on the work of
Ludwig Wittgenstein (1889–1951) and Bertrand Russell (1872–1970), the
famous Vienna Circle, led by Rudolf Carnap (1891–1970), developed the
doctrine of . This doctrine holds that all knowledge can be
characterized by logical theories connected, ultimately, to that
correspond to sensory inputs; thus logical positivism combines
rationalism and empiricism.[^3] The of Carnap and Carl Hempel
(1905–1997) attempted to analyze the acquisition of knowledge from
experience. Carnap’s book *The Logical Structure of the
World* [-@Carnap:1928] defined an explicit computational
procedure for extracting knowledge from elementary experiences. It was
probably the first theory of mind as a computational process.

The final element in the philosophical picture of the mind is the
connection between knowledge and action. This question is vital to AI
because intelligence requires action as well as reasoning. Moreover,
only by understanding how actions are justified can we understand how to
build an agent whose actions are justifiable (or rational). Aristotle
argued (in *De Motu Animalium*) that actions are justified
by a logical connection between goals and knowledge of the action’s
outcome (the last part of this extract also appears on the front cover
of this book, in the original Greek):

But how does it happen that thinking is sometimes accompanied by action
and sometimes not, sometimes by motion, and sometimes not? It looks as
if almost the same thing happens as in the case of reasoning and making
inferences about unchanging objects. But in that case the end is a
speculative proposition $\ldots$ whereas here the conclusion which
results from the two premises is an action. $\ldots$ I need covering; a
cloak is a covering. I need a cloak. What I need, I have to make; I need
a cloak. I have to make a cloak. And the conclusion, the “I have to make
a cloak,” is an action.

In the *Nicomachean Ethics* (Book III. 3, 1112b), Aristotle
further elaborates on this topic, suggesting an algorithm:

We deliberate not about ends, but about means. For a doctor does not
deliberate whether he shall heal, nor an orator whether he shall
persuade, $\ldots$ They assume the end and consider how and by what
means it is attained, and if it seems easily and best produced thereby;
while if it is achieved by one means only they consider
*how* it will be achieved by this and by what means
*this* will be achieved, till they come to the first cause,
$\ldots$ and what is last in the order of analysis seems to be first in
the order of becoming. And if we come on an impossibility, we give up
the search, e.g., if we need money and this cannot be got; but if a
thing appears possible we try to do it.

Aristotle’s algorithm was implemented 2300 years later by Newell and
Simon in their program. We would now call it a regression planning
system (see ).

Goal-based analysis is useful, but does not say what to do when several
actions will achieve the goal or when no action will achieve it
completely. Antoine Arnauld (1612–1694) correctly described a
quantitative formula for deciding what action to take in cases like this
(see ). John Stuart Mill’s (1806–1873) book
*Utilitarianism* @Mill:1863 promoted the idea of rational
decision criteria in all spheres of human activity. The more formal
theory of decisions is discussed in the following section.

### Mathematics {#history-of-math-section}

-   What are the formal rules to draw valid conclusions?

-   What can be computed?

-   How do we reason with uncertain information?

Philosophers staked out some of the fundamental ideas of AI, but the
leap to a formal science required a level of mathematical formalization
in three fundamental areas: logic, computation, and probability.

The idea of formal logic can be traced back to the philosophers of
ancient Greece, but its mathematical development really began with the
work of George Boole (1815–1864), who worked out the details of
propositional, or Boolean, logic @Boole:1847. In 1879, Gottlob Frege
(1848–1925) extended Boole’s logic to include objects and relations,
creating the first-order logic that is used today.[^4] Alfred Tarski
(1902–1983) introduced a theory of reference that shows how to relate
the objects in a logic to objects in the real world.

The next step was to determine the limits of what could be done with
logic and computation. The first nontrivial is thought to be Euclid’s
algorithm for computing greatest common divisors. The word
*algorithm* (and the idea of studying them) comes from
al-Khowarazmi, a Persian mathematician of the 9th century, whose
writings also introduced Arabic numerals and algebra to Europe. Boole
and others discussed algorithms for logical deduction, and, by the late
19th century, efforts were under way to formalize general mathematical
reasoning as logical deduction. In 1930, Kurt Gödel (1906–1978) showed
that there exists an effective procedure to prove any true statement in
the first-order logic of Frege and Russell, but that first-order logic
could not capture the principle of mathematical induction needed to
characterize the natural numbers. In 1931, Gödel showed that limits on
deduction do exist. His showed that in any formal theory as strong as
Peano arithmetic (the elementary theory of natural numbers), there are
true statements that are undecidable in the sense that they have no
proof within the theory.

This fundamental result can also be interpreted as showing that some
functions on the integers cannot be represented by an algorithm—that is,
they cannot be computed. This motivated Alan Turing (1912–1954) to try
to characterize exactly which functions *are* —capable of
being computed. This notion is actually slightly problematic because the
notion of a computation or effective procedure really cannot be given a
formal definition. However, the Church–Turing thesis, which states that
the Turing machine @Turing:1936 is capable of computing any computable
function, is generally accepted as providing a sufficient definition.
Turing also showed that there were some functions that no Turing machine
can compute. For example, no machine can tell *in general*
whether a given program will return an answer on a given input or run
forever.

Although decidability and computability are important to an
understanding of computation, the notion of has had an even greater
impact. Roughly speaking, a problem is called intractable if the time
required to solve instances of the problem grows exponentially with the
size of the instances. The distinction between polynomial and
exponential growth in complexity was first emphasized in the
mid-1960s @Cobham:1964 [@Edmonds:1965]. It is important because
exponential growth means that even moderately large instances cannot be
solved in any reasonable time. Therefore, one should strive to divide
the overall problem of generating intelligent behavior into tractable
subproblems rather than intractable ones.

How can one recognize an intractable problem? The theory of , pioneered
by Steven Cook [-@Cook:1971] and Richard Karp [-@Karp:1972], provides a
method. Cook and Karp showed the existence of large classes of canonical
combinatorial search and reasoning problems that are NP-complete. Any
problem class to which the class of NP-complete problems can be reduced
is likely to be intractable. (Although it has not been proved that
NP-complete problems are necessarily intractable, most theoreticians
believe it.) These results contrast with the optimism with which the
popular press greeted the first computers—“Electronic Super-Brains” that
were “Faster than Einstein!” Despite the increasing speed of computers,
careful use of resources will characterize intelligent systems. Put
crudely, the world is an *extremely* large problem
instance! Work in AI has helped explain why some instances of
NP-complete problems are hard, yet others are easy @Cheeseman+al:1991.

Besides logic and computation, the third great contribution of
mathematics to AI is the theory of . The Italian Gerolamo Cardano
(1501–1576) first framed the idea of probability, describing it in terms
of the possible outcomes of gambling events. In 1654, Blaise Pascal
(1623–1662), in a letter to Pierre Fermat (1601–1665), showed how to
predict the future of an unfinished gambling game and assign average
payoffs to the gamblers. Probability quickly became an invaluable part
of all the quantitative sciences, helping to deal with uncertain
measurements and incomplete theories. James Bernoulli (1654–1705),
Pierre Laplace (1749–1827), and others advanced the theory and
introduced new statistical methods. Thomas Bayes (1702–1761), who
appears on the front cover of this book, proposed a rule for updating
probabilities in the light of new evidence. Bayes’ rule underlies most
modern approaches to uncertain reasoning in AI systems.

### Economics

-   How should we make decisions so as to maximize payoff?

-   How should we do this when others may not go along?

-   How should we do this when the payoff may be far in the future?

The science of economics got its start in 1776, when Scottish
philosopher Adam Smith (1723–1790) published *An Inquiry into the
Nature and Causes of the Wealth of Nations*. While the ancient
Greeks and others had made contributions to economic thought, Smith was
the first to treat it as a science, using the idea that economies can be
thought of as consisting of individual agents maximizing their own
economic well-being. Most people think of economics as being about
money, but economists will say that they are really studying how people
make choices that lead to preferred outcomes. When McDonald’s offers a
hamburger for a dollar, they are asserting that they would prefer the
dollar and hoping that customers will prefer the hamburger. The
mathematical treatment of “preferred outcomes” or was first formalized
by Léon Walras (pronounced “Valrasse”) (1834-1910) and was improved by
Frank Ramsey [-@Ramsey:1931] and later by John von Neumann and Oskar
Morgenstern in their book *The Theory of Games and Economic
Behavior* [-@VonNeumann+Morgenstern:1944].

, which combines probability theory with utility theory, provides a
formal and complete framework for decisions (economic or otherwise) made
under uncertainty—that is, in cases where probabilistic descriptions
appropriately capture the decision maker’s environment. This is suitable
for “large” economies where each agent need pay no attention to the
actions of other agents as individuals. For “small” economies, the
situation is much more like a : the actions of one player can
significantly affect the utility of another (either positively or
negatively). Von Neumann and Morgenstern’s development of  \<see
also\>Luce+Raiffa:1957 included the surprising result that,
for some games, a rational agent should adopt policies that are (or
least appear to be) randomized. Unlike decision theory, game theory does
not offer an unambiguous prescription for selecting actions.

For the most part, economists did not address the third question listed
above, namely, how to make rational decisions when payoffs from actions
are not immediate but instead result from several actions taken
*in sequence*. This topic was pursued in the field of ,
which emerged in World War II from efforts in Britain to optimize radar
installations, and later found civilian applications in complex
management decisions. The work of Richard Bellman [-@Bellman:1957]
formalized a class of sequential decision problems called , which we
study in Chapters [complex-decisions-chapter]
and [reinforcement-learning-chapter].

Work in economics and operations research has contributed much to our
notion of rational agents, yet for many years AI research developed
along entirely separate paths. One reason was the apparent complexity of
making rational decisions. The pioneering AI researcher Herbert Simon
(1916–2001) won the Nobel Prize in economics in 1978 for his early work
showing that models based on —making decisions that are “good enough,”
rather than laboriously calculating an optimal decision—gave a better
description of actual human behavior @Simon:1947. Since the 1990s, there
has been a resurgence of interest in decision-theoretic techniques for
agent systems @Wellman:1995.

### Neuroscience

-   How do brains process information?

is the study of the nervous system, particularly the brain. Although the
exact way in which the brain enables thought is one of the great
mysteries of science, the fact that it *does* enable
thought has been appreciated for thousands of years because of the
evidence that strong blows to the head can lead to mental
incapacitation. It has also long been known that human brains are
somehow different; in about 335 b.c. Aristotle wrote, “Of
all the animals, man has the largest brain in proportion to his
size.”[^5] Still, it was not until the middle of the 18th century that
the brain was widely recognized as the seat of consciousness. Before
then, candidate locations included the heart and the spleen.

Paul Broca’s (1824–1880) study of aphasia (speech deficit) in
brain-damaged patients in 1861 demonstrated the existence of localized
areas of the brain responsible for specific cognitive functions. In
particular, he showed that speech production was localized to the
portion of the left hemisphere now called Broca’s area.[^6] By that
time, it was known that the brain consisted of nerve cells, or , but it
was not until 1873 that Camillo Golgi (1843–1926) developed a staining
technique allowing the observation of individual neurons in the brain
(see ). This technique was used by Santiago Ramon y Cajal (1852–1934) in
his pioneering studies of the brain’s neuronal structures.[^7] Nicolas
was the first to apply mathematical models to the study of the nervous
sytem.

[neuron-figure]

We now have some data on the mapping between areas of the brain and the
parts of the body that they control or from which they receive sensory
input. Such mappings are able to change radically over the course of a
few weeks, and some animals seem to have multiple maps. Moreover, we do
not fully understand how other areas can take over functions when one
area is damaged. There is almost no theory on how an individual memory
is stored.

The measurement of intact brain activity began in 1929 with the
invention by Hans Berger of the electroencephalograph (EEG). The recent
development of functional magnetic resonance imaging
(fMRI) @Ogawa+al:1990 [@Cabeza+Nyberg:2000] is giving neuroscientists
unprecedentedly detailed images of brain activity, enabling measurements
that correspond in interesting ways to ongoing cognitive processes.
These are augmented by advances in single-cell recording of neuron
activity. Individual neurons can be stimulated electrically, chemically,
or even optically @Han+Boyden:2007, allowing neuronal input–output
relationships to be mapped. Despite these advances, we are still a long
way from understanding how cognitive processes actually work.

The truly amazing conclusion is that

a collection of simple cells can lead to thought, action, and
consciousness

or, in the pithy words of John Searle [-@Searle:1992], *brains
cause minds*. The only real alternative theory is mysticism: that
minds operate in some mystical realm that is beyond physical science.

Brains and digital computers have somewhat different properties. shows
that computers have a cycle time that is a million times faster than a
brain. The brain makes up for that with far more storage and
interconnection than even a high-end personal computer, although the
largest supercomputers have a capacity that is similar to the brain’s.
(It should be noted, however, that the brain does not seem to use all of
its neurons simultaneously.) Futurists make much of these numbers,
pointing to an approaching at which computers reach a superhuman level
of performance @Vinge:1993 [@Kurzweil:2005], but the raw comparisons are
not especially informative. Even with a computer of virtually unlimited
capacity, we still would not know how to achieve the brain’s level of
intelligence.

[computer-brain-table]

### Psychology {#history-of-psychology-section}

-   How do humans and animals think and act?

The origins of scientific psychology are usually traced to the work of
the German physicist Hermann von Helmholtz (1821–1894) and his student
Wilhelm Wundt (1832–1920). Helmholtz applied the scientific method to
the study of human vision, and his *Handbook of Physiological
Optics* is even now described as “the single most important
treatise on the physics and physiology of human vision” @Nalwa:1993
[p.15]. In 1879, Wundt opened the first laboratory of experimental
psychology, at the University of Leipzig. Wundt insisted on carefully
controlled experiments in which his workers would perform a perceptual
or associative task while introspecting on their thought processes. The
careful controls went a long way toward making psychology a science, but
the subjective nature of the data made it unlikely that an experimenter
would ever disconfirm his or her own theories. Biologists studying
animal behavior, on the other hand, lacked introspective data and
developed an objective methodology, as described by
H. S. Jennings [-@Jennings:1906] in his influential work *Behavior
of the Lower Organisms*. Applying this viewpoint to humans, the
movement, led by John Watson (1878–1958), rejected *any*
theory involving mental processes on the grounds that introspection
could not provide reliable evidence. Behaviorists insisted on studying
only objective measures of the percepts (or *stimulus*)
given to an animal and its resulting actions (or
*response*). Behaviorism discovered a lot about rats and
pigeons but had less success at understanding humans.

, which views the brain as an information-processing device, can be
traced back at least to the works of William James (1842–1910).
Helmholtz also insisted that perception involved a form of unconscious
logical inference. The cognitive viewpoint was largely eclipsed by
behaviorism in the , but at Cambridge’s Applied Psychology Unit,
directed by Frederic Bartlett (1886–1969), cognitive modeling was able
to flourish. *The Nature of Explanation*, by Bartlett’s
student and successor Kenneth Craik [-@Craik:1943], forcefully
reestablished the legitimacy of such “mental” terms as beliefs and
goals, arguing that they are just as scientific as, say, using pressure
and temperature to talk about gases, despite their being made of
molecules that have neither. Craik specified the three key steps of a
knowledge-based agent: (1) the stimulus must be translated into an
internal representation, (2) the representation is manipulated by
cognitive processes to derive new internal representations, and (3)
these are in turn retranslated back into action. He clearly explained
why this was a good design for an agent:[craik-quote-page]

If the organism carries a “small-scale model” of external reality and of
its own possible actions within its head, it is able to try out various
alternatives, conclude which is the best of them, react to future
situations before they arise, utilize the knowledge of past events in
dealing with the present and future, and in every way to react in a much
fuller, safer, and more competent manner to the emergencies which face
it. @Craik:1943

After Craik’s death in a bicycle accident in 1945, his work was
continued by Donald Broadbent, whose book *Perception and
Communication* [-@Broadbent:1958] was one of the first works to
model psychological phenomena as information processing. Meanwhile, in
the , the development of computer modeling led to the creation of the
field of . The field can be said to have started at a workshop in
September 1956 at MIT. (We shall see that this is just two months after
the conference at which AI itself was “born.”) At the workshop, George
Miller presented *The Magic Number Seven*, Noam Chomsky
presented *Three Models of Language*, and Allen Newell and
Herbert Simon presented *The Logic Theory Machine*. These
three influential papers showed how computer models could be used to
address the psychology of memory, language, and logical thinking,
respectively. It is now a common (although far from universal) view
among psychologists that “a cognitive theory should be like a computer
program” @Anderson:1980; that is, it should describe a detailed
information-processing mechanism whereby some cognitive function might
be implemented.

### Computer engineering

-   How can we build an efficient computer?

For artificial intelligence to succeed, we need two things: intelligence
and an artifact. The computer has been the artifact of choice. The
modern digital electronic computer was invented independently and almost
simultaneously by scientists in three countries embattled in World War
II. The first *operational* computer was the
electromechanical Heath Robinson,[^8] built in 1940 by Alan Turing’s
team for a single purpose: deciphering German messages. In 1943, the
same group developed the Colossus, a powerful general-purpose machine
based on vacuum tubes.[^9] The first operational
*programmable* computer was the Z-3, the invention of
Konrad Zuse in Germany in 1941. Zuse also invented floating-point
numbers and the first high-level programming language, Plankalkül. The
first *electronic* computer, the ABC, was assembled by John
Atanasoff and his student Clifford Berry between 1940 and 1942 at Iowa
State University. Atanasoff’s research received little support or
recognition; it was the ENIAC, developed as part of a secret military
project at the University of Pennsylvania by a team including John
Mauchly and John Eckert, that proved to be the most influential
forerunner of modern computers.

Since that time, each generation of computer hardware has brought an
increase in speed and capacity and a decrease in price. Performance
doubled every 18 months or so until around 2005, when power dissipation
problems led manufacturers to start multiplying the number of CPU cores
rather than the clock speed. Current expectations are that future
increases in power will come from massive parallelism—a curious
convergence with the properties of the brain.

Of course, there were calculating devices before the electronic
computer. The earliest automated machines, dating from the 17th century,
were discussed on . The first *programmable* machine was a
loom, devised in 1805 by Joseph Marie Jacquard
(1752–1834)[jacquard-page], that used punched cards to store
instructions for the pattern to be woven. In the mid-19th century,
Charles Babbage (1792–1871) designed two machines, neither of which he
completed. The Difference Engine was intended to compute mathematical
tables for engineering and scientific projects. It was finally built and
shown to work in 1991 at the Science Museum in London @Swade:2000.
Babbage’s Analytical Engine was far more ambitious: it included
addressable memory, stored programs, and conditional jumps and was the
first artifact capable of universal computation. Babbage’s colleague Ada
Lovelace, daughter of the poet Lord Byron, was perhaps the world’s first
programmer. (The programming language Ada is named after her.) She wrote
programs for the unfinished Analytical Engine and even speculated that
the machine could play chess or compose music.

AI also owes a debt to the software side of computer science, which has
supplied the operating systems, programming languages, and tools needed
to write modern programs (and papers about them). But this is one area
where the debt has been repaid: work in AI has pioneered many ideas that
have made their way back to mainstream computer science, including time
sharing, interactive interpreters, personal computers with windows and
mice, rapid development environments, the linked list data type,
automatic storage management, and key concepts of symbolic, functional,
declarative, and object-oriented programming.

### Control theory and cybernetics

-   How can artifacts operate under their own control?

Ktesibios of Alexandria (c. 250 b.c.) built the first
self-controlling machine: a water clock with a regulator that maintained
a constant flow rate. This invention changed the definition of what an
artifact could do. Previously, only living things could modify their
behavior in response to changes in the environment. Other examples of
self-regulating feedback control systems include the steam engine
governor, created by James Watt (1736–1819), and the thermostat,
invented by Cornelis Drebbel (1572–1633), who also invented the
submarine. The mathematical theory of stable feedback systems was
developed in the 19th century.

The central figure in the creation of what is now called was Norbert
Wiener (1894–1964). Wiener was a brilliant mathematician who worked with
Bertrand Russell, among others, before developing an interest in
biological and mechanical control systems and their connection to
cognition. Like Craik (who also used control systems as psychological
models), Wiener and his colleagues Arturo Rosenblueth and Julian Bigelow
challenged the behaviorist orthodoxy @Rosenblueth+al:1943. They viewed
purposive behavior as arising from a regulatory mechanism trying to
minimize “error”—the difference between current state and goal state. In
the late 1940s, Wiener, along with Warren McCulloch, Walter Pitts, and
John von Neumann, organized a series of influential conferences that
explored the new mathematical and computational models of cognition.
Wiener’s book *Cybernetics* [-@Wiener:1948] became a
bestseller and awoke the public to the possibility of artificially
intelligent machines. Meanwhile, in Britain, W. Ross Ashby @Ashby:1940
pioneered similar ideas. Ashby, Alan Turing, Grey Walter, and others
formed the Ratio Club for “those who had Wiener’s ideas before Wiener’s
book appeared.” Ashby’s *Design for a Brain*
[-@Ashby:1948; -@Ashby:1952] elaborated on his idea that intelligence
could be created by the use of devices containing appropriate feedback
loops to achieve stable adaptive behavior.

Modern control theory, especially the branch known as stochastic optimal
control, has as its goal the design of systems that maximize an over
time. This roughly matches our view of AI: designing systems that behave
optimally. Why, then, are AI and control theory two different fields,
despite the close connections among their founders? The answer lies in
the close coupling between the mathematical techniques that were
familiar to the participants and the corresponding sets of problems that
were encompassed in each world view. Calculus and matrix algebra, the
tools of control theory, lend themselves to systems that are describable
by fixed sets of continuous variables, whereas AI was founded in part as
a way to escape from the these perceived limitations. The tools of
logical inference and computation allowed AI researchers to consider
problems such as language, vision, and planning that fell completely
outside the control theorist’s purview.

### Linguistics

-   How does language relate to thought?

In 1957, B. F. Skinner published *Verbal Behavior*. This
was a comprehensive, detailed account of the behaviorist approach to
language learning, written by the foremost expert in the field. But
curiously, a review of the book became as well known as the book itself,
and served to almost kill off interest in behaviorism. The author of the
review was the linguist Noam Chomsky, who had just published a book on
his own theory, *Syntactic Structures*. Chomsky pointed out
that the behaviorist theory did not address the notion of creativity in
language—it did not explain how a child could understand and make up
sentences that he or she had never heard before. Chomsky’s theory—based
on syntactic models going back to the Indian linguist Panini (c. 350
b.c.)—could explain this, and unlike previous theories, it
was formal enough that it could in principle be programmed.

Modern linguistics and AI, then, were “born” at about the same time, and
grew up together, intersecting in a hybrid field called or . The problem
of understanding language soon turned out to be considerably more
complex than it seemed in 1957. Understanding language requires an
understanding of the subject matter and context, not just an
understanding of the structure of sentences. This might seem obvious,
but it was not widely appreciated until the 1960s. Much of the early
work in (the study of how to put knowledge into a form that a computer
can reason with) was tied to language and informed by research in
linguistics, which was connected in turn to decades of work on the
philosophical analysis of language.

The History of Artificial Intelligence
--------------------------------------

With the background material behind us, we are ready to cover the
development of AI itself.

### The gestation of artificial intelligence (1943–1955)

The first work that is now generally recognized as AI was done by Warren
McCulloch and Walter Pitts [-@McCulloch+Pitts:1943]. They drew on three
sources: knowledge of the basic physiology and function of neurons in
the brain; a formal analysis of propositional logic due to Russell and
Whitehead; and Turing’s theory of computation. They proposed a model of
artificial neurons in which each neuron is characterized as being “on”
or “off,” with a switch to “on” occurring in response to stimulation by
a sufficient number of neighboring neurons. The state of a neuron was
conceived of as “factually equivalent to a proposition which proposed
its adequate stimulus.” They showed, for example, that any computable
function could be computed by some network of connected neurons, and
that all the logical connectives (and, or, not, etc.) could be
implemented by simple net structures. McCulloch and Pitts also suggested
that suitably defined networks could learn. Donald Hebb [-@Hebb:1949]
demonstrated a simple updating rule for modifying the connection
strengths between neurons. His rule, now called , remains an influential
model to this day.

Two undergraduate students at Harvard, Marvin Minsky and Dean Edmonds,
built the first neural network computer in 1950. The , as it was called,
used 3000 vacuum tubes and a surplus automatic pilot mechanism from a
B-24 bomber to simulate a network of 40 neurons. Later, at Princeton,
Minsky studied universal computation in neural networks. His
Ph.D. committee was skeptical about whether this kind of work should be
considered mathematics, but von Neumann reportedly said, “If it isn’t
now, it will be someday.” Minsky was later to prove influential theorems
showing the limitations of neural network research.

There were a number of early examples of work that can be characterized
as AI, but Alan Turing’s vision was perhaps the most influential. He
gave lectures on the topic as early as 1947 at the London Mathematical
Society and articulated a persuasive agenda in his 1950 article
“Computing Machinery and Intelligence.” Therein, he introduced the
Turing Test, machine learning, genetic algorithms, and reinforcement
learning. He proposed the *Child Programme* idea,
explaining “Instead of trying to produce a programme to simulate the
adult mind, why not rather try to produce one which simulated the
child’s?”

### The birth of artificial intelligence (1956)

Princeton was home to another influential figure in AI, John McCarthy.
After receiving his PhD there in 1951 and working for two years as an
instructor, McCarthy moved to Stanford and then to Dartmouth College,
which was to become the official birthplace of the field. McCarthy
convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him
bring together U.S. researchers interested in automata theory, neural
nets, and the study of intelligence. They organized a two-month workshop
at Dartmouth in the summer of 1956. The proposal states:[^10]

> We propose that a 2 month, 10 man study of artificial intelligence be
> carried out during the summer of 1956 at Dartmouth College in Hanover,
> New Hampshire. The study is to proceed on the basis of the conjecture
> that every aspect of learning or any other feature of intelligence can
> in principle be so precisely described that a machine can be made to
> simulate it. An attempt will be made to find how to make machines use
> language, form abstractions and concepts, solve kinds of problems now
> reserved for humans, and improve themselves. We think that a
> significant advance can be made in one or more of these problems if a
> carefully selected group of scientists work on it together for a
> summer.

There were 10 attendees in all, including Trenchard More from Princeton,
Arthur Samuel from IBM, and Ray Solomonoff and Oliver Selfridge from
MIT.

Two researchers from Carnegie Tech,[^11] Allen Newell and Herbert Simon,
rather stole the show. Although the others had ideas and in some cases
programs for particular applications such as checkers, Newell and Simon
already had a reasoning program, the Logic Theorist (), about which
Simon claimed, “We have invented a computer program capable of thinking
non-numerically, and thereby solved the venerable mind–body
problem.”[^12] Soon after the workshop, the program was able to prove
most of the theorems in Chapter 2 of Russell and Whitehead’s
*Principia Mathematica*. Russell was reportedly delighted
when Simon showed him that the program had come up with a proof for one
theorem that was shorter than the one in *Principia*. The
editors of the *Journal of Symbolic Logic* were less
impressed; they rejected a paper coauthored by Newell, Simon, and Logic
Theorist.

The Dartmouth workshop did not lead to any new breakthroughs, but it did
introduce all the major figures to each other. For the next 20 years,
the field would be dominated by these people and their students and
colleagues at MIT, CMU, Stanford, and IBM.

Looking at the proposal for the Dartmouth workshop @McCarthy+al:1955, we
can see why it was necessary for AI to become a separate field. Why
couldn’t all the work done in AI have taken place under the name of
control theory or operations research or decision theory, which, after
all, have objectives similar to those of AI? Or why isn’t AI a branch of
mathematics? The first answer is that AI from the start embraced the
idea of duplicating human faculties such as creativity,
self-improvement, and language use. None of the other fields were
addressing these issues. The second answer is methodology. AI is the
only one of these fields that is clearly a branch of computer science
(although operations research does share an emphasis on computer
simulations), and AI is the only field to attempt to build machines that
will function autonomously in complex, changing environments.

### Early enthusiasm, great expectations (1952–1969)

The early years of AI were full of successes—in a limited way. Given the
primitive computers and programming tools of the time and the fact that
only a few years earlier computers were seen as things that could do
arithmetic and no more, it was astonishing whenever a computer did
anything remotely clever. The intellectual establishment, by and large,
preferred to believe that “a machine can never do $X$.” (See for a long
list of $X$’s gathered by Turing.) AI researchers naturally responded by
demonstrating one $X$ after another. John McCarthy referred to this
period as the “Look, Ma, no hands!” era.

Newell and Simon’s early success was followed up with the General
Problem Solver, or . Unlike Logic Theorist, this program was designed
from the start to imitate human problem-solving protocols. Within the
limited class of puzzles it could handle, it turned out that the order
in which the program considered subgoals and possible actions was
similar to that in which humans approached the same problems. Thus, was
probably the first program to embody the “thinking humanly” approach.
The success of and subsequent programs as models of cognition led to
formulate the famous hypothesis, which states that “a physical symbol
system has the necessary and sufficient means for general intelligent
action.” What they meant is that any system (human or machine)
exhibiting intelligence must operate by manipulating data structures
composed of symbols. We will see later that this hypothesis has been
challenged from many directions.

At IBM, Nathaniel Rochester and his colleagues produced some of the
first AI programs. Herbert Gelernter [-@Gelernter:1959] constructed the
Geometry Theorem Prover, which was able to prove theorems that many
students of mathematics would find quite tricky. Starting in 1952,
Arthur Samuel wrote a series of programs for checkers (draughts) that
eventually learned to play at a strong amateur level. Along the way, he
disproved the idea that computers can do only what they are told to: his
program quickly learned to play a better game than its creator. The
program was demonstrated on television in February 1956, creating a
strong impression. Like Turing, Samuel had trouble finding computer
time. Working at night, he used machines that were still on the testing
floor at IBM’s manufacturing plant. covers game playing, and explains
the learning techniques used by Samuel.

John McCarthy moved from Dartmouth to MIT and there made three crucial
contributions in one historic year: 1958. In MIT AI Lab Memo No. 1,
McCarthy defined the high-level language , which was to become the
dominant AI programming language for the next 30 years. With Lisp,
McCarthy had the tool he needed, but access to scarce and expensive
computing resources was also a serious problem. In response, he and
others at MIT invented time sharing. Also in 1958, McCarthy published a
paper entitled *Programs with Common Sense*, in which he
described the Advice Taker, a hypothetical program that can be seen as
the first complete AI system. Like the Logic Theorist and Geometry
Theorem Prover, McCarthy’s program was designed to use knowledge to
search for solutions to problems. But unlike the others, it was to
embody general knowledge of the world. For example, he showed how some
simple axioms would enable the program to generate a plan to drive to
the airport. The program was also designed to accept new axioms in the
normal course of operation, thereby allowing it to achieve competence in
new areas *without being reprogrammed*. The Advice Taker
thus embodied the central principles of knowledge representation and
reasoning: that it is useful to have a formal, explicit representation
of the world and its workings and to be able to manipulate that
representation with deductive processes. It is remarkable how much of
the 1958 paper remains relevant today.

1958 also marked the year that Marvin Minsky moved to MIT. His initial
collaboration with McCarthy did not last, however. McCarthy stressed
representation and reasoning in formal logic, whereas Minsky was more
interested in getting programs to work and eventually developed an
anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford.
His plan to use logic to build the ultimate Advice Taker was advanced by
J. A. Robinson’s discovery in 1965 of the resolution method (a complete
theorem-proving algorithm for first-order logic; see ). Work at Stanford
emphasized general-purpose methods for logical reasoning. Applications
of logic included Cordell Green’s question-answering and planning
systems @Green:1969 and the Shakey robotics project at the Stanford
Research Institute (SRI). The latter project, discussed further in , was
the first to demonstrate the complete integration of logical reasoning
and physical activity.

Minsky supervised a series of students who chose limited problems that
appeared to require intelligence to solve. These limited domains became
known as . James Slagle’s program [-@Slagle:1963] was able to solve
closed-form calculus integration problems typical of first-year college
courses. Tom Evans’s program [-@Evans:1968] solved geometric analogy
problems that appear in IQ tests. Daniel Bobrow’s program
[-@Bobrow:1967] solved algebra story problems, such as the following:

If the number of customers Tom gets is twice the square of 20 percent of
the number of advertisements he runs, and the number of advertisements
he runs is 45, what is the number of customers Tom gets?

[blocks-world-figure]

The most famous microworld was the blocks world, which consists of a set
of solid blocks placed on a tabletop (or more often, a simulation of a
tabletop), as shown in . A typical task in this world is to rearrange
the blocks in a certain way, using a robot hand that can pick up one
block at a time. The blocks world was home to the vision project of
David Huffman [-@Huffman:1971], the vision and constraint-propagation
work of David Waltz [-@Waltz:1975], the learning theory of Patrick
Winston [-@Winston:1970], the natural-language-understanding program of
Terry Winograd [-@Winograd:1972], and the planner of Scott Fahlman
[-@Fahlman:1974].

Early work building on the neural networks of McCulloch and Pitts also
flourished. The work of Winograd and Cowan [-@Winograd+Cowan:1963]
showed how a large number of elements could collectively represent an
individual concept, with a corresponding increase in robustness and
parallelism. Hebb’s learning methods were enhanced by Bernie
Widrow @Widrow+Hoff:1960 [@Widrow:1962], who called his networks , and
by Frank Rosenblatt [-@Rosenblatt:1962] with his . The @Block+al:1962
says that the learning algorithm can adjust the connection strengths of
a perceptron to match any input data, provided such a match exists.
These topics are covered in .

### A dose of reality (1966–1973)

From the beginning, AI researchers were not shy about making predictions
of their coming successes. The following statement by Herbert Simon in
1957 is often quoted:

It is not my aim to surprise or shock you—but the simplest way I can
summarize is to say that there are now in the world machines that think,
that learn and that create. Moreover, their ability to do these things
is going to increase rapidly until—in a visible future—the range of
problems they can handle will be coextensive with the range to which the
human mind has been applied.

Terms such as “visible future” can be interpreted in various ways, but
Simon also made more concrete predictions: that within 10 years a
computer would be chess champion, and a significant mathematical theorem
would be proved by machine. These predictions came true (or
approximately true) within 40 years rather than 10. Simon’s
overconfidence was due to the promising performance of early AI systems
on simple examples. In almost all cases, however, these early systems
turned out to fail miserably when tried out on wider selections of
problems and on more difficult problems.

The first kind of difficulty arose because most early programs knew
nothing of their subject matter; they succeeded by means of simple
syntactic manipulations. A typical story occurred in early machine
translation efforts, which were generously funded by the U.S. National
Research Council in an attempt to speed up the translation of Russian
scientific papers in the wake of the Sputnik launch in 1957. It was
thought initially that simple syntactic transformations based on the
grammars of Russian and English, and word replacement from an electronic
dictionary, would suffice to preserve the exact meanings of sentences.
The fact is that accurate translation requires background knowledge in
order to resolve ambiguity and establish the content of the sentence.
The famous retranslation of “the spirit is willing but the flesh is
weak” as “the vodka is good but the meat is rotten” illustrates the
difficulties encountered. In 1966, a report by an advisory committee
found that “there has been no machine translation of general scientific
text, and none is in immediate prospect.” All U.S. government funding
for academic translation projects was canceled. Today, machine
translation is an imperfect but widely used tool for technical,
commercial, government, and Internet documents.

The second kind of difficulty was the intractability of many of the
problems that AI was attempting to solve. Most of the early AI programs
solved problems by trying out different combinations of steps until the
solution was found. This strategy worked initially because microworlds
contained very few objects and hence very few possible actions and very
short solution sequences. Before the theory of computational complexity
was developed, it was widely thought that “scaling up” to larger
problems was simply a matter of faster hardware and larger memories. The
optimism that accompanied the development of resolution theorem proving,
for example, was soon dampened when researchers failed to prove theorems
involving more than a few dozen facts.

The fact that a program can find a solution in principle does not mean
that the program contains any of the mechanisms needed to find it in
practice.

The illusion of unlimited computational power was not confined to
problem-solving programs. Early experiments in (now called )
@Friedberg:1958 [@Friedberg+al:1959] were based on the undoubtedly
correct belief that by making an appropriate series of small mutations
to a machine-code program, one can generate a program with good
performance for any particular task. The idea, then, was to try random
mutations with a selection process to preserve mutations that seemed
useful. Despite thousands of hours of CPU time, almost no progress was
demonstrated. Modern genetic algorithms use better representations and
have shown more success.

Failure to come to grips with the “combinatorial explosion” was one of
the main criticisms of AI contained in the Lighthill
report @Lighthill:1973, which formed the basis for the decision by the
British government to end support for AI research in all but two
universities. (Oral tradition paints a somewhat different and more
colorful picture, with political ambitions and personal animosities
whose description is beside the point.)

A third difficulty arose because of some fundamental limitations on the
basic structures being used to generate intelligent behavior. For
example, Minsky and Papert’s book *Perceptrons*
[-@Minsky+Papert:1969] proved that, although perceptrons (a simple form
of neural network) could be shown to learn anything they were capable of
representing, they could represent very little. In particular, a
two-input perceptron (restricted to be simpler than the form Rosenblatt
originally studied) could not be trained to recognize when its two
inputs were different. Although their results did not apply to more
complex, multilayer networks, research funding for neural-net research
soon dwindled to almost nothing. Ironically, the new back-propagation
learning algorithms for multilayer networks that were to cause an
enormous resurgence in neural-net research in the late 1980s were
actually discovered first in 1969 @Bryson+Ho:1969.

### Knowledge-based systems: The key to power? (1969–1979)

The picture of problem solving that had arisen during the first decade
of AI research was of a general-purpose search mechanism trying to
string together elementary reasoning steps to find complete solutions.
Such approaches have been called because, although general, they do not
scale up to large or difficult problem instances. The alternative to
weak methods is to use more powerful, domain-specific knowledge that
allows larger reasoning steps and can more easily handle typically
occurring cases in narrow areas of expertise. One might say that to
solve a hard problem, you have to almost know the answer already.

The program @Buchanan+al:1969 was an early example of this approach. It
was developed at Stanford, where Ed Feigenbaum (a former student of
Herbert Simon), Bruce Buchanan (a philosopher turned computer
scientist), and Joshua Lederberg (a Nobel laureate geneticist) teamed up
to solve the problem of inferring molecular structure from the
information provided by a mass spectrometer. The input to the program
consists of the elementary formula of the molecule (e.g.,
C$_6$H$_{{13}}$NO$_2$) and the mass spectrum giving the masses of the
various fragments of the molecule generated when it is bombarded by an
electron beam. For example, the mass spectrum might contain a peak at
$m={15}$, corresponding to the mass of a methyl (CH$_3$) fragment.

The naive version of the program generated all possible structures
consistent with the formula, and then predicted what mass spectrum would
be observed for each, comparing this with the actual spectrum. As one
might expect, this is intractable for even moderate-sized molecules. The
researchers consulted analytical chemists and found that they worked by
looking for well-known patterns of peaks in the spectrum that suggested
common substructures in the molecule. For example, the following rule is
used to recognize a ketone (C=O) subgroup (which weighs 28):

**if** there are two peaks at $x_1$ and $x_2$ such that\
   (a) $x_1 + x_2 = M+{28}$ ($M$ is the mass of the whole molecule);\
   (b) $x_1 - {28}$ is a high peak;\
   (c) $x_2 - {28}$ is a high peak;\
   (d) At least one of $x_1$ and $x_2$ is high.\
**then** there is a ketone subgroup

Recognizing that the molecule contains a particular substructure reduces
the number of possible candidates enormously. was powerful because

All the relevant theoretical knowledge to solve these problems has been
mapped over from its general form in the [spectrum prediction component]
(“first principles”) to efficient special forms (“cookbook recipes”).
 @Feigenbaum+al:1971

The significance of was that it was the first successful
*knowledge-intensive* system: its expertise derived from
large numbers of special-purpose rules. Later systems also incorporated
the main theme of McCarthy’s Advice Taker approach—the clean separation
of the knowledge (in the form of rules) from the reasoning component.

With this lesson in mind, Feigenbaum and others at Stanford began the
Heuristic Programming Project (HPP) to investigate the extent to which
the new methodology of could be applied to other areas of human
expertise. The next major effort was in the area of medical diagnosis.
Feigenbaum, Buchanan, and Dr. Edward Shortliffe developed to diagnose
blood infections. With about 450 rules, was able to perform as well as
some experts, and considerably better than junior doctors. It also
contained two major differences from . First, unlike the rules, no
general theoretical model existed from which the rules could be deduced.
They had to be acquired from extensive interviewing of experts, who in
turn acquired them from textbooks, other experts, and direct experience
of cases. Second, the rules had to reflect the uncertainty associated
with medical knowledge. incorporated a calculus of uncertainty called
(see ), which seemed (at the time) to fit well with how doctors assessed
the impact of evidence on the diagnosis.

The importance of domain knowledge was also apparent in the area of
understanding natural language. Although Winograd’s system for
understanding natural language had engendered a good deal of excitement,
its dependence on syntactic analysis caused some of the same problems as
occurred in the early machine translation work. It was able to overcome
ambiguity and understand pronoun references, but this was mainly because
it was designed specifically for one area—the blocks world. Several
researchers, including Eugene Charniak, a fellow graduate student of
Winograd’s at MIT, suggested that robust language understanding would
require general knowledge about the world and a general method for using
that knowledge.

At Yale, linguist-turned-AI-researcher Roger Schank emphasized this
point, claiming, “There is no such thing as syntax,” which upset a lot
of linguists but did serve to start a useful discussion. Schank and his
students built a series of programs @Schank+Abelson:1977
[@Wilensky:1978; @Schank+Riesbeck:1981; @Dyer:1983] that all had the
task of understanding natural language. The emphasis, however, was less
on language *per se* and more on the problems of
representing and reasoning with the knowledge required for language
understanding. The problems included representing stereotypical
situations @Cullingford:1981, describing human memory organization
@Rieger:1976 [@Kolodner:1983], and understanding plans and goals
@Wilensky:1983.

The widespread growth of applications to real-world problems caused a
concurrent increase in the demands for workable knowledge representation
schemes. A large number of different representation and reasoning
languages were developed. Some were based on logic—for example, the
Prolog language became popular in Europe, and the family in the United
States. Others, following Minsky’s idea of  [-@Minsky:1975], adopted a
more structured approach, assembling facts about particular object and
event types and arranging the types into a large taxonomic hierarchy
analogous to a biological taxonomy.

### AI becomes an industry (1980–present)

The first successful commercial expert system, , began operation at the
Digital Equipment Corporation @McDermott:1982. The program helped
configure orders for new computer systems; by 1986, it was saving the
company an estimated 40 million a year. By 1988, DEC’s AI group had 40
expert systems deployed, with more on the way. DuPont had 100 in use and
500 in development, saving an estimated 10 million a year. Nearly every
major U.S. corporation had its own AI group and was either using or
investigating expert systems.

In 1981, the Japanese announced the “Fifth Generation” project, a
10-year plan to build intelligent computers running Prolog. In response,
the United States formed the Microelectronics and Computer Technology
Corporation (MCC) as a research consortium designed to assure national
competitiveness. In both cases, AI was part of a broad effort, including
chip design and human-interface research. In Britain, the Alvey report
reinstated the funding that was cut by the Lighthill report.[^13] In all
three countries, however, the projects never met their ambitious goals.

Overall, the AI industry boomed from a few million dollars in 1980 to
billions of dollars in 1988, including hundreds of companies building
expert systems, vision systems, robots, and software and hardware
specialized for these purposes. Soon after that came a period called the
“AI Winter,” in which many companies fell by the wayside as they failed
to deliver on extravagant promises.

### The return of neural networks (1986–present)

In the mid-1980s at least four different groups reinvented the learning
algorithm first found in 1969 by Bryson and Ho. The algorithm was
applied to many learning problems in computer science and psychology,
and the widespread dissemination of the results in the collection
*Parallel Distributed
Processing* @Rumelhart+McClelland:1986 caused great excitement.

These so-called models of intelligent systems were seen by some as
direct competitors both to the symbolic models promoted by Newell and
Simon and to the logicist approach of McCarthy and
others @Smolensky:1988. It might seem obvious that at some level humans
manipulate symbols—in fact, Terrence Deacon’s book *The Symbolic
Species* [-@Deacon:1997] suggests that this is the
*defining characteristic* of humans—but the most ardent
connectionists questioned whether symbol manipulation had any real
explanatory role in detailed models of cognition. This question remains
unanswered, but the current view is that connectionist and symbolic
approaches are complementary, not competing. As occurred with the
separation of AI and cognitive science, modern neural network research
has bifurcated into two fields, one concerned with creating effective
network architectures and algorithms and understanding their
mathematical properties, the other concerned with careful modeling of
the empirical properties of actual neurons and ensembles of neurons.

### AI adopts the scientific method (1987–present)

Recent years have seen a revolution in both the content and the
methodology of work in artificial intelligence.[^14] It is now more
common to build on existing theories than to propose brand-new ones, to
base claims on rigorous theorems or hard experimental evidence rather
than on intuition, and to show relevance to real-world applications
rather than toy examples.

AI was founded in part as a rebellion against the limitations of
existing fields like control theory and statistics, but now it is
embracing those fields. As David McAllester [-@McAllester:1998] put it:

In the early period of AI it seemed plausible that new forms of symbolic
computation, e.g., frames and semantic networks, made much of classical
theory obsolete. This led to a form of isolationism in which AI became
largely separated from the rest of computer science. This isolationism
is currently being abandoned. There is a recognition that machine
learning should not be isolated from information theory, that uncertain
reasoning should not be isolated from stochastic modeling, that search
should not be isolated from classical optimization and control, and that
automated reasoning should not be isolated from formal methods and
static analysis.

In terms of methodology, AI has finally come firmly under the scientific
method. To be accepted, hypotheses must be subjected to rigorous
empirical experiments, and the results must be analyzed statistically
for their importance @Cohen:1995. It is now possible to replicate
experiments by using shared repositories of test data and code.

The field of speech recognition illustrates the pattern. In the 1970s, a
wide variety of different architectures and approaches were tried. Many
of these were rather *ad hoc* and fragile, and were
demonstrated on only a few specially selected examples. In recent years,
approaches based on (HMMs) have come to dominate the area. Two aspects
of HMMs are relevant. First, they are based on a rigorous mathematical
theory. This has allowed speech researchers to build on several decades
of mathematical results developed in other fields. Second, they are
generated by a process of training on a large corpus of real speech
data. This ensures that the performance is robust, and in rigorous blind
tests the HMMs have been improving their scores steadily. Speech
technology and the related field of handwritten character recognition
are already making the transition to widespread industrial and consumer
applications. Note that there is no scientific claim that humans use
HMMs to recognize speech; rather, HMMs provide a mathematical framework
for understanding the problem and support the engineering claim that
they work well in practice.

Machine translation follows the same course as speech recognition. In
the 1950s there was initial enthusiasm for an approach based on
sequences of words, with models learned according to the principles of
information theory. That approach fell out of favor in the 1960s, but
returned in the late 1990s and now dominates the field.

Neural networks also fit this trend. Much of the work on neural nets in
the 1980s was done in an attempt to scope out what could be done and to
learn how neural nets differ from “traditional” techniques. Using
improved methodology and theoretical frameworks, the field arrived at an
understanding in which neural nets can now be compared with
corresponding techniques from statistics, pattern recognition, and
machine learning, and the most promising technique can be applied to
each application. As a result of these developments, so-called
technology has spawned a vigorous new industry.

Judea Pearl’s [-@Pearl:1988] *Probabilistic Reasoning in
Intelligent Systems* led to a new acceptance of probability and
decision theory in AI, following a resurgence of interest epitomized by
Peter Cheeseman’s [-@Cheeseman:1985] article “In Defense of
Probability.” The formalism was invented to allow efficient
representation of, and rigorous reasoning with, uncertain knowledge.
This approach largely overcomes many problems of the probabilistic
reasoning systems of the 1960s and 1970s; it now dominates AI research
on uncertain reasoning and expert systems. The approach allows for
learning from experience, and it combines the best of classical AI and
neural nets. Work by Judea Pearl [-@Pearl:1982] and by Eric Horvitz and
David Heckerman @Horvitz+Heckerman:1986 [@Horvitz+al:1986] promoted the
idea of *normative* expert systems: ones that act
rationally according to the laws of decision theory and do not try to
imitate the thought steps of human experts. The Windows operating system
includes several normative diagnostic expert systems for correcting
problems. Chapters [probability-chapter] to [decision-theory-chapter]
cover this area.

Similar gentle revolutions have occurred in robotics, computer vision,
and knowledge representation. A better understanding of the problems and
their complexity properties, combined with increased mathematical
sophistication, has led to workable research agendas and robust methods.
Although increased formalization and specialization led fields such as
vision and robotics to become somewhat isolated from “mainstream” AI in
the 1990s, this trend has reversed in recent years as tools from machine
learning in particular have proved effective for many problems. The
process of reintegration is already yielding significant benefits

### The emergence of intelligent agents (1995–present)

Perhaps encouraged by the progress in solving the subproblems of AI,
researchers have also started to look at the “whole agent” problem
again. The work of Allen Newell, John Laird, and Paul Rosenbloom on
@Newell:1990 [@Laird+al:1987] is the best-known example of a complete
agent architecture. One of the most important environments for
intelligent agents is the Internet. AI systems have become so common in
Web-based applications that the “-bot” suffix has entered everyday
language. Moreover, AI technologies underlie many Internet tools, such
as search engines, recommender systems, and Web site aggregators.

One consequence of trying to build complete agents is the realization
that the previously isolated subfields of AI might need to be
reorganized somewhat when their results are to be tied together. In
particular, it is now widely appreciated that sensory systems (vision,
sonar, speech recognition, etc.) cannot deliver perfectly reliable
information about the environment. Hence, reasoning and planning systems
must be able to handle uncertainty. A second major consequence of the
agent perspective is that AI has been drawn into much closer contact
with other fields, such as control theory and economics, that also deal
with agents. Recent progress in the control of robotic cars has derived
from a mixture of approaches ranging from better sensors,
control-theoretic integration of sensing, localization and mapping, as
well as a degree of high-level planning.

Despite these successes, some influential founders of AI, including John
McCarthy [-@McCarthy:2007], Marvin Minsky [-@Minsky:2007], Nils Nilsson
[-@Nilsson:1995; -@Nilsson:2005] and Patrick Winston @Beal+Winston:2009,
have expressed discontent with the progress of AI. They think that AI
should put less emphasis on creating ever-improved versions of
applications that are good at a specific task, such as driving a car,
playing chess, or recognizing speech. Instead, they believe AI should
return to its roots of striving for, in Simon’s words, “machines that
think, that learn and that create.” They call the effort or HLAI; their
first symposium was in 2004 @Minsky+al:2004. The effort will require
very large knowledge bases; discuss where these knowledge bases might
come from.

A related idea is the subfield of or AGI @Goertzel+Pennachin:2007, which
held its first conference and organized the *Journal of Artificial
General Intelligence* in 2008. AGI looks for a universal
algorithm for learning and acting in any environment, and has its roots
in the work of Ray , one of the attendees of the original 1956 Dartmouth
conference. Guaranteeing that what we create is really is also a concern
@Yudkowsky:2008 [@Omohundro:2008], one we will return to in .

### The availability of very large data sets (2001–present)

Throughout the 60-year history of computer science, the emphasis has
been on the *algorithm* as the main subject of study. But
some recent work in AI suggests that for many problems, it makes more
sense to worry about the *data* and be less picky about
what algorithm to apply. This is true because of the increasing
availability of very large data sources: for example, trillions of words
of English and billions of images from the Web
@Kilgarriff+Grefenstette:2006; or billions of base pairs of genomic
sequences @Collins+al:2003.

One influential paper in this line was Yarowsky’s [-@Yarowsky:1995] work
on word-sense disambiguation: given the use of the word “plant” in a
sentence, does that refer to flora or factory? Previous approaches to
the problem had relied on human-labeled examples combined with machine
learning algorithms. Yarowsky showed that the task can be done, with
accuracy above 96%, with no labeled examples at all. Instead, given a
very large corpus of unannotated text and just the dictionary
definitions of the two senses—“works, industrial plant” and “flora,
plant life”—one can label examples in the corpus, and from there to
learn new patterns that help label new examples. show that techniques
like this perform even better as the amount of available text goes from
a million words to a billion and that the increase in performance from
using more data exceeds any difference in algorithm choice; a mediocre
algorithm with 100 million words of unlabeled training data outperforms
the best known algorithm with 1 million words.

As another example, discuss the problem of filling in holes in a
photograph. Suppose you use Photoshop to mask out an ex-friend from a
group photo, but now you need to fill in the masked area with something
that matches the background. Hays and Efros defined an algorithm that
searches through a collection of photos to find something that will
match. They found the performance of their algorithm was poor when they
used a collection of only ten thousand photos, but crossed a threshold
into excellent performance when they grew the collection to two million
photos.

Work like this suggests that the “knowledge bottleneck” in AI—the
problem of how to express all the knowledge that a system needs—may be
solved in many applications by learning methods rather than hand-coded
knowledge engineering, provided the learning algorithms have enough data
to go on @Halevy+al:2009. Reporters have noticed the surge of new
applications and have written that “AI Winter” may be yielding to a new
Spring @Havenstein:2005. As writes, “today, many thousands of AI
applications are deeply embedded in the infrastructure of every
industry.”

The State of the Art {#state-of-the-art-section}
--------------------

What can AI do today? A concise answer is difficult because there are so
many activities in so many subfields. Here we sample a few applications;
others appear throughout the book.

**Robotic vehicles**[driving-robot-page]: A driverless
robotic car named sped through the rough terrain of the Mojave dessert
at 22 mph, finishing the 132-mile course first to win the 2005 DARPA
Grand Challenge. is a Volkswagen Touareg outfitted with cameras, radar,
and laser rangefinders to sense the environment and onboard software to
command the steering, braking, and acceleration @Thrun+al:2006. The
following year CMU’s won the Urban Challenge, safely driving in traffic
through the streets of a closed Air Force base, obeying traffic rules
and avoiding pedestrians and other vehicles.

**Speech recognition**: A traveler calling United Airlines
to book a flight can have the entire conversation guided by an automated
speech recognition and dialog management system.

**Autonomous planning and scheduling**: A hundred million
miles from Earth, NASA’s Remote Agent program became the first on-board
autonomous planning program to control the scheduling of operations for
a spacecraft @Jonsson+al:2000[rax]. generated plans from high-level
goals specified from the ground and monitored the execution of those
plans—detecting, diagnosing, and recovering from problems as they
occurred. Successor program @Al-Chang+al:2004 plans the daily operations
for NASA’s Mars Exploration Rovers, and @Cesta+al:2007 did mission
planning—both logistics and science planning—for the European Space
Agency’s Mars Express mission in 2008.

**Game playing**: IBM’s became the first computer program
to defeat the world champion in a chess match when it bested Garry
Kasparov by a score of 3.5 to 2.5 in an exhibition
match @Goodman+Keene:1997. Kasparov said that he felt a “new kind of
intelligence” across the board from him. *Newsweek*
magazine described the match as “The brain’s last stand.” The value of
IBM’s stock increased by 18 billion. Human champions studied Kasparov’s
loss and were able to draw a few matches in subsequent years, but the
most recent human-computer matches have been won convincingly by the
computer.

**Spam fighting**: Each day, learning algorithms classify
over a billion messages as spam, saving the recipient from having to
waste time deleting what, for many users, could comprise 80% or 90% of
all messages, if not classified away by algorithms. Because the spammers
are continually updating their tactics, it is difficult for a static
programmed approach to keep up, and learning algorithms work best
@Sahami+al:1998 [@Goodman+Heckerman:2004].

**Logistics planning**: During the Persian Gulf crisis of
1991, U.S. forces deployed a Dynamic Analysis and Replanning Tool,
@Cross+Walker:1994, to do automated logistics planning and scheduling
for transportation. This involved up to 50,000 vehicles, cargo, and
people at a time, and had to account for starting points, destinations,
routes, and conflict resolution among all parameters. The AI planning
techniques generated in hours a plan that would have taken weeks with
older methods. The Defense Advanced Research Project Agency (DARPA)
stated that this single application more than paid back DARPA’s 30-year
investment in AI.

**Robotics**: The iRobot Corporation has sold over two
million Roomba robotic vacuum cleaners for home use. The company also
deploys the more rugged PackBot to Iraq and Afghanistan, where it is
used to handle hazardous materials, clear explosives, and identify the
location of snipers.

**Machine Translation**: A computer program automatically
translates from Arabic to English, allowing an English speaker to see
the headline “Ardogan Confirms That Turkey Would Not Accept Any
Pressure, Urging Them to Recognize Cyprus.” The program uses a
statistical model built from examples of Arabic-to-English translations
and from examples of English text totaling two trillion
words @Brants+al:2007. None of the computer scientists on the team speak
Arabic, but they do understand statistics and machine learning
algorithms.

These are just a few examples of artificial intelligence systems that
exist today. Not magic or science fiction—but rather science,
engineering, and mathematics, to which this book provides an
introduction.

This chapter defines AI and establishes the cultural background against
which it has developed. Some of the important points are as follows:

-   Different people approach AI with different goals in mind. Two
    important questions to ask are: Are you concerned with thinking or
    behavior? Do you want to model humans or work from an ideal
    standard?

-   In this book, we adopt the view that intelligence is concerned
    mainly with . Ideally, an takes the best possible action in a
    situation. We study the problem of building agents that are
    intelligent in this sense.

-   Philosophers (going back to 400 b.c.) made AI
    conceivable by considering the ideas that the mind is in some ways
    like a machine, that it operates on knowledge encoded in some
    internal language, and that thought can be used to choose what
    actions to take.

-   Mathematicians provided the tools to manipulate statements of
    logical certainty as well as uncertain, probabilistic statements.
    They also set the groundwork for understanding computation and
    reasoning about algorithms.

-   Economists formalized the problem of making decisions that maximize
    the expected outcome to the decision maker.

-   Neuroscientists discovered some facts about how the brain works and
    the ways in which it is similar to and different from computers.

-   Psychologists adopted the idea that humans and animals can be
    considered information-processing machines. Linguists showed that
    language use fits into this model.

-   Computer engineers provided the ever-more-powerful machines that
    make AI applications possible.

-   Control theory deals with designing devices that act optimally on
    the basis of feedback from the environment. Initially, the
    mathematical tools of control theory were quite different from AI,
    but the fields are coming closer together.

-   The history of AI has had cycles of success, misplaced optimism, and
    resulting cutbacks in enthusiasm and funding. There have also been
    cycles of introducing new creative approaches and systematically
    refining the best ones.

-   AI has advanced more rapidly in the past decade because of greater
    use of the scientific method in experimenting with and comparing
    approaches.

-   Recent progress in understanding the theoretical basis for
    intelligence has gone hand in hand with improvements in the
    capabilities of real systems. The subfields of AI have become more
    integrated, and AI has found common ground with other disciplines.

The methodological status of artificial intelligence is investigated in
*The Sciences of the Artificial*, by Herb
Simon [-@Simon:1981], which discusses research areas concerned with
complex artifacts. It explains how AI can be viewed as both science and
mathematics. gives an overview of experimental methodology within AI.

The Turing Test @Turing:1950 is discussed by , who severely criticizes
the usefulness of its instantiation in the Loebner Prize competition,
and by , who argue that the test itself is not helpful for AI. gives
advice for a Turing Test judge. and collect a number of essays on the
Turing Test. *Artificial Intelligence: The Very Idea*, by
John Haugeland [-@Haugeland:1985], gives a readable account of the
philosophical and practical problems of AI. Significant early papers in
AI are anthologized in the collections by and by . The
*Encyclopedia of AI* @Shapiro:1992 contains survey articles
on almost every topic in AI, as does Wikipedia. These articles usually
provide a good entry point into the research literature on each topic.
An insightful and comprehensive history of AI is given by Nils Nillson
[-@Nilsson:2009], one of the early pioneers of the field.

The most recent work appears in the proceedings of the major AI
conferences: the biennial International Joint Conference on AI (IJCAI),
the annual European Conference on AI (ECAI), and the National Conference
on AI, more often known as AAAI, after its sponsoring organization. The
major journals for general AI are *Artificial
Intelligence*, *Computational Intelligence*, the
*IEEE Transactions on Pattern Analysis and Machine
Intelligence*, *IEEE Intelligent Systems*, and the
electronic *Journal of Artificial Intelligence Research*.
There are also many conferences and journals devoted to specific areas,
which we cover in the appropriate chapters. The main professional
societies for AI are the American Association for Artificial
Intelligence (AAAI), the ACM Special Interest Group in Artificial
Intelligence (SIGART), and the Society for Artificial Intelligence and
Simulation of Behaviour (AISB). AAAI’s *AI Magazine*
contains many topical and tutorial articles, and its Web site,
[aaai.org](aaai.org), contains news, tutorials, and background
information. These exercises are intended to stimulate discussion, and
some might be set as term projects. Alternatively, preliminary attempts
can be made now, and these attempts can be reviewed after the completion
of the book.

Define in your own words: (a) intelligence, (b) artificial intelligence,
(c) agent, (d) rationality, (e) logical reasoning.

Read Turing’s original paper on AI @Turing:1950. In the paper, he
discusses several objections to his proposed enterprise and his test for
intelligence. Which objections still carry weight? Are his refutations
valid? Can you think of new objections arising from developments since
he wrote the paper? In the paper, he predicts that, by the year 2000, a
computer will have a 30% chance of passing a five-minute Turing Test
with an unskilled interrogator. What chance do you think a computer
would have today? In another 50 years?

Every year the Loebner Prize is awarded to the program that comes
closest to passing a version of the Turing Test. Research and report on
the latest winner of the Loebner prize. What techniques does it use? How
does it advance the state of the art in AI?

Are reflex actions (such as flinching from a hot stove) rational? Are
they intelligent?

There are well-known classes of problems that are intractably difficult
for computers, and other classes that are provably undecidable. Does
this mean that AI is impossible?

Suppose we extend Evans’s program so that it can score 200 on a standard
IQ test. Would we then have a program more intelligent than a human?
Explain.

The neural structure of the sea slug *Aplysia* has been
widely studied (first by Nobel Laureate Eric Kandel) because it has only
about 20,000 neurons, most of them large and easily manipulated.
Assuming that the cycle time for an *Aplysia* neuron is
roughly the same as for a human neuron, how does the computational
power, in terms of memory updates per second, compare with the high-end
computer described in ?

How could introspection—reporting on one’s inner thoughts—be inaccurate?
Could I be wrong about what I’m thinking? Discuss.

To what extent are the following computer systems instances of
artificial intelligence:

-   Supermarket bar code scanners.

-   Web search engines.

-   Voice-activated telephone menus.

-   Internet routing algorithms that respond dynamically to the state of
    the network.

To what extent are the following computer systems instances of
artificial intelligence:

-   Supermarket bar code scanners.

-   Voice-activated telephone menus.

-   Spelling and grammar correction features in Microsoft Word.

-   Internet routing algorithms that respond dynamically to the state of
    the network.

Many of the computational models of cognitive activities that have been
proposed involve quite complex mathematical operations, such as
convolving an image with a Gaussian or finding a minimum of the entropy
function. Most humans (and certainly all animals) never learn this kind
of mathematics at all, almost no one learns it before college, and
almost no one can compute the convolution of a function with a Gaussian
in their head. What sense does it make to say that the “vision system”
is doing this kind of mathematics, whereas the actual person has no idea
how to do it?

Some authors have claimed that perception and motor skills are the most
important part of intelligence, and that “higher level” capacities are
necessarily parasitic—simple add-ons to these underlying facilities.
Certainly, most of evolution and a large part of the brain have been
devoted to perception and motor skills, whereas AI has found tasks such
as game playing and logical inference to be easier, in many ways, than
perceiving and acting in the real world. Do you think that AI’s
traditional focus on higher-level cognitive abilities is misplaced?

Why would evolution tend to result in systems that act rationally? What
goals are such systems designed to achieve?

Is AI a science, or is it engineering? Or neither or both? Explain.

“Surely computers cannot be intelligent—they can do only what their
programmers tell them.” Is the latter statement true, and does it imply
the former?

“Surely animals cannot be intelligent—they can do only what their genes
tell them.” Is the latter statement true, and does it imply the former?

“Surely animals, humans, and computers cannot be intelligent—they can do
only what their constituent atoms are told to do by the laws of
physics.” Is the latter statement true, and does it imply the former?

Examine the AI literature to discover whether the following tasks can
currently be solved by computers:

1.  Playing a decent game of table tennis (Ping-Pong).

2.  Driving in the center of Cairo, Egypt.

3.  Driving in Victorville, California.

4.  Buying a week’s worth of groceries at the market.

5.  Buying a week’s worth of groceries on the Web.

6.  Playing a decent game of bridge at a competitive level.

7.  Discovering and proving new mathematical theorems.

8.  Writing an intentionally funny story.

9.  Giving competent legal advice in a specialized area of law.

10. Translating spoken English into spoken Swedish in real time.

11. Performing a complex surgical operation.

For the currently infeasible tasks, try to find out what the
difficulties are and predict when, if ever, they will be overcome.

Various subfields of AI have held contests by defining a standard task
and inviting researchers to do their best. Examples include the DARPA
Grand Challenge for robotic cars, The International Planning
Competition, the Robocup robotic soccer league, the TREC information
retrieval event, and contests in machine translation, speech
recognition. Investigate five of these contests, and describe the
progress made over the years. To what degree have the contests advanced
toe state of the art in AI? Do what degree do they hurt the field by
drawing energy away from new ideas?

[^1]: By distinguishing between *human* and
    *rational* behavior, we are not suggesting that humans
    are necessarily “irrational” in the sense of “emotionally unstable”
    or “insane.” One merely need note that we are not perfect: not all
    chess players are grandmasters; and, unfortunately, not everyone
    gets an A on the exam. Some systematic errors in human reasoning are
    cataloged by Kahneman *et al.* [-@Kahneman+al:1982].

[^2]: The *Novum Organum* is an update of Aristotle’s
    *Organon*, or instrument of thought. Thus Aristotle can
    be seen as both an empiricist and a rationalist.

[^3]: In this picture, all meaningful statements can be verified or
    falsified either by experimentation or by analysis of the meaning of
    the words. Because this rules out most of metaphysics, as was the
    intention, logical positivism was unpopular in some circles.

[^4]: Frege’s proposed notation for first-order logic—an arcane
    combination of textual and geometric features—never became popular.

[^5]: Since then, it has been discovered that the tree shrew
    (*Scandentia*) has a higher ratio of brain to body
    mass.

[^6]: Many cite Alexander Hood [-@Hood:1824] as a possible prior source.

[^7]: Golgi persisted in his belief that the brain’s functions were
    carried out primarily in a continuous medium in which neurons were
    embedded, whereas Cajal propounded the “neuronal doctrine.” The two
    shared the Nobel prize in 1906 but gave mutually antagonistic
    acceptance speeches.

[^8]: Heath Robinson was a cartoonist famous for his depictions of
    whimsical and absurdly complicated contraptions for everyday tasks
    such as buttering toast.

[^9]: In the postwar period, Turing wanted to use these computers for AI
    research—for example, one of the first chess programs @Turing:1953.
    His efforts were blocked by the British government.

[^10]: This was the first official usage of McCarthy’s term
    *artificial intelligence*. Perhaps “computational
    rationality” would have been more precise and less threatening, but
    “AI” has stuck. At the 50th anniversary of the Dartmouth conference,
    McCarthy stated that he resisted the terms “computer” or
    “computational” in deference to Norbert Weiner, who was promoting
    analog cybernetic devices rather than digital computers.

[^11]: Now Carnegie Mellon University (CMU).

[^12]: Newell and Simon also invented a list-processing language, , to
    write . They had no compiler and translated it into machine code by
    hand. To avoid errors, they worked in parallel, calling out binary
    numbers to each other as they wrote each instruction to make sure
    they agreed.

[^13]: To save embarrassment, a new field called IKBS (Intelligent
    Knowledge-Based Systems) was invented because Artificial
    Intelligence had been officially canceled.

[^14]: Some have characterized this change as a victory of the —those
    who think that AI theories should be grounded in mathematical
    rigor—over the —those who would rather try out lots of ideas, write
    some programs, and then assess what seems to be working. Both
    approaches are important. A shift toward neatness implies that the
    field has reached a level of stability and maturity. Whether that
    stability will be disrupted by a new scruffy idea is another
    question.
[logic-part]

Logical Agents {#knowledge+logic-chapter}
==============

Humans, it seems, know things; and what they know helps them do things.
These are not empty statements. They make strong claims about how the
intelligence of humans is achieved—not by purely reflex mechanisms but
by processes of that operate on internal of knowledge. In AI, this
approach to intelligence is embodied in .

The problem-solving agents of Chapters [search-chapter]
and [advanced-search-chapter] know things, but only in a very limited,
inflexible sense. For example, the transition model for the
8-puzzle—knowledge of what the actions do—is hidden inside the
domain-specific code of the function. It can be used to predict the
outcome of actions but not to deduce that two tiles cannot occupy the
same space or that states with odd parity cannot be reached from states
with even parity. The atomic representations used by problem-solving
agents are also very limiting. In a partially observable environment, an
agent’s only choice for representing what it knows about the current
state is to list all possible concrete states—a hopeless prospect in
large environments.

introduced the idea of representing states as assignments of values to
variables; this is a step in the right direction, enabling some parts of
the agent to work in a domain-independent way and allowing for more
efficient algorithms. In this chapter and those that follow, we take
this step to its logical conclusion, so to speak—we develop as a general
class of representations to support knowledge-based agents. Such agents
can combine and recombine information to suit myriad purposes. Often,
this process can be quite far removed from the needs of the moment—as
when a mathematician proves a theorem or an astronomer calculates the
earth’s life expectancy. Knowledge-based agents can accept new tasks in
the form of explicitly described goals; they can achieve competence
quickly by being told or learning new knowledge about the environment;
and they can adapt to changes in the environment by updating the
relevant knowledge.

We begin in with the overall agent design. introduces a simple new
environment, the wumpus world, and illustrates the operation of a
knowledge-based agent without going into any technical detail. Then we
explain the general principles of in and the specifics of in . While
less expressive than (), propositional logic illustrates all the basic
concepts of logic; it also comes with well-developed inference
technologies, which we describe in
sections [propositional-inference-section] and [model-checking-section].
Finally, combines the concept of knowledge-based agents with the
technology of propositional logic to build some simple agents for the
wumpus world.

Knowledge-Based Agents {#kb-agent-section}
----------------------

The central component of a knowledge-based agent is its , or KB. A
knowledge base is a set of . (Here “sentence” is used as a technical
term. It is related but not identical to the sentences of English and
other natural languages.) Each sentence is expressed in a language
called a and represents some assertion about the world. Sometimes we
dignify a sentence with the name , when the sentence is taken as given
without being derived from other sentences.

There must be a way to add new sentences to the knowledge base and a way
to query what is known. The standard names for these operations are and
, respectively. Both operations may involve —that is, deriving new
sentences from old. Inference must obey the requirement that when one s
a question of the knowledge base, the answer should follow from what has
been told (or ed) to the knowledge base previously. Later in this
chapter, we will be more precise about the crucial word “follow.” For
now, take it to mean that the inference process should not make things
up as it goes along.

shows the outline of a knowledge-based agent program. Like all our
agents, it takes a percept as input and returns an action. The agent
maintains a knowledge base, ${KB}$, which may initially contain some .

Each time the agent program is called, it does three things. First, it s
the knowledge base what it perceives. Second, it s the knowledge base
what action it should perform. In the process of answering this query,
extensive reasoning may be done about the current state of the world,
about the outcomes of possible action sequences, and so on. Third, the
agent program s the knowledge base which action was chosen, and the
agent executes the action.

[logical-agent-algorithm]

The details of the representation language are hidden inside three
functions that implement the interface between the sensors and actuators
on one side and the core representation and reasoning system on the
other. constructs a sentence asserting that the agent perceived the
given percept at the given time. constructs a sentence that asks what
action should be done at the current time. Finally, constructs a
sentence asserting that the chosen action was executed. The details of
the inference mechanisms are hidden inside and . Later sections will
reveal these details.

The agent in appears quite similar to the agents with internal state
described in . Because of the definitions of and , however, the
knowledge-based agent is not an arbitrary program for calculating
actions. It is amenable to a description at the , where we need specify
only what the agent knows and what its goals are, in order to fix its
behavior. For example, an automated taxi might have the goal of taking a
passenger from San Francisco to Marin County and might know that the
Golden Gate Bridge is the only link between the two locations. Then we
can expect it to cross the Golden Gate Bridge *because it knows
that that will achieve its goal*. Notice that this analysis is
independent of how the taxi works at the . It doesn’t matter whether its
geographical knowledge is implemented as linked lists or pixel maps, or
whether it reasons by manipulating strings of symbols stored in
registers or by propagating noisy signals in a network of neurons.

A knowledge-based agent can be built simply by ing it what it needs to
know. Starting with an empty knowledge base, the agent designer can
sentences one by one until the agent knows how to operate in its
environment. This is called the approach to system building. In
contrast, the approach encodes desired behaviors directly as program
code. In the 1970s and 1980s, advocates of the two approaches engaged in
heated debates. We now understand that a successful agent often combines
both declarative and procedural elements in its design, and that
declarative knowledge can often be compiled into more efficient
procedural code.

We can also provide a knowledge-based agent with mechanisms that allow
it to learn for itself. These mechanisms, which are discussed in ,
create general knowledge about the environment from a series of
percepts. A learning agent can be fully autonomous.

The Wumpus World {#wumpus-world-section}
----------------

In this section we describe an environment in which knowledge-based
agents can show their worth. The is a cave consisting of rooms connected
by passageways. Lurking somewhere in the cave is the terrible wumpus, a
beast that eats anyone who enters its room. The wumpus can be shot by an
agent, but the agent has only one arrow. Some rooms contain bottomless
pits that will trap anyone who wanders into these rooms (except for the
wumpus, which is too big to fall in). The only mitigating feature of
this bleak environment is the possibility of finding a heap of gold.
Although the wumpus world is rather tame by modern computer game
standards, it illustrates some important points about intelligence.

A sample wumpus world is shown in . The precise definition of the task
environment is given, as suggested in , by the PEAS
description:[wumpus-PEAS-page]

+1000 for climbing out of the cave with the gold, –1000 for falling into
a pit or being eaten by the wumpus, –1 for each action taken and –10 for
using up the arrow. The game ends either when the agent dies or when the
agent climbs out of the cave.

A $4\stimes 4$ grid of rooms. The agent always starts in the square
labeled [1,1], facing to the right. The locations of the gold and the
wumpus are chosen randomly, with a uniform distribution, from the
squares other than the start square. In addition, each square other than
the start can be a pit, with probability 0.2.

The agent can move , by ${90}^\circ$, or by ${90}^\circ$. The agent dies
a miserable death if it enters a square containing a pit or a live
wumpus. (It is safe, albeit smelly, to enter a square with a dead
wumpus.) If an agent tries to move forward and bumps into a wall, then
the agent does not move. The action can be used to pick up the gold if
it is in the same square as the agent. The action can be used to fire an
arrow in a straight line in the direction the agent is facing. The arrow
continues until it either hits (and hence kills) the wumpus or hits a
wall. The agent has only one arrow, so only the first action has any
effect. Finally, the action can be used to climb out of the cave, but
only from square [1,1].

The agent has five sensors, each of which gives a single bit of
information:

-   In the square containing the wumpus and in the directly (not
    diagonally) adjacent squares, the agent will perceive a
    *Stench*.

-   In the squares directly adjacent to a pit, the agent will perceive a
    *Breeze*.

-   In the square where the gold is, the agent will perceive a
    *Glitter*.

-   When an agent walks into a wall, it will perceive a
    *Bump*.

-   When the wumpus is killed, it emits a woeful *Scream*
    that can be perceived anywhere in the cave.

The percepts will be given to the agent program in the form of a list of
five symbols; for example, if there is a stench and a breeze, but no
glitter, bump, or scream, the agent program will get
$[{Stench},{Breeze},{None},{None},{None}]$.

We can characterize the wumpus environment along the various dimensions
given in . Clearly, it is discrete, static, and single-agent. (The
wumpus doesn’t move, fortunately.) It is sequential, because rewards may
come only after many actions are taken. It is partially observable,
because some aspects of the state are not directly perceivable: the
agent’s location, the wumpus’s state of health, and the availability of
an arrow. As for the locations of the pits and the wumpus: we could
treat them as unobserved parts of the state that happen to be
immutable—in which case, the transition model for the environment is
completely known; or we could say that the transition model itself is
unknown because the agent doesn’t know which actions are fatal—in which
case, discovering the locations of pits and wumpus completes the agent’s
knowledge of the transition model.

For an agent in the environment, the main challenge is its initial
ignorance of the configuration of the environment; overcoming this
ignorance seems to require logical reasoning. In most instances of the
wumpus world, it is possible for the agent to retrieve the gold safely.
Occasionally, the agent must choose between going home empty-handed and
risking death to find the gold. About 21% of the environments are
utterly unfair, because the gold is in a pit or surrounded by pits.

[wumpus-world-figure]

Let us watch a knowledge-based wumpus agent exploring the environment
shown in . We use an informal knowledge representation language
consisting of writing down symbols in a grid (as in Figures
[wumpus-seq01-figure] and [wumpus-seq35-figure]).

The agent’s initial knowledge base contains the rules of the
environment, as described previously; in particular, it knows that it is
in [1,1] and that [1,1] is a safe square; we denote that with an “A” and
“OK,” respectively, in square [1,1].

The first percept is $[{None},{None},{None},{None},{None}]$,
from which the agent can conclude that its neighboring squares, [1,2]
and [2,1], are free of dangers—they are OK. (a) shows the agent’s state
of knowledge at this point.

[wumpus-seq01-figure]

A cautious agent will move only into a square that it knows to be OK.
Let us suppose the agent decides to move forward to [2,1]. The agent
perceives a breeze (denoted by “B”) in [2,1], so there must be a pit in
a neighboring square. The pit cannot be in [1,1], by the rules of the
game, so there must be a pit in [2,2] or [3,1] or both. The notation
“P?” in (b) indicates a possible pit in those squares. At this point,
there is only one known square that is OK and that has not yet been
visited. So the prudent agent will turn around, go back to [1,1], and
then proceed to [1,2].

[wumpus-seq35-figure]

The agent perceives a stench in [1,2], resulting in the state of
knowledge shown in (a). The stench in [1,2] means that there must be a
wumpus nearby. But the wumpus cannot be in [1,1], by the rules of the
game, and it cannot be in [2,2] (or the agent would have detected a
stench when it was in [2,1]). Therefore, the agent can infer that the
wumpus is in [1,3]. The notation W! indicates this
inference. Moreover, the lack of a breeze in [1,2] implies that there is
no pit in [2,2]. Yet the agent has already inferred that there must be a
pit in either [2,2] or [3,1], so this means it must be in [3,1]. This is
a fairly difficult inference, because it combines knowledge gained at
different times in different places and relies on the lack of a percept
to make one crucial step.

The agent has now proved to itself that there is neither a pit nor a
wumpus in [2,2], so it is OK to move there. We do not show the agent’s
state of knowledge at [2,2]; we just assume that the agent turns and
moves to [2,3], giving us (b). In [2,3], the agent detects a glitter, so
it should grab the gold and then return home.

Note that in each case for which the agent draws a conclusion from the
available information, that conclusion is *guaranteed* to
be correct if the available information is correct. This is a
fundamental property of logical reasoning. In the rest of this chapter,
we describe how to build logical agents that can represent information
and draw conclusions such as those described in the preceding
paragraphs.

Logic {#logic-section}
-----

This section summarizes the fundamental concepts of logical
representation and reasoning. These beautiful ideas are independent of
any of logic’s particular forms. We therefore postpone the technical
details of those forms until the next section, using instead the
familiar example of ordinary arithmetic.

In , we said that knowledge bases consist of sentences. These sentences
are expressed according to the of the representation language, which
specifies all the sentences that are well formed. The notion of syntax
is clear enough in ordinary arithmetic: “$x+y=4$” is a well-formed
sentence, whereas “$x 4 y+=$” is not.

A logic must also define the or meaning of sentences. The semantics
defines the of each sentence with respect to each . For example, the
semantics for arithmetic specifies that the sentence “$x+y\eq
4$” is true in a world where $x$ is 2 and $y$ is 2, but false in a world
where $x$ is 1 and $y$ is 1. In standard logics, every sentence must be
either true or false in each possible world—there is no “in
between.”[^1]

When we need to be precise, we use the term in place of “possible
world.” Whereas possible worlds might be thought of as (potentially)
real environments that the agent might or might not be in, models are
mathematical abstractions, each of which simply fixes the truth or
falsehood of every relevant sentence. Informally, we may think of a
possible world as, for example, having $x$ men and $y$ women sitting at
a table playing bridge, and the sentence $x+y\eq 4$ is true when there
are four people in total. Formally, the possible models are just all
possible assignments of real numbers to the variables $x$ and $y$. Each
such assignment fixes the truth of any sentence of arithmetic whose
variables are $x$ and $y$. If a sentence $\alpha$ is true in model $m$,
we say that $m$ $\alpha$ or sometimes $m$ $\alpha$. We use the notation
$M(\alpha)$ to mean the set of all models of $\alpha$.

Now that we have a notion of truth, we are ready to talk about logical
reasoning. This involves the relation of logical between sentences—the
idea that a sentence *follows logically* from another
sentence. In mathematical notation, we write $$\alpha \entails \beta$$
to mean that the sentence $\alpha$ entails the sentence $\beta$. The
formal definition of entailment is this: $\alpha \entails \beta$ if and
only if, in every model in which $\alpha$ is true, $\beta$ is also true.
Using the notation just introduced, we can write
$$\alpha \entails \beta \mbox{ if and only if } M(\alpha) \subseteq M(\beta)\ .$$
(Note the direction of the ${\subseteq}$ here: if
$\alpha \entails \beta$, then $\alpha$ is a *stronger*
assertion than $\beta$: it rules out *more* possible
worlds.) The relation of entailment is familiar from arithmetic; we are
happy with the idea that the sentence $x=0$ entails the sentence $xy =
0$. Obviously, in any model where $x$ is zero, it is the case that $xy$
is zero (regardless of the value of $y$).

We can apply the same kind of analysis to the wumpus-world reasoning
example given in the preceding section. Consider the situation in (b):
the agent has detected nothing in [1,1] and a breeze in [2,1]. These
percepts, combined with the agent’s knowledge of the rules of the wumpus
world, constitute the KB. The agent is interested (among other things)
in whether the adjacent squares [1,2], [2,2], and [3,1] contain pits.
Each of the three squares might or might not contain a pit, so (for the
purposes of this example) there are $2^3 \eq 8$ possible models. These
eight models are shown in .[^2]

[wumpus-entailment-figure]

The KB can be thought of as a set of sentences or as a single sentence
that asserts all the individual sentences. The KB is false in models
that contradict what the agent knows—for example, the KB is false in any
model in which [1,2] contains a pit, because there is no breeze in
[1,1]. There are in fact just three models in which the KB is true, and
these are shown surrounded by a solid line in . Now let us consider two
possible conclusions:

~1~ =\
~2~ =

We have surrounded the models of $\alpha_1$ and $\alpha_2$ with dotted
lines in Figures [wumpus-entailment-figure](a) and
 [wumpus-entailment-figure](b), respectively. By inspection, we see the
following:
$$\mbox{in every model in which }{KB}\mbox{ is true, } \alpha_1\mbox{ is also true.}$$
Hence, ${KB} \entails \alpha_1$: there is no pit in [1,2]. We can also
see that
$$\mbox{in some models in which }{KB}\mbox{ is true, } \alpha_2\mbox{ is false.}$$
Hence, ${KB} \not\entails \alpha_2$: the agent *cannot*
conclude that there is no pit in [2,2]. (Nor can it conclude that there
*is* a pit in [2,2].)[^3]

The preceding example not only illustrates entailment but also shows how
the definition of entailment can be applied to derive conclusions—that
is, to carry out . The inference algorithm illustrated in is called ,
because it enumerates all possible models to check that $\alpha$ is true
in all models in which ${KB}$ is true, that is, that
$M(KB)\subseteq M(\alpha)$.

In understanding entailment and inference, it might help to think of the
set of all consequences of ${KB}$ as a haystack and of $\alpha$ as a
needle. Entailment is like the needle being in the haystack; inference
is like finding it. This distinction is embodied in some formal
notation: if an inference algorithm $i$ can derive $\alpha$ from
${KB}$, we write $${KB}\vdash_i\alpha\ ,$$ which is pronounced
“$\alpha$ is derived from ${KB}$ by $i$” or “$i$ derives $\alpha$ from
${KB}$.”

An inference algorithm that derives only entailed sentences is called or
. Soundness is a highly desirable property. An unsound inference
procedure essentially makes things up as it goes along—it announces the
discovery of nonexistent needles. It is easy to see that model checking,
when it is applicable,[^4] is a sound procedure.

The property of is also desirable: an inference algorithm is complete if
it can derive any sentence that is entailed. For real haystacks, which
are finite in extent, it seems obvious that a systematic examination can
always decide whether the needle is in the haystack. For many knowledge
bases, however, the haystack of consequences is infinite, and
completeness becomes an important issue.[^5] Fortunately, there are
complete inference procedures for logics that are sufficiently
expressive to handle many knowledge bases.

We have described a reasoning process whose conclusions are guaranteed
to be true in any world in which the premises are true; in particular,

if ${KB}$ is true in the *real* world, then any sentence
$\alpha$ derived from ${KB}$ by a sound inference procedure is also
true in the real world.

So, while an inference process operates on “syntax”—internal physical
configurations such as bits in registers or patterns of electrical blips
in brains—the process *corresponds* to the real-world
relationship whereby some aspect of the real world is the case[^6] by
virtue of other aspects of the real world being the case. This
correspondence between world and representation is illustrated in .

[follows+entails-figure]

The final issue to consider is —the connection between logical reasoning
processes and the real environment in which the agent exists. In
particular,

how do we know that ${KB}$ is true in the real world?

(After all, ${KB}$ is just “syntax” inside the agent’s head.) This is
a philosophical question about which many, many books have been written.
(See .) A simple answer is that the agent’s sensors create the
connection. For example, our wumpus-world agent has a smell sensor. The
agent program creates a suitable sentence whenever there is a smell.
Then, whenever that sentence is in the knowledge base, it is true in the
real world. Thus, the meaning and truth of percept sentences are defined
by the processes of sensing and sentence construction that produce them.
What about the rest of the agent’s knowledge, such as its belief that
wumpuses cause smells in adjacent squares? This is not a direct
representation of a single percept, but a general rule—derived, perhaps,
from perceptual experience but not identical to a statement of that
experience. General rules like this are produced by a sentence
construction process called , which is the subject of . Learning is
fallible. It could be the case that wumpuses cause smells *except
on February 29 in leap years*, which is when they take their
baths. Thus, ${KB}$ may not be true in the real world, but with good
learning procedures, there is reason for optimism.

Propositional Logic: A Very Simple Logic {#propositional-logic-section}
----------------------------------------

We now present a simple but powerful logic called . We cover the syntax
of propositional logic and its semantics—the way in which the truth of
sentences is determined. Then we look at —the relation between a
sentence and another sentence that follows from it—and see how this
leads to a simple algorithm for logical inference. Everything takes
place, of course, in the wumpus world.

### Syntax

The of propositional logic defines the allowable sentences. The consist
of a single . Each such symbol stands for a proposition that can be true
or false. We use symbols that start with an uppercase letter and may
contain other letters or subscripts, for example: $P$, $Q$, $R$,
$W_{1,3}$ and ${North}$. The names are arbitrary but are often chosen
to have some mnemonic value—we use $W_{1,3}$ to stand for the
proposition that the wumpus is in [1,3]. (Remember that symbols such as
$W_{1,3}$ are *atomic*, i.e., $W$, 1, and 3 are not
meaningful parts of the symbol.) There are two proposition symbols with
fixed meanings: ${True}$ is the always-true proposition and
${False}$ is the always-false proposition. are constructed from
simpler sentences, using parentheses and . There are five connectives in
common use:

$\lnot$
:   (not). A sentence such as $\lnot W_{1,3}$ is called the of
    $W_{1,3}$. A is either an atomic sentence (a ) or a negated atomic
    sentence (a ).

$\land$
:   (and). A sentence whose main connective is $\land$, such as
    $W_{1,3} \land P_{3,1}$, is called a ; its parts are the . (The
    $\land$ looks like an “A” for “And.”)

$\lor$
:   (or). A sentence using $\lor$, such as
    $(W_{1,3} \land P_{3,1}) \lor W_{2,2}$, is a of the
    $(W_{1,3} \land P_{3,1})$ and $W_{2,2}$. (Historically, the $\lor$
    comes from the Latin “vel,” which means “or.” For most people, it is
    easier to remember $\lor$ as an upside-down $\land$.)

$\impliessymbol$
:   (implies). A sentence such as
    $(W_{1,3} \land P_{3,1}) \implies \lnot W_{2,2}$ is called an (or
    conditional). Its or is $(W_{1,3} \land P_{3,1})$, and its or is
    $\lnot W_{2,2}$. Implications are also known as or statements. The
    implication symbol is sometimes written in other books as $\supset$
    or $\rightarrow$.

$\lequivsymbol$
:   (if and only if). The sentence $W_{1,3}
    \lequiv \lnot W_{2,2}$ is a . Some other books write this as
    $\equiv$.

[prop-bnf-figure]

gives a formal grammar of propositional logic; see if you are not
familiar with the BNF notation. The BNF grammar by itself is ambiguous;
a sentence with several operators can be parsed by the grammar in
multiple ways. To eliminate the ambiguity we define a precedence for
each operator. The “not” operator ($\lnot$) has the highest precedence,
which means that in the sentence $\lnot A \land B$ the $\lnot$ binds
most tightly, giving us the equivalent of $(\lnot A) \land B$ rather
than $\lnot (A \land B)$. (The notation for ordinary arithmetic is the
same: $-2 + 4$ is 2, not –6.) When in doubt, use parentheses to make
sure of the right interpretation. Square brackets mean the same thing as
parentheses; the choice of square brackets or parentheses is solely to
make it easier for a human to read a sentence.

### Semantics {#propositional-semantics-section}

Having specified the syntax of propositional logic, we now specify its
semantics. The semantics defines the rules for determining the truth of
a sentence with respect to a particular model. In propositional logic, a
model simply fixes the —${true}$ or ${false}$—for every proposition
symbol. For example, if the sentences in the knowledge base make use of
the proposition symbols $P_{1,2}$, $P_{2,2}$, and $P_{3,1}$, then one
possible model is
$$m_1 = \{P_{1,2}\eq {false},\,P_{2,2}\eq {false},\,P_{3,1}\eq {true}\}\ .$$
With three proposition symbols, there are $2^3\eq 8$ possible
models—exactly those depicted in . Notice, however, that the models are
purely mathematical objects with no necessary connection to wumpus
worlds. $P_{1,2}$ is just a symbol; it might mean “there is a pit in
[1,2]” or “I’m in Paris today and tomorrow.”

The semantics for propositional logic must specify how to compute the
truth value of *any* sentence, given a model. This is done
recursively. All sentences are constructed from atomic sentences and the
five connectives; therefore, we need to specify how to compute the truth
of atomic sentences and how to compute the truth of sentences formed
with each of the five connectives. Atomic sentences are easy:

-   ${True}$ is true in every model and ${False}$ is false in every
    model.

-   The truth value of every other proposition symbol must be specified
    directly in the model. For example, in the model $m_1$ given
    earlier, $P_{1,2}$ is false.

For complex sentences, we have five rules, which hold for any
subsentences $P$ and $Q$ in any model $m$ (here “iff” means “if and only
if”):

-   $\lnot P$ is true iff $P$ is false in $m$.

-   $P \land Q$ is true iff both $P$ and $Q$ are true in $m$.

-   $P \lor Q$ is true iff either $P$ or $Q$ is true in $m$.

-   $P \textimplies Q$ is true unless $P$ is true and $Q$ is false in
    $m$.

-   $P \textlequiv Q$ is true iff $P$ and $Q$ are both true or both
    false in $m$.

The rules can also be expressed with that specify the truth value of a
complex sentence for each possible assignment of truth values to its
components. Truth tables for the five connectives are given in . From
these tables, the truth value of any sentence $s$ can be computed with
respect to any model $m$ by a simple recursive evaluation. For example,
the sentence $\lnot P_{1,2}\land (P_{2,2}\lor P_{3,1})$, evaluated in
$m_1$, gives ${true}\land ({false} \lor {true})\eq {true}\land
{true} \eq {true}$. asks you to write the algorithm (), which
computes the truth value of a propositional logic sentence $s$ in a
model $m$.

[tbp] [truth-tables-table]

The truth tables for “and,” “or,” and “not” are in close accord with our
intuitions about the English words. The main point of possible confusion
is that $P \lor Q$ is true when $P$ is true or $Q$ is true *or
both*. A different connective, called “exclusive or” (“xor” for
short), yields false when both disjuncts are true.[^7] There is no
consensus on the symbol for exclusive or; some choices are $\dot{\lor}$
or $\neq$ or $\oplus$.

The truth table for $\impliessymbol$ may not quite fit one’s intuitive
understanding of “$P$ implies $Q$” or “if $P$ then $Q$.” For one thing,
propositional logic does not require any relation of
*causation* or *relevance* between $P$ and
$Q$. The sentence “5 is odd implies Tokyo is the capital of Japan” is a
true sentence of propositional logic (under the normal interpretation),
even though it is a decidedly odd sentence of English. Another point of
confusion is that any implication is true whenever its antecedent is
false. For example, “5 is even implies Sam is smart” is true, regardless
of whether Sam is smart. This seems bizarre, but it makes sense if you
think of “$P
\implies Q$” as saying, “If $P$ is true, then I am claiming that $Q$ is
true. Otherwise I am making no claim.” The only way for this sentence to
be *false* is if $P$ is true but $Q$ is false.

The biconditional, $P\lequivsymbol Q$, is true whenever both
$P\textimplies Q$ and $Q\textimplies P$ are true. In English, this is
often written as “$P$ if and only if $Q$.” Many of the rules of the
wumpus world are best written using $\lequivsymbol$. For example, a
square is breezy *if* a neighboring square has a pit, and a
square is breezy *only if* a neighboring square has a pit.
So we need a biconditional,
$$B_{1,1} \lequiv (P_{1,2} \lor P_{2,1})\ ,$$ where $B_{1,1}$ means that
there is a breeze in [1,1].

### A simple knowledge base

[wumpus-kb-section]

Now that we have defined the semantics for propositional logic, we can
construct a knowledge base for the wumpus world. We focus first on the
*immutable* aspects of the wumpus world, leaving the
mutable aspects for a later section. For now, we need the following
symbols for each $[x,y]$ location:

> $P_{x,y}$ is true if there is a pit in $[x,y]$.\
> $W_{x,y}$ is true if there is a wumpus in $[x,y]$, dead or alive.\
> $B_{x,y}$ is true if the agent perceives a breeze in $[x,y]$.\
> $S_{x,y}$ is true if the agent perceives a stench in $[x,y]$.\

The sentences we write will suffice to derive $\lnot P_{1,2}$ (there is
no pit in [1,2]), as was done informally in . We label each sentence
$R_i$ so that we can refer to them:

-   There is no pit in [1,1]: $$R_1:\quad \lnot P_{1,1}\ .$$

-   A square is breezy if and only if there is a pit in a neighboring
    square. This has to be stated for each square; for now, we include
    just the relevant squares[propositional-breezy-page]:

    $$\begin{aligned}
      R_2:\quad B_{1,1} &\lequiv& (P_{1,2} \lor P_{2,1})\ .\\
      R_3:\quad B_{2,1} &\lequiv& (P_{1,1} \lor P_{2,2}\lor P_{3,1})\ .\end{aligned}$$

-   The preceding sentences are true in all wumpus worlds. Now we
    include the breeze percepts for the first two squares visited in the
    specific world the agent is in, leading up to the situation in (b).

    R~4~:B~1,1~  .\
    R~5~:B~2,1~  .

### A simple inference procedure

Our goal now is to decide whether ${KB}\entails
\alpha$ for some sentence $\alpha$. For example, is $\lnot P_{1,2}$
entailed by our ${KB}$? Our first algorithm for inference is a
model-checking approach that is a direct implementation of the
definition of entailment: enumerate the models, and check that $\alpha$
is true in every model in which ${KB}$ is true. Models are assignments
of ${true}$ or ${false}$ to every proposition symbol. Returning to
our wumpus-world example, the relevant proposition symbols are
$B_{1,1}$, $B_{2,1}$, $P_{1,1}$, $P_{1,2}$, $P_{2,1}$, $P_{2,2}$, and
$P_{3,1}$. With seven symbols, there are $2^7\eq {128}$ possible models;
in three of these, ${KB}$ is true (). In those three models,
$\lnot P_{1,2}$ is true, hence there is no pit in [1,2]. On the other
hand, $P_{2,2}$ is true in two of the three models and false in one, so
we cannot yet tell whether there is a pit in [2,2].

[tbp]

[wumpus-model-table]

[tt-entails-algorithm]

reproduces in a more precise form the reasoning illustrated in . A
general algorithm for deciding entailment in propositional logic is
shown in . Like the algorithm on , performs a recursive enumeration of a
finite space of assignments to symbols. The algorithm is because it
implements directly the definition of entailment, and because it works
for any ${KB}$ and $\alpha$ and always terminates—there are only
finitely many models to examine.

Of course, “finitely many” is not always the same as “few.” If ${KB}$
and $\alpha$ contain $n$ symbols in all, then there are $2^n$ models.
Thus, the time complexity of the algorithm is $O(2^n)$. (The space
complexity is only $O(n)$ because the enumeration is depth-first.) Later
in this chapter we show algorithms that are much more efficient in many
cases. Unfortunately, propositional entailment is (i.e., probably no
easier than NP-complete—see ), so

every known inference algorithm for propositional logic has a worst-case
complexity that is exponential in the size of the input.

Propositional Theorem Proving {#propositional-inference-section}
-----------------------------

So far, we have shown how to determine entailment by *model
checking*: enumerating models and showing that the sentence must
hold in all models. In this section, we show how entailment can be done
by —applying rules of inference directly to the sentences in our
knowledge base to construct a proof of the desired sentence without
consulting models. If the number of models is large but the length of
the proof is short, then theorem proving can be more efficient than
model checking.

Before we plunge into the details of theorem-proving algorithms, we will
need some additional concepts related to entailment. The first concept
is : two sentences $\alpha$ and $\beta$ are logically equivalent if they
are true in the same set of models. We write this as
$\alpha \equiv \beta$. For example, we can easily show (using truth
tables) that $P\land Q$ and $Q\land P$ are logically equivalent; other
equivalences are shown in . These equivalences play much the same role
in logic as arithmetic identities do in ordinary mathematics. An
alternative definition of equivalence is as follows: any two sentences
$\alpha$ and $\beta$ are equivalent only if each of them entails the
other: $$\alpha\equiv\beta \quad\mbox{if and only if} \quad
  \alpha\entails\beta \mbox{ and } \beta\entails\alpha \ .$$

[tbp] [logical-equivalence-table]

The second concept we will need is . A sentence is valid if it is true
in *all* models. For example, the sentence $P\lor \lnot P$
is valid. Valid sentences are also known as —they are
*necessarily* true. Because the sentence ${True}$ is true
in all models, every valid sentence is logically equivalent to
${True}$. What good are valid sentences? From our definition of
entailment, we can derive the , which was known to the ancient Greeks:

> For any sentences $\alpha$ and $\beta$, $\alpha\entails \beta$ if and
> only if the sentence $(\alpha \textimplies \beta)$ is valid.

( asks for a proof.)[deduction-theorem-page] Hence, we can decide if
$\alpha \models \beta$ by checking that $(\alpha \textimplies \beta)$ is
true in every model—which is essentially what the inference algorithm in
does—or by proving that $(\alpha \textimplies \beta)$ is equivalent to
${True}$. Conversely, the deduction theorem states that every valid
implication sentence describes a legitimate inference.

The final concept we will need is . A sentence is satisfiable if it is
true in, or satisfied by, *some* model. For example, the
knowledge base given earlier, ($R_1\land R_2\land
R_3\land R_4\land R_5$), is satisfiable because there are three models
in which it is true, as shown in . Satisfiability can be checked by
enumerating the possible models until one is found that satisfies the
sentence. The problem of determining the satisfiability of sentences in
propositional logic—the problem—was the first problem proved to be .
Many problems in computer science are really satisfiability problems.
For example, all the constraint satisfaction problems in ask whether the
constraints are satisfiable by some assignment.

Validity and satisfiability are of course connected: $\alpha$ is valid
iff $\lnot \alpha$ is unsatisfiable; contrapositively, $\alpha$ is
satisfiable iff $\lnot
\alpha$ is not valid. We also have the following useful result:

> $\alpha\entails \beta$ if and only if the sentence
> $(\alpha \land \lnot \beta)$ is unsatisfiable.

Proving $\beta$ from $\alpha$ by checking the unsatisfiability of
$(\alpha \land \lnot \beta)$ corresponds exactly to the standard
mathematical proof technique of (literally, “reduction to an absurd
thing”). It is also called proof by or proof by [refutation-page]. One
assumes a sentence $\beta$ to be false and shows that this leads to a
contradiction with known axioms $\alpha$. This contradiction is exactly
what is meant by saying that the sentence $(\alpha \land \lnot \beta)$
is unsatisfiable.

### Inference and proofs

This section covers that can be applied to derive a —a chain of
conclusions that leads to the desired goal. The best-known rule is
called (Latin for *mode that affirms*) and is written
$$\frac{\alpha \implies \beta,\qquad \alpha}{\beta} \ .$$ The notation
means that, whenever any sentences of the form $\alpha
\textimplies \beta$ and $\alpha$ are given, then the sentence $\beta$
can be inferred. For example, if
$({WumpusAhead} \land {WumpusAlive})
\textimplies {Shoot}$ and $({WumpusAhead}
\land {WumpusAlive})$ are given, then ${Shoot}$ can be inferred.

Another useful inference rule is , which says that, from a conjunction,
any of the conjuncts can be inferred:
$$\frac{\alpha \land \beta}{\alpha} \ .$$ For example, from
$({WumpusAhead} \land {WumpusAlive})$, ${WumpusAlive}$ can be
inferred.

By considering the possible truth values of $\alpha$ and $\beta$, one
can show easily that Modus Ponens and And-Elimination are sound once and
for all. These rules can then be used in any particular instances where
they apply, generating sound inferences without the need for enumerating
models.

All of the logical equivalences in can be used as inference rules. For
example, the equivalence for biconditional elimination yields the two
inference rules
$$\frac{\alpha\lequiv \beta}{(\alpha\implies \beta)\land (\beta\implies \alpha)}
\qquad\mbox{and}\qquad
\frac{(\alpha\implies \beta)\land (\beta\implies \alpha)}{\alpha\lequiv \beta}\ .$$
Not all inference rules work in both directions like this. For example,
we cannot run Modus Ponens in the opposite direction to obtain
$\alpha \textimplies \beta$ and $\alpha$ from $\beta$.

Let us see how these inference rules and equivalences can be used in the
wumpus world. We start with the knowledge base containing $R_1$ through
$R_5$ and show how to prove $\lnot P_{1,2}$, that is, there is no pit in
[1,2]. First, we apply biconditional elimination to $R_2$ to obtain
$$R_6:\quad (B_{1,1} \implies (P_{1,2} \lor P_{2,1}))\ \land \ 
            ((P_{1,2} \lor P_{2,1}) \implies B_{1,1})\ .$$ Then we apply
And-Elimination to $R_6$ to obtain
$$R_7:\quad ((P_{1,2} \lor P_{2,1}) \implies B_{1,1})\ .$$ Logical
equivalence for contrapositives gives
$$R_8:\quad (\lnot B_{1,1} \implies \lnot (P_{1,2} \lor P_{2,1}))\ .$$
Now we can apply Modus Ponens with $R_8$ and the percept $R_4$ (i.e.,
$\lnot B_{1,1}$), to obtain
$$R_9:\quad \lnot (P_{1,2} \lor P_{2,1})\ .$$ Finally, we apply De
Morgan’s rule, giving the conclusion
$$R_{{10}}:\quad \lnot P_{1,2} \land \lnot P_{2,1}\ .$$ That is, neither
[1,2] nor [2,1] contains a pit.

We found this proof by hand, but we can apply any of the search
algorithms in to find a sequence of steps that constitutes a proof. We
just need to define a proof problem as follows:

-   Initial State: the initial knowledge base.

-   Actions: the set of actions consists of all the
    inference rules applied to all the sentences that match the top half
    of the inference rule.

-   Result: the result of an action is to add the sentence
    in the bottom half of the inference rule.

-   Goal: the goal is a state that contains the sentence we
    are trying to prove.

Thus, searching for proofs is an alternative to enumerating models. In
many practical cases

finding a proof can be more efficient because the proof can ignore
irrelevant propositions, no matter how many of them there are.

For example, the proof given earlier leading to $\lnot P_{1,2} \land
\lnot P_{2,1}$ does not mention the propositions $B_{2,1}$, $P_{1,1}$,
$P_{2,2}$, or $P_{3,1}$. They can be ignored because the goal
proposition, $P_{1,2}$, appears only in sentence $R_2$; the other
propositions in $R_2$ appear only in $R_4$ and $R_2$; so $R_1$, $R_3$,
and $R_5$ have no bearing on the proof. The same would hold even if we
added a million more sentences to the knowledge base; the simple
truth-table algorithm, on the other hand, would be overwhelmed by the
exponential explosion of models.

One final property of logical systems is , which says that the set of
entailed sentences can only *increase* as information is
added to the knowledge base.[^8] For any sentences $\alpha$ and $\beta$,
$$\mbox{if} \quad {KB}\entails \alpha \quad \mbox{then} \quad {KB}\land \beta \entails \alpha\ .$$
For example, suppose the knowledge base contains the additional
assertion $\beta$ stating that there are exactly eight pits in the
world. This knowledge might help the agent draw
*additional* conclusions, but it cannot invalidate any
conclusion $\alpha$ already inferred—such as the conclusion that there
is no pit in [1,2]. Monotonicity means that inference rules can be
applied whenever suitable premises are found in the knowledge base—the
conclusion of the rule must follow *regardless of what else is in
the knowledge base*.

### Proof by resolution {#pl-resolution-section}

We have argued that the inference rules covered so far are
*sound*, but we have not discussed the question of
*completeness* for the inference algorithms that use them.
Search algorithms such as iterative deepening search () are complete in
the sense that they will find any reachable goal, but if the available
inference rules are inadequate, then the goal is not reachable—no proof
exists that uses only those inference rules. For example, if we removed
the biconditional elimination rule, the proof in the preceding section
would not go through. The current section introduces a single inference
rule, , that yields a complete inference algorithm when coupled with any
complete search algorithm.

We begin by using a simple version of the resolution rule in the wumpus
world. Let us consider the steps leading up to (a): the agent returns
from [2,1] to [1,1] and then goes to [1,2], where it perceives a stench,
but no breeze. We add the following facts to the knowledge base:

R~11~:B~1,2~ .\
R~12~:B~1,2~ (P~1,1~ P~2,2~P~1,3~) .

By the same process that led to $R_{{10}}$ earlier, we can now derive
the absence of pits in [2,2] and [1,3] (remember that [1,1] is already
known to be pitless):

R~13~:P~2,2~ .\
R~14~:P~1,3~ .

We can also apply biconditional elimination to $R_3$, followed by Modus
Ponens with $R_5$, to obtain the fact that there is a pit in [1,1],
[2,2], or [3,1]: $$R_{{15}}:\quad P_{1,1} \lor P_{2,2}\lor P_{3,1}\ .$$
Now comes the first application of the resolution rule: the literal
$\lnot P_{2,2}$ in $R_{{13}}$ *resolves with* the literal
$P_{2,2}$ in $R_{{15}}$ to give the
$$R_{{16}}:\quad P_{1,1} \lor P_{3,1}\ .$$ In English; if there’s a pit
in one of [1,1], [2,2], and [3,1] and it’s not in [2,2], then it’s in
[1,1] or [3,1]. Similarly, the literal $\lnot P_{1,1}$ in $R_{1}$
resolves with the literal $P_{1,1}$ in $R_{{16}}$ to give
$$R_{{17}}:\quad P_{3,1}\ .$$ In English: if there’s a pit in [1,1] or
[3,1] and it’s not in [1,1], then it’s in [3,1]. These last two
inference steps are examples of the inference rule,
$$\frac {\ell_1 \lor \cdots\lor \ell_k,\qquad m}
        {\ell_1 \lor \cdots\lor \ell_{i-1}\lor \ell_{i+1}\lor\cdots \lor \ell_k}\ ,$$
where each $\ell$ is a literal and $\ell_i$ and $m$ are (i.e., one is
the negation of the other). Thus, the unit resolution rule takes a —a
disjunction of literals—and a literal and produces a new clause. Note
that a single literal can be viewed as a disjunction of one literal,
also known as a .

The unit resolution rule can be generalized to the
full[pl-resolution-page] rule,
$$\frac {\ell_1 \lor \cdots\lor \ell_k,\qquad m_1 \lor \cdots\lor m_n}
        {\ell_1 \lor \cdots\lor \ell_{i-1}\lor \ell_{i+1}\lor\cdots\lor \ell_k
        \lor m_1 \lor \cdots \lor m_{j-1}\lor m_{j+1}\lor\cdots\lor m_n} \ ,$$
where $\ell_i$ and $m_j$ are complementary literals. This says that
resolution takes two clauses and produces a new clause containing all
the literals of the two original clauses *except* the two
complementary literals. For example, we have
$$\frac{P_{1,1} \lor P_{3,1},\qquad\lnot P_{1,1}\lor \lnot P_{2,2}}
       {P_{3,1} \lor \lnot P_{2,2}}\ .$$ There is one more technical
aspect of the resolution rule: the resulting clause should contain only
one copy of each literal.[^9] The removal of multiple copies of literals
is called . For example, if we resolve $(A\lor B)$ with
$(A\lor \lnot B)$, we obtain $(A\lor A)$, which is reduced to just $A$.

The *soundness* of the resolution rule can be seen easily
by considering the literal $\ell_i$ that is complementary to literal
$m_j$ in the other clause. If $\ell_i$ is true, then $m_j$ is false, and
hence $m_1 \lor \cdots \lor m_{j-1}\lor
m_{j+1}\lor\cdots\lor m_n$ must be true, because $m_1 \lor
\cdots\lor m_n$ is given. If $\ell_i$ is false, then
$\ell_1 \lor \cdots\lor
\ell_{i-1}\lor \ell_{i+1}\lor\cdots\lor \ell_k$ must be true because
$\ell_1 \lor \cdots\lor \ell_k$ is given. Now $\ell_i$ is either true or
false, so one or other of these conclusions holds—exactly as the
resolution rule states.

What is more surprising about the resolution rule is that it forms the
basis for a family of *complete* inference procedures.

A resolution-based theorem prover can, for any sentences $\alpha$ and
$\beta$ in propositional logic, decide whether $\alpha\models\beta$.

The next two subsections explain how resolution accomplishes this.

#### Conjunctive normal form

The resolution rule applies only to clauses (that is, disjunctions of
literals), so it would seem to be relevant only to knowledge bases and
queries consisting of clauses. How, then, can it lead to a complete
inference procedure for all of propositional logic? The answer is that

every sentence of propositional logic is logically equivalent to a
conjunction of clauses.

A sentence expressed as a conjunction of clauses is said to be in or
(see ). We now describe a procedure for converting to CNF.[pl-cnf-page]
We illustrate the procedure by converting the sentence
$B_{1,1} \textlequiv (P_{1,2} \lor
P_{2,1})$ into CNF. The steps are as follows:

1.  Eliminate $\lequivsymbol$, replacing $\alpha\textlequiv \beta$ with
    $(\alpha\implies
    \beta)\land (\beta\implies \alpha)$.
    $$(B_{1,1} \implies (P_{1,2} \lor P_{2,1})) \land 
       ((P_{1,2} \lor P_{2,1})\implies B_{1,1})\ .$$

2.  Eliminate $\impliessymbol$, replacing $\alpha\textimplies \beta$
    with $\lnot
    \alpha\lor \beta$:
    $$(\lnot B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land 
       (\lnot(P_{1,2} \lor P_{2,1})\lor B_{1,1})\ .$$

3.  CNF requires $\lnot$ to appear only in literals, so we “move $\lnot$
    inwards” by repeated application of the following equivalences from
    :

    > $\lnot(\lnot \alpha) \equiv \alpha\ \ $ (double-negation
    > elimination)\
    > $\lnot(\alpha\land \beta) \equiv (\lnot \alpha \lor \lnot \beta)\ \ $
    > (De Morgan)\
    > $\lnot(\alpha\lor \beta) \equiv (\lnot \alpha \land \lnot \beta)\ \ $
    > (De Morgan)

    In the example, we require just one application of the last rule:
    $$(\lnot B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land 
       ((\lnot P_{1,2} \land \lnot P_{2,1})\lor B_{1,1})\ .$$

4.  Now we have a sentence containing nested ${\land}$ and ${\lor}$
    operators applied to literals. We apply the distributivity law from
    , distributing ${\lor}$ over ${\land}$ wherever possible.
    $$(\lnot B_{1,1} \lor P_{1,2} \lor P_{2,1}) \land 
       (\lnot P_{1,2} \lor B_{1,1}) \land (\lnot P_{2,1} \lor B_{1,1})\ .$$

The original sentence is now in CNF, as a conjunction of three clauses.
It is much harder to read, but it can be used as input to a resolution
procedure.

#### A resolution algorithm

Inference procedures based on resolution work by using the principle of
proof by contradiction introduced on . That is, to show that
${KB}\entails \alpha$, we show that $({KB} \land \lnot \alpha)$ is
unsatisfiable. We do this by proving a contradiction.

A resolution algorithm is shown in . First,
$({KB} \land \lnot \alpha)$ is converted into CNF. Then, the
resolution rule is applied to the resulting clauses. Each pair that
contains complementary literals is resolved to produce a new clause,
which is added to the set if it is not already present. The process
continues until one of two things happens:

-   there are no new clauses that can be added, in which case ${KB}$
    does not entail $\alpha$; or,

-   two clauses resolve to yield the *empty* clause, in
    which case ${KB}$ entails $\alpha$.

The empty clause—a disjunction of no disjuncts—is equivalent to
${False}$ because a disjunction is true only if at least one of its
disjuncts is true. Another way to see that an empty clause represents a
contradiction is to observe that it arises only from resolving two
complementary unit clauses such as $P$ and $\lnot P$.

[pl-resolution-algorithm]

[wumpus-resolution-figure]

We can apply the resolution procedure to a very simple inference in the
wumpus world. When the agent is in [1,1], there is no breeze, so there
can be no pits in neighboring squares. The relevant knowledge base is
$${KB} = R_{2} \land R_4 = (B_{1,1} \lequiv (P_{1,2} \lor P_{2,1}))
\land \lnot B_{1,1}$$ and we wish to prove $\alpha$ which is, say,
$\lnot P_{1,2}$. When we convert $({KB} \land \lnot \alpha)$ into CNF,
we obtain the clauses shown at the top of . The second row of the figure
shows clauses obtained by resolving pairs in the first row. Then, when
$P_{1,2}$ is resolved with $\lnot P_{1,2}$, we obtain the empty clause,
shown as a small square. Inspection of reveals that many resolution
steps are pointless. For example, the clause $B_{1,1}\lor
\lnot B_{1,1} \lor P_{1,2}$ is equivalent to ${True} \lor P_{1,2}$
which is equivalent to ${True}$. Deducing that ${True}$ is true is
not very helpful. Therefore, any clause in which two complementary
literals appear can be discarded.

#### Completeness of resolution

To conclude our discussion of resolution, we now show why is complete.
To do this, we introduce the ${RC}(S)$ of a set of clauses $S$, which
is the set of all clauses derivable by repeated application of the
resolution rule to clauses in $S$ or their derivatives. The resolution
closure is what computes as the final value of the variable člauses. It
is easy to see that ${RC}(S)$ must be finite, because there are only
finitely many distinct clauses that can be constructed out of the
symbols $P_1,\ldots,P_k$ that appear in $S$. (Notice that this would not
be true without the factoring step that removes multiple copies of
literals.) Hence, always terminates.

The completeness theorem for resolution in propositional logic is called
the :[ground-resolution-page]

> If a set of clauses is unsatisfiable, then the resolution closure of
> those clauses contains the empty clause.

This theorem is proved by demonstrating its contrapositive: if the
closure ${RC}(S)$ does *not* contain the empty clause,
then $S$ is satisfiable. In fact, we can construct a model for $S$ with
suitable truth values for $P_1,\ldots,P_k$. The construction procedure
is as follows:

-   For $i$ from 1 to $k$,

    -   If a clause in ${RC}(S)$ contains the literal $\lnot P_i$ and
        all its other literals are false under the assignment chosen for
        $P_1, \ldots, P_{i-1}$, then assign ${false}$ to $P_i$.

    -   Otherwise, assign ${true}$ to $P_i$.

This assignment to $P_1,\ldots,P_k$ is a model of $S$. To see this,
assume the opposite—that, at some stage $i$ in the sequence, assigning
symbol $P_i$ causes some clause $C$ to become false. For this to happen,
it must be the case that all the *other* literals in $C$
must already have been falsified by assignments to $P_1,\ldots,P_{i-1}$.
Thus, $C$ must now look like either
$({false} \lor {false} \lor \cdots {false} \lor P_i)$ or like
$({false} \lor {false} \lor \cdots {false} \lor \lnot P_i)$. If
just one of these two is in $RC(S)$, then the algorithm will assign the
appropriate truth value to $P_i$ to make $C$ true, so $C$ can only be
falsified if *both* of these clauses are in $RC(S)$. Now,
since $RC(S)$ is closed under resolution, it will contain the resolvent
of these two clauses, and that resolvent will have all of its literals
already falsified by the assignments to $P_1,\ldots,P_{i-1}$. This
contradicts our assumption that the first falsified clause appears at
stage $i$. Hence, we have proved that the construction never falsifies a
clause in $RC(S)$; that is, it produces a model of $RC(S)$ and thus a
model of $S$ itself (since $S$ is contained in $RC(S)$).

### Horn clauses and definite clauses {#definite-clause-section}

The completeness of resolution makes it a very important inference
method. In many practical situations, however, the full power of
resolution is not needed. Some real-world knowledge bases satisfy
certain restrictions on the form of sentences they contain, which
enables them to use a more restricted and efficient inference algorithm.

One such restricted form is the , which is a disjunction of literals of
which *exactly one is positive*. For example, the clause
$(\lnot L_{1,1} \lor \lnot {Breeze} \lor
B_{1,1})$ is a definite clause, whereas
$(\lnot B_{1,1} \lor P_{1,2} \lor
P_{2,1})$ is not.[Horn-page]

Slightly more general is the , which is a disjunction of literals of
which *at most one is positive*. So all definite clauses
are Horn clauses, as are clauses with no positive literals; these are
called . Horn clauses are closed under resolution: if you resolve two
Horn clauses, you get back a Horn clause.

[cnf-bnf-figure]

Knowledge bases containing only definite clauses are interesting for
three reasons:

1.  Every definite clause can be written as an implication whose premise
    is a conjunction of positive literals and whose conclusion is a
    single positive literal. (See .) For example, the definite clause
    $(\lnot L_{1,1} \lor \lnot {Breeze}
      \lor B_{1,1})$ can be written as the implication $(L_{1,1} \land
      {Breeze}) \textimplies B_{1,1}$. In the implication form, the
    sentence is easier to understand: it says that if the agent is in
    [1,1] and there is a breeze, then [1,1] is breezy. In Horn form, the
    premise is called the and the conclusion is called the . A sentence
    consisting of a single positive literal, such as $L_{1,1}$, is
    called a . It too can be written in implication form as
    ${True} \textimplies L_{1,1}$, but it is simpler to write just
    $L_{1,1}$.

2.  Inference with Horn clauses can be done through the and algorithms,
    which we explain next. Both of these algorithms are natural, in that
    the inference steps are obvious and easy for humans to follow. This
    type of inference is the basis for , which is discussed in .

3.  Deciding entailment with Horn clauses can be done in time that is
    *linear* in the size of the knowledge base—a pleasant
    surprise.

### Forward and backward chaining

The forward-chaining algorithm (ǨBq) determines if a single proposition
symbol q—the query—is entailed by a knowledge base of definite clauses.
It begins from known facts (positive literals) in the knowledge base. If
all the premises of an implication are known, then its conclusion is
added to the set of known facts. For example, if $L_{1,1}$ and
${Breeze}$ are known and
$(L_{1,1} \land {Breeze}) \textimplies B_{1,1}$ is in the knowledge
base, then $B_{1,1}$ can be added. This process continues until the
query q is added or until no further inferences can be made. The
detailed algorithm is shown in ; the main point to remember is that it
runs in linear time.

[pl-fc-algorithm]

[pl-horn-example-figure]

The best way to understand the algorithm is through an example and a
picture. (a) shows a simple knowledge base of Horn clauses with $A$ and
$B$ as known facts. (b) shows the same knowledge base drawn as an (see
). In AND–OR graphs, multiple links joined by an arc indicate a
conjunction—every link must be proved—while multiple links without an
arc indicate a disjunction—any link can be proved. It is easy to see how
forward chaining works in the graph. The known leaves (here, $A$ and
$B$) are set, and inference propagates up the graph as far as possible.
Wherever a conjunction appears, the propagation waits until all the
conjuncts are known before proceeding. The reader is encouraged to work
through the example in detail.

It is easy to see that forward chaining is : every inference is
essentially an application of Modus Ponens. Forward chaining is also :
every entailed atomic sentence will be derived. The easiest way to see
this is to consider the final state of the ǐnferred table (after the
algorithm reaches a where no new inferences are possible). The table
contains ťrue for each symbol inferred during the process, and false for
all other symbols. We can view the table as a logical model; moreover,

every definite clause in the original KB is true in this model.

To see this, assume the opposite, namely that some clause
$a_1\land\ldots\land a_k\textimplies b$ is false in the model. Then
$a_1\land\ldots\land a_k$ must be true in the model and $b$ must be
false in the model. But this contradicts our assumption that the
algorithm has reached a fixed point! We can conclude, therefore, that
the set of atomic sentences inferred at the fixed point defines a model
of the original KB. Furthermore, any atomic sentence $q$ that is
entailed by the KB must be true in all its models and in this model in
particular. Hence, every entailed atomic sentence $q$ must be inferred
by the algorithm. [pl-fc-completeness-page]

Forward chaining is an example of the general concept of reasoning—that
is, reasoning in which the focus of attention starts with the known
data. It can be used within an agent to derive conclusions from incoming
percepts, often without a specific query in mind. For example, the
wumpus agent might its percepts to the knowledge base using an
incremental forward-chaining algorithm in which new facts can be added
to the agenda to initiate new inferences. In humans, a certain amount of
data-driven reasoning occurs as new information arrives. For example, if
I am indoors and hear rain starting to fall, it might occur to me that
the picnic will be canceled. Yet it will probably not occur to me that
the seventeenth petal on the largest rose in my neighbor’s garden will
get wet; humans keep forward chaining under careful control, lest they
be swamped with irrelevant consequences.

The backward-chaining algorithm, as its name suggests, works backward
from the query. If the query q is known to be true, then no work is
needed. Otherwise, the algorithm finds those implications in the
knowledge base whose conclusion is q. If all the premises of one of
those implications can be proved true (by backward chaining), then q is
true. When applied to the query $Q$ in , it works back down the graph
until it reaches a set of known facts, $A$ and $B$, that forms the basis
for a proof. The algorithm is essentially identical to the algorithm in
. As with forward chaining, an efficient implementation runs in linear
time.

Backward chaining is a form of . It is useful for answering specific
questions such as “What shall I do now?” and “Where are my keys?” Often,
the cost of backward chaining is *much less* than linear in
the size of the knowledge base, because the process touches only
relevant facts.

Effective Propositional Model Checking {#model-checking-section}
--------------------------------------

In this section, we describe two families of efficient algorithms for
general propositional inference based on model checking: One approach
based on backtracking search, and one on local hill-climbing search.
These algorithms are part of the “technology” of propositional logic.
This section can be skimmed on a first reading of the chapter.

The algorithms we describe are for checking satisfiability: the SAT
problem. (As noted earlier, testing entailment, $\alpha\models\beta$,
can be done by testing *un*satisfiability of
$\alpha \land \lnot\beta$.) We have already noted the connection between
finding a satisfying model for a logical sentence and finding a solution
for a constraint satisfaction problem, so it is perhaps not surprising
that the two families of algorithms closely resemble the backtracking
algorithms of and the local search algorithms of . They are, however,
extremely important in their own right because so many combinatorial
problems in computer science can be reduced to checking the
satisfiability of a propositional sentence. Any improvement in
satisfiability algorithms has huge consequences for our ability to
handle complexity in general.

### A complete backtracking algorithm

The first algorithm we consider is often called the , after the seminal
paper by Martin Davis and Hilary Putnam [-@Davis+Putnam:1960]. The
algorithm is in fact the version described by Davis, Logemann, and
Loveland [-@Davis+al:1962], so we will call it after the initials of all
four authors. takes as input a sentence in conjunctive normal form—a set
of clauses. Like and , it is essentially a recursive, depth-first
enumeration of possible models. It embodies three improvements over the
simple scheme of :

-   *Early termination*: The algorithm detects whether the
    sentence must be true or false, even with a partially completed
    model. A clause is true if *any* literal is true, even
    if the other literals do not yet have truth values; hence, the
    sentence as a whole could be judged true even before the model is
    complete. For example, the sentence $(A\lor B)\land (A\lor C)$ is
    true if $A$ is true, regardless of the values of $B$ and $C$.
    Similarly, a sentence is false if *any* clause is
    false, which occurs when each of its literals is false. Again, this
    can occur long before the model is complete. Early termination
    avoids examination of entire subtrees in the search space.

-   *Pure symbol heuristic*: A is a symbol that always
    appears with the same “sign” in all clauses. For example, in the
    three clauses $(A\lor \lnot B)$, $(\lnot B\lor \lnot C)$, and
    $(C\lor
      A)$, the symbol $A$ is pure because only the positive literal
    appears, $B$ is pure because only the negative literal appears, and
    $C$ is impure. It is easy to see that if a sentence has a model,
    then it has a model with the pure symbols assigned so as to make
    their literals ťrue, because doing so can never make a clause false.
    Note that, in determining the purity of a symbol, the algorithm can
    ignore clauses that are already known to be true in the model
    constructed so far. For example, if the model contains
    $B\eq {false}$, then the clause $(\lnot B\lor \lnot
      C)$ is already true, and in the remaining clauses $C$ appears only
    as a positive literal; therefore $C$ becomes pure.

    [dpll-algorithm]

-   *Unit clause heuristic*: A was defined earlier as a
    clause with just one literal. In the context of , it also means
    clauses in which all literals but one are already assigned false by
    the model. For example, if the model contains $B\eq {true}$, then
    $(\lnot B\lor \lnot C)$ simplifies to $\lnot C$, which is a unit
    clause. Obviously, for this clause to be true, $C$ must be set to
    false. The unit clause heuristic assigns all such symbols before
    branching on the remainder. One important consequence of the
    heuristic is that any attempt to prove (by refutation) a literal
    that is already in the knowledge base will succeed immediately ().
    Notice also that assigning one unit clause can create another unit
    clause—for example, when $C$ is set to false, $(C\lor A)$ becomes a
    unit clause, causing ťrue to be assigned to $A$. This “cascade” of
    forced assignments is called . It resembles the process of forward
    chaining with definite clauses, and indeed, if the CNF expression
    contains only definite clauses then DPLL essentially replicates
    forward chaining. (See .)

The algorithm is shown in , which gives the the essential skeleton of
the search process.

What does not show are the tricks that enable SAT solvers to scale up to
large problems. It is interesting that most of these tricks are in fact
rather general, and we have seen them before in other guises:

1.  (as seen with Tasmania in CSPs): As assigns truth values to
    variables, the set of clauses may become separated into disjoint
    subsets, called , that share no unassigned variables. Given an
    efficient way to detect when this occurs, a solver can gain
    considerable speed by working on each component separately.

2.  (as seen in for CSPs): Our simple implementation of uses an
    arbitrary variable ordering and always tries the value before . The
    (see ) suggests choosing the variable that appears most frequently
    over all remaining clauses.

3.  (as seen in for CSPs): Many problems that cannot be solved in hours
    of run time with chronological backtracking can be solved in seconds
    with intelligent backtracking that backs up all the way to the
    relevant point of conflict. All SAT solvers that do intelligent
    backtracking use some form of to record conflicts so that they won’t
    be repeated later in the search. Usually a limited-size set of
    conflicts is kept, and rarely used ones are dropped.

4.  (as seen on for hill-climbing): Sometimes a run appears not to be
    making progress. In this case, we can start over from the top of the
    search tree, rather than trying to continue. After restarting,
    different random choices (in variable and value selection) are made.
    Clauses that are learned in the first run are retained after the
    restart and can help prune the search space. Restarting does not
    guarantee that a solution will be found faster, but it does reduce
    the variance on the time to solution.

5.  (as seen in many algorithms): The speedup methods used in DPLL
    itself, as well as the tricks used in modern solvers, require fast
    indexing of such things as “the set of clauses in which variable
    $X_i$ appears as a positive literal.” This task is complicated by
    the fact that the algorithms are interested only in the clauses that
    have not yet been satisfied by previous assignments to variables, so
    the indexing structures must be updated dynamically as the
    computation proceeds.

With these enhancements, modern solvers can handle problems with tens of
millions of variables. They have revolutionized areas such as hardware
verification and security protocol verification, which previously
required laborious, hand-guided proofs.

### Local search algorithms

We have seen several local search algorithms so far in this book,
including () and (). These algorithms can be applied directly to
satisfiability problems, provided that we choose the right evaluation
function. Because the goal is to find an assignment that satisfies every
clause, an evaluation function that counts the number of unsatisfied
clauses will do the job. In fact, this is exactly the measure used by
the algorithm for CSPs (). All these algorithms take steps in the space
of complete assignments, flipping the truth value of one symbol at a
time. The space usually contains many local minima, to escape from which
various forms of randomness are required. In recent years, there has
been a great deal of experimentation to find a good balance between
greediness and randomness.

One of the simplest and most effective algorithms to emerge from all
this work is called (). On every iteration, the algorithm picks an
unsatisfied clause and picks a symbol in the clause to flip. It chooses
randomly between two ways to pick which symbol to flip: (1) a
“min-conflicts” step that minimizes the number of unsatisfied clauses in
the new state and (2) a “random walk” step that picks the symbol
randomly.

[walksat-algorithm]

When returns a model, the input sentence is indeed satisfiable, but when
it returns failure, there are two possible causes: either the sentence
is unsatisfiable or we need to give the algorithm more time. If we set
$\v{max\_flips}\eq \infty$ and $p>0$, will eventually return a model (if
one exists), because the random-walk steps will eventually hit upon the
solution. Alas, if max\_flips is infinity and the sentence is
unsatisfiable, then the algorithm never terminates!

For this reason, is most useful when we expect a solution to exist—for
example, the problems discussed in Chapters [search-chapter]
and [csp-chapter] usually have solutions. On the other hand, cannot
always detect *unsatisfiability*, which is required for
deciding entailment. For example, an agent cannot
*reliably* use to prove that a square is safe in the wumpus
world. Instead, it can say, “I thought about it for an hour and couldn’t
come up with a possible world in which the square *isn’t*
safe.” This may be a good empirical indicator that the square is safe,
but it’s certainly not a proof.

### The landscape of random SAT problems

Some SAT problems are harder than others. *Easy* problems
can be solved by any old algorithm, but because we know that SAT is
NP-complete, at least some problem instances must require exponential
run time. In , we saw some surprising discoveries about certain kinds of
problems. For example, the $n$-queens problem—thought to be quite tricky
for backtracking search algorithms—turned out to be trivially easy for
local search methods, such as min-conflicts. This is because solutions
are very densely distributed in the space of assignments, and any
initial assignment is guaranteed to have a solution nearby. Thus,
$n$-queens is easy because it is .

When we look at satisfiability problems in conjunctive normal form, an
underconstrained problem is one with relatively *few*
clauses constraining the variables. For example, here is a randomly
generated 3-CNF sentence with five symbols and five clauses:

(DBC)(BAC)(CBE)\
(ED B)(BEC) .

Sixteen of the 32 possible assignments are models of this sentence, so,
on average, it would take just two random guesses to find a model. This
is an easy satisfiability problem, as are most such underconstrained
problems. On the other hand, an *overconstrained* problem
has many clauses relative to the number of variables and is likely to
have no solutions.

To go beyond these basic intuitions, we must define exactly how random
sentences are generated. The notation $CNF_k(m,n)$ denotes a $k$-CNF
sentence with $m$ clauses and $n$ symbols, where the clauses are chosen
uniformly, independently, and without replacement from among all clauses
with $k$ different literals, which are positive or negative at random.
(A symbol may not appear twice in a clause, nor may a clause appear
twice in a sentence.)

Given a source of random sentences, we can measure the probability of
satisfiability. (a) plots the probability for $CNF_3(m,50)$, that is,
sentences with 50 variables and 3 literals per clause, as a function of
the clause/symbol ratio, $m/n$. As we expect, for small $m/n$ the
probability of satisfiability is close to 1, and at large $m/n$ the
probability is close to 0. The probability drops fairly sharply around
$m/n\eq {4.3}$. Empirically, we find that the “cliff” stays in roughly
the same place (for $k\eq 3$) and gets sharper and sharper as $n$
increases. Theoretically, the says that for every $k \ge 3$, there is a
threshold ratio $r_k$ such that, as $n$ goes to infinity, the
probability that $CNF_k(n,rn)$ is satisfiable becomes 1 for all values
of $r$ below the threshold, and 0 for all values above. The conjecture
remains unproven.

[random-3sat-figure]

Now that we have a good idea where the satisfiable and unsatisfiable
problems are, the next question is, where are the hard problems? It
turns out that they are also often at the threshold value. (b) shows
that 50-symbol problems at the threshold value of 4.3 are about 20 times
more difficult to solve than those at a ratio of 3.3. The
underconstrained problems are easiest to solve (because it is so easy to
guess a solution); the overconstrained problems are not as easy as the
underconstrained, but still are much easier than the ones right at the
threshold.

Agents Based on Propositional Logic {#wumpus-agent-section}
-----------------------------------

In this section, we bring together what we have learned so far in order
to construct wumpus world agents that use propositional logic. The first
step is to enable the agent to deduce, to the extent possible, the state
of the world given its percept history. This requires writing down a
complete logical model of the effects of actions. We also show how the
agent can keep track of the world efficiently without going back into
the percept history for each inference. Finally, we show how the agent
can use logical inference to construct plans that are guaranteed to
achieve its goals.

### The current state of the world {#successor-state-section}

As stated at the beginning of the chapter, a logical agent operates by
deducing what to do from a knowledge base of sentences about the world.
The knowledge base is composed of axioms—general knowledge about how the
world works—and percept sentences obtained from the agent’s experience
in a particular world. In this section, we focus on the problem of
deducing the current state of the wumpus world—where am I, is that
square safe, and so on.

We began collecting axioms in . The agent knows that the starting square
contains no pit ($\lnot P_{1,1}$) and no wumpus ($\lnot W_{1,1}$).
Furthermore, for each square, it knows that the square is breezy if and
only if a neighboring square has a pit; and a square is smelly if and
only if a neighboring square has a wumpus. Thus, we include a large
collection of sentences of the following form:

B~1,1~ (P~1,2~ P~2,1~)\
S~1,1~ (W~1,2~ W~2,1~)\
\

The agent also knows that there is exactly one wumpus. This is expressed
in two parts. First, we have to say that there is *at least
one* wumpus:
$$W_{1,1} \lor W_{1,2} \lor \cdots\lor W_{4,3} \lor W_{4,4}\ .$$ Then,
we have to say that there is *at most one* wumpus. For each
pair of locations, we add a sentence saying that at least one of them
must be wumpus-free:

W~1,1~ W~1,2~\
W~1,1~ W~1,3~\
\
W~4,3~ W~4,4~ .

So far, so good. Now let’s consider the agent’s percepts. If there is
currently a stench, one might suppose that a proposition ${Stench}$
should be added to the knowledge base. This is not quite right, however:
if there was no stench at the previous time step, then
$\lnot {Stench}$ would already be asserted, and the new assertion
would simply result in a contradiction. The problem is solved when we
realize that a percept asserts something *only about the current
time*. Thus, if the time step (as supplied to in ) is 4, then we
add ${Stench}^4$ to the knowledge base, rather than
${Stench}$—neatly avoiding any contradiction with
$\lnot {Stench}^3$. The same goes for the breeze, bump, glitter, and
scream percepts.

The idea of associating propositions with time steps extends to any
aspect of the world that changes over time. For example, the initial
knowledge base includes $L_{1,1}^0$—the agent is in square $[1,1]$ at
time 0—as well as ${FacingEast}^0$, ${HaveArrow}^0$, and
${WumpusAlive}^0$. We use the word (from the Latin
*fluens*, flowing) to refer an aspect of the world that
changes. “Fluent” is a synonym for “state variable,” in the sense
described in the discussion of factored representations in on . Symbols
associated with permanent aspects of the world do not need a time
superscript and are sometimes called .

We can connect stench and breeze percepts directly to the properties of
the squares where they are experienced through the location fluent as
follows.[^10] For any time step $t$ and any square $[x,y]$, we assert

L~x,y~^t^ (Breeze^t^ B~x,y~)\
L~x,y~^t^ (Stench^t^ S~x,y~) .

Now, of course, we need axioms that allow the agent to keep track of
fluents such as $L_{x,y}^t$. These fluents change as the result of
actions taken by the agent, so, in the terminology of , we need to write
down the of the wumpus world as a set of logical sentences.

First, we need proposition symbols for the occurrences of actions. As
with percepts, these symbols are indexed by time; thus, ${Forward}^0$
means that the agent executes the action at time 0. By convention, the
percept for a given time step happens first, followed by the action for
that time step, followed by a transition to the next time step.

To describe how the world changes, we can try writing that specify the
outcome of an action at the next time step. For example, if the agent is
at location $[1,1]$ facing east at time 0 and goes ${Forward}$, the
result is that the agent is in square $[2,1]$ and no longer is in
$[1,1]$:

$$L_{1,1}^0 \land {FacingEast}^0 \land {Forward}^0 \implies  (L_{2,1}^1
\land \lnot L_{1,1}^1)\ .
\label{forward-effect-axiom-equation}$$

We would need one such sentence for each possible time step, for each of
the 16 squares, and each of the four orientations. We would also need
similar sentences for the other actions: , , , , and .

Let us suppose that the agent does decide to move at time 0 and asserts
this fact into its knowledge base. Given the effect axiom in , combined
with the initial assertions about the state at time 0, the agent can now
deduce that it is in $[2,1]$. That is,
$\noprog{Ask}({KB},L_{2,1}^1)\eq {true}$. So far, so good.
Unfortunately, the news elsewhere is less good: if we
$\noprog{Ask}({KB},{HaveArrow}^1)$, the answer is ${false}$, that
is, the agent cannot prove it still has the arrow; nor can it prove it
*doesn’t* have it! The information has been lost because
the effect axiom fails to state what remains *unchanged* as
the result of an action. The need to do this gives rise to the .[^11]
One possible solution to the frame problem would be to add explicitly
asserting all the propositions that remain the same. For example, for
each time $t$ we would have

^t^ (^t^ ^t+1^)\
^t^ (^t^ ^t+1^)\

where we explicitly mention every proposition that stays unchanged from
time $t$ to time $t+1$ under the action ${Forward}$. Although the
agent now knows that it still has the arrow after moving forward and
that the wumpus hasn’t died or come back to life, the proliferation of
frame axioms seems remarkably inefficient. In a world with $m$ different
actions and $n$ fluents, the set of frame axioms will be of size
$O(mn)$. This specific manifestation of the frame problem is sometimes
called the . Historically, the problem was a significant one for AI
researchers; we explore it further in the notes at the end of the
chapter.

The representational frame problem is significant because the real world
has very many fluents, to put it mildly. Fortunately for us humans, each
action typically changes no more than some small number $k$ of those
fluents—the world exhibits . Solving the representational frame problem
requires defining the transition model with a set of axioms of size
$O(mk)$ rather than size $O(mn)$. There is also an : the problem of
projecting forward the results of a $t$ step plan of action in time
$O(kt)$ rather than $O(nt)$.

The solution to the problem involves changing one’s focus from writing
axioms about *actions* to writing axioms about
*fluents*. Thus, for each fluent $F$, we will have an axiom
that defines the truth value of $F^{t+1}$ in terms of fluents (including
$F$ itself) at time $t$ and the actions that may have occurred at time
$t$. Now, the truth value of $F^{t+1}$ can be set in one of two ways:
either the action at time $t$ causes $F$ to be true at $t+1$, or $F$ was
already true at time $t$ and the action at time $t$ does not cause it to
be false. An axiom of this form is called a and has this schema:
$$F^{t+1} \lequiv 
  {ActionCausesF}^{t} \lor (F^{t} \land \lnot {ActionCausesNotF}^{t}) \ .$$
One of the simplest successor-state axioms is the one for
${HaveArrow}$. Because there is no action for reloading, the
${ActionCausesF}^{t}$ part goes away and we are left with

$${HaveArrow}^{t+1} \lequiv ({HaveArrow}^t \land \lnot {Shoot}^t) \ .
\label{arrow-ss-axiom-equation}$$

For the agent’s location, the successor-state axioms are more elaborate.
For example, $L_{1,1}^{t+1}$ is true if either (a) the agent moved from
$[1,2]$ when facing south, or from $[2,1]$ when facing west; or (b)
$L_{1,1}^{t}$ was already true and the action did not cause movement
(either because the action was not ${Forward}$ or because the action
bumped into a wall). Written out in propositional logic, this becomes

$$\begin{aligned}
L_{1,1}^{t+1} &\lequiv&     (L_{1,1}^{t} \land (\lnot {Forward}^{t} \lor {Bump}^{t+1})) \nonumber\\
          &       \quad \lor& (L_{1,2}^{t} \land ({South}^{t} \land {Forward}^{t})) \label{location-ss-axiom-equation}\\
          &       \quad \lor& (L_{2,1}^{t} \land ({West}^{t} \land {Forward}^{t})) \ .\nonumber\end{aligned}$$

asks you to write out axioms for the remaining wumpus world fluents.

Given a complete set of successor-state axioms and the other axioms
listed at the beginning of this section, the agent will be able to and
answer any answerable question about the current state of the world. For
example, in the initial sequence of percepts and actions is

$$\begin{aligned}
\lnot {Stench}^0 \land \lnot {Breeze}^0 \land \lnot {Glitter}^0 \land \lnot {Bump}^0 \land \lnot {Scream}^0 &;&
{Forward}^0\\
\lnot {Stench}^1 \land {Breeze}^1 \land \lnot {Glitter}^1 \land \lnot {Bump}^1 \land \lnot {Scream}^1 &;&
{TurnRight}^1\\
\lnot {Stench}^2 \land {Breeze}^2 \land \lnot {Glitter}^2 \land \lnot {Bump}^2 \land \lnot {Scream}^2 &;&
{TurnRight}^2\\
\lnot {Stench}^3 \land {Breeze}^3 \land \lnot {Glitter}^3 \land \lnot {Bump}^3 \land \lnot {Scream}^3 &;&
{Forward}^3\\
\lnot {Stench}^4 \land \lnot {Breeze}^4 \land \lnot {Glitter}^4 \land \lnot {Bump}^4 \land \lnot {Scream}^4 &;&
{TurnRight}^4\\
\lnot {Stench}^5 \land \lnot {Breeze}^5 \land \lnot {Glitter}^5 \land \lnot {Bump}^5 \land \lnot {Scream}^5 &;&
{Forward}^5\\
{Stench}^6 \land \lnot {Breeze}^6 \land \lnot {Glitter}^6 \land \lnot {Bump}^6 \land \lnot {Scream}^6\end{aligned}$$

At this point, we have $\noprog{Ask}({KB},L_{1,2}^6)\eq {true}$, so
the agent knows where it is. Moreover,
$\noprog{Ask}({KB},W_{1,3})\eq {true}$ and
$\noprog{Ask}({KB},P_{3,1})\eq {true}$, so the agent has found the
wumpus and one of the pits. The most important question for the agent is
whether a square is OK to move into, that is, the square contains no pit
nor live wumpus. It’s convenient to add axioms for this, having the form
$${OK}_{x,y}^t \lequiv \lnot P_{x,y} \land \lnot (W_{x,y} \land {WumpusAlive}^t)\ .$$
Finally, $\noprog{Ask}({KB},{OK}_{2,2}^6)\eq true$, so the square
$[2,2]$ is OK to move into. In fact, given a sound and complete
inference algorithm such as DPLL, the agent can answer any answerable
question about which squares are OK—and can do so in just a few
milliseconds for small-to-medium wumpus worlds.

Solving the representational and inferential frame problems is a big
step forward, but a pernicious problem remains: we need to confirm that
*all* the necessary preconditions of an action hold for it
to have its intended effect. We said that the ${Forward}$ action moves
the agent ahead unless there is a wall in the way, but there are many
other unusual exceptions that could cause the action to fail: the agent
might trip and fall, be stricken with a heart attack, be carried away by
giant bats, etc. Specifying all these exceptions is called the
[qualification-problem-page]. There is no complete solution within
logic; system designers have to use good judgment in deciding how
detailed they want to be in specifying their model, and what details
they want to leave out. We will see in that probability theory allows us
to summarize all the exceptions without explicitly naming them.

### A hybrid agent {#wumpus-hybrid-agent-section}

The ability to deduce various aspects of the state of the world can be
combined fairly straightforwardly with condition–action rules and with
problem-solving algorithms from Chapters [search-chapter]
and [advanced-search-chapter] to produce a for the wumpus world. shows
one possible way to do this. The agent program maintains and updates a
knowledge base as well as a current plan. The initial knowledge base
contains the *atemporal* axioms—those that don’t depend
on $t$, such as the axiom relating the breeziness of squares to the
presence of pits. At each time step, the new percept sentence is added
along with all the axioms that depend on $t$, such as the
successor-state axioms. (The next section explains why the agent doesn’t
need axioms for *future* time steps.) Then, the agent uses
logical inference, by ing questions of the knowledge base, to work out
which squares are safe and which have yet to be visited.

The main body of the agent program constructs a plan based on a
decreasing priority of goals. First, if there is a glitter, the program
constructs a plan to grab the gold, follow a route back to the initial
location, and climb out of the cave. Otherwise, if there is no current
plan, the program plans a route to the closest safe square that it has
not visited yet, making sure the route goes through only safe squares.
Route planning is done with A search, not with . If there are no safe
squares to explore, the next step—if the agent still has an arrow—is to
try to make a safe square by shooting at one of the possible wumpus
locations. These are determined by asking where $(KB, \lnot W_{x,y})$ is
false—that is, where it is *not* known that there is
*not* a wumpus. The function (not shown) uses to plan a
sequence of actions that will line up this shot. If this fails, the
program looks for a square to explore that is not provably unsafe—that
is, a square for which $(KB, \lnot
{OK}^t_{x,y})$ returns false. If there is no such square, then the
mission is impossible and the agent retreats to $[1,1]$ and climbs out
of the cave.

[hybrid-wumpus-agent-algorithm]

### Logical state estimation {#logical-state-estimation-section}

The agent program in works quite well, but it has one major weakness: as
time goes by, the computational expense involved in the calls to goes up
and up. This happens mainly because the required inferences have to go
back further and further in time and involve more and more proposition
symbols. Obviously, this is unsustainable—we cannot have an agent whose
time to process each percept grows in proportion to the length of its
life! What we really need is a *constant* update time—that
is, independent of $t$. The obvious answer is to save, or , the results
of inference, so that the inference process at the next time step can
build on the results of earlier steps instead of having to start again
from scratch.

As we saw in , the past history of percepts and all their ramifications
can be replaced by the —that is, some representation of the set of all
possible current states of the world.[^12] The process of updating the
belief state as new percepts arrive is called . Whereas in the belief
state was an explicit list of states, here we can use a logical sentence
involving the proposition symbols associated with the current time step,
as well as the atemporal symbols. For example, the logical sentence

$${WumpusAlive}^1 \land L_{2,1}^1 \land B_{2,1} \land (P_{3,1} \lor
  P_{2,2})
\label{wumpus-belief-state-equation}$$

represents the set of all states at time 1 in which the wumpus is alive,
the agent is at $[2,1]$, that square is breezy, and there is a pit in
$[3,1]$ or $[2,2]$ or both.

Maintaining an exact belief state as a logical formula turns out not to
be easy. If there are $n$ fluent symbols for time $t$, then there are
$2^n$ possible states—that is, assignments of truth values to those
symbols. Now, the set of belief states is the powerset (set of all
subsets) of the set of physical states. There are $2^n$ physical states,
hence $2^{2^n}$ belief states. Even if we used the most compact possible
encoding of logical formulas, with each belief state represented by a
unique binary number, we would need numbers with
$\log_2(2^{2^n})\eq 2^n$ bits to label the current belief state. That
is, exact state estimation may require logical formulas whose size is
exponential in the number of symbols.

One very common and natural scheme for *approximate* state
estimation is to represent belief states as conjunctions of literals,
that is, 1-CNF formulas. To do this, the agent program simply tries to
prove $X^t$ and $\lnot X^t$ for each symbol $X^t$ (as well as each
atemporal symbol whose truth value is not yet known), given the belief
state at $t-1$. The conjunction of provable literals becomes the new
belief state, and the previous belief state is discarded.

It is important to understand that this scheme may lose some information
as time goes along. For example, if the sentence in were the true belief
state, then neither $P_{3,1}$ nor $P_{2,2}$ would be provable
individually and neither would appear in the 1-CNF belief
state[1cnf-belief-state-page]. ( explores one possible solution to this
problem.) On the other hand, because every literal in the 1-CNF belief
state is proved from the previous belief state, and the initial belief
state is a true assertion, we know that entire 1-CNF belief state must
be true. Thus,

the set of possible states represented by the 1-CNF belief state
includes all states that are in fact possible given the full percept
history.

As illustrated in , the 1-CNF belief state acts as a simple outer
envelope, or , around the exact belief state. We see this idea of
conservative approximations to complicated sets as a recurring theme in
many areas of AI.

[wiggly-belief-state-figure]

### Making plans by propositional inference {#satplan-section}

The agent in uses logical inference to determine which squares are safe,
but uses A search to make plans. In this section, we show how to make
plans by logical inference. The basic idea is very simple:

1.  Construct a sentence that includes

    1.  ${Init}^0$, a collection of assertions about the initial
        state;

    2.  ${Transition}^1, \ldots, {Transition}^t$, the
        successor-state axioms for all possible actions at each time up
        to some maximum time $t$;

    3.  the assertion that the goal is achieved at time $t$:
        ${HaveGold}^t \land  {ClimbedOut}^t$.

2.  Present the whole sentence to a SAT solver. If the solver finds a
    satisfying model, then the goal is achievable; if the sentence is
    unsatisfiable, then the planning problem is impossible.

3.  Assuming a model is found, extract from the model those variables
    that represent actions and are assigned ${true}$. Together they
    represent a plan to achieve the goals.

A propositional planning procedure, , is shown in . It implements the
basic idea just given, with one twist. Because the agent does not know
how many steps it will take to reach the goal, the algorithm tries each
possible number of steps $t$, up to some maximum conceivable plan length
$T_{{\rm max}}$. In this way, it is guaranteed to find the shortest plan
if one exists. Because of the way searches for a solution, this approach
cannot be used in a partially observable environment; would just set the
unobservable variables to the values it needs to create a solution.

[satplan-agent-algorithm]

The key step in using is the construction of the knowledge base. It
might seem, on casual inspection, that the wumpus world axioms in
suffice for steps 1(a) and 1(b) above. There is, however, a significant
difference between the requirements for entailment (as tested by ) and
those for satisfiability. Consider, for example, the agent’s location,
initially $[1,1]$, and suppose the agent’s unambitious goal is to be in
$[2,1]$ at time 1. The initial knowledge base contains $L_{1,1}^0$ and
the goal is $L_{2,1}^1$. Using , we can prove $L_{2,1}^1$ if
$\act{Forward}^0$ is asserted, and, reassuringly, we cannot prove
$L_{2,1}^1$ if, say, $\act{Shoot}^0$ is asserted instead. Now, will find
the plan $[\act{Forward}^0]$; so far, so good. Unfortunately, also finds
the plan $[\act{Shoot}^0]$. How could this be? To find out, we inspect
the model that constructs: it includes the assignment $L_{2,1}^0$, that
is, the agent can be in $[2,1]$ at time 1 by being there at time 0 and
shooting. One might ask, “Didn’t we say the agent is in $[1,1]$ at time
0?” Yes, we did, but we didn’t tell the agent that it can’t be in two
places at once! For entailment, $L_{2,1}^0$ is unknown and cannot,
therefore, be used in a proof; for satisfiability, on the other hand,
$L_{2,1}^0$ is unknown and can, therefore, be set to whatever value
helps to make the goal true. For this reason, is a good debugging tool
for knowledge bases because it reveals places where knowledge is
missing. In this particular case, we can fix the knowledge base by
asserting that, at each time step, the agent is in exactly one location,
using a collection of sentences similar to those used to assert the
existence of exactly one wumpus. Alternatively, we can assert
$\lnot L_{x,y}^0$ for all locations other than $[1,1]$; the
successor-state axiom for location takes care of subsequent time steps.
The same fixes also work to make sure the agent has only one
orientation.

has more surprises in store, however. The first is that it finds models
with impossible actions, such as shooting with no arrow. To understand
why, we need to look more carefully at what the successor-state axioms
(such as ) say about actions whose preconditions are not satisfied. The
axioms *do* predict correctly that nothing will happen when
such an action is executed (see ), but they do *not* say
that the action cannot be executed! To avoid generating plans with
illegal actions, we must add stating that an action occurrence requires
the preconditions to be satisfied.[^13] For example, we need to say, for
each time $t$, that $${Shoot}^t \implies {HaveArrow}^t\ .$$ This
ensures that if a plan selects the action at any time, it must be the
case that the agent has an arrow at that time.

’s second surprise is the creation of plans with multiple simultaneous
actions. For example, it may come up with a model in which both
${Forward}^0$ and ${Shoot}^0$ are true, which is not allowed. To
eliminate this problem, we introduce : for every pair of actions $A_i^t$
and $A_j^t$ we add the axiom $$\lnot A_i^t \lor \lnot A_j^t \ .$$ It
might be pointed out that walking forward and shooting at the same time
is not so hard to do, whereas, say, shooting and grabbing at the same
time is rather impractical. By imposing action exclusion axioms only on
pairs of actions that really do interfere with each other, we can allow
for plans that include multiple simultaneous actions—and because finds
the shortest legal plan, we can be sure that it will take advantage of
this capability.

To summarize, finds models for a sentence containing the initial state,
the goal, the successor-state axioms, the precondition axioms, and the
action exclusion axioms. It can be shown that this collection of axioms
is sufficient, in the sense that there are no longer any spurious
“solutions.” Any model satisfying the propositional sentence will be a
valid plan for the original problem. Modern SAT-solving technology makes
the approach quite practical. For example, a DPLL-style solver has no
difficulty in generating the 11-step solution for the wumpus world
instance shown in .

This section has described a declarative approach to agent construction:
the agent works by a combination of asserting sentences in the knowledge
base and performing logical inference. This approach has some weaknesses
hidden in phrases such as “for each time $t$” and “for each square
$[x,y]$.” For any practical agent, these phrases have to be implemented
by code that generates instances of the general sentence schema
automatically for insertion into the knowledge base. For a wumpus world
of reasonable size—one comparable to a smallish computer game—we might
need a $100\stimes 100$ board and 1000 time steps, leading to knowledge
bases with tens or hundreds of millions of sentences. Not only does this
become rather impractical, but it also illustrates a deeper problem: we
know something about the wumpus world—namely, that the “physics” works
the same way across all squares and all time steps—that we cannot
express directly in the language of propositional logic. To solve this
problem, we need a more expressive language, one in which phrases like
“for each time $t$” and “for each square $[x,y]$” can be written in a
natural way. First-order logic, described in , is such a language; in
first-order logic a wumpus world of any size and duration can be
described in about ten sentences rather than ten million or ten
trillion.

We have introduced knowledge-based agents and have shown how to define a
logic with which such agents can reason about the world. The main points
are as follows:

-   Intelligent agents need knowledge about the world in order to reach
    good decisions.

-   Knowledge is contained in agents in the form of in a that are stored
    in a .

-   A knowledge-based agent is composed of a knowledge base and an
    inference mechanism. It operates by storing sentences about the
    world in its knowledge base, using the inference mechanism to infer
    new sentences, and using these sentences to decide what action to
    take.

-   A representation language is defined by its , which specifies the
    structure of sentences, and its , which defines the of each sentence
    in each or .

-   The relationship of between sentences is crucial to our
    understanding of reasoning. A sentence $\alpha$ entails another
    sentence $\beta$ if $\beta$ is true in all worlds where $\alpha$ is
    true. Equivalent definitions include the of the sentence
    $\alpha\textimplies\beta$ and the of the sentence
    $\alpha \land \lnot \beta$.

-   Inference is the process of deriving new sentences from old ones.
    inference algorithms derive *only* sentences that are
    entailed; algorithms derive *all* sentences that are
    entailed.

-   is a simple language consisting of and . It can handle propositions
    that are known true, known false, or completely unknown.

-   The set of possible models, given a fixed propositional vocabulary,
    is finite, so entailment can be checked by enumerating models.
    Efficient inference algorithms for propositional logic include
    backtracking and local search methods and can often solve large
    problems quickly.

-   are patterns of sound inference that can be used to find proofs. The
    rule yields a complete inference algorithm for knowledge bases that
    are expressed in . and are very natural reasoning algorithms for
    knowledge bases in .

-   methods such as can be used to find solutions. Such algorithms are
    sound but not complete.

-   Logical involves maintaining a logical sentence that describes the
    set of possible states consistent with the observation history. Each
    update step requires inference using the transition model of the
    environment, which is built from that specify how each changes.

-   Decisions within a logical agent can be made by SAT solving: finding
    possible models specifying future action sequences that reach the
    goal. This approach works only for fully observable or sensorless
    environments.

-   Propositional logic does not scale to environments of unbounded size
    because it lacks the expressive power to deal concisely with time,
    space, and universal patterns of relationships among objects.

John McCarthy’s paper “Programs with Common Sense” @McCarthy:1958
[@McCarthy:1968] promulgated the notion of agents that use logical
reasoning to mediate between percepts and actions. It also raised the
flag of declarativism, pointing out that telling an agent what it needs
to know is an elegant way to build software. Allen
Newell’s [-@Newell:1982] article “The Knowledge Level” makes the case
that rational agents can be described and analyzed at an abstract level
defined by the knowledge they possess rather than the programs they run.
The declarative and procedural approaches to AI are analyzed in depth by
. The debate was revived by, among others, and , and continues to this
day @Shaparau+al:2008. Meanwhile, the declarative approach has spread
into other areas of computer science such as networking @Loo+al:2006.

Logic itself had its origins in ancient Greek philosophy and
mathematics. Various logical principles—principles connecting the
syntactic structure of sentences with their truth and falsity, with
their meaning, or with the validity of arguments in which they
figure—are scattered in the works of Plato. The first known systematic
study of logic was carried out by Aristotle, whose work was assembled by
his students after his death in 322 b.c. as a treatise
called the *Organon*. Aristotle’s were what we would now
call inference rules. Although the syllogisms included elements of both
propositional and first-order logic, the system as a whole lacked the
compositional properties required to handle sentences of arbitrary
complexity.

The closely related Megarian and Stoic schools (originating in the fifth
century b.c. and continuing for several centuries
thereafter) began the systematic study of the basic logical connectives.
The use of truth tables for defining connectives is due to Philo of
Megara. The Stoics took five basic inference rules as valid without
proof, including the rule we now call . They derived a number of other
rules from these five, using, among other principles, the deduction
theorem () and were much clearer about the notion of proof than was
Aristotle. A good account of the history of Megarian and Stoic logic is
given by Benson Mates [-@Mates:1953].

The idea of reducing logical inference to a purely mechanical process
applied to a formal language is due to Wilhelm Leibniz (1646–1716),
although he had limited success in implementing the ideas. George
Boole [-@Boole:1847] introduced the first comprehensive and workable
system of formal logic in his book *The Mathematical Analysis of
Logic*. Boole’s logic was closely modeled on the ordinary algebra
of real numbers and used substitution of logically equivalent
expressions as its primary inference method. Although Boole’s system
still fell short of full propositional logic, it was close enough that
other mathematicians could quickly fill in the gaps. described
conjunctive normal form, while was introduced much later by Alfred
Horn [-@Horn:1951]. The first comprehensive exposition of modern
propositional logic (and first-order logic) is found in Gottlob
Frege’s [-@Frege:1879] *Begriffschrift* (“Concept Writing”
or “Conceptual Notation”).

The first mechanical device to carry out logical inferences was
constructed by the third Earl of Stanhope (1753–1816). The Stanhope
Demonstrator could handle syllogisms and certain inferences in the
theory of probability. William Stanley Jevons, one of those who improved
upon and extended Boole’s work, constructed his “logical piano” in 1869
to perform inferences in Boolean logic. An entertaining and instructive
history of these and other early mechanical devices for reasoning is
given by Martin Gardner [-@Gardner:1968]. The first published computer
program for logical inference was the Logic Theorist of Newell, Shaw,
and Simon [-@Newell+al:1957]. This program was intended to model human
thought processes. Martin Davis [-@Davis:1957] had actually designed a
program that came up with a proof in 1954, but the Logic Theorist’s
results were published slightly earlier.

Truth tables as a method of testing validity or unsatisfiability in
propositional logic were introduced independently by Emil
Post [-@Post:1921] and Ludwig Wittgenstein [-@Wittgenstein:1922]. In the
1930s, a great deal of progress was made on inference methods for
first-order logic. In particular, Gödel [-@Goedel:1930]
showed that a complete procedure for inference in first-order logic
could be obtained via a reduction to propositional logic, using
Herbrand’s theorem @Herbrand:1930. We take up this history again in ;
the important point here is that the development of efficient
propositional algorithms in the 1960s was motivated largely by the
interest of mathematicians in an effective theorem prover for
first-order logic. The Davis–Putnam algorithm @Davis+Putnam:1960 was the
first effective algorithm for propositional resolution but was in most
cases much less efficient than the backtracking algorithm introduced two
years later [-@Davis+al:1962]. The full resolution rule and a proof of
its completeness appeared in a seminal paper by
J. A. Robinson [-@Robinson:1965], which also showed how to do
first-order reasoning without resort to propositional techniques.

Stephen Cook [-@Cook:1971] showed that deciding satisfiability of a
sentence in propositional logic (the SAT problem) is . Since deciding
entailment is equivalent to deciding unsatisfiability, it is . Many
subsets of propositional logic are known for which the satisfiability
problem is polynomially solvable; Horn clauses are one such subset. The
linear-time forward-chaining algorithm for Horn clauses is due to , who
describe their algorithm as a dataflow process similar to the
propagation of signals in a circuit.

Early theoretical investigations showed that has polynomial average-case
complexity for certain natural distributions of problems. This
potentially exciting fact became less exciting when showed that the same
problems could be solved in constant time simply by guessing random
assignments. The random-generation method described in the chapter
produces much harder problems. Motivated by the empirical success of
local search on these problems, showed that a simple hill-climbing
algorithm can solve *almost all* satisfiability problem
instances very quickly, suggesting that hard problems are rare.
Moreover, exhibited a randomized hill-climbing algorithm whose
*worst-case* expected run time on 3-SAT problems (that is,
satisfiability of 3-CNF sentences) is $O({1.333}^n)$—still exponential,
but substantially faster than previous worst-case bounds. The current
record is $O({1.324}^n)$ @Iwama+Tamaki:2004. and exhibit families of
3-SAT instances for which all known DPLL-like algorithms require
exponential running time.

On the practical side, efficiency gains in propositional solvers have
been marked. Given ten minutes of computing time, the original algorithm
in 1962 could only solve problems with no more than 10 or 15 variables.
By 1995 the solver @Li+Anbulagan:1997 could handle 1,000 variables,
thanks to optimized data structures for indexing variables. Two crucial
contributions were the indexing technique of , which makes unit
propagation very efficient, and the introduction of clause (i.e.,
constraint) learning techniques from the CSP community by . Using these
ideas, and spurred by the prospect of solving industrial-scale circuit
verification problems, developed the solver, which could handle problems
with millions of variables. Beginning in 2002, SAT competitions have
been held regularly; most of the winning entries have either been
descendants of or have used the same general approach.
 @Pipatsrisawat+Darwiche:2007, the 2007 winner, falls in the latter
category. Also noteworthy is  @Een+Sorensson:2003, an open-source
implementation available at http://minisat.se that is
designed to be easily modified and improved. The current landscape of
solvers is surveyed by .

Local search algorithms for were tried by various authors throughout the
1980s; all of the algorithms were based on the idea of minimizing the
number of unsatisfied clauses @Hansen+Jaumard:1990. A particularly
effective algorithm was developed by and independently by , who called
it and showed that it was capable of solving a wide range of very hard
problems very quickly. The algorithm described in the chapter is due to
.

The “” in satisfiability of random $k$-SAT problems was first observed
by and has given rise to a great deal of theoretical and empirical
research—due, in part, to the obvious connection to phase transition
phenomena in statistical physics. observed phase transitions in several
CSPs and conjecture that all NP-hard problems have a phase transition.
located the 3-SAT transition at a clause/variable ratio of around 4.26,
noting that this coincides with a sharp peak in the run time of their
SAT solver. provide an excellent summary of the early literature on the
problem.

The current state of theoretical understanding is summarized by . The
states that, for each $k$, there is a sharp satisfiability threshold
$r_k$, such that as the number of variables $n\rightarrow\infty$,
instances below the threshold are *satisfiable* with
probability 1, while those above the threshold are
*unsatisfiable* with probability 1. The conjecture was not
quite proved by : a sharp threshold exists but its location might depend
on $n$ even as $n\rightarrow\infty$. Despite significant progress in
asymptotic analysis of the threshold location for large
$k$ @Achlioptas+Peres:2004 [@Achlioptas+al:2007], all that can be proved
for $k\eq 3$ is that it lies in the range [3.52,4.51]. Current theory
suggests that a peak in the run time of a SAT solver is not necessarily
related to the satisfiability threshold, but instead to a phase
transition in the solution distribution and structure of SAT instances.
Empirical results due to support this view. In fact, algorithms such as
 @Mezard+al:2002 [@Maneva+al:2007] take advantage of special properties
of random SAT instances near the satisfiability threshold and greatly
outperform general SAT solvers on such instances.

The best sources for information on satisfiability, both theoretical and
practical, are the *Handbook of
Satisfiability* @Biere+al:2009 and the regular
*International Conferences on Theory and Applications of
Satisfiability Testing*, known as SAT.

The idea of building agents with propositional logic can be traced back
to the seminal paper of , which initiated the field of neural networks.
Contrary to popular supposition, the paper was concerned with the
implementation of a Boolean circuit-based agent design in the brain.
Circuit-based agents, which perform computation by propagating signals
in hardware circuits rather than running algorithms in general-purpose
computers, have received little attention in AI, however. The most
notable exception is the work of Stan Rosenschein @Rosenschein:1985
[@Kaelbling+Rosenschein:1990], who developed ways to compile
circuit-based agents from declarative descriptions of the task
environment. (Rosenschein’s approach is described at some length in the
second edition of this book.) The work of Rod
Brooks [-@Brooks:1986; -@Brooks:1989] demonstrates the effectiveness of
circuit-based designs for controlling robots—a topic we take up in .
argues that circuit-based designs are *all* that is needed
for AI—that representation and reasoning are cumbersome, expensive, and
unnecessary. In our view, neither approach is sufficient by itself. show
how a hybrid agent design not too different from our wumpus agent has
been used to control NASA spacecraft, planning sequences of actions and
diagnosing and recovering from faults.

The general problem of keeping track of a partially observable
environment was introduced for state-based representations in . Its
instantiation for propositional representations was studied by , who
identified several classes of environments that admit efficient
state-estimation algorithms and showed that for several other classes
the problem is intractable. The problem, which involves determining what
propositions hold true after an action sequence is executed, can be seen
as a special case of state estimation with empty percepts. Many authors
have studied this problem because of its importance in planning; some
important hardness results were established by . The idea of
representing a belief state with propositions can be traced to .

Logical state estimation, of course, requires a logical representation
of the effects of actions—a key problem in AI since the late 1950s. The
dominant proposal has been the formalism @McCarthy:1963, which is
couched within first-order logic. We discuss situation calculus, and
various extensions and alternatives, in Chapters [planning-chapter]
and [kr-chapter]. The approach taken in this chapter—using temporal
indices on propositional variables—is more restrictive but has the
benefit of simplicity. The general approach embodied in the algorithm
was proposed by . Later generations of were able to take advantage of
the advances in SAT solvers, described earlier, and remain among the
most effective ways of solving difficult problems @Kautz:2006.

The was first recognized by . Many researchers considered the problem
unsolvable within first-order logic, and it spurred a great deal of
research into nonmonotonic logics. Philosophers from  to  have cited the
frame problem as one symptom of the inevitable failure of the entire AI
enterprise. The solution of the frame problem with successor-state
axioms is due to Ray Reiter [-@Reiter:1991]. identifies the as a
separate idea and provides a solution. In retrospect, one can see that
Rosenschein’s [-@Rosenschein:1985] agents were using circuits that
implemented successor-state axioms, but Rosenschein did not notice that
the frame problem was thereby largely solved. explains why the
discrete-event control theory models typically used by engineers do not
have to explicitly deal with the frame problem: because they are dealing
with prediction and control, not with explanation and reasoning about
counterfactual situations.

Modern propositional solvers have wide applicability in industrial
applications. The application of propositional inference in the
synthesis of computer hardware is now a standard technique having many
large-scale deployments @Nowick+al:1993. The satisfiability checker was
used to detect a previously unknown vulnerability in a Web browser user
sign-on protocol @Armando+al:2008.

The wumpus world was invented by Gregory Yob [-@Yob:1975]. Ironically,
Yob developed it because he was bored with games played on a rectangular
grid: the topology of his original wumpus world was a dodecahedron, and
we put it back in the boring old grid. Michael Genesereth was the first
to suggest that the wumpus world be used as an agent testbed.

Suppose the agent has progressed to the point shown in (a), , having
perceived nothing in [1,1], a breeze in [2,1], and a stench in [1,2],
and is now concerned with the contents of [1,3], [2,2], and [3,1]. Each
of these can contain a pit, and at most one can contain a wumpus.
Following the example of , construct the set of possible worlds. (You
should find 32 of them.) Mark the worlds in which the KB is true and
those in which each of the following sentences is true:

~2~ =\
~3~ =

Hence show that ${KB} \entails \alpha_2$ and
${KB} \entails \alpha_3$.

(Adapted from .) Given the following, can you prove that the unicorn is
mythical? How about magical? Horned?

> If the unicorn is mythical, then it is immortal, but if it is not
> mythical, then it is a mortal mammal. If the unicorn is either
> immortal or a mammal, then it is horned. The unicorn is magical if it
> is horned.

[truth-value-exercise] Consider the problem of deciding whether a
propositional logic sentence is true in a given model.

1.  Write a recursive algorithm () that returns ťrue if and only if the
    sentence $s$ is true in the model $m$ (where $m$ assigns a truth
    value for every symbol in $s$). The algorithm should run in time
    linear in the size of the sentence. (Alternatively, use a version of
    this function from the online code repository.)

2.  Give three examples of sentences that can be determined to be true
    or false in a *partial* model that does not specify a
    truth value for some of the symbols.

3.  Show that the truth value (if any) of a sentence in a partial model
    cannot be determined efficiently in general.

4.  Modify your algorithm so that it can sometimes judge truth from
    partial models, while retaining its recursive structure and linear
    run time. Give three examples of sentences whose truth in a partial
    model is *not* detected by your algorithm.

5.  Investigate whether the modified algorithm makes more efficient.

Which of the following are correct?

1.  ${False} \models {True}$.

2.  ${True} \models {False}$.

3.  $(A\land B)  \models (A\lequiv B)$.

4.  $A\lequiv B \models A \lor B$.

5.  $A\lequiv B \models \lnot A \lor B$.

6.  $(A\land B)\implies C \models (A\implies C)\lor(B\implies C)$.

7.  $(C\lor (\lnot A \land \lnot B)) \equiv ((A\implies C) \land (B \implies C))$.

8.  $(A\lor B) \land (\lnot C\lor\lnot D\lor E) \models (A\lor B)$.

9.  $(A\lor B) \land (\lnot C\lor\lnot D\lor E) \models (A\lor B) \land (\lnot D\lor E)$.

10. $(A\lor B) \land \lnot(A \implies B)$ is satisfiable.

11. $(A\lequiv B) \land (\lnot A \lor B)$ is satisfiable.

12. $(A\lequiv B) \lequiv C$ has the same number of models as
    $(A\lequiv B)$ for any fixed set of proposition symbols that
    includes $A$, $B$, $C$.

Which of the following are correct?

1.  ${False} \models {True}$.

2.  ${True} \models {False}$.

3.  $(A\land B)  \models (A\lequiv B)$.

4.  $A\lequiv B \models A \lor B$.

5.  $A\lequiv B \models \lnot A \lor B$.

6.  $(A\lor B) \land (\lnot C\lor\lnot D\lor E) \models (A\lor B\lor C) \land (B\land C\land D\implies E)$.

7.  $(A\lor B) \land (\lnot C\lor\lnot D\lor E) \models (A\lor B) \land (\lnot D\lor E)$.

8.  $(A\lor B) \land \lnot(A \implies B)$ is satisfiable.

9.  $(A\land B)\implies C \models (A\implies C)\lor(B\implies C)$.

10. $(C\lor (\lnot A \land \lnot B)) \equiv ((A\implies C) \land (B \implies C))$.

11. $(A\lequiv B) \land (\lnot A \lor B)$ is satisfiable.

12. $(A\lequiv B) \lequiv C$ has the same number of models as
    $(A\lequiv B)$ for any fixed set of proposition symbols that
    includes $A$, $B$, $C$.

[deduction-theorem-exercise] Prove each of the following assertions:

1.  $\alpha$ is valid if and only if ${True}\entails \alpha$.

2.  For any $\alpha$, ${False}\entails\alpha$.

3.  $\alpha\entails \beta$ if and only if the sentence
    $(\alpha \implies \beta)$ is valid.

4.  $\alpha \equiv \beta$ if and only if the sentence
    $(\alpha\lequiv\beta)$ is valid.

5.  $\alpha\entails \beta$ if and only if the sentence
    $(\alpha \land \lnot \beta)$ is unsatisfiable.

Prove, or find a counterexample to, each of the following assertions:

1.  If $\alpha\models\gamma$ or $\beta\models\gamma$ (or both) then
    $(\alpha\land \beta)\models\gamma$

2.  If $(\alpha\land \beta)\models\gamma$ then $\alpha\models\gamma$ or
    $\beta\models\gamma$ (or both).

3.  If $\alpha\models (\beta \lor \gamma)$ then $\alpha \models \beta$
    or $\alpha \models \gamma$ (or both).

Prove, or find a counterexample to, each of the following assertions:

1.  If $\alpha\models\gamma$ or $\beta\models\gamma$ (or both) then
    $(\alpha\land \beta)\models\gamma$

2.  If $\alpha\models (\beta \land \gamma)$ then $\alpha \models \beta$
    and $\alpha \models \gamma$.

3.  If $\alpha\models (\beta \lor \gamma)$ then $\alpha \models \beta$
    or $\alpha \models \gamma$ (or both).

Consider a vocabulary with only four propositions, $A$, $B$, $C$, and
$D$. How many models are there for the following sentences?

1.  $B\lor C$.

2.  $\lnot A\lor \lnot B \lor \lnot C \lor \lnot D$.

3.  $(A\implies B) \land A \land \lnot B \land C \land D$.

We have defined four binary logical connectives.

1.  Are there any others that might be useful?

2.  How many binary connectives can there be?

3.  Why are some of them not very useful?

[logical-equivalence-exercise]Using a method of your choice, verify each
of the equivalences in ().

[propositional-validity-exercise]Decide whether each of the following
sentences is valid, unsatisfiable, or neither. Verify your decisions
using truth tables or the equivalence rules of ().

1.  ${Smoke} \implies {Smoke}$

2.  ${Smoke} \implies {Fire}$

3.  $({Smoke} \implies {Fire}) \implies (\lnot {Smoke} \implies \lnot {Fire})$

4.  ${Smoke} \lor {Fire} \lor \lnot {Fire}$

5.  $(({Smoke} \land {Heat}) \implies {Fire}) 
            \lequiv (({Smoke} \implies {Fire}) \lor ({Heat} \implies {Fire}))$

6.  $({Smoke} \implies {Fire}) \implies 
            (({Smoke} \land {Heat}) \implies {Fire}) $

7.  ${Big} \lor {Dumb} \lor ({Big} \implies {Dumb})$

[propositional-validity-exercise]Decide whether each of the following
sentences is valid, unsatisfiable, or neither. Verify your decisions
using truth tables or the equivalence rules of ().

1.  ${Smoke} \implies {Smoke}$

2.  ${Smoke} \implies {Fire}$

3.  $({Smoke} \implies {Fire}) \implies (\lnot {Smoke} \implies \lnot {Fire})$

4.  ${Smoke} \lor {Fire} \lor \lnot {Fire}$

5.  $(({Smoke} \land {Heat}) \implies {Fire}) 
            \lequiv (({Smoke} \implies {Fire}) \lor ({Heat} \implies {Fire}))$

6.  ${Big} \lor {Dumb} \lor ({Big} \implies {Dumb})$

7.  $({Big} \land {Dumb}) \lor \lnot {Dumb}$

[cnf-proof-exercise] Any propositional logic sentence is logically
equivalent to the assertion that each possible world in which it would
be false is not the case. From this observation, prove that any sentence
can be written in CNF.

Use resolution to prove the sentence $\lnot A \land \lnot B$ from the
clauses in .

[inf-exercise] This exercise looks into the relationship between clauses
and implication sentences.

1.  Show that the clause $(\lnot P_1 \lor \cdots \lor \lnot P_m \lor Q)$
    is logically equivalent to the implication sentence
    $(P_1 \land \cdots \land P_m) \textimplies Q$.

2.  Show that every clause (regardless of the number of positive
    literals) can be written in the form
    $(P_1 \land \cdots \land P_m) \textimplies (Q_1 \lor \cdots \lor Q_n)$,
    where the $P$s and $Q$s are proposition symbols. A knowledge base
    consisting of such sentences is in or @Kowalski:1979.

3.  Write down the full resolution rule for sentences in implicative
    normal form.

According to some political pundits, a person who is radical ($R$) is
electable ($E$) if he/she is conservative ($C$), but otherwise is not
electable.

1.  Which of the following are correct representations of this
    assertion?

    1.  $(R\land E)\iff C$

    2.  $R\implies (E\iff C)$

    3.  $R\implies ((C\implies E) \lor \lnot E)$

2.  Which of the sentences in (a) can be expressed in Horn form?

This question considers representing satisfiability (SAT) problems as
CSPs.

1.  Draw the constraint graph corresponding to the SAT problem
    $$(\lnot X_1 \lor X_2) \land (\lnot X_2 \lor X_3) \land \ldots \land (\lnot X_{n-1} \lor X_n)$$
    for the particular case $n\eq 5$.

2.  How many solutions are there for this general SAT problem as a
    function of $n$?

3.  Suppose we apply () to find *all* solutions to a SAT
    CSP of the type given in (a). (To find *all* solutions
    to a CSP, we simply modify the basic algorithm so it continues
    searching after each solution is found.) Assume that variables are
    ordered $X_1,\ldots,X_n$ and ${false}$ is ordered before
    ${true}$. How much time will the algorithm take to terminate?
    (Write an $O(\cdot)$ expression as a function of $n$.)

4.  We know that SAT problems in Horn form can be solved in linear time
    by forward chaining (unit propagation). We also know that every
    tree-structured binary CSP with discrete, finite domains can be
    solved in time linear in the number of variables (). Are these two
    facts connected? Discuss.

This question considers representing satisfiability (SAT) problems as
CSPs.

1.  Draw the constraint graph corresponding to the SAT problem
    $$(\lnot X_1 \lor X_2) \land (\lnot X_2 \lor X_3) \land \ldots \land (\lnot X_{n-1} \lor X_n)$$
    for the particular case $n\eq 4$.

2.  How many solutions are there for this general SAT problem as a
    function of $n$?

3.  Suppose we apply () to find *all* solutions to a SAT
    CSP of the type given in (a). (To find *all* solutions
    to a CSP, we simply modify the basic algorithm so it continues
    searching after each solution is found.) Assume that variables are
    ordered $X_1,\ldots,X_n$ and ${false}$ is ordered before
    ${true}$. How much time will the algorithm take to terminate?
    (Write an $O(\cdot)$ expression as a function of $n$.)

4.  We know that SAT problems in Horn form can be solved in linear time
    by forward chaining (unit propagation). We also know that every
    tree-structured binary CSP with discrete, finite domains can be
    solved in time linear in the number of variables (). Are these two
    facts connected? Discuss.

Explain why every nonempty propositional clause, by itself, is
satisfiable. Prove rigorously that every set of five 3-SAT clauses is
satisfiable, provided that each clause mentions exactly three distinct
variables. What is the smallest set of such clauses that is
unsatisfiable? Construct such a set.

A propositional *2-CNF* expression is a conjunction of
clauses, each containing *exactly 2* literals, e.g.,
$$(A\lor B) \land (\lnot A \lor C) \land (\lnot B \lor D) \land (\lnot
  C \lor G) \land (\lnot D \lor G)\ .$$

1.  Prove using resolution that the above sentence entails $G$.

2.  Two clauses are *semantically distinct* if they are not
    logically equivalent. How many semantically distinct 2-CNF clauses
    can be constructed from $n$ proposition symbols?

3.  Using your answer to (b), prove that propositional resolution always
    terminates in time polynomial in $n$ given a 2-CNF sentence
    containing no more than $n$ distinct symbols.

4.  Explain why your argument in (c) does not apply to 3-CNF.

Prove each of the following assertions:

1.  Every pair of propositional clauses either has no resolvents, or all
    their resolvents are logically equivalent.

2.  There is no clause that, when resolved with itself, yields (after
    factoring) the clause $(\lnot P \lor \lnot Q)$.

3.  If a propositional clause $C$ can be resolved with a copy of itself,
    it must be logically equivalent to ${True}$.

Consider the following sentence:
$$[ ({Food} \implies {Party}) \lor ({Drinks} \implies {Party}) ] \implies [ ( {Food} \land {Drinks} )  \implies {Party}]\ .$$

1.  Determine, using enumeration, whether this sentence is valid,
    satisfiable (but not valid), or unsatisfiable.

2.  Convert the left-hand and right-hand sides of the main implication
    into CNF, showing each step, and explain how the results confirm
    your answer to (a).

3.  Prove your answer to (a) using resolution.

[dnf-exercise] A sentence is in (DNF) if it is the disjunction of
conjunctions of literals. For example, the sentence
$(A \land B \land \lnot C) \lor (\lnot A \land C) \lor (B \land \lnot C)$
is in DNF.

1.  Any propositional logic sentence is logically equivalent to the
    assertion that some possible world in which it would be true is in
    fact the case. From this observation, prove that any sentence can be
    written in DNF.

2.  Construct an algorithm that converts any sentence in propositional
    logic into DNF. (*Hint*: The algorithm is similar to
    the algorithm for conversion to CNF given in .)

3.  Construct a simple algorithm that takes as input a sentence in DNF
    and returns a satisfying assignment if one exists, or reports that
    no satisfying assignment exists.

4.  Apply the algorithms in (b) and (c) to the following set of
    sentences:

    A B\
    B C\
    C A .

5.  Since the algorithm in (b) is very similar to the algorithm for
    conversion to CNF, and since the algorithm in (c) is much simpler
    than any algorithm for solving a set of sentences in CNF, why is
    this technique not used in automated reasoning?

[convert-clausal-exercise] Convert the following set of sentences to
clausal form.

> S1: $A \lequiv (B \lor E)$.\
> S2: $E \implies D$.\
> S3: $C \land F \implies \lnot B$.\
> S4: $E \implies B$.\
> S5: $B \implies F$.\
> S6: $B \implies C$

Give a trace of the execution of DPLL on the conjunction of these
clauses.

[convert-clausal-exercise] Convert the following set of sentences to
clausal form.

> S1: $A \lequiv (C \lor E)$.\
> S2: $E \implies D$.\
> S3: $B \land F \implies \lnot C$.\
> S4: $E \implies C$.\
> S5: $C \implies F$.\
> S6: $C \implies B$

Give a trace of the execution of DPLL on the conjunction of these
clauses.

Is a randomly generated 4-CNF sentence with $n$ symbols and $m$ clauses
more or less likely to be solvable than a randomly generated 3-CNF
sentence with $n$ symbols and $m$ clauses? Explain.

[minesweeper-exercise] Minesweeper, the well-known computer game, is
closely related to the wumpus world. A world is a rectangular grid of
$N$ squares with $M$ invisible mines scattered among them. Any square
may be probed by the agent; instant death follows if a mine is probed.
Minesweeper indicates the presence of mines by revealing, in each probed
square, the *number* of mines that are directly or
diagonally adjacent. The goal is to probe every unmined square.

1.  Let $X_{i,j}$ be true iff square $[i,j]$ contains a mine. Write down
    the assertion that exactly two mines are adjacent to [1,1] as a
    sentence involving some logical combination of $X_{i,j}$
    propositions.

2.  Generalize your assertion from (a) by explaining how to construct a
    CNF sentence asserting that $k$ of $n$ neighbors contain mines.

3.  Explain precisely how an agent can use to prove that a given square
    does (or does not) contain a mine, ignoring the global constraint
    that there are exactly $M$ mines in all.

4.  Suppose that the global constraint is constructed from your method
    from part (b). How does the number of clauses depend on $M$ and $N$?
    Suggest a way to modify so that the global constraint does not need
    to be represented explicitly.

5.  Are any conclusions derived by the method in part (c) invalidated
    when the global constraint is taken into account?

6.  Give examples of configurations of probe values that induce
    *long-range dependencies* such that the contents of a
    given unprobed square would give information about the contents of a
    far-distant square. (*Hint*: consider an $N\stimes 1$
    board.)

[known-literal-exercise] How long does it take to prove
${KB}\entails\alpha$ using when $\alpha$ is a literal *already
contained in* ${KB}$? Explain.

[dpll-fc-exercise] Trace the behavior of on the knowledge base in when
trying to prove $Q$, and compare this behavior with that of the
forward-chaining algorithm.

Write a successor-state axiom for the ${Locked}$ predicate, which
applies to doors, assuming the only actions available are ${Lock}$ and
${Unlock}$.

Discuss what is meant by *optimal* behavior in the wumpus
world. Show that the is not optimal, and suggest ways to improve it.

Suppose an agent inhabits a world with two states, $S$ and $\lnot S$,
and can do exactly one of two actions, $a$ and $b$. Action $a$ does
nothing and action $b$ flips from one state to the other. Let $S^t$ be
the proposition that the agent is in state $S$ at time $t$, and let
$a^t$ be the proposition that the agent does action $a$ at time $t$
(similarly for $b^t$).

1.  Write a successor-state axiom for $S^{t+1}$.

2.  Convert the sentence in (a) into CNF.

3.  Show a resolution refutation proof that if the agent is in $\lnot S$
    at time $t$ and does $a$, it will still be in $\lnot S$ at time
    $t+1$.

[ss-axiom-exercise] provides some of the successor-state axioms required
for the wumpus world. Write down axioms for all remaining fluent
symbols.

[hybrid-wumpus-exercise]the to use the 1-CNF logical state estimation
method described on . We noted on that page that such an agent will not
be able to acquire, maintain, and use more complex beliefs such as the
disjunction $P_{3,1}\lor P_{2,2}$. Suggest a method for overcoming this
problem by defining additional proposition symbols, and try it out in
the wumpus world. Does it improve the performance of the agent?

[^1]: , discussed in , allows for degrees of truth.

[^2]: Although the figure shows the models as partial wumpus worlds,
    they are really nothing more than assignments of ${true}$ and
    ${false}$ to the sentences “there is a pit in [1,2]” etc. Models,
    in the mathematical sense, do not need to have ’orrible ’airy
    wumpuses in them.

[^3]: The agent can calculate the *probability* that there
    is a pit in [2,2]; shows how.

[^4]: Model checking works if the space of models is finite—for example,
    in wumpus worlds of fixed size. For arithmetic, on the other hand,
    the space of models is infinite: even if we restrict ourselves to
    the integers, there are infinitely many pairs of values for $x$ and
    $y$ in the sentence $x+y=4$.

[^5]: Compare with the case of infinite search spaces in , where
    depth-first search is not complete.

[^6]: As Wittgenstein [-@Wittgenstein:1922] put it in his famous
    *Tractatus*: “The world is everything that is the
    case.”

[^7]: Latin has a separate word, *aut*, for exclusive or.

[^8]: logics, which violate the monotonicity property, capture a common
    property of human reasoning: changing one’s mind. They are discussed
    in .

[^9]: If a clause is viewed as a *set* of literals, then
    this restriction is automatically respected. Using set notation for
    clauses makes the resolution rule much cleaner, at the cost of
    introducing additional notation.

[^10]: conveniently glossed over this requirement.

[^11]: The name “frame problem” comes from “frame of reference” in
    physics—the assumed stationary background with respect to which
    motion is measured. It also has an analogy to the frames of a movie,
    in which normally most of the background stays constant while
    changes occur in the foreground.

[^12]: We can think of the percept history itself as a representation of
    the belief state, but one that makes inference increasingly
    expensive as the history gets longer.

[^13]: Notice that the addition of precondition axioms means that we
    need not include preconditions for actions in the successor-state
    axioms.
Knowledge Representation {#kr-chapter}
========================

The previous chapters described the technology for knowledge-based
agents: the syntax, semantics, and proof theory of propositional and
first-order logic, and the implementation of agents that use these
logics. In this chapter we address the question of what
*content* to put into such an agent’s knowledge base—how to
represent facts about the world.

introduces the idea of a general ontology, which organizes everything in
the world into a hierarchy of categories. covers the basic categories of
objects, substances, and measures; covers events, and discusses
knowledge about beliefs. We then return to consider the technology for
reasoning with this content: discusses reasoning systems designed for
efficient inference with categories, and discusses reasoning with
default information. brings all the knowledge together in the context of
an Internet shopping environment.

Ontological Engineering {#ontology-section}
-----------------------

In “toy” domains, the choice of representation is not that important;
many choices will work. Complex domains such as shopping on the Internet
or driving a car in traffic require more general and flexible
representations. This chapter shows how to create these representations,
concentrating on general concepts—such as *Events, Time, Physical
Objects*, and *Beliefs*—that occur in many different
domains. Representing these abstract concepts is sometimes called .

[ontology-figure]

The prospect of representing *everything* in the world is
daunting. Of course, we won’t actually write a complete description of
everything—that would be far too much for even a 1000-page textbook—but
we will leave placeholders where new knowledge for any domain can fit
in. For example, we will define what it means to be a physical object,
and the details of different types of objects—robots, televisions,
books, or whatever—can be filled in later. This is analogous to the way
that designers of an object-oriented programming framework (such as the
Java Swing graphical framework) define general concepts like
*Window*, expecting users to use these to define more
specific concepts like *SpreadsheetWindow*. The general
framework of concepts is called an because of the convention of drawing
graphs with the general concepts at the top and the more specific
concepts below them, as in .

Before considering the ontology further, we should state one important
caveat. We have elected to use first-order logic to discuss the content
and organization of knowledge, although certain aspects of the real
world are hard to capture in FOL. The principal difficulty is that most
generalizations have exceptions or hold only to a degree. For example,
although “tomatoes are red” is a useful rule, some tomatoes are green,
yellow, or orange. Similar exceptions can be found to almost all the
rules in this chapter. The ability to handle exceptions and uncertainty
is extremely important, but is orthogonal to the task of understanding
the general ontology. For this reason, we delay the discussion of
exceptions until of this chapter, and the more general topic of
reasoning with uncertainty until .

Of what use is an upper ontology? Consider the ontology for circuits in
. It makes many simplifying assumptions: time is omitted completely;
signals are fixed and do not propagate; the structure of the circuit
remains constant. A more general ontology would consider signals at
particular times, and would include the wire lengths and propagation
delays. This would allow us to simulate the timing properties of the
circuit, and indeed such simulations are often carried out by circuit
designers. We could also introduce more interesting classes of gates,
for example, by describing the technology (TTL, CMOS, and so on) as well
as the input–output specification. If we wanted to discuss reliability
or diagnosis, we would include the possibility that the structure of the
circuit or the properties of the gates might change spontaneously. To
account for stray capacitances, we would need to represent where the
wires are on the board.

If we look at the wumpus world, similar considerations apply. Although
we do represent time, it has a simple structure: Nothing happens except
when the agent acts, and all changes are instantaneous. A more general
ontology, better suited for the real world, would allow for simultaneous
changes extended over time. We also used a ${Pit}$ predicate to say
which squares have pits. We could have allowed for different kinds of
pits by having several individuals belonging to the class of pits, each
having different properties. Similarly, we might want to allow for other
animals besides wumpuses. It might not be possible to pin down the exact
species from the available percepts, so we would need to build up a
biological taxonomy to help the agent predict the behavior of
cave-dwellers from scanty clues.

For any special-purpose ontology, it is possible to make changes like
these to move toward greater generality. An obvious question then
arises: do all these ontologies converge on a general-purpose ontology?
After centuries of philosophical and computational investigation, the
answer is “Maybe.” In this section, we present one general-purpose
ontology that synthesizes ideas from those centuries. Two major
characteristics of general-purpose ontologies distinguish them from
collections of special-purpose ontologies:

-   A general-purpose ontology should be applicable in more or less any
    special-purpose domain (with the addition of domain-specific
    axioms). This means that no representational issue can be finessed
    or brushed under the carpet.

-   In any sufficiently demanding domain, different areas of knowledge
    must be *unified*, because reasoning and problem
    solving could involve several areas simultaneously. A robot
    circuit-repair system, for instance, needs to reason about circuits
    in terms of electrical connectivity and physical layout, and about
    time, both for circuit timing analysis and estimating labor costs.
    The sentences describing time therefore must be capable of being
    combined with those describing spatial layout and must work equally
    well for nanoseconds and minutes and for angstroms and meters.

We should say up front that the enterprise of general ontological
engineering has so far had only limited success. None of the top AI
applications (as listed in ) make use of a shared ontology—they all use
special-purpose knowledge engineering. Social/political considerations
can make it difficult for competing parties to agree on an ontology. As
Tom Gruber [-@Gruber:2004] says, “Every ontology is a treaty—a social
agreement—among people with some common motive in sharing.” When
competing concerns outweigh the motivation for sharing, there can be no
common ontology. Those ontologies that do exist have been created along
four routes:

1.  By a team of trained ontologist/logicians, who architect the
    ontology and write axioms. The system was mostly built this way
    @Lenat+Guha:1990.

2.  By importing categories, attributes, and values from an existing
    database or databases. was built by importing structured facts from
    Wikipedia @Bizer+al:2007.

3.  By parsing text documents and extracting information from them. was
    built by reading a large corpus of Web pages @Banko+Etzioni:2008.

4.  By enticing unskilled amateurs to enter commonsense knowledge. The
    system was built by volunteers who proposed facts in English
    @Singh+al:2002 [@Chklovski+Gil:2005].

Categories and Objects {#category-section}
----------------------

The organization of objects into is a vital part of knowledge
representation. Although interaction with the world takes place at the
level of individual objects,

much reasoning takes place at the level of categories.

For example, a shopper would normally have the goal of buying a
basketball, rather than a *particular* basketball such as
${BB}{}_{9}$. Categories also serve to make predictions about objects
once they are classified. One infers the presence of certain objects
from perceptual input, infers category membership from the perceived
properties of the objects, and then uses category information to make
predictions about the objects. For example, from its green and yellow
mottled skin, one-foot diameter, ovoid shape, red flesh, black seeds,
and presence in the fruit aisle, one can infer that an object is a
watermelon; from this, one infers that it would be useful for fruit
salad.

There are two choices for representing categories in first-order logic:
predicates and objects. That is, we can use the predicate
${Basketball}(b)$, or we can [^1] the category as an object,
${Basketballs}$. We could then say ${Member}(b,
{Basketballs})$, which we will abbreviate as $b \elt {Basketballs}$,
to say that $b$ is a member of the category of basketballs. We say
${Subset}({Basketballs}, {Balls})$, abbreviated as
${Basketballs} \subset
{Balls}$, to say that ${Basketballs}$ is a of ${Balls}$. We will
use subcategory, subclass, and subset interchangeably.

Categories serve to organize and simplify the knowledge base through .
If we say that all instances of the category ${Food}$ are edible, and
if we assert that ${Fruit}$ is a subclass of ${Food}$ and
${Apples}$ is a subclass of ${Fruit}$, then we can infer that every
apple is edible. We say that the individual apples the property of
edibility, in this case from their membership in the ${Food}$
category.

Subclass relations organize categories into a , or . Taxonomies have
been used explicitly for centuries in technical fields. The largest such
taxonomy organizes about 10 million living and extinct species, many of
them beetles,[^2] into a single hierarchy; library science has developed
a taxonomy of all fields of knowledge, encoded as the Dewey Decimal
system; and tax authorities and other government departments have
developed extensive taxonomies of occupations and commercial products.
Taxonomies are also an important aspect of general commonsense
knowledge.

First-order logic makes it easy to state facts about categories, either
by relating objects to categories or by quantifying over their members.
Here are some types of facts, with examples of each:

-   An object is a member of a category.\
    ${BB}{}_{9} \elt {Basketballs}$

-   A category is a subclass of another category.\
    ${Basketballs} \subset {Balls}$

-   All members of a category have some properties.\
    $\NoAll{x} (x \elt {Basketballs}) \implies {Spherical}(x)$

-   Members of a category can be recognized by some properties.\
    $\NoAll{x} {Orange}(x) \land {Round}(x) \land {Diameter}(x) \eq {9.5}'' \land x \elt {Balls}
    \implies x \elt {Basketballs}$

-   A category as a whole has some properties.\
    ${Dogs} \elt \v{DomesticatedSpecies}$

Notice that because ${Dogs}$ is a category and is a member of
$\v{DomesticatedSpecies}$, the latter must be a category of categories.
Of course there are exceptions to many of the above rules (punctured
basketballs are not spherical); we deal with these exceptions later.

Although subclass and member relations are the most important ones for
categories, we also want to be able to state relations between
categories that are not subclasses of each other. For example, if we
just say that ${Males}$ and ${Females}$ are subclasses of
${Animals}$, then we have not said that a male cannot be a female. We
say that two or more categories are if they have no members in common.
And even if we know that males and females are disjoint, we will not
know that an animal that is not a male must be a female, unless we say
that males and females constitute an of the animals. A disjoint
exhaustive decomposition is known as a . The following examples
illustrate these three concepts: $$\begin{array}{l}
   {Disjoint}(\{{Animals},{Vegetables}\}) \\
   {ExhaustiveDecomposition}(\{{Americans},{Canadians},{Mexicans}\},\\
    \qquad\qquad\qquad\qquad {NorthAmericans}) \\
   {Partition}(\{{Males},{Females}\},{Animals})\ . \end{array}$$
(Note that the ${ExhaustiveDecomposition}$ of ${NorthAmericans}$ is
not a ${Partition}$, because some people have dual citizenship.) The
three predicates are defined as follows: $$\begin{array}{l}
  \NoAll{s} {Disjoint}(s) \lequiv (\All{c_1,c_2} c_1 \elt s \land c_2 \elt s \land c_1 \neq c_2 
  \implies {Intersection}(c_1 ,c_2) \eq  \{\;\}) \\
\NoAll{s,c} {ExhaustiveDecomposition}(s,c) \lequiv
 (\All{i} i \elt c \lequiv
    \Exi{c_2} c_2 \elt s \land i\elt c_2)\\
\NoAll{s,c} {Partition}(s,c) \lequiv {Disjoint}(s) \land
{ExhaustiveDecomposition}(s,c)\ .
   \end{array}$$

Categories can also be *defined* by providing necessary and
sufficient conditions for membership. For example, a bachelor is an
unmarried adult male:
$$\NoAll{x}  x \elt {Bachelors} \lequiv {Unmarried}(x) \land x \elt {Adults} \land  x \elt {Males}\ .$$
As we discuss in the sidebar on natural kinds on , strict logical
definitions for categories are neither always possible nor always
necessary.

### Physical composition

The idea that one object can be part of another is a familiar one. One’s
nose is part of one’s head, Romania is part of Europe, and this chapter
is part of this book. We use the general ${PartOf}$ relation to say
that one thing is part of another. Objects can be grouped into
${PartOf}$ hierarchies, reminiscent of the ${Subset}$ hierarchy:
$$\begin{array}{l}
{PartOf}({Bucharest},{Romania})\\
{PartOf}({Romania},{EasternEurope})\\
{PartOf}({EasternEurope},{Europe})\\
{PartOf}({Europe},{Earth})\ .
\end{array}$$ The ${PartOf}$ relation is transitive and reflexive;
that is,

(x,y) (y,z) (x,z)  .\
(x, x) .

Therefore, we can conclude ${PartOf}({Bucharest},{Earth})$.

Categories of are often characterized by structural relations among
parts. For example, a biped has two legs attached to a body:

$$\begin{aligned}
\NoAll{a} {Biped}(a) &\implies&
          \Exi{l_1,l_2,b} {Leg}(l_1) \land {Leg}(l_2) \land {Body}(b)\ \land\\
&&\;\;   {PartOf}(l_1,a) \land {PartOf}(l_2,a) \land {PartOf}(b,a)\ \land \\
&&\;\;   {Attached}(l_1,b) \land {Attached}(l_2,b) \ \land\\
&&\;\;   l_1 \neq l_2 \land [\All{l_3} {Leg}(l_3) \land
                    {PartOf}(l_3,a) \implies (l_3\eq l_1 \lor l_3\eq l_2)]\ .\end{aligned}$$

The notation for “exactly two” is a little awkward; we are forced to say
that there are two legs, that they are not the same, and that if anyone
proposes a third leg, it must be the same as one of the other two. In ,
we describe a formalism called description logic makes it easier to
represent constraints like “exactly two.”

We can define a ${PartPartition}$ relation analogous to the
${Partition}$ relation for categories. (See .) An object is composed
of the parts in its ${PartPartition}$ and can be viewed as deriving
some properties from those parts. For example, the mass of a composite
object is the sum of the masses of the parts. Notice that this is not
the case with categories, which have no mass, even though their elements
might.

It is also useful to define composite objects with definite parts but no
particular structure. For example, we might want to say “The apples in
this bag weigh two pounds.” The temptation would be to ascribe this
weight to the *set* of apples in the bag, but this would be
a mistake because the set is an abstract mathematical concept that has
elements but does not have weight. Instead, we need a new concept, which
we will call a . For example, if the apples are ${Apple}{}_1$,
${Apple}{}_2$, and ${Apple}{}_3$, then
$${BunchOf}(\{{Apple}{}_1,{Apple}{}_2,{Apple}{}_3\})$$ denotes
the composite object with the three apples as parts (not elements). We
can then use the bunch as a normal, albeit unstructured, object. Notice
that ${BunchOf}(\{x\})\eq x$. Furthermore, ${BunchOf}({Apples})$
is the composite object consisting of all apples—not to be confused with
${Apples}$, the category or set of all apples.

We can define ${BunchOf}$ in terms of the ${PartOf}$ relation.
Obviously, each element of $s$ is part of ${BunchOf}(s)$:
$$\All{x} x \elt s \implies {PartOf}(x, {BunchOf}(s))\ .$$
Furthermore, ${BunchOf}(s)$ *is the smallest object satisfying
this condition*. In other words, ${BunchOf}(s)$ must be part of
any object that has all the elements of $s$ as parts:
$$\All{y} [\All{x} x \elt s \implies {PartOf}(x, y)] \implies
    {PartOf}({BunchOf}(s),y)\ .$$ These axioms are an example of a
general technique called , which means defining an object as the
smallest one satisfying certain conditions.

### Measurements

In both scientific and commonsense theories of the world, objects have
height, mass, cost, and so on. The values that we assign for these
properties are called . Ordinary quantitative measures are quite easy to
represent. We imagine that the universe includes abstract “measure
objects,” such as the *length* that is the length of this
line segment: . We can call this length 1.5 inches or 3.81 centimeters.
Thus, the same length has different names in our language.We represent
the length with a that takes a number as argument. (An alternative
scheme is explored in .) If the line segment is called $L_1$, we can
write
$${Length}(L_1) \eq  {Inches}({1.5}) \eq  {Centimeters}({3.81})\ .$$
Conversion between units is done by equating multiples of one unit to
another:

(2.54 d) (d) .\

Similar axioms can be written for pounds and kilograms, seconds and
days, and dollars and cents. Measures can be used to describe objects as
follows:

(~12~) (9.5) .\
(~12~) (19) .\
 d (d) (24) .

Note that ${\DollarSign}(1)$ is *not* a dollar bill! One
can have two dollar bills, but there is only one object named
${\DollarSign}(1)$. Note also that, while ${Inches}(0)$ and
${Centimeters}(0)$ refer to the same zero length, they are not
identical to other zero measures, such as ${Seconds}(0)$.

Simple, quantitative measures are easy to represent. Other measures
present more of a problem, because they have no agreed scale of values.
Exercises have difficulty, desserts have deliciousness, and poems have
beauty, yet numbers cannot be assigned to these qualities. One might, in
a moment of pure accountancy, dismiss such properties as useless for the
purpose of logical reasoning; or, still worse, attempt to impose a
numerical scale on beauty. This would be a grave mistake, because it is
unnecessary. The most important aspect of measures is not the particular
numerical values, but the fact that measures can be
*ordered*.

Although measures are not numbers, we can still compare them, using an
ordering symbol such as $>$. For example, we might well believe that
Norvig’s exercises are tougher than Russell’s, and that one scores less
on tougher exercises:

e~1~ e~2~ (,e~1~) (,e~2~)\
 (e~1~) \> (e~2~) .\
 e~1~ e~2~ (e~1~) \> (e~2~)\
 (e~1~) \< (e~2~) .

This is enough to allow one to decide which exercises to do, even though
no numerical values for difficulty were ever used. (One does, however,
have to discover who wrote which exercises.) These sorts of monotonic
relationships among measures form the basis for the field of , a
subfield of AI that investigates how to reason about physical systems
without plunging into detailed equations and numerical simulations.
Qualitative physics is discussed in the historical notes section.

### Objects: Things and stuff

The real world can be seen as consisting of primitive objects (e.g.,
atomic particles) and composite objects built from them. By reasoning at
the level of large objects such as apples and cars, we can overcome the
complexity involved in dealing with vast numbers of primitive objects
individually. There is, however, a significant portion of reality that
seems to defy any obvious —division into distinct objects. We give this
portion the generic name . For example, suppose I have some butter and
an aardvark in front of me. I can say there is one aardvark, but there
is no obvious number of “butter-objects,” because any part of a
butter-object is also a butter-object, at least until we get to very
small parts indeed. This is the major distinction between
*stuff* and *things*. If we cut an aardvark in
half, we do not get two aardvarks (unfortunately).

The English language distinguishes clearly between *stuff*
and *things*. We say “an aardvark,” but, except in
pretentious California restaurants, one cannot say “a butter.” Linguists
distinguish between , such as aardvarks, holes, and theorems, and , such
as butter, water, and energy. Several competing ontologies claim to
handle this distinction. Here we describe just one; the others are
covered in the historical notes section.

To represent *stuff* properly, we begin with the obvious.
We need to have as objects in our ontology at least the gross “lumps” of
*stuff* we interact with. For example, we might recognize a
lump of butter as the one left on the table the night before; we might
pick it up, weigh it, sell it, or whatever. In these senses, it is an
object just like the aardvark. Let us call it ${Butter}{}_3$. We also
define the category ${Butter}$. Informally, its elements will be all
those things of which one might say “It’s butter,” including
${Butter}{}_3$. With some caveats about very small parts that we w
omit for now, any part of a butter-object is also a butter-object:
$$\NoAll{b,p} b \elt {Butter} \land {PartOf}(p,b) \implies p \elt {Butter}\ .$$
We can now say that butter melts at around degrees centigrade:
$$\NoAll{x} b \elt {Butter} \implies {MeltingPoint}(b,{Centigrade}({30}))\ .$$
We could go on to say that butter is yellow, is less dense than water,
is soft at room temperature, has a high fat content, and so on. On the
other hand, butter has no particular size, shape, or weight. We can
define more specialized categories of butter such as
${UnsaltedButter}$, which is also a kind of *stuff*. Note
that the category ${PoundOfButter}$, which includes as members all
butter-objects weighing one pound, is not a kind of
*stuff*. If we cut a pound of butter in half, we do not,
alas, get two pounds of butter.

What is actually going on is this: some properties are : they belong to
the very substance of the object, rather than to the object as a whole.
When you cut an instance of *stuff* in half, the two pieces
retain the intrinsic properties—things like density, boiling point,
flavor, color, ownership, and so on. On the other hand, their
properties—weight, length, shape, and so on—are not retained under
subdivision. A category of objects that includes in its definition only
*intrinsic* properties is then a substance, or mass noun; a
class that includes *any* extrinsic properties in its
definition is a count noun. The category $\v{Stuff}$ is the most general
substance category, specifying no intrinsic properties. The category
${Thing}$ is the most general discrete object category, specifying no
extrinsic properties.

Events {#event-section}
------

In , we showed how situation calculus represents actions and their
effects. Situation calculus is limited in its applicability: it was
designed to describe a world in which actions are discrete,
instantaneous, and happen one at a time. Consider a continuous action,
such as filling a bathtub. Situation calculus can say that the tub is
empty before the action and full when the action is done, but it can’t
talk about what happens *during* the action. It also can’t
describe two actions happening at the same time—such as brushing one’s
teeth while waiting for the tub to fill. To handle such cases we
introduce an alternative formalism known as , which is based on points
of time rather than on situations.[^3]

Event calculus reifies fluents and events. The fluent
${At}({Shankar},{Berkeley})$ is an object that refers to the fact
of Shankar being in Berkeley, but does not by itself say anything about
whether it is true. To assert that a fluent is actually true at some
point in time we use the predicate $\Holds$, as in
$\Holds({At}({Shankar},{Berkeley}), t)$.

Events are described as instances of event categories.[^4] The event
$E_1$ of Shankar flying from San Francisco to Washington, D.C. is
described as
$$E_1 \in Flyings \land {Flyer}(E_1, {Shankar}) \land {Origin}(E_1, {SF}) \land {Destination}(E_1, {DC}) \,.$$
If this is too verbose, we can define an alternative three-argument
version of the category of flying events and say
$$E_1 \in Flyings({Shankar}, {SF}, {DC}) \,.$$ We then use
${Happens}(E_1, i)$ to say that the event $E_1$ took place over the
time interval $i$, and we say the same thing in functional form with
${Extent}(E_1)\eq i$. We represent time intervals by a (start, end)
pair of times; that is, $i = (t_1, t_2)$ is the time interval that
starts at $t_1$ and ends at $t_2$. The complete set of predicates for
one version of the event calculus is

(f, t) & &\
(e, i) & &\
(e, f, t) & &\
(e, f, t) & &\
(f, i) & &\
(f, i) & &

We assume a distinguished event, ${Start}$, that describes the initial
state by saying which fluents are initiated or terminated at the start
time. We define $\Holds$ by saying that a fluent holds at a point in
time if the fluent was initiated by an event at some time in the past
and was not made false (clipped) by an intervening event. A fluent does
not hold if it was terminated by an event and not made true (restored)
by another event. Formally, the axioms are:

(e, (t~1~,t~2~)) (e, f, t~1~) (f, (t~1~, t)) t~1~ \< t\
(f, t)\
(e, (t~1~, t~2~)) (e, f, t~1~) (f, (t~1~, t)) t~1~ \< t\
(f, t)\

where ${Clipped}$ and ${Restored}$ are defined by

(f, (t~1~, t~2~))\
 (e, (t,t~3~)) t~1~ t \< t~2~ (e, f, t)\
(f, (t~1~, t~2~))\
 (e, (t,t~3~)) t~1~ t \< t~2~ (e, f, t)\

It is convenient to extend $\Holds$ to work over intervals as well as
time points; a fluent holds over an interval if it holds on every point
within the interval:
$$\Holds(f, (t_1, t_2)) \lequiv [\All{t} (t_1 \le t < t_2) \implies \Holds(f, t)]$$
Fluents and actions are defined with domain-specific axioms that are
similar to successor-state axioms. For example, we can say that the only
way a wumpus-world agent gets an arrow is at the start, and the only way
to use up an arrow is to shoot it:

(e, , t) e =\
(e, , t) e

By reifying events we make it possible to add any amount of arbitrary
information about them. For example, we can say that Shankar’s flight
was bumpy with ${Bumpy}(E_1)$. In an ontology where events are $n$-ary
predicates, there would be no way to add extra information like this;
moving to an $n+1$-ary predicate isn’t a scalable solution.

We can extend event calculus to make it possible to represent
simultaneous events (such as two people being necessary to ride a
seesaw), exogenous events (such as the wind blowing and changing the
location of an object), continuous events (such as the level of water in
the bathtub continuously rising) and other complications.

### Processes

[process-section]

The events we have seen so far are what we call —they have a definite
structure. Shankar’s trip has a beginning, middle, and end. If
interrupted halfway, the event would be something different—it would not
be a trip from San Francisco to Washington, but instead a trip from San
Francisco to somewhere over Kansas. On the other hand, the category of
events denoted by ${Flyings}$ has a different quality. If we take a
small interval of Shankar’s flight, say, the third 20-minute segment
(while he waits anxiously for a bag of peanuts), that event is still a
member of ${Flyings}$. In fact, this is true for any subinterval.

Categories of events with this property are called categories or
categories. Any process $e$ that happens over an interval also happens
over any subinterval:

$$(e \in {Processes}) \land {Happens}(e, (t_1, t_4)) \land (t_1 < t_2 < t_3 < t_4) \implies {Happens}(e, (t_2, t_3)) \,.$$

The distinction between liquid and nonliquid events is exactly analogous
to the difference between substances, or *stuff*, and
individual objects, or *things*. In fact, some have called
liquid events , whereas substances like butter are .

### Time intervals

Event calculus opens us up to the possibility of talking about time, and
time intervals. We will consider two kinds of time intervals: moments
and extended intervals. The distinction is that only moments have zero
duration: $$\begin{array}{l}
   {Partition}(\{{Moments},{ExtendedIntervals}\},{Intervals}) \\
   \NoAll{i} i \elt {Moments} \lequiv
            {Duration}(i) \eq  {Seconds}(0) \ . \\
   \end{array}$$ Next we invent a time scale and associate points on
that scale with moments, giving us absolute times. The time scale is
arbitrary; we measure it in seconds and say that the moment at midnight
(GMT) on January 1, 1900, has time 0. The functions ${Begin}$ and
${End}$ pick out the earliest and latest moments in an interval, and
the function ${Time}$ delivers the point on the time scale for a
moment. The function ${Duration}$ gives the difference between the end
time and the start time.

​(i) (i)(((i)) - ((i))) .\
(()) (0)  .\
(()) (3187324800)  .\
(()) (3218860800) .\
() (31536000) .

To make these numbers easier to read, we also introduce a function
${Date}$, which takes six arguments (hours, minutes, seconds, day,
month, and year) and returns a time point: $$\begin{array}{l}
{Time}({Begin}({AD}{2001})) \eq  {Date}(0,0,0,1,{Jan},{2001}) \\
{Date}(0, {20}, {21}, {24}, 1, {1995}) \eq  {Seconds}({3000000000})\ .
\end{array}$$ Two intervals ${Meet}$ if the end time of the first
equals the start time of the second. The complete set of interval
relations, as proposed by , is shown graphically in and logically below:

(i,j) && (i) (j)\
(i,j) && (i) \< (j)\
(j,i) && (i,j)\
(i,j) && (j) \< (i) \< (i) \< (j)\
(i,j) && (i) \< (j) \< (i) \< (j)\
(i,j) && (i) = (j)\
(i,j) && (i) = (j)\
(i,j) && (i) = (j) (i) = (j)

These all have their intuitive meaning, with the exception of : we tend
to think of overlap as symmetric (if *i* overlaps
*j* then *j* overlaps *i*), but
in this definition, ${Overlap}(i, j)$ only holds if *i*
begins before *j*. To say that the reign of Elizabeth II
immediately followed that of George VI, and the reign of Elvis
overlapped with the 1950s, we can write the following:

((), ()) .\
(,())  .\
() ()  .\
() () .

[time-intervals-figure]

[president-usa-figure]

### Fluents and objects

[fluents-and-objects-section]

Physical objects can be viewed as generalized events, in the sense that
a physical object is a chunk of space–time. For example, ${USA}$ can
be thought of as an event that began in, say, 1776 as a union of 13
states and is still in progress today as a union of 50. We can describe
the changing properties of ${USA}$ using state fluents, such as
${Population}({USA})$. A property of the USA that changes every four
or eight years, barring mishaps, is its president. One might propose
that ${President}({USA})$ is a logical term that denotes a different
object at different times. Unfortunately, this is not possible, because
a term denotes exactly one object in a given model structure. (The term
${President}({USA},t)$ can denote different objects, depending on
the value of $t$, but our ontology keeps time indices separate from
fluents.) The only possibility is that ${President}({USA})$ denotes
a single object that consists of different people at different times. It
is the object that is George Washington from 1789 to 1797, John Adams
from 1797 to 1801, and so on, as in . To say that George Washington was
president throughout 1790, we can write
$$\Holds({Equals}({President}({USA}), {GeorgeWashington}), {AD}{1790})\ .$$
We use the function symbol ${Equals}$ rather than the standard logical
predicate $=$, because we cannot have a predicate as an argument to
$\Holds$, and because the interpretation is *not* that
${GeorgeWashington}$ and ${President}({USA})$ are logically
identical in 1790; logical identity is not something that can change
over time. The identity is between the subevents of each object that are
defined by the period 1790.

Mental Events and Mental Objects {#belief-section}
--------------------------------

The agents we have constructed so far have beliefs and can deduce new
beliefs. Yet none of them has any knowledge *about* beliefs
or *about* deduction. Knowledge about one’s own knowledge
and reasoning processes is useful for controlling inference. For
example, suppose Alice asks “what is the square root of 1764” and Bob
replies “I don’t know.” If Alice insists “think harder,” Bob should
realize that with some more thought, this question can in fact be
answered. On the other hand, if the question were “Is your mother
sitting down right now?” then Bob should realize that thinking harder is
unlikely to help. Knowledge about the knowledge of other agents is also
important; Bob should realize that his mother knows whether she is
sitting or not, and that asking her would be a way to find out.

What we need is a model of the mental objects that are in someone’s head
(or something’s knowledge base) and of the mental processes that
manipulate those mental objects. The model does not have to be detailed.
We do not have to be able to predict how many milliseconds it will take
for a particular agent to make a deduction. We will be happy just to be
able to conclude that mother knows whether or not she is sitting.

We begin with the that an agent can have toward mental objects:
attitudes such as ${Believes}$, ${Knows}$, ${Wants}$,
${Intends}$, and ${Informs}$. The difficulty is that these attitudes
do not behave like “normal” predicates. For example, suppose we try to
assert that Lois knows that Superman can fly:
$${Knows}({Lois},{CanFly}({Superman})) \,.$$ One minor issue
with this is that we normally think of ${CanFly}({Superman})$ as a
sentence, but here it appears as a term. That issue can be patched up
just be reifying ${CanFly}({Superman})$; making it a fluent. A more
serious problem is that, if it is true that Superman is Clark Kent, then
we must conclude that Lois knows that Clark can fly:

( = ) (,())\
(,()) .

This is a consequence of the fact that equality reasoning is built into
logic. Normally that is a good thing; if our agent knows that $2+2=4$
and $4<5$, then we want our agent to know that $2+2<5$. This property is
called —it doesn’t matter what term a logic uses to refer to an object,
what matters is the object that the term names. But for propositional
attitudes like *believes* and *knows*, we
would like to have referential opacity—the terms used *do*
matter, because not all agents know which terms are co-referential.

is designed to address this problem. Regular logic is concerned with a
single modality, the modality of truth, allowing us to express
“*P* is true.” Modal logic includes special modal operators
that take sentences (rather than terms) as arguments. For example,
“*A* knows *P*” is represented with the
notation $\mbf{K}_A P$, where $\mbf{K}$ is the modal operator for
knowledge. It takes two arguments, an agent (written as the subscript)
and a sentence. The syntax of modal logic is the same as first-order
logic, except that sentences can also be formed with modal operators.

The semantics of modal logic is more complicated. In first-order logic a
contains a set of objects and an interpretation that maps each name to
the appropriate object, relation, or function. In modal logic we want to
be able to consider both the possibility that Superman’s secret identity
is Clark and that it isn’t. Therefore, we will need a more complicated
model, one that consists of a collection of rather than just one true
world. The worlds are connected in a graph by , one relation for each
modal operator. We say that world $w_1$ is accessible from world $w_0$
with respect to the modal operator $\mbf{K}_A$ if everything in $w_1$ is
consistent with what $A$ knows in $w_0$, and we write this as
${Acc}(\mbf{K}_A, w_0,
w_1)$. In diagrams such as we show accessibility as an arrow between
possible worlds. As an example, in the real world, Bucharest is the
capital of Romania, but for an agent that did not know that, other
possible worlds are accessible, including ones where the capital of
Romania is Sibiu or Sofia. Presumably a world where $2 + 2 = 5$ would
not be accessible to any agent.

In general, a knowledge atom $\mbf{K}_A P$ is true in world $w$ if and
only if $P$ is true in every world accessible from $w$. The truth of
more complex sentences is derived by recursive application of this rule
and the normal rules of first-order logic. That means that modal logic
can be used to reason about nested knowledge sentences: what one agent
knows about another agent’s knowledge. For example, we can say that,
even though Lois doesn’t know whether Superman’s secret identity is
Clark Kent, she does know that Clark knows:
$$\mbf{K}_{{Lois}} [\mbf{K}_{{Clark}} {Identity}({Superman},{Clark}) \lor
\mbf{K}_{{Clark}} \lnot {Identity}({Superman},{Clark})]$$ shows
some possible worlds for this domain, with accessibility relations for
Lois and Superman.

[possible-worlds-figure]

In the top-left diagram, it is common knowledge that
Superman knows his own identity, and neither he nor Lois has seen the
weather report. So in $w_0$ the worlds $w_0$ and $w_2$ are accessible to
Superman; maybe rain is predicted, maybe not. For Lois all four worlds
are accessible from each other; she doesn’t know anything about the
report or if Clark is Superman. But she does know that Superman knows
whether he is Clark, because in every world that is accessible to Lois,
either Superman knows $I$, or he knows $\lnot I$. Lois does not know
which is the case, but either way she knows Superman knows.

In the top-right diagram it is common knowledge that Lois
has seen the weather report. So in $w_4$ she knows rain is predicted and
in $w_6$ she knows rain is not predicted. Superman does not know the
report, but he knows that Lois knows, because in every world that is
accessible to him, either she knows $R$ or she knows $\lnot R$.

In the bottom diagram we represent the scenario where it is
common knowledge that Superman knows his identity, and Lois might or
might not have seen the weather report. We represent this by combining
the two top scenarios, and adding arrows to show that Superman does not
know which scenario actually holds. Lois does know, so we don’t need to
add any arrows for her. In $w_0$ Superman still knows $I$ but not $R$,
and now he does not know whether Lois knows $R$. From what Superman
knows, he might be in $w_0$ or $w_2$, in which case Lois does not know
whether $R$ is true, or he could be in $w_4$, in which case she knows
$R$, or $w_6$, in which case she knows $\lnot R$.

There are an infinite number of possible worlds, so the trick is to
introduce just the ones you need to represent what you are trying to
model. A new possible world is needed to talk about different possible
facts (e.g., rain is predicted or not), or to talk about different
states of knowledge (e.g., does Lois know that rain is predicted). That
means two possible worlds, such as $w_4$ and $w_0$ in , might have the
same base facts about the world, but differ in their accessibility
relations, and therefore in facts about knowledge.

Modal logic solves some tricky issues with the interplay of quantifiers
and knowledge. The English sentence “Bond knows that someone is a spy”
is ambiguous. The first reading is that there is a particular someone
who Bond knows is a spy; we can write this as
$$\Exi{x} \mbf{K}_{{Bond}} {Spy}(x) \,,$$ which in modal logic means
that there is an $x$ that, in all accessible worlds, Bond knows to be a
spy. The second reading is that Bond just knows that there is at least
one spy: $$\mbf{K}_{{Bond}} \Exi{x} {Spy}(x) \,.$$ The modal logic
interpretation is that in each accessible world there is an $x$ that is
a spy, but it need not be the same $x$ in each world.

Now that we have a modal operator for knowledge, we can write axioms for
it. First, we can say that agents are able to draw deductions; if an
agent knows $P$ and knows that $P$ implies $Q$, then the agent knows
$Q$:
$$(\mbf{K}_a P \land \mbf{K}_a (P \implies Q)) \implies \mbf{K}_a Q \,.$$
From this (and a few other rules about logical identities) we can
establish that $\mbf{K}_A (P \lor \lnot P)$ is a tautology; every agent
knows every proposition $P$ is either true or false. On the other hand,
$(\mbf{K}_A P) \lor (\mbf{K}_A \lnot P)$ is not a tautology; in general,
there will be lots of propositions that an agent does not know to be
true and does not know to be false.

It is said (going back to Plato) that knowledge is justified true
belief. That is, if it is true, if you believe it, and if you have an
unassailably good reason, then you know it. That means that if you know
something, it must be true, and we have the axiom:
$$\mbf{K}_a P \implies P \ .$$ Furthermore, logical agents should be
able to introspect on their own knowledge. If they know something, then
they know that they know it:
$$\mbf{K}_a P \implies \mbf{K}_a (\mbf{K}_a P) \ .$$ We can define
similar axioms for belief (often denoted by $\mbf{B}$) and other
modalities. However, one problem with the modal logic approach is that
it assumes [logical-omniscience] on the part of agents. That is, if an
agent knows a set of axioms, then it knows all consequences of those
axioms. This is on shaky ground even for the somewhat abstract notion of
knowledge, but it seems even worse for belief, because belief has more
connotation of referring to things that are physically represented in
the agent, not just potentially derivable. There have been attempts to
define a form of limited rationality for agents; to say that agents
believe those assertions that can be derived with the application of no
more than $k$ reasoning steps, or no more than $s$ seconds of
computation. These attempts have been generally unsatisfactory.

Reasoning Systems for Categories {#hierarchy-section}
--------------------------------

Categories are the primary building blocks of large-scale knowledge
representation schemes. This section describes systems specially
designed for organizing and reasoning with categories. There are two
closely related families of systems: provide graphical aids for
visualizing a knowledge base and efficient algorithms for inferring
properties of an object on the basis of its category membership; and
provide a formal language for constructing and combining category
definitions and efficient algorithms for deciding subset and superset
relationships between categories.

### Semantic networks {#semantic-network-section}

In 1909, Charles S. Peirce proposed a graphical notation of nodes and
edges called that he called “the logic of the future.” Thus began a
long-running debate between advocates of “logic” and advocates of
“semantic networks.” Unfortunately, the debate obscured the fact that
semantics networks—at least those with well-defined
semantics—*are* a form of logic. The notation that semantic
networks provide for certain kinds of sentences is often more
convenient, but if we strip away the “human interface” issues, the
underlying concepts—objects, relations, quantification, and so on—are
the same.

There are many variants of semantic networks, but all are capable of
representing individual objects, categories of objects, and relations
among objects. A typical graphical notation displays object or category
names in ovals or boxes, and connects them with labeled links. For
example, has a ${MemberOf}$ link between ${Mary}$ and
${FemalePersons}$, corresponding to the logical assertion
${Mary} \elt {FemalePersons}$; similarly, the ${SisterOf}$ link
between ${Mary}$ and ${John}$ corresponds to the assertion
${SisterOf}({Mary},{John})$. We can connect categories using
${SubsetOf}$ links, and so on. It is such fun drawing bubbles and
arrows that one can get carried away. For example, we know that persons
have female persons as mothers, so can we draw a ${HasMother}$ link
from ${Persons}$ to ${FemalePersons}$? The answer is no, because
${HasMother}$ is a relation between a person and his or her mother,
and categories do not have mothers.[^5]

For this reason, we have used a special notation—the double-boxed
link—in . This link asserts that
$$\All{x} x\elt {Persons} \implies [\All{y} {HasMother}(x,y) \implies y\elt {FemalePersons}]\,.$$
We might also want to assert that persons have two legs—that is,
$$\All{x} x\elt {Persons} \implies {Legs}(x,2)\ .$$ As before, we
need to be careful not to assert that a category has legs; the
single-boxed link in is used to assert properties of every member of a
category.

[semantic-network-figure]

The semantic network notation makes it convenient to perform reasoning
of the kind introduced in . For example, by virtue of being a person,
Mary inherits the property of having two legs. Thus, to find out how
many legs Mary has, the inheritance algorithm follows the ${MemberOf}$
link from ${Mary}$ to the category she belongs to, and then follows
${SubsetOf}$ links up the hierarchy until it finds a category for
which there is a boxed ${Legs}$ link—in this case, the ${Persons}$
category. The simplicity and efficiency of this inference mechanism,
compared with logical theorem proving, has been one of the main
attractions of semantic networks.

Inheritance becomes complicated when an object can belong to more than
one category or when a category can be a subset of more than one other
category; this is called . In such cases, the inheritance algorithm
might find two or more conflicting values answering the query. For this
reason, multiple inheritance is banned in some (OOP) languages, such as
Java, that use inheritance in a class hierarchy. It is usually allowed
in semantic networks, but we defer discussion of that until .

[flying-network-figure]

The reader might have noticed an obvious drawback of semantic network
notation, compared to first-order logic: the fact that links between
bubbles represent only *binary* relations. For example, the
sentence ${Fly}({Shankar}, {NewYork}, {NewDelhi},{Yesterday})$
cannot be asserted directly in a semantic network. Nonetheless, we
*can* obtain the effect of $n$-ary assertions by reifying
the proposition itself as an event belonging to an appropriate event
category. shows the semantic network structure for this particular
event. Notice that the restriction to binary relations forces the
creation of a rich ontology of reified concepts.

Reification of propositions makes it possible to represent every ground,
function-free atomic sentence of first-order logic in the semantic
network notation. Certain kinds of universally quantified sentences can
be asserted using inverse links and the singly boxed and doubly boxed
arrows applied to categories, but that still leaves us a long way short
of full first-order logic. Negation, disjunction, nested function
symbols, and existential quantification are all missing. Now it is
*possible* to extend the notation to make it equivalent to
first-order logic—as in Peirce’s existential graphs—but doing so negates
one of the main advantages of semantic networks, which is the simplicity
and transparency of the inference processes. Designers can build a large
network and still have a good idea about what queries will be efficient,
because (a) it is easy to visualize the steps that the inference
procedure will go through and (b) in some cases the query language is so
simple that difficult queries cannot be posed. In cases where the
expressive power proves to be too limiting, many semantic network
systems provide for to fill in the gaps. Procedural attachment is a
technique whereby a query about (or sometimes an assertion of) a certain
relation results in a call to a special procedure designed for that
relation rather than a general inference algorithm.

One of the most important aspects of semantic networks is their ability
to represent for categories. Examining carefully, one notices that John
has one leg, despite the fact that he is a person and all persons have
two legs. In a strictly logical KB, this would be a contradiction, but
in a semantic network, the assertion that all persons have two legs has
only default status; that is, a person is assumed to have two legs
unless this is contradicted by more specific information. The default
semantics is enforced naturally by the inheritance algorithm, because it
follows links upwards from the object itself (John in this case) and
stops as soon as it finds a value. We say that the default is by the
more specific value. Notice that we could also override the default
number of legs by creating a category of ${OneLeggedPersons}$, a
subset of ${Persons}$ of which ${John}$ is a member.

We can retain a strictly logical semantics for the network if we say
that the ${Legs}$ assertion for ${Persons}$ includes an exception
for John:
$$\All{x} x\elt {Persons} \land x\neq {John} \implies {Legs}(x,2)\ .$$
For a *fixed* network, this is semantically adequate but
will be much less concise than the network notation itself if there are
lots of exceptions. For a network that will be updated with more
assertions, however, such an approach fails—we really want to say that
any persons as yet unknown with one leg are exceptions too. goes into
more depth on this issue and on default reasoning in general.

### Description logics {#description-logic-section}

The syntax of first-order logic is designed to make it easy to say
things about objects. are notations that are designed to make it easier
to describe definitions and properties of categories. Description logic
systems evolved from semantic networks in response to pressure to
formalize what the networks mean while retaining the emphasis on
taxonomic structure as an organizing principle.

The principal inference tasks for description logics are (checking if
one category is a subset of another by comparing their definitions) and
(checking whether an object belongs to a category).. Some systems also
include of a category definition—whether the membership criteria are
logically satisfiable.

The language @Borgida+al:1989 is a typical description logic. The syntax
of descriptions is shown in .[^6] For example, to say that bachelors are
unmarried adult males we would write
$${Bachelor} = {And}({Unmarried},{Adult},{Male})\ .$$ The
equivalent in first-order logic would be
$$\NoAll{x} {Bachelor}(x) \lequiv {Unmarried}(x) \land {Adult}(x) \land {Male}(x)\ .$$
Notice that the description logic has an an algebra of operations on
predicates, which of course we can’t do in first-order logic. Any
description in can be translated into an equivalent first-order
sentence, but some descriptions are more straightforward in . For
example, to describe the set of men with at least three sons who are all
unemployed and married to doctors, and at most two daughters who are all
professors in physics or math departments, we would
use[description-logic-ex]

(,(3,), (2,),\
(,(,,(,))),\
(,(,(,,)))) .

We leave it as an exercise to translate this into first-order logic.

[classic-figure]

Perhaps the most important aspect of description logics is their
emphasis on tractability of inference. A problem instance is solved by
describing it and then asking if it is subsumed by one of several
possible solution categories. In standard first-order logic systems,
predicting the solution time is often impossible. It is frequently left
to the user to engineer the representation to detour around sets of
sentences that seem to be causing the system to take several weeks to
solve a problem. The thrust in description logics, on the other hand, is
to ensure that subsumption-testing can be solved in time polynomial in
the size of the descriptions.[^7]

This sounds wonderful in principle, until one realizes that it can only
have one of two consequences: either hard problems cannot be stated at
all, or they require exponentially large descriptions! However, the
tractability results do shed light on what sorts of constructs cause
problems and thus help the user to understand how different
representations behave. For example, description logics usually lack
*negation* and *disjunction*. Each forces
first-order logical systems to go through a potentially exponential case
analysis in order to ensure completeness. allows only a limited form of
disjunction in the ${Fills}$ and ${OneOf}$ constructs, which permit
disjunction over explicitly enumerated individuals but not over
descriptions. With disjunctive descriptions, nested definitions can lead
easily to an exponential number of alternative routes by which one
category can subsume another.

Reasoning with Default Information {#nonmon-section}
----------------------------------

In the preceding section, we saw a simple example of an assertion with
default status: people have two legs. This default can be overridden by
more specific information, such as that Long John Silver has one leg. We
saw that the inheritance mechanism in semantic networks implements the
overriding of defaults in a simple and natural way. In this section, we
study defaults more generally, with a view toward understanding the
*semantics* of defaults rather than just providing a
procedural mechanism.

### Circumscription and default logic

We have seen two examples of reasoning processes that violate the
property of logic that was proved in .[^8] In this chapter we saw that a
property inherited by all members of a category in a semantic network
could be overridden by more specific information for a subcategory. In ,
we saw that under the closed-world assumption, if a proposition $\alpha$
is not mentioned in ${KB}$ then ${KB} \entails \lnot \alpha$, but
${KB} \land \alpha
\entails \alpha$.

Simple introspection suggests that these failures of monotonicity are
widespread in commonsense reasoning. It seems that humans often “jump to
conclusions.” For example, when one sees a car parked on the street, one
is normally willing to believe that it has four wheels even though only
three are visible. Now, probability theory can certainly provide a
conclusion that the fourth wheel exists with high probability, yet, for
most people, the possibility of the car’s not having four wheels
*does not arise unless some new evidence presents itself*.
Thus, it seems that the four-wheel conclusion is reached *by
default*, in the absence of any reason to doubt it. If new
evidence arrives—for example, if one sees the owner carrying a wheel and
notices that the car is jacked up—then the conclusion can be retracted.
This kind of reasoning is said to exhibit , because the set of beliefs
does not grow monotonically over time as new evidence arrives. have been
devised with modified notions of truth and entailment in order to
capture such behavior. We will look at two such logics that have been
studied extensively: circumscription and default logic.

can be seen as a more powerful and precise version of the closed-world
assumption. The idea is to specify particular predicates that are
assumed to be “as false as possible”—that is, false for every object
except those for which they are known to be true. For example, suppose
we want to assert the default rule that birds fly. We would introduce a
predicate, say ${Abnormal}{}_1(x)$, and write
$${Bird}(x) \land \lnot {Abnormal}{}_1(x) \implies {Flies}(x)\ .$$
If we say that ${Abnormal}{}_1$ is to be , a circumscriptive reasoner
is entitled to assume $\lnot {Abnormal}{}_1(x)$ unless
${Abnormal}{}_1(x)$ is known to be true. This allows the conclusion
${Flies}({Tweety})$ to be drawn from the premise
${Bird}({Tweety})$, but the conclusion no longer holds if
${Abnormal}{}_1({Tweety})$ is asserted.

Circumscription can be viewed as an example of a logic. In such logics,
a sentence is entailed (with default status) if it is true in all
*preferred* models of the KB, as opposed to the requirement
of truth in *all* models in classical logic. For
circumscription, one model is preferred to another if it has fewer
abnormal objects.[^9] Let us see how this idea works in the context of
multiple inheritance in semantic networks. The standard example for
which multiple inheritance is problematic is called the “Nixon diamond.”
It arises from the observation that Richard Nixon was both a Quaker (and
hence by default a pacifist) and a Republican (and hence by default not
a pacifist). We can write this as follows:

() ()  .\
(x) ~2~(x) (x) .\
(x) ~3~(x) (x) .

If we circumscribe ${Abnormal}{}_2$ and ${Abnormal}{}_3$, there are
two preferred models: one in which ${Abnormal}{}_2({Nixon})$ and
${Pacifist}({Nixon})$ hold and one in which
${Abnormal}{}_3({Nixon})$ and $\lnot {Pacifist}({Nixon})$ hold.
Thus, the circumscriptive reasoner remains properly agnostic as to
whether Nixon was a pacifist. If we wish, in addition, to assert that
religious beliefs take precedence over political beliefs, we can use a
formalism called to give preference to models where ${Abnormal}{}_3$
is minimized.

is a formalism in which can be written to generate contingent,
nonmonotonic conclusions. A default rule looks like this:
$${Bird}(x) : {Flies}(x) / {Flies}(x)\ .$$ This rule means that if
${Bird}(x)$ is true, and if ${Flies}(x)$ is consistent with the
knowledge base, then ${Flies}(x)$ may be concluded by default. In
general, a default rule has the form $$P : J_1, \ldots,  J_n / C$$ where
$P$ is called the prerequisite, $C$ is the conclusion, and $J_i$ are the
justifications—if any one of them can be proven false, then the
conclusion cannot be drawn. Any variable that appears in $J_i$ or $C$
must also appear in $P$. The Nixon-diamond example can be represented in
default logic with one fact and two default rules:

() () .\
(x) : (x) / (x) .\
(x) : (x) / (x) .

To interpret what the default rules mean, we define the notion of an of
a default theory to be a maximal set of consequences of the theory. That
is, an extension $S$ consists of the original known facts and a set of
conclusions from the default rules, such that no additional conclusions
can be drawn from $S$ and the justifications of every default conclusion
in $S$ are consistent with $S$. As in the case of the preferred models
in circumscription, we have two possible extensions for the Nixon
diamond: one wherein he is a pacifist and one wherein he is not.
Prioritized schemes exist in which some default rules can be given
precedence over others, allowing some ambiguities to be resolved.

Since 1980, when nonmonotonic logics were first proposed, a great deal
of progress has been made in understanding their mathematical
properties. There are still unresolved questions, however. For example,
if “Cars have four wheels” is false, what does it mean to have it in
one’s knowledge base? What is a good set of default rules to have? If we
cannot decide, for each rule separately, whether it belongs in our
knowledge base, then we have a serious problem of nonmodularity.
Finally, how can beliefs that have default status be used to make
decisions? This is probably the hardest issue for default reasoning.
Decisions often involve tradeoffs, and one therefore needs to compare
the *strengths* of belief in the outcomes of different
actions, and the *costs* of making a wrong decision. In
cases where the same kinds of decisions are being made repeatedly, it is
possible to interpret default rules as “threshold probability”
statements. For example, the default rule “My brakes are always OK”
really means “The probability that my brakes are OK, given no other
information, is sufficiently high that the optimal decision is for me to
drive without checking them.” When the decision context changes—for
example, when one is driving a heavily laden truck down a steep mountain
road—the default rule suddenly becomes inappropriate, even though there
is no new evidence of faulty brakes. These considerations have led some
researchers to consider how to embed default reasoning within
probability theory or utility theory.

### Truth maintenance systems {#tms-section}

We have seen that many of the inferences drawn by a knowledge
representation system will have only default status, rather than being
absolutely certain. Inevitably, some of these inferred facts will turn
out to be wrong and will have to be retracted in the face of new
information. This process is called .[^10] Suppose that a knowledge base
${KB}$ contains a sentence $P$—perhaps a default conclusion recorded
by a forward-chaining algorithm, or perhaps just an incorrect
assertion—and we want to execute (${KB}$, $\lnot P$). To avoid
creating a contradiction, we must first execute (${KB}$, $P$). This
sounds easy enough. Problems arise, however, if any
*additional* sentences were inferred from $P$ and asserted
in the KB. For example, the implication $P\,{\implies}\, Q$ might have
been used to add $Q$. The obvious “solution”—retracting all sentences
inferred from $P$—fails because such sentences may have other
justifications besides $P$. For example, if $R$ and $R\,{\implies}\, Q$
are also in the KB, then $Q$ does not have to be removed after all. , or
TMSs, are designed to handle exactly these kinds of complications.

One simple approach to truth maintenance is to keep track of the order
in which sentences are told to the knowledge base by numbering them from
$P_1$ to $P_n$. When the call (${KB}$, $P_i$) is made, the system
reverts to the state just before $P_i$ was added, thereby removing both
$P_i$ and any inferences that were derived from $P_i$. The sentences
$P_{i+1}$ through $P_n$ can then be added again. This is simple, and it
guarantees that the knowledge base will be consistent, but retracting
$P_i$ requires retracting and reasserting $n-i$ sentences as well as
undoing and redoing all the inferences drawn from those sentences. For
systems to which many facts are being added—such as large commercial
databases—this is impractical.

A more efficient approach is the justification-based truth maintenance
system, or . In a JTMS, each sentence in the knowledge base is annotated
with a consisting of the set of sentences from which it was inferred.
For example, if the knowledge base already contains $P\,{\implies}\, Q$,
then $(P)$ will cause $Q$ to be added with the justification
$\{P,\; P\,{\implies}\, Q\}$. In general, a sentence can have any number
of justifications. Justifications make retraction efficient. Given the
call $(P)$, the JTMS will delete exactly those sentences for which $P$
is a member of every justification. So, if a sentence $Q$ had the single
justification $\{P,\; P\,{\implies}\, Q\}$, it would be removed; if it
had the additional justification $\{P, \; P \lor R \implies Q\}$, it
would still be removed; but if it also had the justification
$\{R, \; P \lor R
\implies Q\}$, then it would be spared. In this way, the time required
for retraction of $P$ depends only on the number of sentences derived
from $P$ rather than on the number of other sentences added since $P$
entered the knowledge base.

The JTMS assumes that sentences that are considered once will probably
be considered again, so rather than deleting a sentence from the
knowledge base entirely when it loses all justifications, we merely mark
the sentence as being *out* of the knowledge base. If a
subsequent assertion restores one of the justifications, then we mark
the sentence as being back *in*. In this way, the JTMS
retains all the inference chains that it uses and need not rederive
sentences when a justification becomes valid again.

In addition to handling the retraction of incorrect information, TMSs
can be used to speed up the analysis of multiple hypothetical
situations. Suppose, for example, that the Romanian Olympic Committee is
choosing sites for the swimming, athletics, and equestrian events at the
2048 Games to be held in Romania. For example, let the first hypothesis
be ${Site}({Swimming},{Pitesti})$,
${Site}({Athletics},{Bucharest})$, and
${Site}({Equestrian},{Arad})$. A great deal of reasoning must then
be done to work out the logistical consequences and hence the
desirability of this selection. If we want to consider
${Site}({Athletics},{Sibiu})$ instead, the TMS avoids the need to
start again from scratch. Instead, we simply retract
${Site}({Athletics},{Bucharest})$ and assert
${Site}({Athletics},{Sibiu})$ and the TMS takes care of the
necessary revisions. Inference chains generated from the choice of
Bucharest can be reused with Sibiu, provided that the conclusions are
the same.

An assumption-based truth maintenance system, or , makes this type of
context-switching between hypothetical worlds particularly efficient. In
a JTMS, the maintenance of justifications allows you to move quickly
from one state to another by making a few retractions and assertions,
but at any time only one state is represented. An ATMS represents
*all* the states that have ever been considered at the same
time. Whereas a JTMS simply labels each sentence as being
*in* or *out*, an ATMS keeps track, for each
sentence, of which assumptions would cause the sentence to be true. In
other words, each sentence has a label that consists of a set of
assumption sets. The sentence holds just in those cases in which all the
assumptions in one of the assumption sets hold.

Truth maintenance systems also provide a mechanism for generating .
Technically, an explanation of a sentence $P$ is a set of sentences $E$
such that $E$ entails $P$. If the sentences in $E$ are already known to
be true, then $E$ simply provides a sufficient basis for proving that
$P$ must be the case. But explanations can also include —sentences that
are not known to be true, but would suffice to prove $P$ if they were
true. For example, one might not have enough information to prove that
one’s car won’t start, but a reasonable explanation might include the
assumption that the battery is dead. This, combined with knowledge of
how cars operate, explains the observed nonbehavior. In most cases, we
will prefer an explanation $E$ that is minimal, meaning that there is no
proper subset of $E$ that is also an explanation. An ATMS can generate
explanations for the “car won’t start” problem by making assumptions
(such as “gas in car” or “battery dead”) in any order we like, even if
some assumptions are contradictory. Then we look at the label for the
sentence “car won’t start” to read off the sets of assumptions that
would justify the sentence.

The exact algorithms used to implement truth maintenance systems are a
little complicated, and we do not cover them here. The computational
complexity of the truth maintenance problem is at least as great as that
of propositional inference—that is, NP-hard. Therefore, you should not
expect truth maintenance to be a panacea. When used carefully, however,
a TMS can provide a substantial increase in the ability of a logical
system to handle complex environments and hypotheses.

The Internet Shopping World {#shopping-section}
---------------------------

In this final section we put together all we have learned to encode
knowledge for a shopping research agent that helps a buyer find product
offers on the Internet. The shopping agent is given a product
description by the buyer and has the task of producing a list of Web
pages that offer such a product for sale, and ranking which offers are
best. In some cases the buyer’s product description will be precise, as
in *Canon Rebel XTi digital camera*, and the task is then
to find the store(s) with the best offer. In other cases the description
will be only partially specified, as in *digital camera for under
300*, and the agent will have to compare different products.

[html-figure]

The shopping agent’s environment is the entire in its full
complexity—not a toy simulated environment. The agent’s percepts are Web
pages, but whereas a human Web user would see pages displayed as an
array of pixels on a screen, the shopping agent will perceive a page as
a character string consisting of ordinary words interspersed with
formatting commands in the HTML markup language. shows a Web page and a
corresponding HTML character string. The perception problem for the
shopping agent involves extracting useful information from percepts of
this kind.

Clearly, perception on Web pages is easier than, say, perception while
driving a taxi in Cairo. Nonetheless, there are complications to the
Internet perception task. The Web page in is simple compared to real
shopping sites, which may include CSS, cookies, Java, Javascript, Flash,
robot exclusion protocols, malformed HTML, sound files, movies, and text
that appears only as part of a JPEG image. An agent that can deal with
*all* of the Internet is almost as complex as a robot that
can move in the real world. We concentrate on a simple agent that
ignores most of these complications.

The agent’s first task is to collect product offers that are relevant to
a query. If the query is “laptops,” then a Web page with a review of the
latest high-end laptop would be relevant, but if it doesn’t provide a
way to buy, it isn’t an offer. For now, we can say a page is an offer if
it contains the words “buy” or “price” or “add to cart” within an HTML
link or form on the page. For example, if the page contains a string of
the form “`<a` …`add to cart` …`</a`” then it is an offer. This could be
represented in first-order logic, but it is more straightforward to
encode it into program code. We show how to do more sophisticated
information extraction in .

### Following links

The strategy is to start at the home page of an online store and
consider all pages that can be reached by following relevant links.[^11]
The agent will have knowledge of a number of stores, for example:

(, ) .\
 (, ) .\
 (, ) .

These stores classify their goods into product categories, and provide
links to the major categories from their home page. Minor categories can
be reached through a chain of relevant links, and eventually we will
reach offers. In other words, a page is relevant to the query if it can
be reached by a chain of zero or more relevant category links from a
store’s home page, and then from one more link to the product offer. We
can define relevance:

(, )\
 (, )\
 (, ~2~, ) (~2~, )\
 = ()  .

Here the predicate ${Link}({from}, {to})$ means that there is a
hyperlink from the URL to the URL. To define what counts as a , we need
to follow not just any old hyperlinks, but only those links whose
associated anchor text indicates that the link is relevant to the
product query. For this, we use
${LinkText}({from}, {to}, {text})$ to mean that there is a link
between and with as the anchor text. A chain of links between two URLs,
and , is relevant to a description $d$ if the anchor text of each link
is a relevant category name for $d$. The existence of the chain itself
is determined by a recursive definition, with the empty chain
(${start}\eq {end}$) as the base case:

(, , ) ( = )\
 ( (, u, ) (, )\
 (u, , )) .

[shopping-categories-figure]

Now we must define what it means for to be a for . First, we need to
relate strings to the categories they name. This is done using the
predicate ${Name}(s,c)$, which says that string $s$ is a name for
category $c$—for example, we might assert that
${Name}(\qt{laptops},{LaptopComputers})$. Some more examples of the
predicate appear in (b). Next, we define relevance. Suppose that is
“laptops.” Then ${RelevantCategoryName}({query}, {text})$ is true
when one of the following holds:

-   The and name the same category—e.g., “notebooks” and “laptops.”

-   The names a supercategory such as “computers.”

-   The names a subcategory such as “ultralight notebooks.”

The logical definition of ${RelevantCategoryName}$ is as follows:

$$\begin{aligned}
\lefteqn{{RelevantCategoryName}({query}, {text}) \lequiv {}} \nonumber\\
&&  \Exi{c_1,c_2} {Name}({query}, c_1) \land {Name}({text}, c_2) \land (c_1 \subseteq c_2 \lor c_2 \subseteq c_1)\ .
\label{relevant-category-equation}\end{aligned}$$

Otherwise, the anchor text is irrelevant because it names a category
outside this line, such as “clothes” or “lawn & garden.”

To follow relevant links, then, it is essential to have a rich hierarchy
of product categories. The top part of this hierarchy might look like
(a). It will not be feasible to list *all* possible
shopping categories, because a buyer could always come up with some new
desire and manufacturers will always come out with new products to
satisfy them (electric kneecap warmers?). Nonetheless, an ontology of
about a thousand categories will serve as a very useful tool for most
buyers.

In addition to the product hierarchy itself, we also need to have a rich
vocabulary of names for categories. Life would be much easier if there
were a one-to-one correspondence between categories and the character
strings that name them. We have already seen the problem of —two names
for the same category, such as “laptop computers” and “laptops.” There
is also the problem of —one name for two or more different categories.
For example, if we add the sentence
$${Name}(\qt{CDs}, {CertificatesOfDeposit})$$ to the knowledge base
in (b), then “CDs” will name two different categories.

Synonymy and ambiguity can cause a significant increase in the number of
paths that the agent has to follow, and can sometimes make it difficult
to determine whether a given page is indeed relevant. A much more
serious problem is the very broad range of descriptions that a user can
type and category names that a store can use. For example, the link
might say “laptop” when the knowledge base has only “laptops” or the
user might ask for “a computer I can fit on the tray table of an
economy-class airline seat.” It is impossible to enumerate in advance
all the ways a category can be named, so the agent will have to be able
to do additional reasoning in some cases to determine if the relation
holds. In the worst case, this requires full natural language
understanding, a topic that we will defer to . In practice, a few simple
rules—such as allowing “laptop” to match a category named “laptops”—go a
long way. asks you to develop a set of such rules after doing some
research into online stores.

Given the logical definitions from the preceding paragraphs and suitable
knowledge bases of product categories and naming conventions, are we
ready to apply an inference algorithm to obtain a set of relevant offers
for our query? Not quite! The missing element is the
${Contents}({url})$ function, which refers to the HTML page at a
given URL. The agent doesn’t have the page contents of every URL in its
knowledge base; nor does it have explicit rules for deducing what those
contents might be. Instead, we can arrange for the right HTTP procedure
to be executed whenever a subgoal involves the function. In this way, it
appears to the inference engine as if the entire Web is inside the
knowledge base. This is an example of a general technique called ,
whereby particular predicates and functions can be handled by
special-purpose methods.

### Comparing offers

Let us assume that the reasoning processes of the preceding section have
produced a set of offer pages for our “laptops” query. To compare those
offers, the agent must extract the relevant information—price, speed,
disk size, weight, and so on—from the offer pages. This can be a
difficult task with real Web pages, for all the reasons mentioned
previously. A common way of dealing with this problem is to use programs
called to extract information from a page. The technology of information
extraction is discussed in . For now we assume that wrappers exist, and
when given a page and a knowledge base, they add assertions to the
knowledge base. Typically, a hierarchy of wrappers would be applied to a
page: a very general one to extract dates and prices, a more specific
one to extract attributes for computer-related products, and if
necessary a site-specific one that knows the format of a particular
store. Given a page on the example.com site with the text
$$\mbox{{\tt IBM ThinkBook 970.  Our price: {\DollarSign}399.00}}$$
followed by various technical specifications, we would like a wrapper to
extract information such as the following:

c\
 (c, ) (c, )\
 (c, (14)) (c, )\
 (c, (2)) (c, (1.2))\
 (, c) (, )\
 (, )\
 (, (399)) (, ) .

This example illustrates several issues that arise when we take
seriously the task of knowledge engineering for commercial transactions.
For example, notice that the price is an attribute of the
*offer*, not the product itself. This is important because
the offer at a given store may change from day to day even for the same
individual laptop; for some categories—such as houses and paintings—the
same individual object may even be offered simultaneously by different
intermediaries at different prices. There are still more complications
that we have not handled, such as the possibility that the price depends
on the method of payment and on the buyer’s qualifications for certain
discounts. The final task is to compare the offers that have been
extracted. For example, consider these three offers:

A:  .\
B:  .\
C:  .

$C$ is by $A$; that is, $A$ is cheaper and faster, and they are
otherwise the same. In general, $X$ dominates $Y$ if $X$ has a better
value on at least one attribute, and is not worse on any attribute. But
neither $A$ nor $B$ dominates the other. To decide which is better we
need to know how the buyer weighs CPU speed and price against memory and
disk space. The general topic of preferences among multiple attributes
is addressed in ; for now, our shopping agent will simply return a list
of all undominated offers that meet the buyer’s description. In this
example, both $A$ and $B$ are undominated. Notice that this outcome
relies on the assumption that everyone prefers cheaper prices, faster
processors, and more storage. Some attributes, such as screen size on a
notebook, depend on the user’s particular preference (portability versus
visibility); for these, the shopping agent will just have to ask the
user.

The shopping agent we have described here is a simple one; many
refinements are possible. Still, it has enough capability that with the
right domain-specific knowledge it can actually be of use to a shopper.
Because of its declarative construction, it extends easily to more
complex applications. The main point of this section is to show that
some knowledge representation—in particular, the product hierarchy—is
necessary for such an agent, and that once we have some knowledge in
this form, the rest follows naturally.

By delving into the details of how one represents a variety of
knowledge, we hope we have given the reader a sense of how real
knowledge bases are constructed and a feeling for the interesting
philosophical issues that arise. The major points are as follows:

-   Large-scale knowledge representation requires a general-purpose
    ontology to organize and tie together the various specific domains
    of knowledge.

-   A general-purpose ontology needs to cover a wide variety of
    knowledge and should be capable, in principle, of handling any
    domain.

-   Building a large, general-purpose ontology is a significant
    challenge that has yet to be fully realized, although current
    frameworks seem to be quite robust.

-   We presented an based on categories and the event calculus. We
    covered categories, subcategories, parts, structured objects,
    measurements, substances, events, time and space, change, and
    beliefs.

-   Natural kinds cannot be defined completely in logic, but properties
    of natural kinds can be represented.

-   Actions, events, and time can be represented either in situation
    calculus or in more expressive representations such as event
    calculus. Such representations enable an agent to construct plans by
    logical inference.

-   We presented a detailed analysis of the Internet shopping domain,
    exercising the general ontology and showing how the domain knowledge
    can be used by a shopping agent.

-   Special-purpose representation systems, such as and , have been
    devised to help in organizing a hierarchy of categories. is an
    important form of inference, allowing the properties of objects to
    be deduced from their membership in categories.

-   The , as implemented in logic programs, provides a simple way to
    avoid having to specify lots of negative information. It is best
    interpreted as a that can be overridden by additional information.

-   , such as and , are intended to capture default reasoning in
    general.

-   handle knowledge updates and revisions efficiently.

claims that formal knowledge representation research began with
classical Indian theorizing about the grammar of Shastric Sanskrit,
which dates back to the first millennium b.c. In the West,
the use of definitions of terms in ancient Greek mathematics can be
regarded as the earliest instance: Aristotle’s
*Metaphysics* (literally, what comes after the book on
physics) is a near-synonym for *Ontology*. Indeed, the
development of technical terminology in any field can be regarded as a
form of knowledge representation.

Early discussions of representation in AI tended to focus on
“*problem* representation” rather than
“*knowledge* representation.” (See, for example,
Amarel’s [-@Amarel:1968] discussion of the Missionaries and Cannibals
problem.) In the 1970s, AI emphasized the development of “expert
systems” (also called “knowledge-based systems”) that could, if given
the appropriate domain knowledge, match or exceed the performance of
human experts on narrowly defined tasks. For example, the first expert
system, @Feigenbaum+al:1971 [@Lindsay+al:1980], interpreted the output
of a mass spectrometer (a type of instrument used to analyze the
structure of organic chemical compounds) as accurately as expert
chemists. Although the success of was instrumental in convincing the
AI research community of the importance of knowledge representation, the
representational formalisms used in are highly specific to the domain of
chemistry. Over time, researchers became interested in standardized
knowledge representation formalisms and ontologies that could streamline
the process of creating new expert systems. In so doing, they ventured
into territory previously explored by philosophers of science and of
language. The discipline imposed in AI by the need for one’s theories to
“work” has led to more rapid and deeper progress than was the case when
these problems were the exclusive domain of philosophy (although it has
at times also led to the repeated reinvention of the wheel).

The creation of comprehensive taxonomies or classifications dates back
to ancient times. Aristotle (384–322 b.c.) strongly
emphasized classification and categorization schemes. His
*Organon*, a collection of works on logic assembled by his
students after his death, included a treatise called
*Categories* in which he attempted to construct what we
would now call an upper ontology. He also introduced the notions of and
for lower-level classification. Our present system of biological
classification, including the use of “binomial nomenclature”
(classification via genus and species in the technical sense), was
invented by the Swedish biologist Carolus Linnaeus, or Carl von Linne
(1707–1778). The problems associated with natural kinds and inexact
category boundaries have been addressed by , , , and , among others.

Interest in larger-scale ontologies is increasing, as documented by the
*Handbook on Ontologies* @Staab:2004. The
project @Lenat+Guha:1990 [@Matuszek+al:2006] has released a
150,000-concept ontology, with an upper ontology similar to the one in
as well as specific concepts like “OLED Display” and “iPhone,” which is
a type of “cellular phone,” which in turn is a type of “consumer
electronics,” “phone,” “wireless communication device,” and other
concepts. The project extracts structured data from Wikipedia;
specifically from Infoboxes: the boxes of attribute/value pairs that
accompany many Wikipedia articles @Wu+Weld:2008 [@Bizer+al:2007]. As of
mid-2009, contains 2.6 million concepts, with about 100 facts per
concept. The working group P1600.1 created the Suggested Upper Merged
Ontology (SUMO) @Niles+Pease:2001 [@Pease+Niles:2002], which contains
about 1000 terms in the upper ontology and links to over 20,000
domain-specific terms. describe algorithms for efficiently managing a
very large ontology. A survey of techniques for extracting knowledge
from Web pages is given by .

On the Web, representation languages are emerging. @Brickley+Guha:2004
allows for assertions to be made in the form of relational triples, and
provides some means for evolving the meaning of names over time.
@Smith+al:2004 is a description logic that supports inferences over
these triples. So far, usage seems to be inversely proportional to
representational complexity: the traditional HTML and CSS formats
account for over 99% of Web content, followed by the simplest
representation schemes, such as microformats @Khare:2006 and RDFa
@Adida+Birbeck:2008, which use HTML and XHTML markup to add attributes
to literal text. Usage of sophisticated RDF and OWL ontologies is not
yet widespread, and the full vision of the  @Berners-Lee+al:2001 has not
yet been realized. The conferences on *Formal Ontology in
Information Systems* (FOIS) contain many interesting papers on
both general and domain-specific ontologies.

The taxonomy used in this chapter was developed by the authors and is
based in part on their experience in the project and in part on work by
Hwang and Schubert [-@Hwang+Schubert:1993] and
Davis [-@Davis:1990; -@Davis:2005]. An inspirational discussion of the
general project of commonsense knowledge representation appears in
Hayes’s [-@Hayes:1978; -@Hayes:1985] “Naive Physics Manifesto.”

Successful deep ontologies within a specific field include the Gene
Ontology project @Consortium:2008 and CML, the Chemical Markup Language
@Murray-Rust+al:2003.

Doubts about the feasibility of a single ontology for *all*
knowledge are expressed by , , , and , who states, “the initial project
of building one single ontology …has …largely been abandoned.”

The event calculus was introduced by to handle continuous time, and
there have been several variations @Sadri+Kowalski:1995 [@Shanahan:1997]
and overviews @Shanahan:1999b [@Mueller:2006]. show how the logic of
events maps onto the language we use to talk about events. An
alternative to the event and situation calculi is the fluent calculus
@Thielscher:1999. James Allen introduced time intervals for the same
reason @Allen:1984, arguing that intervals were much more natural than
situations for reasoning about extended and concurrent events. Peter
Ladkin [-@Ladkin:1986; -@Ladkin:1986a] introduced “concave” time
intervals (intervals with gaps; essentially, unions of ordinary “convex”
time intervals) and applied the techniques of mathematical abstract
algebra to time representation. Allen [-@Allen:1991] systematically
investigates the wide variety of techniques available for time
representation; analyze algorithms for temporal reasoning. There are
significant commonalities between the event-based ontology given in this
chapter and an analysis of events due to the philosopher Donald
Davidson [-@Davidson:1980]. The in Pat Hayes’s [-@Hayes:1985a] ontology
of liquids and the in McDermott’s [-@McDermott:1985] theory of plans
were also important influences on the field and this chapter.

The question of the ontological status of substances has a long history.
proposed that substances were abstract entities entirely distinct from
physical objects; he would say ${MadeOf}({Butter}{}_3,{Butter})$
rather than ${Butter}{}_3 \elt
{Butter}$. This leads to a substance hierarchy in which, for example,
${UnsaltedButter}$ is a more specific substance than ${Butter}$. The
position adopted in this chapter, in which substances are categories of
objects, was championed by Richard Montague [-@Montague:1973]. It has
also been adopted in the project. Copeland [-@Copeland:1993] mounts a
serious, but not invincible, attack. The alternative approach mentioned
in the chapter, in which butter is one object consisting of all buttery
objects in the universe, was proposed originally by the Polish logician
Leśniewski [-@Lesniewski:1916]. His (the name is derived from the Greek
word for “part”) used the part–whole relation as a substitute for
mathematical set theory, with the aim of eliminating abstract entities
such as sets. A more readable exposition of these ideas is given by ,
and Goodman’s *The Structure of Appearance*
[-@Goodman:1977] applies the ideas to various problems in knowledge
representation. While some aspects of the mereological approach are
awkward—for example, the need for a separate inheritance mechanism based
on part–whole relations—the approach gained the support of
Quine [-@Quine:1960]. Harry Bunt [-@Bunt:1985] has provided an extensive
analysis of its use in knowledge representation. cover parts, wholes,
and the spatial locations.

[knowledge-belief-history] Mental objects have been the subject of
intensive study in philosophy and AI. There are three main approaches.
The one taken in this chapter, based on modal logic and possible worlds,
is the classical approach from philosophy @Hintikka:1962
[@Kripke:1963; @Hughes+Cresswell:1996]. The book *Reasoning about
Knowledge* @Fagin+al:1995 provides a thorough introduction. The
second approach is a first-order theory in which mental objects are
fluents. and describe this approach. It relies on the possible-worlds
formalism, and builds on work by Robert . The third approach is a , in
which mental objects are represented by character strings. A string is
just a complex term denoting a list of symbols, so
${CanFly}({Clark})$ can be represented by the list of symbols
$[C,a,n,F,l,y,(,C,l,a,r,k,)]$. The syntactic theory of mental objects
was first studied in depth by Kaplan and
Montague [-@Kaplan+Montague:1960], who showed that it led to paradoxes
if not handled carefully. Ernie provides an excellent comparison of the
syntactic and modal theories of knowledge.

The Greek philosopher Porphyry (c. 234–305 a.d.),
commenting on ’s *Categories*, drew what might qualify as
the first semantic network. Charles S. Peirce [-@Peirce:1909] developed
existential graphs as the first semantic network formalism using modern
logic. Ross Quillian [-@Quillian:1961], driven by an interest in human
memory and language processing, initiated work on semantic networks
within AI. An influential paper by Marvin Minsky [-@Minsky:1975]
presented a version of semantic networks called ; a frame was a
representation of an object or category, with attributes and relations
to other objects or categories. The question of semantics arose quite
acutely with respect to Quillian’s semantic networks (and those of
others who followed his approach), with their ubiquitous and very vague
“IS-A links” Woods’s [-@Woods:1975] famous article “What’s In a Link?”
drew the attention of AI researchers to the need for precise semantics
in knowledge representation formalisms. Brachman [-@Brachman:1979]
elaborated on this point and proposed solutions. Patrick
Hayes’s [-@Hayes:1979] “The Logic of Frames” cut even deeper, claiming
that “Most of ‘frames’ is just a new syntax for parts of first-order
logic.” Drew McDermott’s [-@McDermott:1978] “Tarskian Semantics, or, No
Notation without Denotation!” argued that the model-theoretic approach
to semantics used in first-order logic should be applied to all
knowledge representation formalisms. This remains a controversial idea;
notably, McDermott himself has reversed his position in “A Critique of
Pure Reason” @McDermott:1987. Selman and
Levesque [-@Selman+Levesque:1993] discuss the complexity of inheritance
with exceptions, showing that in most formulations it is .

The development of description logics is the most recent stage in a long
line of research aimed at finding useful subsets of first-order logic
for which inference is computationally tractable. Hector Levesque and
Ron Brachman [-@Levesque+Brachman:1987] showed that certain logical
constructs—notably, certain uses of disjunction and negation—were
primarily responsible for the intractability of logical inference.
Building on the system @Schmolze+Lipkis:1983, several researchers
developed systems that incorporate theoretical complexity analysis, most
notably  @Brachman+al:1983 and Classic @Borgida+al:1989. The result has
been a marked increase in the speed of inference and a much better
understanding of the interaction between complexity and expressiveness
in reasoning systems. summarize the state of the art, and present a
comprehensive handbook of description logic. Against this trend, Doyle
and Patil [-@Doyle+Patil:1991] have argued that restricting the
expressiveness of a language either makes it impossible to solve certain
problems or encourages the user to circumvent the language restrictions
through nonlogical means.

The three main formalisms for dealing with nonmonotonic inference—
@McCarthy:1980, @Reiter:1980, and modal @McDermott+Doyle:1980—were all
introduced in one special issue of the AI Journal. discuss the merits of
the variants, given 25 years of hindsight. Answer set programming can be
seen as an extension of negation as failure or as a refinement of
circumscription; the underlying theory of stable model semantics was
introduced by , and the leading answer set programming systems are
@Eiter+al:1998 and @Niemela+al:2000. The disk drive example comes from
the user manual @Syrjanen:2000. discusses the use of answer set
programming for planning. give a good overview of the various approaches
to nonmonotonic logic. covers the negation-as-failure approach to logic
programming and . show that every Prolog program without negation has a
unique minimal model. Recent years have seen renewed interest in
applications of nonmonotonic logics to large-scale knowledge
representation systems. The systems for handling insurance-benefit
inquiries was perhaps the first commercially successful application of a
nonmonotonic inheritance system @Morgenstern:1998. discusses the
application of answer set programming to planning. A variety of
nonmonotonic reasoning systems based on logic programming are documented
in the proceedings of the conferences on *Logic Programming and
Nonmonotonic Reasoning* (LPNMR).

The study of truth maintenance systems began with the TMS @Doyle:1979
and RUP @McAllester:1980 systems, both of which were essentially JTMSs.
explain in depth how TMSs can be used in AI applications. show how an
efficient incremental TMS called an ITMS makes it feasible to plan the
operations of a spacecraft in real time.

This chapter could not cover *every* area of knowledge
representation in depth. The three principal topics omitted are the
following:

: Qualitative physics is a subfield of knowledge representation
concerned specifically with constructing a logical, nonnumeric theory of
physical objects and processes. The term was coined by Johan de
Kleer [-@DeKleer:1975], although the enterprise could be said to have
started in Fahlman’s [-@Fahlman:1974] , a sophisticated planner for
constructing complex towers of blocks. Fahlman discovered in the process
of designing it that most of the effort (80%, by his estimate) went into
modeling the physics of the blocks world to calculate the stability of
various subassemblies of blocks, rather than into planning per se. He
sketches a hypothetical naive-physics-like process to explain why young
children can solve -like problems without access to the high-speed
floating-point arithmetic used in ’s physical modeling.
Hayes [-@Hayes:1985a] uses “histories”—four-dimensional slices of
space-time similar to Davidson’s events—to construct a fairly complex
naive physics of liquids. Hayes was the first to prove that a bath with
the plug in will eventually overflow if the tap keeps running and that a
person who falls into a lake will get wet all over. gives an update to
the ontology of liquids that describes the pouring of liquids into
containers.

De Kleer and Brown [-@DeKleer+Brown:1985], Ken Forbus [-@Forbus:1985],
and Benjamin independently and almost simultaneously developed systems
that can reason about a physical system based on qualitative
abstractions of the underlying equations. Qualitative physics soon
developed to the point where it became possible to analyze an impressive
variety of complex physical systems @Yip:1991. Qualitative techniques
have been used to construct novel designs for clocks, windshield wipers,
and six-legged walkers @Subramanian+Wang:1994. The collection
*Readings in Qualitative Reasoning about Physical
Systems* @Weld+DeKleer:1990 an encyclopedia article by , and a
handbook article by introduce to the field.

: The reasoning necessary to navigate in the wumpus world and shopping
world is trivial in comparison to the rich spatial structure of the real
world. The earliest serious attempt to capture commonsense reasoning
about space appears in the work of Ernest
Davis [-@Davis:1986; -@Davis:1990]. The region connection calculus of
supports a form of qualitative spatial reasoning and has led to new
kinds of geographical information systems; see also @Davis:2006. As with
qualitative physics, an agent can go a long way, so to speak, without
resorting to a full metric representation. When such a representation is
necessary, techniques developed in robotics () can be used.

: Psychological reasoning involves the development of a working
*psychology* for artificial agents to use in reasoning
about themselves and other agents. This is often based on so-called folk
psychology, the theory that humans in general are believed to use in
reasoning about themselves and other humans. When AI researchers provide
their artificial agents with psychological theories for reasoning about
other agents, the theories are frequently based on the researchers’
description of the logical agents’ own design. Psychological reasoning
is currently most useful within the context of natural language
understanding, where divining the speaker’s intentions is of paramount
importance.

collects papers by leading researchers in knowledge representation,
summarizing 40 years of work in the field. The proceedings of the
international conferences on *Principles of Knowledge
Representation and Reasoning* provide the most up-to-date sources
for work in this area. *Readings in Knowledge
Representation* @Brachman+Levesque:1985 and *Formal
Theories of the Commonsense World* @Hobbs+Moore:1985 are
excellent anthologies on knowledge representation; the former focuses
more on historically important papers in representation languages and
formalisms, the latter on the accumulation of the knowledge itself. , ,
and provide textbook introductions to knowledge representation,
contributes a handbook, and a special issue of AI Journal covers recent
progress @Davis+Morgenstern:2004. The biennial conference on
*Theoretical Aspects of Reasoning About Knowledge* (TARK)
covers applications of the theory of knowledge in AI, economics, and
distributed systems.

Define an ontology in first-order logic for tic-tac-toe. The ontology
should contain situations, actions, squares, players, marks (X, O, or
blank), and the notion of winning, losing, or drawing a game. Also
define the notion of a forced win (or draw): a position from which a
player can force a win (or draw) with the right sequence of actions.
Write axioms for the domain. (Note: The axioms that enumerate the
different squares and that characterize the winning positions are rather
long. You need not write these out in full, but indicate clearly what
they look like.)

You are to create a system for advising computer science undergraduates
on what courses to take over an extended period in order to satisfy the
program requirements. (Use whatever requirements are appropriate for
your institution.) First, decide on a vocabulary for representing all
the information, and then represent it; then formulate a query to the
system that will return a legal program of study as a solution. You
should allow for some tailoring to individual students, in that your
system should ask what courses or equivalents the student has already
taken, and not generate programs that repeat those courses.

Suggest ways in which your system could be improved—for example to take
into account knowledge about student preferences, the workload, good and
bad instructors, and so on. For each kind of knowledge, explain how it
could be expressed logically. Could your system easily incorporate this
information to find all feasible programs of study for a student? Could
it find the *best* program?

shows the top levels of a hierarchy for everything. Extend it to include
as many real categories as possible. A good way to do this is to cover
all the things in your everyday life. This includes objects and events.
Start with waking up, and proceed in an orderly fashion noting
everything that you see, touch, do, and think about. For example, a
random sampling produces music, news, milk, walking, driving, gas, Soda
Hall, carpet, talking, Professor Fateman, chicken curry, tongue, 7, sun,
the daily newspaper, and so on.

You should produce both a single hierarchy chart (on a large sheet of
paper) and a listing of objects and categories with the relations
satisfied by members of each category. Every object should be in a
category, and every category should be in the hierarchy.

[windows-exercise] Develop a representational system for reasoning about
windows in a window-based computer interface. In particular, your
representation should be able to describe:

-   The state of a window: minimized, displayed, or nonexistent.

-   Which window (if any) is the active window.

-   The position of every window at a given time.

-   The order (front to back) of overlapping windows.

-   The actions of creating, destroying, resizing, and moving windows;
    changing the state of a window; and bringing a window to the front.
    Treat these actions as atomic; that is, do not deal with the issue
    of relating them to mouse actions. Give axioms describing the
    effects of actions on fluents. You may use either event or situation
    calculus.

Assume an ontology containing *situations,*
*actions,* *integers* (for $x$ and $y$
coordinates) and *windows*. Define a language over this
ontology; that is, a list of constants, function symbols, and predicates
with an English description of each. If you need to add more categories
to the ontology (e.g., pixels), you may do so, but be sure to specify
these in your write-up. You may (and should) use symbols defined in the
text, but be sure to list these explicitly.

State the following in the language you developed for the previous
exercise:

1.  In situation $S_0$, window $W_1$ is behind $W_2$ but sticks out on
    the left and right. Do *not* state exact coordinates
    for these; describe the *general* situation.

2.  If a window is displayed, then its top edge is higher than its
    bottom edge.

3.  After you create a window $w$, it is displayed.

4.  A window can be minimized if it is displayed.

State the following in the language you developed for the previous
exercise:

1.  In situation $S_0$, window $W_1$ is behind $W_2$ but sticks out on
    the top and bottom. Do *not* state exact coordinates
    for these; describe the *general* situation.

2.  If a window is displayed, then its top edge is higher than its
    bottom edge.

3.  After you create a window $w$, it is displayed.

4.  A window can be minimized only if it is displayed.

(Adapted from an example by Doug Lenat.) Your mission is to capture, in
logical form, enough knowledge to answer a series of questions about the
following simple scenario:

> Yesterday John went to the North Berkeley Safeway supermarket and
> bought two pounds of tomatoes and a pound of ground beef.

Start by trying to represent the content of the sentence as a series of
assertions. You should write sentences that have straightforward logical
structure (e.g., statements that objects have certain properties, that
objects are related in certain ways, that all objects satisfying one
property satisfy another). The following might help you get started:

-   Which classes, objects, and relations would you need? What are their
    parents, siblings and so on? (You will need events and temporal
    ordering, among other things.)

-   Where would they fit in a more general hierarchy?

-   What are the constraints and interrelationships among them?

-   How detailed must you be about each of the various concepts?

To answer the questions below, your knowledge base must include
background knowledge. You’ll have to deal with what kind of things are
at a supermarket, what is involved with purchasing the things one
selects, what the purchases will be used for, and so on. Try to make
your representation as general as possible. To give a trivial example:
don’t say “People buy food from Safeway,” because that won’t help you
with those who shop at another supermarket. Also, don’t turn the
questions into answers; for example, question (c) asks “Did John buy any
meat?”—not “Did John buy a pound of ground beef?”

Sketch the chains of reasoning that would answer the questions. If
possible, use a logical reasoning system to demonstrate the sufficiency
of your knowledge base. Many of the things you write might be only
approximately correct in reality, but don’t worry too much; the idea is
to extract the common sense that lets you answer these questions at all.
A truly complete answer to this question is *extremely*
difficult, probably beyond the state of the art of current knowledge
representation. But you should be able to put together a consistent set
of axioms for the limited questions posed here.

1.  Is John a child or an adult? [Adult]

2.  Does John now have at least two tomatoes? [Yes]

3.  Did John buy any meat? [Yes]

4.  If Mary was buying tomatoes at the same time as John, did he see
    her? [Yes]

5.  Are the tomatoes made in the supermarket? [No]

6.  What is John going to do with the tomatoes? [Eat them]

7.  Does Safeway sell deodorant? [Yes]

8.  Did John bring some money or a credit card to the supermarket? [Yes]

9.  Does John have less money after going to the supermarket? [Yes]

Make the necessary additions or changes to your knowledge base from the
previous exercise so that the questions that follow can be answered.
Include in your report a discussion of your changes, explaining why they
were needed, whether they were minor or major, and what kinds of
questions would necessitate further changes.

1.  Are there other people in Safeway while John is there? [Yes—staff!]

2.  Is John a vegetarian? [No]

3.  Who owns the deodorant in Safeway? [Safeway Corporation]

4.  Did John have an ounce of ground beef? [Yes]

5.  Does the Shell station next door have any gas? [Yes]

6.  Do the tomatoes fit in John’s car trunk? [Yes]

Represent the following seven sentences using and extending the
representations developed in the chapter:

1.  Water is a liquid between 0 and 100 degrees.

2.  Water boils at 100 degrees.

3.  The water in John’s water bottle is frozen.

4.  Perrier is a kind of water.

5.  John has Perrier in his water bottle.

6.  All liquids have a freezing point.

7.  A liter of water weighs more than a liter of alcohol.

[part-decomposition-exercise]Write definitions for the following:

1.  ${ExhaustivePartDecomposition}$

2.  ${PartPartition}$

3.  ${PartwiseDisjoint}$

These should be analogous to the definitions for
${ExhaustiveDecomposition}$, ${Partition}$, and ${Disjoint}$. Is
it the case that ${PartPartition}(s,{BunchOf}(s))$? If so, prove it;
if not, give a counterexample and define sufficient conditions under
which it does hold.

[alt-measure-exercise] An alternative scheme for representing measures
involves applying the units function to an abstract length object. In
such a scheme, one would write ${Inches}({Length}(L_1)) = {1.5}$.
How does this scheme compare with the one in the chapter? Issues include
conversion axioms, names for abstract quantities (such as “50 dollars”),
and comparisons of abstract measures in different units (50 inches is
more than 50 centimeters).

Write a set of sentences that allows one to calculate the price of an
individual tomato (or other object), given the price per pound. Extend
the theory to allow the price of a bag of tomatoes to be calculated.

[namematch-exercise] Add sentences to extend the definition of the
predicate ${Name}(s, c)$ so that a string such as “laptop computer”
matches the appropriate category names from a variety of stores. Try to
make your definition general. Test it by looking at ten online stores,
and at the category names they give for three different categories. For
example, for the category of laptops, we found the names “Notebooks,”
“Laptops,” “Notebook Computers,” “Notebook,” “Laptops and Notebooks,”
and “Notebook PCs.” Some of these can be covered by explicit ${Name}$
facts, while others could be covered by sentences for handling plurals,
conjunctions, etc.

Write event calculus axioms to describe the actions in the wumpus world.

State the interval-algebra relation that holds between every pair of the
following real-world events:

> $LK$: The life of President Kennedy.\
> $IK$: The infancy of President Kennedy.\
> $PK$: The presidency of President Kennedy.\
> $LJ$: The life of President Johnson.\
> $PJ$: The presidency of President Johnson.\
> $LO$: The life of President Obama.

This exercise concerns the problem of planning a route for a robot to
take from one city to another. The basic action taken by the robot is
${Go}(x,y)$, which takes it from city $x$ to city $y$ if there is a
route between those cities. ${Road}(x, y)$ is true if and only if
there is a road connecting cities $x$ and $y$; if there is, then
${Distance}(x, y)$ gives the length of the road. See the map on for an
example. The robot begins in Arad and must reach Bucharest.

1.  Write a suitable logical description of the initial situation of the
    robot.

2.  Write a suitable logical query whose solutions provide possible
    paths to the goal.

3.  Write a sentence describing the ${Go}$ action.

4.  Now suppose that the robot consumes fuel at the rate of .02 gallons
    per mile. The robot starts with 20 gallons of fuel. Augment your
    representation to include these considerations.

5.  Now suppose some of the cities have gas stations at which the robot
    can fill its tank. Extend your representation and write all the
    rules needed to describe gas stations, including the ${Fillup}$
    action.

ways to extend the event calculus to handle *simultaneous*
events. Is it possible to avoid a combinatorial explosion of axioms?

[exchange-rates-exercise]Construct a representation for exchange rates
between currencies that allows for daily fluctuations.

[fixed-definition-exercise]Define the predicate ${Fixed}$, where
${Fixed}({Location}(x))$ means that the location of object $x$ is
fixed over time.

Describe the event of trading something for something else. Describe
buying as a kind of trading in which one of the objects traded is a sum
of money.

The two preceding exercises assume a fairly primitive notion of
ownership. For example, the buyer starts by *owning* the
dollar bills. This picture begins to break down when, for example, one’s
money is in the bank, because there is no longer any specific collection
of dollar bills that one owns. The picture is complicated still further
by borrowing, leasing, renting, and bailment. Investigate the various
commonsense and legal concepts of ownership, and propose a scheme by
which they can be represented formally.

[card-on-forehead-exercise] (Adapted from .) Consider a game played with
a deck of just 8 cards, 4 aces and 4 kings. The three players, Alice,
Bob, and Carlos, are dealt two cards each. Without looking at them, they
place the cards on their foreheads so that the other players can see
them. Then the players take turns either announcing that they know what
cards are on their own forehead, thereby winning the game, or saying “I
don’t know.” Everyone knows the players are truthful and are perfect at
reasoning about beliefs.

1.  Game 1. Alice and Bob have both said “I don’t know.” Carlos sees
    that Alice has two aces (A-A) and Bob has two kings (K-K). What
    should Carlos say? (*Hint*: consider all three possible
    cases for Carlos: A-A, K-K, A-K.)

2.  Describe each step of Game 1 using the notation of modal logic.

3.  Game 2. Carlos, Alice, and Bob all said “I don’t know” on their
    first turn. Alice holds K-K and Bob holds A-K. What should Carlos
    say on his second turn?

4.  Game 3. Alice, Carlos, and Bob all say “I don’t know” on their first
    turn, as does Alice on her second turn. Alice and Bob both hold A-K.
    What should Carlos say?

5.  Prove that there will always be a winner to this game.

The assumption of *logical omniscience,* discussed on , is
of course not true of any actual reasoners. Rather, it is an
*idealization* of the reasoning process that may be more or
less acceptable depending on the applications. Discuss the
reasonableness of the assumption for each of the following applications
of reasoning about knowledge:

1.  Partial knowledge adversary games, such as card games. Here one
    player wants to reason about what his opponent knows about the state
    of the game.

2.  Chess with a clock. Here the player may wish to reason about the
    limits of his opponent’s or his own ability to find the best move in
    the time available. For instance, if player A has much more time
    left than player B, then A will sometimes make a move that greatly
    complicates the situation, in the hopes of gaining an advantage
    because he has more time to work out the proper strategy.

3.  A shopping agent in an environment in which there are costs of
    gathering information.

4.  Reasoning about public key cryptography, which rests on the
    intractability of certain computational problems.

The assumption of *logical omniscience,* discussed on , is
of course not true of any actual reasoners. Rather, it is an
*idealization* of the reasoning process that may be more or
less acceptable depending on the applications. Discuss the
reasonableness of the assumption for each of the following applications
of reasoning about knowledge:

1.  Chess with a clock. Here the player may wish to reason about the
    limits of his opponent’s or his own ability to find the best move in
    the time available. For instance, if player A has much more time
    left than player B, then A will sometimes make a move that greatly
    complicates the situation, in the hopes of gaining an advantage
    because he has more time to work out the proper strategy.

2.  A shopping agent in an environment in which there are costs of
    gathering information.

3.  An automated tutoring program for math, which reasons about what
    students understand.

4.  Reasoning about public key cryptography, which rests on the
    intractability of certain computational problems.

Translate the following description logic expression (from ) into
first-order logic, and comment on the result:

(,(3,), (2,),\
(,(,,(,))),\
(,(,(,,)))) .

Recall that inheritance information in semantic networks can be captured
logically by suitable implication sentences. This exercise investigates
the efficiency of using such sentences for inheritance.

1.  Consider the information in a used-car catalog such as Kelly’s Blue
    Book—for example, that 1973 Dodge vans are (or perhaps were once)
    worth 575. Suppose all this information (for 11,000 models) is
    encoded as logical sentences, as suggested in the chapter. Write
    down three such sentences, including that for 1973 Dodge vans. How
    would you use the sentences to find the value of a
    *particular* car, given a backward-chaining theorem
    prover such as Prolog?

2.  Compare the time efficiency of the backward-chaining method for
    solving this problem with the inheritance method used in semantic
    nets.

3.  Explain how forward chaining allows a logic-based system to solve
    the same problem efficiently, assuming that the KB contains only the
    11,000 sentences about prices.

4.  Describe a situation in which neither forward nor backward chaining
    on the sentences will allow the price query for an individual car to
    be handled efficiently.

5.  Can you suggest a solution enabling this type of query to be solved
    efficiently in all cases in logic systems? (*Hint:*
    Remember that two cars of the same year and model have the same
    price.)

[natural-stupidity-exercise]One might suppose that the syntactic
distinction between unboxed links and singly boxed links in semantic
networks is unnecessary, because singly boxed links are always attached
to categories; an inheritance algorithm could simply assume that an
unboxed link attached to a category is intended to apply to all members
of that category. Show that this argument is fallacious, giving examples
of errors that would arise.

One part of the shopping process that was not covered in this chapter is
checking for compatibility between items. For example, if a digital
camera is ordered, what accessory batteries, memory cards, and cases are
compatible with the camera? Write a knowledge base that can determine
the compatibility of a set of items and suggest replacements or
additional items if the shopper makes a choice that is not compatible.
The knowledge base should works with at least one line of products and
extend easily to other lines.

[shopping-grammar-exercise] A complete solution to the problem of
inexact matches to the buyer’s description in shopping is very difficult
and requires a full array of natural language processing and information
retrieval techniques. (See Chapters [nlp1-chapter]
and [nlp-english-chapter].) One small step is to allow the user to
specify minimum and maximum values for various attributes. The buyer
must use the following grammar for product descriptions:

& & \*\
 & &\
 & &\
 &&

Here, ${Category}$ names a product category, ${Attribute}$ is some
feature such as “CPU” or “price,” and ${Value}$ is the target value
for the attribute. So the query “computer with at least a 2.5 GHz CPU
for under 500” must be re-expressed as “computer with CPU $>$ 2.5 GHz
and price $<$ 500.” Implement a shopping agent that accepts descriptions
in this language.

[buying-exercise]Our description of Internet shopping omitted the
all-important step of actually *buying* the product.
Provide a formal logical description of buying, using event calculus.
That is, define the sequence of events that occurs when a buyer submits
a credit-card purchase and then eventually gets billed and receives the
product.

[^1]: Turning a proposition into an object is called , from the Latin
    word *res*, or thing. John McCarthy proposed the term
    “thingification,” but it never caught on.

[^2]: The famous biologist J. B. S. Haldane deduced “An inordinate
    fondness for beetles” on the part of the Creator.

[^3]: The terms “event” and “action” may be used interchangeably.
    Informally, “action” connotes an agent while “event” connotes the
    possibility of agentless actions.

[^4]: Some versions of event calculus do not distinguish event
    categories from instances of the categories.

[^5]: Several early systems failed to distinguish between properties of
    members of a category and properties of the category as a whole.
    This can lead directly to inconsistencies, as pointed out by Drew
    McDermott [-@McDermott:1976] in his article “Artificial Intelligence
    Meets Natural Stupidity.” Another common problem was the use of
    ${IsA}$ links for both subset and membership relations, in
    correspondence with English usage: “a cat is a mammal” and “Fifi is
    a cat.” See for more on these issues.

[^6]: Notice that the language does *not* allow one to
    simply state that one concept, or category, is a subset of another.
    This is a deliberate policy: subsumption between categories must be
    derivable from some aspects of the descriptions of the categories.
    If not, then something is missing from the descriptions.

[^7]: provides efficient subsumption testing in practice, but the
    worst-case run time is exponential.

[^8]: Recall that monotonicity requires all entailed sentences to remain
    entailed after new sentences are added to the KB. That is, if
    ${KB} \entails \alpha$ then ${KB} \land \beta \entails \alpha$.

[^9]: For the closed-world assumption, one model is preferred to another
    if it has fewer true atoms—that is, preferred models are models.
    There is a natural connection between the closed-world assumption
    and definite-clause KBs, because the fixed point reached by forward
    chaining on definite-clause KBs is the unique minimal model. See for
    more on this point.

[^10]: Belief revision is often contrasted with , which occurs when a
    knowledge base is revised to reflect a change in the world rather
    than new information about a fixed world. Belief update combines
    belief revision with reasoning about time and change; it is also
    related to the process of described in .

[^11]: An alternative to the link-following strategy is to use an
    Internet search engine; the technology behind , information
    retrieval, will be covered in .
[io-part]

Natural Language Processing {#nlp1-chapter}
===========================

is set apart from other species by the capacity for language. Somewhere
around 100,000 years ago, humans learned how to speak, and about 7,000
years ago learned to write. Although chimpanzees, dolphins, and other
animals have shown vocabularies of hundreds of signs, only humans can
reliably communicate an unbounded number of qualitatively different
messages on any topic using discrete signs.

Of course, there are other attributes that are uniquely human: no other
species wears clothes, creates representational art, or watches three
hours of television a day. But when Alan Turing proposed his Test (see
), he based it on language, not art or TV. There are two main reasons
why we want our computer agents to be able to process natural languages:
first, to communicate with humans, a topic we take up in , and second,
to acquire information from written language, the focus of this chapter.

There are over a trillion pages of information on the Web, almost all of
it in natural language. An agent that wants to do needs to understand
(at least partially) the ambiguous, messy languages that humans use. We
examine the problem from the point of view of specific
information-seeking tasks: text classification, information retrieval,
and information extraction. One common factor in addressing these tasks
is the use of : models that predict the probability distribution of
language expressions.

Language Models
---------------

Formal languages, such as the programming languages Java or Python, have
precisely defined language models. A can be defined as a set of strings;
“print(2 + 2)” is a legal program in the language Python,
whereas “2)+(2 print” is not. Since there are an infinite
number of legal programs, they cannot be enumerated; instead they are
specified by a set of rules called a . Formal languages also have rules
that define the meaning or of a program; for example, the rules say that
the “meaning” of “2 + 2” is 4, and the meaning of
“1/0” is that an error is signaled.

Natural languages, such as English or Spanish, cannot be characterized
as a definitive set of sentences. Everyone agrees that “Not to be
invited is sad” is a sentence of English, but people disagree on the
grammaticality of “To be not invited is sad.” Therefore, it is more
fruitful to define a natural language model as a probability
distribution over sentences rather than a definitive set. That is,
rather than asking if a string of *words* is or is not a
member of the set defining the language, we instead ask for
$P(S\eq\v{words})$—what is the probability that a random sentence would
be *words*.

Natural languages are also . “He saw her duck” can mean either that he
saw a waterfowl belonging to her, or that he saw her move to evade
something. Thus, again, we cannot speak of a single meaning for a
sentence, but rather of a probability distribution over possible
meanings.

Finally, natural languages are difficult to deal with because they are
very large, and constantly changing. Thus, our language models are, at
best, an approximation. We start with the simplest possible
approximations and move up from there.

### *N*-gram character models

Ultimately, a written text is composed of —letters, digits, punctuation,
and spaces in English (and more exotic characters in some other
languages). Thus, one of the simplest language models is a probability
distribution over sequences of characters. As in , we write $P(c_{1:N})$
for the probability of a sequence of $N$ characters, $c_1$ through
$c_N$. In one Web collection, $P(\mbox{``the''})\eq 0.027$ and
$P(\mbox{``zgq''})\eq
0.000000002$. A sequence of written symbols of length $n$ is called an
$n$-gram (from the Greek root for writing or letters), with special case
“unigram” for 1-gram, “bigram” for 2-gram, and “trigram” for 3-gram. A
model of the probability distribution of $n$-letter sequences is thus
called an . (But be careful: we can have $n$-gram models over sequences
of words, syllables, or other units; not just over characters.)

An $n$-gram model is defined as a of order $n-1$. Recall from that in a
Markov chain the probability of character $c_i$ depends only on the
immediately preceding characters, not on any other characters. So in a
trigram model (Markov chain of order 2) we have
$$P(c_i\given c_{1:i-1}) = P(c_i \given c_{i-2:i-1}) \ .$$ We can define
the probability of a sequence of characters $P(c_{1:N})$ under the
trigram model by first factoring with the chain rule and then using the
Markov assumption:
$$P(c_{1:N}) = \prod_{i\eq 1}^N P(c_i\given c_{1:i-1}) =
\prod_{i\eq 1}^N P(c_i\given c_{i-2:i-1}) \ .$$ For a trigram character
model in a language with 100 characters, $\pv({C_i}|C_{i-2:i-1})$ has a
million entries, and can be accurately estimated by counting character
sequences in a body of text of 10 million characters or more. We call a
body of text a (plural *corpora*), from the Latin word for
*body*.

What can we do with $n$-gram character models? One task for which they
are well suited is : given a text, determine what natural language it is
written in. This is a relatively easy task; even with short texts such
as “Hello, world” or “Wie geht es dir,” it is easy to identify the first
as English and the second as German. Computer systems identify languages
with greater than 99% accuracy; occasionally, closely related languages,
such as Swedish and Norwegian, are confused.

One approach to language identification is to first build a trigram
character model of each candidate language, $P(c_i\given c_{i-2:i-1},
\ell)$, where the variable $\ell$ ranges over languages. For each $\ell$
the model is built by counting trigrams in a corpus of that language.
(About 100,000 characters of each language are needed.) That gives us a
model of $\pv({Text}\given {Language})$, but we want to select the
most probable language given the text, so we apply Bayes’ rule followed
by the Markov assumption to get the most probable language:

$$\begin{aligned}
\ell^* &=& \argmax_{\ell} \, P(\ell \given c_{1:N})  \\
       &=& \argmax_{\ell} \, P(\ell) P(c_{1:N}\given\ell)   \\
       &=& \argmax_{\ell} \, P(\ell) \prod_{i\eq 1}^{N} P(c_i\given c_{i-2:i-1}, \ell)\end{aligned}$$

The trigram model can be learned from a corpus, but what about the prior
probability $P(\ell)$? We may have some estimate of these values; for
example, if we are selecting a random Web page we know that English is
the most likely language and that the probability of Macedonian will be
less than 1%. The exact number we select for these priors is not
critical because the trigram model usually selects one language that is
several orders of magnitude more probable than any other.

Other tasks for character models include spelling correction, genre
classification, and named-entity recognition. Genre classification means
deciding if a text is a news story, a legal document, a scientific
article, etc. While many features help make this classification, counts
of punctuation and other character $n$-gram features go a long way
@Kessler+al:1997. Named-entity recognition is the task of finding names
of things in a document and deciding what class they belong to. For
example, in the text “Mr. Sopersteen was prescribed aciphex,” we should
recognize that “Mr. Sopersteen” is the name of a person and “aciphex” is
the name of a drug. Character-level models are good for this task
because they can associate the character sequence “ex” (“ex” followed by
a space) with a drug name and “steen” with a person name, and thereby
identify words that they have never seen before.

### Smoothing *n*-gram models {#smoothing-ngram-section}

The major complication of $n$-gram models is that the training corpus
provides only an estimate of the true probability distribution. For
common character sequences such as “th” any English corpus will give a
good estimate: about 1.5% of all trigrams. On the other hand, “ht” is
very uncommon—no dictionary words start with ht. It is likely that the
sequence would have a count of zero in a training corpus of standard
English. Does that mean we should assign
$P(\mbox{``{\textvisiblespace}th''})\eq 0$? If we did, then the text
“The program issues an http request” would have an English probability
of zero, which seems wrong. We have a problem in generalization: we want
our language models to generalize well to texts they haven’t seen yet.
Just because we have never seen “http” before does not mean that our
model should claim that it is impossible. Thus, we will adjust our
language model so that sequences that have a count of zero in the
training corpus will be assigned a small nonzero probability (and the
other counts will be adjusted downward slightly so that the probability
still sums to 1). The process od adjusting the probability of
low-frequency counts is called .

The[ngram-smoothing-page] simplest type of smoothing was suggested by
Pierre-Simon Laplace in the 18th century: he said that, in the lack of
further information, if a random Boolean variable $X$ has been false in
all $n$ observations so far then the estimate for $P(X\eq{true})$
should be $1/(n+2)$. That is, he assumes that with two more trials, one
might be true and one false. Laplace smoothing (also called add-one
smoothing) is a step in the right direction, but performs relatively
poorly. A better approach is a , in which we start by estimating
$n$-gram counts, but for any particular sequence that has a low (or
zero) count, we back off to $(n-1)$-grams. is a backoff model that
combines trigram, bigram, and unigram models by linear interpolation. It
defines the probability estimate as

(c~i~|c~i-2:i-1~) = ~3~ P(c~i~|c~i-2:i-1~) + ~2~ P(c~i~|c~i-1~) + ~1~
P(c~i~) ,

where $\lambda_3 + \lambda_2 + \lambda_1 \eq 1$. The parameter values
$\lambda_i$ can be fixed, or they can be trained with an
expectation–maximization algorithm. It is also possible to have the
values of $\lambda_i$ depend on the counts: if we have a high count of
trigrams, then we weigh them relatively more; if only a low count, then
we put more weight on the bigram and unigram models. One camp of
researchers has developed ever more sophisticated smoothing models,
while the other camp suggests gathering a larger corpus so that even
simple smoothing models work well. Both are getting at the same goal:
reducing the variance in the language model.

One complication: note that the expression $P(c_i\given c_{i-2:i-1})$
asks for $P(c_1\given c_{{\mbox{-}1}:0})$ when $i=1$, but there are no
characters before $c_1$. We can introduce artificial characters, for
example, defining $c_0$ to be a space character or a special “begin
text” character. Or we can fall back on lower-order Markov models, in
effect defining $c_{\mbox{-}1:0}$ to be the empty sequence and thus
$P(c_1\given c_{\mbox{-}1:0})\eq P(c_1)$.

### Model evaluation

With so many possible $n$-gram models—unigram, bigram, trigram,
interpolated smoothing with different values of $\lambda$, etc.—how do
we know what model to choose? We can evaluate a model with
cross-validation. Split the corpus into a training corpus and a
validation corpus. Determine the parameters of the model from the
training data. Then evaluate the model on the validation corpus.

The evaluation can be a task-specific metric, such as measuring accuracy
on language identification. Alternatively we can have a task-independent
model of language quality: calculate the probability assigned to the
validation corpus by the model; the higher the probability the better.
This metric is inconvenient because the probability of a large corpus
will be a very small number, and floating-point underflow becomes an
issue. A different way of describing the probability of a sequence is
with a measure called , defined as
$${Perplexity}(c_{1:N}) = P(c_{1:N})^{- \frac{1}{N}} \ .$$ Perplexity
can be thought of as the reciprocal of probability, normalized by
sequence length. It can also be thought of as the weighted average
branching factor of a model. Suppose there are 100 characters in our
language, and our model says they are all equally likely. Then for a
sequence of any length, the perplexity will be 100. If some characters
are more likely than others, and the model reflects that, then the model
will have a perplexity less than 100.

### *N*-gram word models

Now we turn to $n$-gram models over words rather than characters. All
the same mechanism applies equally to word and character models. The
main difference is that the —the set of symbols that make up the corpus
and the model—is larger. There are only about 100 characters in most
languages, and sometimes we build character models that are even more
restrictive, for example by treating “A” and “a” as the same symbol or
by treating all punctuation as the same symbol. But with word models we
have at least tens of thousands of symbols, and sometimes millions. The
wide range is because it is not clear what constitutes a word. In
English a sequence of letters surrounded by spaces is a word, but in
some languages, like Chinese, words are not separated by spaces, and
even in English many decisions must be made to have a clear policy on
word boundaries: how many words are in “ne’er-do-well”? Or in
“(Tel:1-800-960-5660x123)”?

Word $n$-gram models need to deal with words. With character models, we
didn’t have to worry about someone inventing a new letter of the
alphabet.[^1] But with word models there is always the chance of a new
word that was not seen in the training corpus, so we need to model that
explicitly in our language model. This can be done by adding just one
new word to the vocabulary: \<UNK\>, standing for the
unknown word. We can estimate $n$-gram counts for \<UNK\>
by this trick: go through the training corpus, and the first time any
individual word appears it is previously unknown, so replace it with the
symbol \<UNK\>. All subsequent appearances of the word
remain unchanged. Then compute $n$-gram counts for the corpus as usual,
treating \<UNK\> just like any other word. Then when an
unknown word appears in a test set, we look up its probability under
\<UNK\>. Sometimes multiple unknown-word symbols are used,
for different classes. For example, any string of digits might be
replaced with \<NUM\>, or any email address with
\<EMAIL\>.

To get a feeling for what word models can do, we built unigram, bigram,
and trigram models over the words in this book and then randomly sampled
sequences of words from the models. The results are

logical are as are confusion a may right tries agent goal the was
$\ldots$\
*Bigram:* systems are very similar computational approach
would be represented $\ldots$\
*Trigram:* planning and scheduling are integrated the
success of naive bayes model is $\ldots$

Even with this small sample, it should be clear that the unigram model
is a poor approximation of either English or the content of an AI
textbook, and that the bigram and trigram models are much better. The
models agree with this assessment: the perplexity was 891 for the
unigram model, 142 for the bigram model and 91 for the trigram model.

With the basics of $n$-gram models—both character- and
word-based—established, we can turn now to some language tasks.

Text Classification
-------------------

We now consider in depth the task of , also known as : given a text of
some kind, decide which of a predefined set of classes it belongs to.
Language identification and genre classification are examples of text
classification, as is sentiment analysis (classifying a movie or product
review as positive or negative) and (classifying an email message as
spam or not-spam). Since “not-spam” is awkward, researchers have coined
the term for not-spam. We can treat spam detection as a problem in
supervised learning. A training set is readily available: the positive
(spam) examples are in my spam folder, the negative (ham) examples are
in my inbox. Here is an excerpt:

Spam: Wholesale Fashion Watches -57% today. Designer watches for cheap
...\
Spam: You can buy ViagraFr\$1.85 All Medications at unbeatable prices!
...\
Spam: WE CAN TREAT ANYTHING YOU SUFFER FROM JUST TRUST US ...\
Spam: Sta.rt earn\*ing the salary yo,u d-eserve by o’btaining the
prope,r crede’ntials!\
\
Ham: The practical significance of hypertree width in identifying more
...\
Ham: Abstract: We will motivate the problem of social identity
clustering: ...\
Ham: Good to see you my friend. Hey Peter, It was good to hear from you.
...\
Ham: PDS implies convexity of the resulting optimization problem (Kernel
Ridge ...

From this excerpt we can start to get an idea of what might be good
features to include in the supervised learning model. Word $n$-grams
such as “for cheap” and “You can buy” seem to be indicators of spam
(although they would have a nonzero probability in ham as well).
Character-level features also seem important: spam is more likely to be
all uppercase and to have punctuation embedded in words. Apparently the
spammers thought that the word bigram “you deserve” would be too
indicative of spam, and thus wrote “yo,u d-eserve” instead. A character
model should detect this. We could either create a full character
$n$-gram model of spam and ham, or we could handcraft features such as
“number of punctuation marks embedded in words.”

Note that we have two complementary ways of talking about
classification. In the language-modeling approach, we define one
$n$-gram language model for $\pv({Message}\given {spam})$ by
training on the spam folder, and one model for
$\pv({Message}\given {ham})$ by training on the inbox. Then we can
classify a new message with an application of Bayes’ rule:
$$\argmax_{{c} \in \{{spam}, {ham}\}} P({c}\given {message})
= \argmax_{{c} \in \{{spam}, {ham}\}} 
 P({message}\given {c})\, P({c}) \ .$$ where $P({c})$ is
estimated just by counting the total number of spam and ham messages.
This approach works well for spam detection, just as it did for language
identification.

In the machine-learning approach we represent the message as a set of
feature/value pairs and apply a classification algorithm $h$ to the
feature vector $\X$. We can make the language-modeling and
machine-learning approaches compatible by thinking of the $n$-grams as
features. This is easiest to see with a unigram model. The features are
the words in the vocabulary: “a,” “aardvark,” $\ldots$, and the values
are the number of times each word appears in the message. That makes the
feature vector large and sparse. If there are 100,000 words in the
language model, then the feature vector has length 100,000, but for a
short email message almost all the features will have count zero. This
unigram representation has been called the model. You can think of the
model as putting the words of the training corpus in a bag and then
selecting words one at a time. The notion of order of the words is lost;
a unigram model gives the same probability to any permutation of a text.
Higher-order $n$-gram models maintain some local notion of word order.

With bigrams and trigrams the number of features is squared or cubed,
and we can add in other, non-$n$-gram features: the time the message was
sent, whether a URL or an image is part of the message, an ID number for
the sender of the message, the sender’s number of previous spam and ham
messages, and so on. The choice of features is the most important part
of creating a good spam detector—more important than the choice of
algorithm for processing the features. In part this is because there is
a lot of training data, so if we can propose a feature, the data can
accurately determine if it is good or not. It is necessary to constantly
update features, because spam detection is an ; the spammers modify
their spam in response to the spam detector’s changes.

It can be expensive to run algorithms on a very large feature vector, so
often a process of is used to keep only the features that best
discriminate between spam and ham. For example, the bigram “of the” is
frequent in English, and may be equally frequent in spam and ham, so
there is no sense in counting it. Often the top hundred or so features
do a good job of discriminating between classes.

Once we have chosen a set of features, we can apply any of the
supervised learning techniques we have seen; popular ones for text
categorization include $k$-nearest-neighbors, support vector machines,
decision trees, naive Bayes, and logistic regression. All of these have
been applied to spam detection, usually with accuracy in the 98%–99%
range. With a carefully designed feature set, accuracy can exceed 99.9%.

### Classification by data compression

Another way to think about classification is as a problem in . A
lossless compression algorithm takes a sequence of symbols, detects
repeated patterns in it, and writes a description of the sequence that
is more compact than the original. For example, the text
“0.142857142857142857” might be compressed to “0.[142857]\*3.”
Compression algorithms work by building dictionaries of subsequences of
the text, and then referring to entries in the dictionary. The example
here had only one dictionary entry, “142857.”

In effect, compression algorithms are creating a language model. The LZW
algorithm in particular directly models a maximum-entropy probability
distribution. To do classification by compression, we first lump
together all the spam training messages and compress them as a unit. We
do the same for the ham. Then when given a new message to classify, we
append it to the spam messages and compress the result. We also append
it to the ham and compress that. Whichever class compresses better—adds
the fewer number of additional bytes for the new message—is the
predicted class. The idea is that a spam message will tend to share
dictionary entries with other spam messages and thus will compress
better when appended to a collection that already contains the spam
dictionary.

Experiments with compression-based classification on some of the
standard corpora for text classification—the 20-Newsgroups data set, the
Reuters-10 Corpora, the Industry Sector corpora—indicate that whereas
running off-the-shelf compression algorithms like gzip, RAR, and can be
quite slow, their accuracy is comparable to traditional classification
algorithms. This is interesting in its own right, and also serves to
point out that there is promise for algorithms that use character
$n$-grams directly with no preprocessing of the text or feature
selection: they seem to be captiring some real patterns.

Information Retrieval {#ir-section}
---------------------

is the task of finding documents that are relevant to a user’s need for
information. The best-known examples of information retrieval systems
are search engines on the . A Web user can type a query such as [AI
book][^2] into a search engine and see a list of relevant pages. In this
section, we will see how such systems are built. An information
retrieval (henceforth ) system can be characterized by

1.  **A corpus of documents.** Each system must decide what
    it wants to treat as a document: a paragraph, a page, or a multipage
    text.

2.  **Queries posed in a **. A query specifies what the
    user wants to know. The query language can be just a list of words,
    such as [AI book]; or it can specify a phrase of words that must be
    adjacent, as in [“AI book”]; it can contain Boolean operators as in
    [AI AND book]; it can include non-Boolean operators such as [AI NEAR
    book] or [AI book site:www.aaai.org].

3.  **A .** This is the subset of documents that the IR
    system judges to be to the query. By *relevant*, we
    mean likely to be of use to the person who posed the query, for the
    particular information need expressed in the query.

4.  **A of the result set.** This can be as simple as a
    ranked list of document titles or as complex as a rotating color map
    of the result set projected onto a three-dimensional space, rendered
    as a two-dimensional display.

The earliest IR systems worked on a . Each word in the document
collection is treated as a Boolean feature that is true of a document if
the word occurs in the document and false if it does not. So the feature
“retrieval” is true for the current chapter but false for . The query
language is the language of Boolean expressions over features. A
document is relevant only if the expression evaluates to true. For
example, the query [information AND retrieval] is true for the current
chapter and false for .

This model has the advantage of being simple to explain and implement.
However, it has some disadvantages. First, the degree of relevance of a
document is a single bit, so there is no guidance as to how to order the
relevant documents for presentation. Second, Boolean expressions are
unfamiliar to users who are not programmers or logicians. Users find it
unintuitive that when they want to know about farming in the states of
Kansas *and* Nebraska they need to issue the query [farming
(Kansas OR Nebraska)]. Third, it can be hard to formulate an appropriate
query, even for a skilled user. Suppose we try [information AND
retrieval AND models AND optimization] and get an empty result set. We
could try [information OR retrieval OR models OR optimization], but if
that returns too many results, it is difficult to know what to try next.

### IR scoring functions

Most IR systems have abandoned the Boolean model and use models based on
the statistics of word counts. We describe the , which comes from the
Okapi project of Stephen Robertson and Karen Sparck Jones at London’s
City College, and has been used in search engines such as the
open-source project.

A scoring function takes a document and a query and returns a numeric
score; the most relevant documents have the highest scores. In the BM25
function, the score is a linear weighted combination of scores for each
of the words that make up the query. Three factors affect the weight of
a query term: First, the frequency with which a query term appears in a
document (also known as ${TF}$ for term frequency). For the query
[farming in Kansas], documents that mention “farming” frequently will
have higher scores. Second, the inverse document frequency of the term,
or ${IDF}$. The word “in” appears in almost every document, so it has
a high document frequency, and thus a low inverse document frequency,
and thus it is not as important to the query as “farming” or “Kansas.”
Third, the length of the document. A million-word document will probably
mention all the query words, but may not actually be about the query. A
short document that mentions all the words is a much better candidate.

The BM25 function takes all three of these into account. We assume we
have created an index of the $N$ documents in the corpus so that we can
look up ${TF}(q_i, d_j)$, the count of the number of times word $q_i$
appears in document $d_j$. We also assume a table of document frequency
counts, ${DF}(q_i)$, that gives the number of documents that contain
the word $q_i$. Then, given a document $d_j$ and a query consisting of
the words $q_{1:N}$, we have
$$BM25(d_j,q_{1:N}) = \sum_{i=1}^{N}IDF(q_i)\cdot\frac{{TF}(q_i,d_j)\cdot(k + 1)}
         {{TF}(q_i,d_j)+k \cdot (1 - b +
b\cdot\frac{\left|d_j\right|}{L})} \ ,$$ where $\left|d_j\right|$ is the
length of document $d_j$ in words, and $L$ is the average document
length in the corpus: $L = \sum_i |d_i|/N$. We have two parameters, $k$
and $b$, that can be tuned by cross-validation; typical values are
$k = 2.0$ and $b = 0.75$. ${IDF}(q_i)$ is the inverse document
frequency of word $q_i$, given by
$${IDF}(q_i) = \log \frac{N - {DF}(q_i) + 0.5}{{DF}(q_i) + 0.5} \ .$$
Of course, it would be impractical to apply the BM25 scoring function to
every document in the corpus. Instead, systems create an ahead of time
that lists, for each vocabulary word, the documents that contain the
word. This is called the for the word. Then when given a query, we
intersect the hit lists of the query words and only score the documents
in the intersection.

### IR system evaluation

How do we know whether an IR system is performing well? We undertake an
experiment in which the system is given a set of queries and the result
sets are scored with respect to human relevance judgments.
Traditionally, there have been two measures used in the scoring: recall
and precision. We explain them with the help of an example. Imagine that
an IR system has returned a result set for a single query, for which we
know which documents are and are not relevant, out of a corpus of 100
documents. The document counts in each category are given in the
following table:

                  In result set   Not in result set
  -------------- --------------- -------------------
        Relevant       30                20
    Not relevant       10                40

measures the proportion of documents in the result set that are actually
relevant. In our example, the precision is ${30}/({30}+{10}) \eq
.{75}$. The false positive rate is $1 - .{75} \eq
.{25}$. measures the proportion of all the relevant documents in the
collection that are in the result set. In our example, recall is
${30}/({30}+{20}) \eq .{60}$. The false negative rate is
$1 - .{60} \eq .{40}$. In a very large document collection, such as the
, recall is difficult to compute, because there is no easy way to
examine every page on the Web for relevance. All we can do is either
estimate recall by sampling or ignore recall completely and just judge
precision. In the case of a Web search engine, there may be thousands of
documents in the result set, so it makes more sense to measure precision
for several different sizes, such as “P@10” (precision in the top 10
results) or “P@50,” rather than to estimate precision in the entire
result set.

It is possible to trade off precision against recall by varying the size
of the result set returned. In the extreme, a system that returns every
document in the document collection is guaranteed a recall of 100%, but
will have low precision. Alternately, a system could return a single
document and have low recall, but a decent chance at 100% precision. A
summary of both measures is the $F_1$ score, a single number that is the
harmonic mean of precision and recall, $2PR/(P+R)$.

### IR refinements

There are many possible refinements to the system described here, and
indeed Web search engines are continually updating their algorithms as
they discover new approaches and as the Web grows and changes.

One common refinement is a better model of the effect of document length
on relevance. observed that simple document length normalization schemes
tend to favor short documents too much and long documents not enough.
They propose a *pivoted* document length normalization
scheme; the idea is that the pivot is the document length at which the
old-style normalization is correct; documents shorter than that get a
boost and longer ones get a penalty.

The BM25 scoring function uses a word model that treats all words as
completely independent, but we know that some words are correlated:
“couch” is closely related to both “couches” and “sofa.” Many IR systems
attempt to account for these correlations.

For example, if the query is [couch], it would be a shame to exclude
from the result set those documents that mention “COUCH” or “couches”
but not “couch.” Most IR systems do of “COUCH” to “couch,” and some use
a algorithm to reduce “couches” to the stem form “couch,” both in the
query and the documents. This typically yields a small increase in
recall (on the order of 2% for English). However, it can harm precision.
For example, stemming “stocking” to “stock” will tend to decrease
precision for queries about either foot coverings or financial
instruments, although it could improve recall for queries about
warehousing. Stemming algorithms based on rules (e.g., remove “-ing”)
cannot avoid this problem, but algorithms based on dictionaries (don’t
remove “-ing” if the word is already listed in the dictionary) can.
While stemming has a small effect in English, it is more important in
other languages. In German, for example, it is not uncommon to see words
like “Lebensversicherungsgesellschaftsangestellter” (life insurance
company employee). Languages such as Finnish, Turkish, Inuit, and Yupik
have recursive morphological rules that in principle generate words of
unbounded length.

The next step is to recognize , such as “sofa” for “couch.” As with
stemming, this has the potential for small gains in recall, but can hurt
precision. A user who gives the query [Tim Couch] wants to see results
about the football player, not sofas. The problem is that “languages
abhor absolute synonyms just as nature abhors a vacuum” @Cruse:1986.
That is, anytime there are two words that mean the same thing, speakers
of the language conspire to evolve the meanings to remove the confusion.
Related words that are not synonyms also play an important role in
ranking—terms like “leather”, “wooden,” or “modern” can serve to confirm
that the document really is about “couch.” Synonyms and related words
can be found in dictionaries or by looking for correlations in documents
or in queries—if we find that many users who ask the query [new sofa]
follow it up with the query [new couch], we can in the future alter [new
sofa] to be [new sofa OR new couch].

As a final refinement, IR can be improved by considering —data outside
of the text of the document. Examples include human-supplied keywords
and publication data. On the Web, hypertext between documents are a
crucial source of information.

### The PageRank algorithm

[^3] was one of the two original ideas that set Google’s search apart
from other Web search engines when it was introduced in 1997. (The other
innovation was the use of anchor text—the underlined text in a
hyperlink—to index a page, even though the anchor text was on a
*different* page than the one being indexed.) PageRank was
invented to solve the problem of the tyranny of ${TF}$ scores: if the
query is [IBM], how do we make sure that IBM’s home page,
[ibm.com](ibm.com), is the first result, even if another page mentions
the term “IBM” more frequently? The idea is that [ibm.com](ibm.com) has
many in-links (links to the page), so it should be ranked higher: each
in-link is a vote for the quality of the linked-to page. But if we only
counted in-links, then it would be possible for a Web spammer to create
a network of pages and have them all point to a page of his choosing,
increasing the score of that page. Therefore, the PageRank algorithm is
designed to weight links from high-quality sites more heavily. What is a
high-quality site? One that is linked to by other high-quality sites.
The definition is recursive, but we will see that the recursion bottoms
out properly. The PageRank for a page $p$ is defined as:
$$PR(p) = \frac{1-d}{N} + d \sum_i \frac{PR({in}_i)}{C({in}_i)} \ ,$$
where $PR(p)$ is the PageRank of page $p$, $N$ is the total number of
pages in the corpus, ${in}_i$ are the pages that link in to $p$, and
$C({in}_i)$ is the count of the total number of out-links on page
${in}_i$. The constant $d$ is a damping factor. It can be understood
through the : imagine a Web surfer who starts at some random page and
begins exploring. With probability $d$ (we’ll assume $d\eq 0.85$) the
surfer clicks on one of the links on the page (choosing uniformly among
them), and with probability $1-d$ she gets bored with the page and
restarts on a random page anywhere on the Web. The PageRank of page $p$
is then the probability that the random surfer will be at page $p$ at
any point in time. PageRank can be computed by an iterative procedure:
start with all pages having $PR(p)\eq 1$, and iterate the algorithm,
updating ranks until they converge.

[HITS-algorithm]

### The HITS algorithm

The Hyperlink-Induced Topic Search algorithm, also known as “” or , is
another influential link-analysis algorithm (see ). HITS differs from
PageRank in several ways. First, it is a query-dependent measure: it
rates pages with respect to a query. That means that it must be computed
anew for each query—a computational burden that most search engines have
elected not to take on. Given a query, HITS first finds a set of pages
that are relevant to the query. It does that by intersecting hit lists
of query words, and then adding pages in the link neighborhood of these
pages—pages that link to or are linked from one of the pages in the
original relevant set.

Each page in this set is considered an on the query to the degree that
other pages in the relevant set point to it. A page is considered a to
the degree that it points to other authoritative pages in the relevant
set. Just as with PageRank, we don’t want to merely count the number of
links; we want to give more value to the high-quality hubs and
authorities. Thus, as with PageRank, we iterate a process that updates
the authority score of a page to be the sum of the hub scores of the
pages that point to it, and the hub score to be the sum of the authority
scores of the pages it points to. If we then normalize the scores and
repeat $k$ times, the process will converge.

Both PageRank and HITS played important roles in developing our
understanding of Web information retrieval. These algorithms and their
extensions are used in ranking billions of queries daily as search
engines steadily develop better ways of extracting yet finer signals of
search relevance.

### Question answering {#question-answering-section}

Information retrieval is the task of finding documents that are relevant
to a query, where the query may be a question, or just a topic area or
concept. is a somewhat different task, in which the query really is a
question, and the answer is not a ranked list of documents but rather a
short response—a sentence, or even just a phrase. There have been
question-answering NLP (natural language processing) systems since the
1960s, but only since 2001 have such systems used Web information
retrieval to radically increase their breadth of coverage.

The system @Banko+al:2002 is a typical Web-based question-answering
system. It is based on the intuition that most questions will be
answered many times on the Web, so question answering should be thought
of as a problem in precision, not recall. We don’t have to deal with all
the different ways that an answer might be phrased—we only have to find
one of them. For example, consider the query [Who killed Abraham
Lincoln?] Suppose a system had to answer that question with access only
to a single encyclopedia, whose entry on Lincoln said

John Wilkes Booth altered history with a bullet. He will forever be
known as the man who ended Abraham Lincoln’s life.

To use this passage to answer the question, the system would have to
know that ending a life can be a killing, that “He” refers to Booth, and
several other linguistic and semantic facts.

does not attempt this kind of sophistication—it knows nothing about
pronoun reference, or about killing, or any other verb. It does know 15
different kinds of questions, and how they can be rewritten as queries
to a search engine. It knows that [Who killed Abraham Lincoln] can be
rewritten as the query [\* killed Abraham Lincoln] and as [Abraham
Lincoln was killed by \*]. It issues these rewritten queries and
examines the results that come back—not the full Web pages, just the
short summaries of text that appear near the query terms. The results
are broken into 1-, 2-, and 3-grams and tallied for frequency in the
result sets and for weight: an $n$-gram that came back from a very
specific query rewrite (such as the exact phrase match query [“Abraham
Lincoln was killed by \*”]) would get more weight than one from a
general query rewrite, such as [Abraham OR Lincoln OR killed]. We would
expect that “John Wilkes Booth” would be among the highly ranked
$n$-grams retrieved, but so would “Abraham Lincoln” and “the
assassination of” and “Ford’s Theatre.”

Once the $n$-grams are scored, they are filtered by expected type. If
the original query starts with “who,” then we filter on names of people;
for “how many” we filter on numbers, for “when,” on a date or time.
There is also a filter that says the answer should not be part of the
question; together these should allow us to return “John Wilkes Booth”
(and not “Abraham Lincoln”) as the highest-scoring response.

In some cases the answer will be longer than three words; since the
components responses only go up to 3-grams, a longer response would have
to be pieced together from shorter pieces. For example, in a system that
used only bigrams, the answer “John Wilkes Booth” could be pieced
together from high-scoring pieces “John Wilkes” and “Wilkes Booth.”

At the Text Retrieval Evaluation Conference (TREC), was rated as one of
the top systems, beating out competitors with the ability to do far more
complex language understanding. relies upon the breadth of the content
on the Web rather than on its own depth of understanding. It won’t be
able to handle complex inference patterns like associating “who killed”
with “ended the life of.” But it knows that the Web is so vast that it
can afford to ignore passages like that and wait for a simple passage it
can handle.

Information Extraction {#ie-section}
----------------------

is the process of acquiring knowledge by skimming a text and looking for
occurrences of a particular class of object and for relationships among
objects. A typical task is to extract instances of addresses from Web
pages, with database fields for street, city, state, and zip code; or
instances of storms from weather reports, with fields for temperature,
wind speed, and precipitation. In a limited domain, this can be done
with high accuracy. As the domain gets more general, more complex
linguistic models and more complex learning techniques are necessary. We
will see in how to define complex language models of the phrase
structure (noun phrases and verb phrases) of English. But so far there
are no complete models of this kind, so for the limited needs of
information extraction, we define limited models that approximate the
full English model, and concentrate on just the parts that are needed
for the task at hand. The models we describe in this section are
approximations in the same way that the simple 1-CNF logical model in ()
is an approximations of the full, wiggly, logical model.

In this section we describe six different approaches to information
extraction, in order of increasing complexity on several dimensions:
deterministic to stochastic, domain-specific to general, hand-crafted to
learned, and small-scale to large-scale.

### Finite-state automata for information extraction

The simplest type of information extraction system is an system that
assumes that the entire text refers to a single object and the task is
to extract attributes of that object. For example, we mentioned in the
problem of extracting from the text “IBM ThinkBook 970. Our price:
399.00” the set of attributes {Manufacturer=IBM, Model=ThinkBook970,
Price=399.00}. We can address this problem by defining a (also known as
a pattern) for each attribute we would like to extract. The template is
defined by a finite state automaton, the simplest example of which is
the , or regex. Regular expressions are used in Unix commands such as
grep, in programming languages such as Perl, and in word processors such
as Microsoft Word. The details vary slightly from one tool to another
and so are best learned from the appropriate manual, but here we show
how to build up a regular expression template for prices in dollars:

ll & matches any digit from 0 to 9\
+ & matches one or more digits\
 & matches a period followed by two digits\
([.][0-9][0-9])? & matches a period followed by two digits,
or nothing\
+([.][0-9][0-9])? & matches 249.99 or 1.23 or 1000000 or …

Templates are often defined with three parts: a prefix regex, a target
regex, and a postfix regex. For prices, the target regex is as above,
the prefix would look for strings such as “price:” and the postfix could
be empty. The idea is that some clues about an attribute come from the
attribute value itself and some come from the surrounding text.

If a regular expression for an attribute matches the text exactly once,
then we can pull out the portion of the text that is the value of the
attribute. If there is no match, all we can do is give a default value
or leave the attribute missing; but if there are several matches, we
need a process to choose among them. One strategy is to have several
templates for each attribute, ordered by priority. So, for example, the
top-priority template for price might look for the prefix “our price:”;
if that is not found, we look for the prefix “price:” and if that is not
found, the empty prefix. Another strategy is to take all the matches and
find some way to choose among them. For example, we could take the
lowest price that is within 50% of the highest price. That will select
78.00 as the target from the text “List price 99.00, special sale price
78.00, shipping 3.00.”

One step up from attribute-based extraction systems are systems, which
deal with multiple objects and the relations among them. Thus, when
these systems see the text “249.99,” they need to determine not just
that it is a price, but also which object has that price. A typical
relational-based extraction system is , which handles news stories about
corporate mergers and acquisitions. It can read the story

Bridgestone Sports Co. said Friday it has set up a joint venture in
Taiwan with a local concern and a Japanese trading house to produce golf
clubs to be shipped to Japan.

and extract the relations:

e (e, ) (e, )\
 (e, ) (e, )\
 (e, ) .

A relational extraction system can be built as a series of . That is,
the system consists of a series of small, efficient finite-state
automata (FSAs), where each automaton receives text as input, transduces
the text into a different format, and passes it along to the next
automaton. consists of five stages:

1.  Tokenization

2.  Complex-word handling

3.  Basic-group handling

4.  Complex-phrase handling

5.  Structure merging

’s first stage is , which segments the stream of characters into tokens
(words, numbers, and punctuation). For English, tokenization can be
fairly simple; just separating characters at white space or punctuation
does a fairly good job. Some tokenizers also deal with markup languages
such as HTML, SGML, and XML.

The second stage handles , including collocations such as “set up” and
“joint venture,” as well as proper names such as “Bridgestone Sports
Co.” These are recognized by a combination of lexical entries and
finite-state grammar rules. For example, a company name might be
recognized by the rule

CapitalizedWord+ (“Company” “Co” “Inc” “Ltd”)

The third stage handles , meaning noun groups and verb groups. The idea
is to chunk these into units that will be managed by the later stages.
We will see how to write a complex description of noun and verb phrases
in , but here we have simple rules that only approximate the complexity
of English, but have the advantage of being representable by finite
state automata. The example sentence would emerge from this stage as the
following sequence of tagged groups:

       1 NG: Bridgestone Sports Co.  10 NG: a local concern
       2 VG: said                    11 CJ: and
       3 NG: Friday                  12 NG: a Japanese trading house
       4 NG: it                      13 VG: to produce
       5 VG: had set up              14 NG: golf clubs
       6 NG: a joint venture         15 VG: to be shipped
       7 PR: in                      16 PR: to
       8 NG: Taiwan                  17 NG: Japan
       9 PR: with

Here NG means noun group, VG is verb group, PR is preposition, and CJ is
conjunction.

The fourth stage combines the basic groups into . Again, the aim is to
have rules that are finite-state and thus can be processed quickly, and
that result in unambiguous (or nearly unambiguous) output phrases. One
type of combination rule deals with domain-specific events. For example,
the rule

Company+ SetUp JointVenture (“with” Company+)?

captures one way to describe the formation of a joint venture. This
stage is the first one in the cascade where the output is placed into a
database template as well as being placed in the output stream. The
final stage that were built up in the previous step. If the next
sentence says “The joint venture will start production in January,” then
this step will notice that there are two references to a joint venture,
and that they should be merged into one. This is an instance of the
discussed in .

In general, finite-state template-based information extraction works
well for a restricted domain in which it is possible to predetermine
what subjects will be discussed, and how they will be mentioned. The
cascaded transducer model helps modularize the necessary knowledge,
easing construction of the system. These systems work especially well
when they are reverse-engineering text that has been generated by a
program. For example, a shopping site on the Web is generated by a
program that takes database entries and formats them into Web pages; a
template-based extractor then recovers the original database.
Finite-state information extraction is less successful at recovering
information in highly variable format, such as text written by humans on
a variety of subjects.

### Probabilistic models for information extraction

When information extraction must be attempted from noisy or varied
input, simple finite-state approaches fare poorly. It is too hard to get
all the rules and their priorities right; it is better to use a
probabilistic model rather than a rule-based model. The simplest
probabilistic model for sequences with hidden state is the hidden Markov
model, or .

Recall from that an HMM models a progression through a sequence of
hidden states, $\x_t$, with an observation $\e_t$ at each step. To apply
HMMs to information extraction, we can either build one big HMM for all
the attributes or build a separate HMM for each attribute. We’ll do the
second. The observations are the words of the text, and the hidden
states are whether we are in the target, prefix, or postfix part of the
attribute template, or in the background (not part of a template). For
example, here is a brief text and the most probable (Viterbi) path for
that text for two HMMs, one trained to recognize the speaker in a talk
announcement, and one trained to recognize dates. The “-” indicates a
background state:

lllllllllllll Text: & There & will & be & a & seminar & by
& Dr. & Andrew & McCallum & on & Friday\
Speaker:& - & - & - & - & & & & & & & -\
Date: & - & - & - & - & - & - & - & - & - & &

[freitag-figure]

HMMs have two big advantages over FSAs for extraction. First, HMMs are
probabilistic, and thus tolerant to noise. In a regular expression, if a
single expected character is missing, the regex fails to match; with
HMMs there is graceful degradation with missing characters/words, and we
get a probability indicating the degree of match, not just a Boolean
match/fail. Second, HMMs can be trained from data; they don’t require
laborious engineering of templates, and thus they can more easily be
kept up to date as text changes over time.

Note that we have assumed a certain level of structure in our HMM
templates: they all consist of one or more target states, and any prefix
states must precede the targets, postfix states most follow the targets,
and other states must be background. This structure makes it easier to
learn HMMs from examples. With a partially specified structure, the
forward–backward algorithm can be used to learn both the transition
probabilities $\pv(\X_t\given \X_{t-1})$ between states and the
observation model, $\pv(\E_t\given \X_t)$, which says how likely each
word is in each state. For example, the word “Friday” would have high
probability in one or more of the target states of the date HMM, and
lower probability elsewhere.

With sufficient training data, the HMM automatically learns a structure
of dates that we find intuitive: the date HMM might have one target
state in which the high-probability words are “Monday,” “Tuesday,” etc.,
and which has a high-probability transition to a target state with words
“Jan”, “January,” “Feb,” etc. shows the HMM for the speaker of a talk
announcement, as learned from data. The prefix covers expressions such
as “Speaker:” and “seminar by,” and the target has one state that covers
titles and first names and another state that covers initials and last
names.

Once the HMMs have been learned, we can apply them to a text, using the
Viterbi algorithm to find the most likely path through the HMM states.
One approach is to apply each attribute HMM separately; in this case you
would expect most of the HMMs to spend most of their time in background
states. This is appropriate when the extraction is sparse—when the
number of extracted words is small compared to the length of the text.

The other approach is to combine all the individual attributes into one
big HMM, which would then find a path that wanders through different
target attributes, first finding a speaker target, then a date target,
etc. Separate HMMs are better when we expect just one of each attribute
in a text and one big HMM is better when the texts are more free-form
and dense with attributes. With either approach, in the end we have a
collection of target attribute observations, and have to decide what to
do with them. If every expected attribute has one target filler then the
decision is easy: we have an instance of the desired relation. If there
are multiple fillers, we need to decide which to choose, as we discussed
with template-based systems. HMMs have the advantage of supplying
probability numbers that can help make the choice. If some targets are
missing, we need to decide if this is an instance of the desired
relation at all, or if the targets found are false positives. A machine
learning algorithm can be trained to make this choice.

### Conditional random fields for information extraction

One issue with HMMs for the information extraction task is that they
model a lot of probabilities that we don’t really need. An HMM is a
generative model; it models the full joint probability of observations
and hidden states, and thus can be used to generate samples. That is, we
can use the HMM model not only to parse a text and recover the speaker
and date, but also to generate a random instance of a text containing a
speaker and a date. Since we’re not interested in that task, it is
natural to ask whether we might be better off with a model that doesn’t
bother modeling that possibility. All we need in order to understand a
text is a , one that models the conditional probability of the hidden
attributes given the observations (the text). Given a text $\e_{1:N}$,
the conditional model finds the hidden state sequence $\X_{1:N}$ that
maximizes $P(\X_{1:N}\given \e_{1:N})$.

Modeling this directly gives us some freedom. We don’t need the
independence assumptions of the Markov model—we can have an $\x_t$ that
is dependent on $\x_1$. A framework for this type of model is the , or
CRF, which models a conditional probability distribution of a set of
target variables given a set of observed variables. Like Bayesian
networks, CRFs can represent many different structures of dependencies
among the variables. One common structure is the for representing Markov
dependencies among variables in a temporal sequence. Thus, HMMs are the
temporal version of naive Bayes models, and linear-chain CRFs are the
temporal version of logistic regression, where the predicted target is
an entire state sequence rather than a single binary variable.

Let $\e_{1:N}$ be the observations (e.g., words in a document), and
$\x_{1:N}$ be the sequence of hidden states (e.g., the prefix, target,
and postfix states). A linear-chain conditional random field defines a
conditional probability distribution:
$$\pv(\x_{1:N}|\e_{1:N}) = \alpha \, e^{\left[\sum_{i=1}^{N} F(\x_{i-1},\x_i,\e,i)\right]} \ ,$$
where $\alpha$ is a normalization factor (to make sure the probabilities
sum to 1), and $F$ is a feature function defined as the weighted sum of
a collection of $k$ component feature functions:
$$F(\x_{i-1},\x_i,\e,i) = \sum_k \lambda_k \, f_k(\x_{i-1},\x_i,\e,i) \ .$$
The $\lambda_{k}$ parameter values are learned with a MAP (maximum a
posteriori) estimation procedure that maximizes the conditional
likelihood of the training data. The feature functions are the key
components of a CRF. The function $f_k$ has access to a pair of adjacent
states, $\x_{i-1}$ and $\x_i$, but also the entire observation (word)
sequence $\e$, and the current position in the temporal sequence, $i$.
This gives us a lot of flexibility in defining features. We can define a
simple feature function, for example one that produces a value of 1 if
the current word is Andrew and the current state is
speaker:
$$f_1(\x_{i -1}, \x_i, \e, i)  =  \left\{ \begin{array}{ll}
                      1 & \mbox{if~} \x_i = \mbox{~{\sc speaker} and~}  \e_i = \mbox{~{\sc Andrew}}\\
                      0 & \mbox{otherwise}
                      \end{array} \right.$$ How are features like these
used? It depends on their corresponding weights. If $\lambda_1 > 0$,
then whenever $f_1$ is true, it increases the probability of the hidden
state sequence $\x_{1:N}$ . This is another way of saying “the CRF model
should prefer the target state speaker for the word
Andrew.” If on the other hand $\lambda_1 < 0$, the CRF
model will try to avoid this association, and if $\lambda_1 = 0$, this
feature is ignored. Parameter values can be set manually or can be
learned from data. Now consider a second feature function:
$$f_2(\x_{i -1}, \x_i, \e, i)  =  \left\{ \begin{array}{ll}
                      1 & \mbox{if~} \x_i = \mbox{~{\sc speaker} and~} \e_{i+1} = \mbox{~{\sc said}}\\
                      0 & \mbox{otherwise}
                      \end{array} \right.$$ This feature is true if the
current state is speaker and the next word is “said.” One
would therefore expect a positive $\lambda_2$ value to go with the
feature. More interestingly, note that both $f_1$ and $f_2$ can hold at
the same time for a sentence like “Andrew said ….” In this case, the two
features overlap each other and both boost the belief in $\x_1 \eq$
speaker. Because of the independence assumption, HMMs
cannot use overlapping features; CRFs can. Furthermore, a feature in a
CRF can use any part of the sequence $\e_{1:N}$. Features can also be
defined over transitions between states. The features we defined here
were binary, but in general, a feature function can be any real-valued
function. For domains where we have some knowledge about the types of
features we would like to include, the CRF formalism gives us a great
deal of flexibility in defining them. This flexibility can lead to
accuracies that are higher than with less flexible models such as HMMs.

### Ontology extraction from large corpora

So far we have thought of information extraction as finding a specific
set of relations (e.g., speaker, time, location) in a specific text
(e.g., a talk announcement). A different application of extraction
technology is building a large knowledge base or ontology of facts from
a corpus. This is different in three ways: First it is open-ended—we
want to acquire facts about all types of domains, not just one specific
domain. Second, with a large corpus, this task is dominated by
precision, not recall—just as with question answering on the Web ().
Third, the results can be statistical aggregates gathered from multiple
sources, rather than being extracted from one specific text.

For example, looked at the problem of learning an ontology of concept
categories and subcategories from a large corpus. (In 1992, a large
corpus was a 1000-page encyclopedia; today it would be a
100-million-page Web corpus.) The work concentrated on templates that
are very general (not tied to a specific domain) and have high precision
(are almost always correct when they match) but low recall (do not
always match). Here is one of the most productive templates:

( )\* ()? (( $|$ ) )?  .

Here the bold words and commas must appear literally in the text, but
the parentheses are for grouping, the asterisk means *repetition
of zero or more*, and the question mark means
*optional.* is a variable standing for a noun phrase;
describes how to identify noun phrases; for now just assume that we know
some words are nouns and other words (such as verbs) that we can
reliably assume are not part of a simple noun phrase. This template
matches the texts “diseases such as rabies affect your dog” and
“supports network protocols such as DNS,” concluding that rabies is a
disease and DNS is a network protocol. Similar templates can be
constructed with the key words “including,” “especially,” and “or
other.” Of course these templates will fail to match many relevant
passages, like “Rabies is a disease.” That is intentional. The “ is a ”
template does indeed sometimes denote a subcategory relation, but it
often means something else, as in “There is a God” or “She is a little
tired.” With a large corpus we can afford to be picky; to use only the
high-precision templates. We’ll miss many statements of a subcategory
relationship, but most likely we’ll find a paraphrase of the statement
somewhere else in the corpus in a form we can use.

### Automated template construction

The *subcategory* relation is so fundamental that is
worthwhile to handcraft a few templates to help identify instances of it
occurring in natural language text. But what about the thousands of
other relations in the world? There aren’t enough AI grad students in
the world to create and debug templates for all of them. Fortunately, it
is possible to *learn* templates from a few examples, then
use the templates to learn more examples, from which more templates can
be learned, and so on. In one of the first experiments of this kind,
started with a data set of just five examples:

(“Isaac Asimov”, “The Robots of Dawn”)\
(“David Brin”, “Startide Rising”)\
(“James Gleick”, “Chaos—Making a New Science”)\
(“Charles Dickens”, “Great Expectations”)\
(“William Shakespeare”, “The Comedy of Errors”)

Clearly these are examples of the author–title relation, but the
learning system had no knowledge of authors or titles. The words in
these examples were used in a search over a Web corpus, resulting in 199
matches. Each match is defined as a tuple of seven strings,
$$(\mbox{{\em Author, Title, Order, Prefix, Middle, Postfix, URL}}) \ ,$$
where *Order* is true if the author came first and false if
the title came first, *Middle* is the characters between
the author and title, *Prefix* is the 10 characters before
the match, *Suffix* is the 10 characters after the match,
and *URL* is the Web address where the match was made.

Given a set of matches, a simple template-generation scheme can find
templates to explain the matches. The language of templates was designed
to have a close mapping to the matches themselves, to be amenable to
automated learning, and to emphasize high precision (possibly at the
risk of lower recall). Each template has the same seven components as a
match. The and are regexes consisting of any characters (but beginning
and ending in letters) and constrained to have a length from half the
minimum length of the examples to twice the maximum length. The prefix,
middle, and postfix are restricted to literal strings, not regexes. The
middle is the easiest to learn: each distinct middle string in the set
of matches is a distinct candidate template. For each such candidate,
the template’s *Prefix* is then defined as the longest
common suffix of all the prefixes in the matches, and the
*Postfix* is defined as the longest common prefix of all
the postfixes in the matches. If either of these is of length zero, then
the template is rejected. The *URL* of the template is
defined as the longest prefix of the URLs in the matches.

In the experiment run by Brin, the first 199 matches generated three
templates. The most productive template was

\<LI\>\<B\> *Title* \</B\> by
*Author* (\
*URL*: www.sff.net/locus/c

The three templates were then used to retrieve 4047 more (author, title)
examples. The examples were then used to generate more templates, and so
on, eventually yielding over 15,000 titles. Given a good set of
templates, the system can collect a good set of examples. Given a good
set of examples, the system can build a good set of templates.

The biggest weakness in this approach is the sensitivity to noise. If
one of the first few templates is incorrect, errors can propagate
quickly. One way to limit this problem is to not accept a new example
unless it is verified by multiple templates, and not accept a new
template unless it discovers multiple examples that are also found by
other templates.

### Machine reading

Automated template construction is a big step up from handcrafted
template construction, but it still requires a handful of labeled
examples of each relation to get started. To build a large ontology with
many thousands of relations, even that amount of work would be onerous;
we would like to have an extraction system with *no* human
input of any kind—a system that could read on its own and build up its
own database. Such a system would be relation-independent; would work
for any relation. In practice, these systems work on *all*
relations in parallel, because of the I/O demands of large corpora. They
behave less like a traditional information-extraction system that is
targeted at a few relations and more like a human reader who learns from
the text itself; because of this the field has been called .

A representative machine-reading system is @Banko+Etzioni:2008. uses to
boost its performance, but it needs something to bootstrap from. In the
case of , specific patterns (e.g., *such as*) provided the
bootstrap, and for , it was a set of five author–title pairs. For , the
original inspiration was a taxonomy of eight very general syntactic
templates, as shown in . It was felt that a small number of templates
like this could cover most of the ways that relationships are expressed
in English. The actual bootsrapping starts from a set of labelled
examples that are extracted from the , a corpus of parsed sentences. For
example, from the parse of the sentence “Einstein received the Nobel
Prize in 1921,” is able to extract the relation (“Einstein,” “received,”
“Nobel Prize”).

Given a set of labeled examples of this type, trains a linear-chain CRF
to extract further examples from unlabeled text. The features in the CRF
include function words like “to” and “of” and “the,” but not nouns and
verbs (and not noun phrases or verb phrases). Because is
domain-independent, it cannot rely on predefined lists of nouns and
verbs.

[tbhp] [textrunner-table]

achieves a precision of 88% and recall of 45% ($F_1$ of 60%) on a large
Web corpus. has extracted hundreds of millions of facts from a corpus of
a half-billion Web pages. For example, even though it has no predefined
medical knowledge, it has extracted over 2000 answers to the query [what
kills bacteria]; correct answers include antibiotics, ozone, chlorine,
Cipro, and broccoli sprouts. Questionable answers include “water,” which
came from the sentence “Boiling water for at least 10 minutes will kill
bacteria.” It would be better to attribute this to “boiling water”
rather than just “water.”

With the techniques outlined in this chapter and continual new
inventions, we are starting to get closer to the goal of machine
reading.

The main points of this chapter are as follows:

-   Probabilistic language models based on $n$-grams recover a
    surprising amount of information about a language. They can perform
    well on such diverse tasks as language identification, spelling
    correction, genre classification, and named-entity recognition.

-   These language models can have millions of features, so feature
    selection and preprocessing of the data to reduce noise is
    important.

-   can be done with naive Bayes $n$-gram models or with any of the
    classification algorithms we have previously discussed.
    Classification can also be seen as a problem in data compression.

-   systems use a very simple language model based on bags of words, yet
    still manage to perform well in terms of and on very large corpora
    of text. On Web corpora, link-analysis algorithms improve
    performance.

-   can be handled by an approach based on information retrieval, for
    questions that have multiple answers in the corpus. When more
    answers are available in the corpus, we can use techniques that
    emphasize precision rather than recall.

-   systems use a more complex model that includes limited notions of
    syntax and semantics in the form of templates. They can be built
    from finite-state automata, HMMs, or conditional random fields, and
    can be learned from examples.

-   In building a statistical language system, it is best to devise a
    model that can make good use of available , even if the model seems
    overly simplistic.

$N$-gram letter models for language modeling were proposed by . Claude
Shannon @Shannon+Weaver:1949 was the first to generate $n$-gram word
models of English. Chomsky [-@Chomsky:1956; -@Chomsky:1957] pointed out
the limitations of finite-state models compared with context-free
models, concluding, “Probabilistic models give no particular insight
into some of the basic problems of syntactic structure.” This is true,
but probabilistic models *do* provide insight into some
*other* basic problems—problems that context-free models
ignore. Chomsky’s remarks had the unfortunate effect of scaring many
people away from statistical models for two decades, until these models
reemerged for use in speech recognition @Jelinek:1976.

show how to apply character $n$-gram models to genre classification, and
describe named-entity recognition with character models. describe the
Google $n$-gram corpus of 13 million unique words from a trillion words
of Web text; it is now publicly available. The model gets its name from
a passage from linguist Zellig Harris [-@Harris:1954], “language is not
merely a bag of words but a tool with particular properties.” gives some
examples of tasks that can be accomplished with $n$-gram models.

Add-one smoothing, first suggested by Pierre-Simon Laplace
[-@Laplace:1816], was formalized by , and is due to , who used it for
speech recognition. Other techniques include Witten–Bell smoothing
[-@Witten+Bell:1991], @Church+Gale:1991 and Kneser–Ney smoothing
[-@Kneser+Ney:1995]. and survey smoothing techniques.

Simple $n$-gram letter and word models are not the only possible
probabilistic models. describe a probabilistic text model called that
views a document as a mixture of topics, each with its own distribution
of words. This model can be seen as an extension and rationalization of
the model of @Deerwester+al:1990 (see also ) and is also related to the
multiple-cause mixture model of @Sahami+al:1996.

and survey text-classification techniques. uses statistical learning
theory and support vector machines to give a theoretical analysis of
when classification will be successful. report an accuracy of 96% in
classifying Reuters news articles into the “Earnings” category. report
accuracy up to 95% with a naive Bayes classifier, and up to 98.6% with a
Bayes classifier that accounts for some dependencies among features.
surveys forty years of application of naive Bayes techniques to text
classification and retrieval. show that simple linear classifiers can
often achieve accuracy almost as good as more complex models and are
more efficient to evaluate. show how to use the EM algorithm to label
unlabeled documents, thus learning a better classification model.
describe compression algorithms for classification, and show the deep
connection between the LZW compression algorithm and maximum-entropy
language models.

Many of the $n$-gram model techniques are also used in problems.
Biostatistics and probabilistic NLP are coming closer together, as each
deals with long, structured sequences chosen from an alphabet of
constituents.

The field of is experiencing a regrowth in interest, sparked by the wide
usage of Internet searching. gives an early overview and introduces the
probability ranking principle. and are the first textbooks to cover
Web-based search as well as traditional IR. covers user interfaces for
Web search. The TREC conference, organized by the U.S. government’s
National Institute of Standards and Technology (NIST), hosts an annual
competition for IR systems and publishes proceedings with results. In
the first seven years of the competition, performance roughly doubled.

The most popular model for IR is the @Salton+al:1975. Salton’s work
dominated the early years of the field. There are two alternative
probabilistic models, one due to and one by and . show that the models
are based on the same joint probability distribution, but that the
choice of model has implications for training the parameters. describe
the and describe how BM25 can be improved with a machine learning
approach that incorporates click data—examples of past search queies and
the results that were clicked on.

Brin and Page [-@Brin+Page:1998] describe the PageRank algorithm and the
implementation of a Web search engine. Kleinberg [-@Kleinberg:1999]
describes the HITS algorithm. investigate a log of a billion Web
searches. The journal *Information Retrieval* and the
proceedings of the annual *SIGIR* conference cover recent
developments in the field.

Early information extraction programs include @Bobrow+al:1977 and
@DeJong:1982. Recent information extraction has been pushed forward by
the annual Message Understand Conferences (MUC), sponsored by the U.S.
government. The finite-state system was done by . It was based in part
on the idea from of using FSAs as approximations to phrase-structure
grammars. Surveys of template-based systems are given by , , and . Large
databases of facts were extracted by , , , and .

discuss HMMs for Information Extraction. CRFs were introduced by ; an
example of their use for information extraction is described in
@McCallum:2003 and a tutorial with practical guidance is given by
@Sutton+McCallum:2007. gives a comprehensive survey.

present the question-answering system; a similar system is due to .
discuss a contest-winning question-answering system. Two early
influential approaches to automated knowledge engineering were by , who
showed that an automatically constructed dictionary performed almost as
well as a carefully handcrafted domain-specific dictionary, and by , who
showed that the task of word sense classification (see ) could be
accomplished through unsupervised training on a corpus of unlabeled text
with accuracy as good as supervised methods.

The idea of simultaneously extracting templates and examples from a
handful of labeled examples was developed independently and
simultaneously by , who called it and by , who called it DIPRE (Dual
Iterative Pattern Relation Extraction). You can see why the term
*cotraining* has stuck. Similar early work, under the name
of bootstrapping, was done by . The method was advanced by the
@Agichtein+Gravano:2003 and @Etzioni+al:2005 systems. Machine reading
was introduced by and and is the focus of the project @Banko+al:2007
[@Banko+Etzioni:2008].

This chapter has focused on natural language text, but it is also
possible to do information extraction based on the physical structure or
layout of text rather than on the linguistic structure. HTML lists and
tables in both HTML and relational databases are home to data that can
be extracted and consolidated @Hurst:2000
[@Pinto+al:2003; @Cafarella+al:2008].

The Association for Computational Linguistics (ACL) holds regular
conferences and publishes the journal *Computational
Linguistics*. There is also an International Conference on
Computational Linguistics (COLING). The textbook by covers statistical
language processing, while give a comprehensive introduction to speech
and natural language processing.

This exercise explores the quality of the $n$-gram model of language.
Find or create a monolingual corpus of 100,000 words or more. Segment it
into words, and compute the frequency of each word. How many distinct
words are there? Also count frequencies of bigrams (two consecutive
words) and trigrams (three consecutive words). Now use those frequencies
to generate language: from the unigram, bigram, and trigram models, in
turn, generate a 100-word text by making random choices according to the
frequency counts. Compare the three generated texts with actual
language. Finally, calculate the perplexity of each model.

Write a program to do of words without spaces. Given a string, such as
the URL
“thelongestlistofthelongeststuffatthelongestdomainnameatlonglast.com,”
return a list of component words: [“the,” “longest,” “list,” $\ldots$].
This task is useful for parsing URLs, for spelling correction when words
runtogether, and for languages such as Chinese that do not have spaces
between words. It can be solved with a unigram or bigram word model and
a dynamic programming algorithm similar to the Viterbi algorithm.

of word distribution states the following: Take a large corpus of text,
count the frequency of every word in the corpus, and then rank these
frequencies in decreasing order. Let $f_{I}$ be the $I$th largest
frequency in this list; that is, $f_{1}$ is the frequency of the most
common word (usually “the”), $f_{2}$ is the frequency of the second most
common word, and so on. Zipf’s law states that $f_{I}$ is approximately
equal to $\alpha / I$ for some constant $\alpha$. The law tends to be
highly accurate except for very small and very large values of $I$.

Choose a corpus of at least 20,000 words of online text, and verify
Zipf’s law experimentally. Define an error measure and find the value of
$\alpha$ where Zipf’s law best matches your experimental data. Create a
log–log graph plotting $f_{I}$ vs. $I$ and $\alpha/I$ vs. $I$. (On a
log–log graph, the function $\alpha/I$ is a straight line.) In carrying
out the experiment, be sure to eliminate any formatting tokens (e.g.,
HTML tags) and normalize upper and lower case.

(Adapted from .) In this exercise you will develop a classifier for
authorship: given a text, the classifier predicts which of two candidate
authors wrote the text. Obtain samples of text from two different
authors. Separate them into training and test sets. Now train a language
model on the training set. You can choose what features to use;
$n$-grams of words or letters are the easiest, but you can add
additional features that you think may help. Then compute the
probability of the text under each language model and chose the most
probable model. Assess the accuracy of this technique. How does accuracy
change as you alter the set of features? This subfield of linguistics is
called ; its successes include the identification of the author of the
disputed *Federalist Papers* @Mosteller+Wallace:1964 and
some disputed works of Shakespeare @Hope:1994. produce good results with
a simple letter bigram model.

This exercise concerns the classification of . Create a corpus of spam
email and one of non-spam mail. Examine each corpus and decide what
features appear to be useful for classification: unigram words? bigrams?
message length, sender, time of arrival? Then train a classification
algorithm (decision tree, naive Bayes, SVM, logistic regression, or some
other algorithm of your choosing) on a training set and report its
accuracy on a test set.

Create a test set of ten queries, and pose them to three major Web
search engines. Evaluate each one for precision at 1, 3, and 10
documents. Can you explain the differences between engines?

Try to ascertain which of the search engines from the previous exercise
are using case folding, stemming, synonyms, and spelling correction.

Estimate how much storage space is necessary for the index to a 100
billion-page corpus of Web pages. Show the assumptions you made.

Write a regular expression or a short program to extract company names.
Test it on a corpus of business news articles. Report your recall and
precision.

Consider the problem of trying to evaluate the quality of an IR system
that returns a ranked list of answers (like most Web search engines).
The appropriate measure of quality depends on the presumed model of what
the searcher is trying to achieve, and what strategy she employs. For
each of the following models, propose a corresponding numeric measure.

1.  The searcher will look at the first twenty answers returned, with
    the objective of getting as much relevant information as possible.

2.  The searcher needs only one relevant document, and will go down the
    list until she finds the first one.

3.  The searcher has a fairly narrow query and is able to examine all
    the answers retrieved. She wants to be sure that she has seen
    everything in the document collection that is relevant to her query.
    (E.g., a lawyer wants to be sure that she has found
    *all* relevant precedents, and is willing to spend
    considerable resources on that.)

4.  The searcher needs just one document relevant to the query, and can
    afford to pay a research assistant for an hour’s work looking
    through the results. The assistant can look through 100 retrieved
    documents in an hour. The assistant will charge the searcher for the
    full hour regardless of whether he finds it immediately or at the
    end of the hour.

5.  The searcher will look through all the answers. Examining a document
    has cost \$$A$; finding a relevant document has value \$$B$; failing
    to find a relevant document has cost \$$C$ for each relevant
    document not found.

6.  The searcher wants to collect as many relevant documents as
    possible, but needs steady encouragement. She looks through the
    documents in order. If the documents she has looked at so far are
    mostly good, she will continue; otherwise, she will stop.

[^1]: With the possible exception of the groundbreaking work of T. .

[^2]: We denote a search query as [*query*]. Square
    brackets are used rather than quotation marks so that we can
    distinguish the query [“two words”] from [two words].

[^3]: The name stands both for Web pages and for coinventor Larry Page
    @Brin+Page:1998.
Natural Language for Communication {#nlp-english-chapter}
==================================

[nlp2-chapter]

is the intentional exchange of information brought about by the
production and perception of drawn from a shared system of conventional
signs. Most animals use signs to represent important messages: food
here, predator nearby, approach, withdraw, let’s mate. In a partially
observable world, communication can help agents be successful because
they can learn information that is observed or inferred by others.
Humans are the most chatty of all species, and if computer agents are to
be helpful, they’ll need to learn to speak the language. In this chapter
we look at language models for communication. Models aimed at deep
understanding of a conversation necessarily need to be more complex than
the simple models aimed at, say, spam classification. We start with
grammatical models of the phrase structure of sentences, add semantics
to the model, and then apply it to machine translation and speech
recognition.

Phrase Structure Grammars {#prob-lang-section}
-------------------------

[e0-grammar-section]

The $n$-gram language models of were based on sequences of words. The
big issue for these models is —with a vocabulary of, say, $10^5$ words,
there are $10^{15}$ trigram probabilities to estimate, and so a corpus
of even a trillion words will not be able to supply reliable estimates
for all of them. We can address the problem of sparsity through
generalization. From the fact that “black dog” is more frequent than
“dog black” and similar observations, we can form the generalization
that adjectives tend to come before nouns in English (whereas they tend
to follow nouns in French: “chien noir” is more frequent). Of course
there are always exceptions; “galore” is an adjective that follows the
noun it modifies. Despite the exceptions, the notion of a (also known as
a ) such as *noun* or *adjective* is a useful
generalization—useful in its own right, but more so when we string
together lexical categories to form such as *noun phrase*
or *verb phrase*, and combine these syntactic categories
into trees representing the of sentences: nested phrases, each marked
with a category.

There have been many competing language models based on the idea of
phrase structure; we will describe a popular model called the , or
PCFG.[^1] A is a collection of rules that defines a as a set of
allowable strings of words. “Context-free” is described in the sidebar
on , and “probabilistic” means that the grammar assigns a probability to
every string. Here is a PCFG rule: $$\begin{array}{rcl}
\bnf{VP} & \bnfeq & \bnf{Verb}                   \bl [{0.70}] \\
        & \bnfor & \bnf{VP} \bl  \bnf{NP}            \bl [{0.30}] \ . \\
\end{array}$$ Here (*verb phrase*) and (*noun
phrase*) are . The grammar also refers to actual words, which are
called . This rule is saying that with probability 0.70 a verb phrase
consists solely of a verb, and with probability 0.30 it is a followed by
an . Appendix B describes non-probabilistic context-free grammars.

We now define a grammar for a tiny fragment of English that is suitable
for communication between agents exploring the wumpus world. We call
this language . Later sections improve on to make it slightly closer to
real English. We are unlikely ever to devise a complete grammar for
English, if only because no two persons would agree entirely on what
constitutes valid English.

### The lexicon of 

First we define the , or list of allowable words. The words are grouped
into the lexical categories familiar to dictionary users: nouns,
pronouns, and names to denote things; verbs to denote events; adjectives
to modify nouns; adverbs to modify verbs; and function words: articles
(such as *the*), prepositions (*in*), and
conjunctions (*and*). shows a small lexicon for the
language .

Each of the categories ends in $\ldots$ to indicate that there are other
words in the category. For nouns, names, verbs, adjectives, and adverbs,
it is infeasible even in principle to list all the words. Not only are
there tens of thousands of members in each class, but new ones–like
*iPod* or *biodiesel*—are being added
constantly. These five categories are called . For the categories of
pronoun, relative pronoun, article, preposition, and conjunction we
could have listed all the words with a little more work. These are
called ; they have a small number of words (a dozen or so). Closed
classes change over the course of centuries, not months. For example,
“thee” and “thou” were commonly used pronouns in the 17th century, were
on the decline in the 19th, and are seen today only in poetry and some
regional dialects.

[e0-lexicon-figure]

[e0-grammar-figure]

[pcfg-tree-figure]

### The Grammar of 

The next step is to combine the words into phrases. shows a grammar for
, with rules for each of the six syntactic categories and an example for
each rewrite rule.[^2] shows a for the sentence “Every wumpus smells.”
The parse tree gives a constructive proof that the string of words is
indeed a sentence according to the rules of . The grammar generates a
wide range of English sentences such as the following:

John is in the pit

The wumpus that stinks is in 2 2

Mary is in Boston and the wumpus is near 3 2

Unfortunately, the grammar : that is, it generates sentences that are
not grammatical, such as “Me go Boston” and “I smell pits wumpus John.”
It also : there are many sentences of English that it rejects, such as
“I think the wumpus is smelly.” We will see how to learn a better
grammar later; for now we concentrate on what we can do with the grammar
we have.

Syntactic Analysis (Parsing) {#parsing-section}
----------------------------

[earley-section]

is the process of analyzing a string of words to uncover its phrase
structure, according to the rules of a grammar. shows that we can start
with the symbol and search top down for a tree that has the words as its
leaves, or we can start with the words and search bottom up for a tree
that culminates in an . Both top-down and bottom-up parsing can be
inefficient, however, because they can end up repeating effort in areas
of the search space that lead to dead ends. Consider the following two
sentences:

> Have the students in section 2 of Computer Science 101 take the exam.\
> Have the students in section 2 of Computer Science 101 taken the exam?

Even though they share the first 10 words, these sentences have very
different parses, because the first is a command and the second is a
question. A left-to-right parsing algorithm would have to guess whether
the first word is part of a command or a question and will not be able
to tell if the guess is correct until at least the eleventh word,
*take* or *taken.* If the algorithm guesses
wrong, it will have to backtrack all the way to the first word and
reanalyze the whole sentence under the other interpretation.

[tbp] [parse-trace-table]

To avoid this source of inefficiency we can use dynamic programming:

every time we analyze a substring, store the results so we won’t have to
reanalyze it later.

For example, once we discover that “the students in section 2 of
Computer Science 101” is an , we can record that result in a data
structure known as a . Algorithms that do this are called . Because we
are dealing with context-free grammars, any phrase that was found in the
context of one branch of the search space can work just as well in any
other branch of the search space. There are many types of chart parsers;
we describe a bottom-up version called the , after its inventors, John
Cocke, Daniel Younger, and Tadeo Kasami.

[cyk-algorithm]

The CYK algorithm is shown in . Note that it requires a grammar with all
rules in one of two very specific formats: lexical rules of the form ,
and syntactic rules of the form . This grammar format, called , may seem
restrictive, but it is not: any context-free grammar can be
automatically transformed into Chomsky Normal Form. leads you through
the process.

The CYK algorithm uses space of $O(n^2m)$ for the $P$ table, where $n$
is the number of words in the sentence, and $m$ is the number of
nonterminal symbols in the grammar, and takes time $O(n^3m)$. (Since $m$
is constant for a particular grammar, this is commonly described as
$O(n^3)$.) No algorithm can do better for general context-free grammars,
although there are faster algorithms on more restricted grammars. In
fact, it is quite a trick for the algorithm to complete in $O(n^3)$
time, given that it is possible for a sentence to have an exponential
number of parse trees. Consider the sentence

> Fall leaves fall and spring leaves spring.

It is ambiguous because each word (except “and”) can be either a noun or
a verb, and “fall” and “spring” can be adjectives as well. (For example,
one meaning of “Fall leaves fall” is equivalent to \`\`Autumn abandons
autumn.) With the sentence has four parses:

 \
 \
 \
 .

If we had $c$ two-ways-ambiguous conjoined subsentences, we would have
$2^c$ ways of choosing parses for the subsentences.[^3] How does the CYK
algorithm process these $2^c$ parse trees in $O(c^3)$ time? The answer
is that it doesn’t examine all the parse trees; all it has to do is
compute the probability of the most probable tree. The subtrees are all
represented in the P table, and with a little work we could enumerate
them all (in exponential time), but the beauty of the CYK algorithm is
that we don’t have to enumerate them unless we want to.

In practice we are usually not interested in all parses; just the best
one or best few. Think of the CYK algorithm as defining the complete
state space defined by the “apply grammar rule” operator. It is possible
to search just part of this space using $A^*$ search. Each state in this
space is a list of items (words or categories), as shown in the
bottom-up parse table (). The start state is a list of words, and a goal
state is the single item . The cost of a state is the inverse of its
probability as defined by the rules applied so far, and there are
various heuristics to estimate the remaining distance to the goal; the
best heuristics come from machine learning applied to a corpus of
sentences. With the $A^*$ algorithm we don’t have to search the entire
state space, and we are guaranteed that the first parse found will be
the most probable.

### Learning probabilities for PCFGs

A PCFG has many rules, with a probability for each rule. This suggests
that the grammar from data might be better than a knowledge engineering
approach. Learning is easiest if we are given a corpus of correctly
parsed sentences, commonly called a . The @Marcus+al:1993 is the best
known; it consists of 3 million words which have been annotated with
part of speech and parse-tree structure, using human labor assisted by
some automated tools. shows an annotated tree from the Penn Treebank.

[treebank-figure]

Given a corpus of trees, we can create a PCFG just by counting (and
smoothing). In the example above, there are two nodes of the form
$[\bnf{S} [\bnf{NP} \ldots] [\bnf{VP} \ldots]]$. We would count these,
and all the other subtrees with root in the corpus. If there are 100,000
nodes of which 60,000 are of this form, then we create the rule:

 .

What if a treebank is not available, but we have a corpus of raw
unlabeled sentences? It is still possible to learn a grammar from such a
corpus, but it is more difficult. First of all, we actually have two
problems: learning the structure of the grammar rules and learning the
probabilities associated with each rule. (We have the same distinction
in learning Bayes nets.) We’ll assume that we’re given the lexical and
syntactic category names. (If not, we can just assume categories
$X_1,\ldots X_n$ and use cross-validation to pick the best value of
$n$.) We can then assume that the grammar includes every possible ( ) or
( ) rule, although many of these rules will have probability 0 or close
to 0.

We can then use an expectation–maximization (EM) approach, just as we
did in learning HMMs. The parameters we are trying to learn are the rule
probabilities; we start them off at random or uniform values. The hidden
variables are the parse trees: we don’t know whether a string of words
$w_i \ldots w_j$ is or is not generated by a rule ($\bnf{X}
\bnfeq \ldots$). The E step estimates the probability that each
subsequence is generated by each rule. The M step then estimates the
probability of each rule. The whole computation can be done in a
dynamic-programming fashion with an algorithm called the in analogy to
the forward–backward algorithm for HMMs.

The inside–outside algorithm seems magical in that it induces a grammar
from unparsed text. But it has several drawbacks. First, the parses that
are assigned by the induced grammars are often difficult to understand
and unsatisfying to linguists. This makes it hard to combine handcrafted
knowledge with automated induction. Second, it is slow: $O(n^3m^3)$,
where $n$ is the number of words in a sentence and $m$ is the number of
grammar categories. Third, the space of probability assignments is very
large, and empirically it seems that getting stuck in local maxima is a
severe problem. Alternatives such as simulated annealing can get closer
to the global maximum, at a cost of even more computation. conclude that
inside–outside is “computationally intractable for realistic problems.”

However, progress can be made if we are willing to step outside the
bounds of learning solely from unparsed text. One approach is to learn
from : to seed the process with a dozen or two rules, similar to the
rules in . From there, more complex rules can be learned more easily,
and the resulting grammar parses English with an overall recall and
precision for sentences of about 80% @Haghighi+Klein:2006. Another
approach is to use treebanks, but in addition to learning PCFG rules
directly from the bracketings, also learning distinctions that are not
in the treebank. For example, not that the tree in makes the distinction
between and . The latter is used for the pronoun “she,” the former for
the pronoun “her.” We will explore this issue in ; for now let us just
say that there are many ways in which it would be useful to a category
like —grammar induction systems that use treebanks but automatically
split categories do better than those that stick with the original
category set @Petrov+Klein:2007c. The error rates for automatically
learned grammars are still about 50% higher than for hand-constructed
grammar, but the gap is decreasing.

### Comparing context-free and Markov models

The problem with PCFGs is that they are context-free. That means that
the difference between $P$(“eat a banana”) and $P$(“eat a bandanna”)
depends only on $P$( “banana”) versus $P$( “bandanna”) and not on the
relation between “eat” and the respective objects. A Markov model of
order two or more, given a sufficiently large corpus,
*will* know that “eat a banana” is more probable. We can
combine a PCFG and Markov model to get the best of both. The simplest
approach is to estimate the probability of a sentence with the geometric
mean of the probabilities computed by both models. Then we would know
that “eat a banana” is probable from both the grammatical and lexical
point of view. But it still wouldn’t pick up the relation between “eat”
and “banana” in “eat a slightly aging but still palatable banana”
because here the relation is more than two words away. Increasing the
order of the Markov model won’t get at the relation precisely; to do
that we can use a PCFG, as described in the next section.

Another problem with PCFGs is that they tend to have too strong a
preference for shorter sentences. In a corpus such as the *Wall
Street Journal*, the average length of a sentence is about 25
words. But a PCFG will usually assign fairly high probability to many
short sentences, such as “He slept,” whereas in the
*Journal* we’re more likely to see something like “It has
been reported by a reliable source that the allegation that he slept is
credible.” It seems that the phrases in the *Journal*
really are not context-free; instead the writers have an idea of the
expected sentence length and use that length as a soft global constraint
on their sentences. This is hard to reflect in a PCFG.

Augmented Grammars and Semantic Interpretation {#augmentation-section}
----------------------------------------------

In this section we see how to extend context-free grammars—to say that,
for example, not every is independent of context, but rather, certain s
are more likely to appear in one context, and others in another context.

### Lexicalized PCFGs

To get at the relationship between the verb “eat” and the nouns “banana”
versus “bandanna,” we can use a ,[LPCFG-page] in which the probabilities
for a rule depend on the relationship between words in the parse tree,
not just on the adjacency of words in a sentence. Of course, we can’t
have the probability depend on every word in the tree, because we won’t
have enough training data to estimate all those probabilities. It is
useful to introduce the notion of the of a phrase—the most important
word. Thus, “eat” is the head of the “eat a banana” and “banana” is the
head of the “a banana.” We use the notation $\bnf{VP}(v)$ to denote a
phrase with category whose head word is $v$. We say that the category is
with the head variable $v$. Here is an that describes the verb–object
relation:

ll (v) (v) (n) &[P~1~(v, n)]\
(v) (v) &[P~2~(v)]\
(n) (a) (j) (n) &[P~3~(n, a)]\
() &[p~n~]\
…& …\

Here the probability $P_1(v, n)$ depends on the head words $v$ and $n$.
We would set this probability to be relatively high when $v$ is “eat”
and $n$ is “banana,” and low when $n$ is “bandanna.” Note that since we
are considering only heads, the distinction between “eat a banana” and
“eat a rancid banana” will not be caught by these probabilities. Another
issue with this approach is that, in a vocabulary with, say, 20,000
nouns and 5,000 verbs, $P_1$ needs 100 million probability estimates.
Only a few percent of these can come from a corpus; the rest will have
to come from smoothing (see ). For example, we can estimate$P_1(v, n)$
for a $(v,n)$ pair that we have not seen often (or at all) by backing
off to a model that depends only on $v$. These objectless probabilities
are still very useful; they can capture the distinction between a
transitive verb like “eat”—which will have a high value for $P_1$ and a
low value for $P_2$—and an intransitive verb like “sleep,” which will
have the reverse. It is quite feasible to learn these probabilities from
a treebank.

### Formal definition of augmented grammar rules

Augmented rules are complicated, so we will give them a formal
definition by showing how an augmented rule can be translated into a
logical sentence. The sentence will have the form of a definite clause
(see ), so the result is called a , or DCG. We’ll use as an example a
version of a rule from the lexicalized grammar for with one new piece of
notation:
$$\bnf{NP}(n) \bnfeq \bnf{Article}(a) \bl \bnf{Adjs}(j) \bl \bnf{Noun}(n) \bl \{{Compatible}(j,n)\} \ .$$
The new aspect here is the notation $\{{constraint}\}$ to denote a
logical constraint on some of the variables; the rule only holds when
the constraint is true. Here the predicate ${Compatible}(j,n)$ is
meant to test whether adjective $j$ and noun $n$ are compatible; it
would be defined by a series of assertions such as
${Compatible}(\bnft{black}, \bnft{dog})$. We can convert this grammar
rule into a definite clause by (1) reversing the order of right- and
left-hand sides, (2) making a conjunction of all the constituents and
constraints, (3) adding a variable $s_i$ to the list of arguments for
each constituent to represent the sequence of words spanned by the
constituent, (4) adding a term for the concatenation of words,
${Append}(s_1,\ldots)$, to the list of arguments for the root of the
tree. That gives us

(a, s~1~) (j, s~2~) (n, s~3~) (j, n)\
(n, (s~1~, s~2~, s~3~))  .

This definite clause says that if the predicate ${Article}$ is true of
a head word $a$ and a string $s_1$, and ${Adjs}$ is similarly true of
a head word $j$ and a string $s_2$, and ${Noun}$ is true of a head
word $n$ and a string $s_3$, and if $j$ and $n$ are compatible, then the
predicate $NP$ is true of the head word $n$ and the result of appending
strings $s_1$, $s_2$, and $s_3$.

The DCG translation left out the probabilities, but we could put them
back in: just augment each constituent with one more variable
representing the probability of the constituent, and augment the root
with a variable that is the product of the constituent probabilities
times the rule probability.

The translation from grammar rule to definite clause allows us to talk
about parsing as logical inference. This makes it possible to reason
about languages and strings in many different ways. For example, it
means we can do bottom-up parsing using forward chaining or top-down
parsing using backward chaining. In fact, parsing natural language with
DCGs was one of the first applications of (and motivations for) the
logic programming language. It is sometimes possible to run the process
backward and do as well as parsing. For example, skipping ahead to (), a
logic program could be given the semantic form
${Loves}({John}, {Mary})$ and apply the definite-clause rules to
deduce
$$S({Loves}({John}, {Mary}), [\bnft{John}, \bnft{loves}, \bnft{Mary}]) \ .$$
This works for toy examples, but serious language-generation systems
need more control over the process than is afforded by the DCG rules
alone.

[e1-grammar-figure]

### Case agreement and subject–verb agreement {#case-agreement-section}

We saw in that the simple grammar for overgenerates, producing
nonsentences such as “Me smell a stench.” To avoid this problem, our
grammar would have to know that “me” is not a valid when it is the
subject of a sentence. Linguists say that the pronoun “I” is in the
subjective case, and “me” is in the objective case.[^4] We can account
for this by splitting $\bnf{NP}$ into two categories, $\bnf{NP}_S$ and
$\bnf{NP}_O$, to stand for noun phrases in the subjective and objective
case, respectively. We would also need to split the category
${Pronoun}$ into the two categories ${Pronoun}{}_S$ (which includes
“I”) and ${Pronoun}{}_O$ (which includes “me”). The top part of shows
the grammar for ; we call the resulting language . Notice that all the
rules must be duplicated, once for $\bnf{NP}_S$ and once for
$\bnf{NP}_O$.

Unfortunately, still overgenerates. English requires for person and
number of the subject and main verb of a sentence. For example, if “I”
is the subject, then “I smell” is grammatical, but “I smells” is not. If
“it” is the subject, we get the reverse. In English, the agreement
distinctions are minimal: most verbs have one form for third-person
singular subjects (he, she, or it), and a second form for all other
combinations of person and number. There is one exception: the verb “to
be” has three forms, “I am / you are / he is.” So one distinction (case)
splits $NP$ two ways, another distinction (person and number) splits
$NP$ three ways, and as we uncover other distinctions we would end up
with an exponential number of subscripted $NP$ forms if we took the
approach of . Augmentations are a better approach: they can represent an
exponential number of forms as a single rule.

In the bottom of we see (part of) an augmented grammar for the language
, which handles case agreement, subject–verb agreement, and head words.
We have just one $NP$ category, but ${NP}(c,{pn},{head})$ has
three augmentations: $c$ is a parameter for case, ${pn}$ is a
parameter for person and number, and ${head}$ is a parameter for the
head word of the phrase. The other categories also are augmented with
heads and other arguments. Let’s consider one rule in detail:
$$\bnf{S}({head}) \bnfeq \bnf{NP}({Sbj},{pn},{h})  \bl \bnf{VP}({pn},\bnf{head}) \ .$$
This rule is easiest to understand right-to-left: when an and a are
conjoined they form an , but only if the has the subjective () case and
the person and number () of the and are identical. If that holds, then
we have an whose head is the same as the head of the . Note the head of
the , denoted by the dummy variable , is not part of the augmentation of
the . The lexical rules for fill in the values of the parameters and are
also best read right-to-left. For example, the rule
$$\bnf{Pronoun}({Sbj},{1S},\bnft{I}) \bnfeq \bnft{I}$$ says that “I”
can be interpreted as a in the subjective case, first-person singular,
with head “I.” For simplicity we have omitted the probabilities for
these rules, but augmentation does work with probabilities. Augmentation
can also work with automated learning mechanisms. show how a learning
algorithm can automatically split the category into $\bnf{NP}_S$ and
$\bnf{NP}_O$.

[arithmetic-semantics-figure]

[parse2-figure]

### Semantic interpretation {#sem-prag-section}

To show how to add semantics to a grammar, we start with an example that
is simpler than English: the semantics of arithmetic expressions. shows
a grammar for arithmetic expressions, where each rule is augmented with
a variable indicating the semantic interpretation of the phrase. The
semantics of a digit such as “3” is the digit itself. The semantics of
an expression such as “3 + 4” is the operator “+” applied to the
semantics of the phrase “3” and the phrase “4.” The rules obey the
principle of —the semantics of a phrase is a function of the semantics
of the subphrases. shows the parse tree for $3+(4\div 2)$ according to
this grammar. The root of the parse tree is ${Exp}(5)$, an expression
whose semantic interpretation is 5.

Now let’s move on to the semantics of English, or at least of . We start
by determining what semantic representations we want to associate with
what phrases. We use the simple example sentence “John loves Mary.” The
“John” should have as its semantic interpretation the logical term
$\bnf{John}$, and the sentence as a whole should have as its
interpretation the logical sentence ${Loves}({John},{Mary})$. That
much seems clear. The complicated part is the “loves Mary.” The semantic
interpretation of this phrase is neither a logical term nor a complete
logical sentence. Intuitively, “loves Mary” is a description that might
or might not apply to a particular person. (In this case, it applies to
John.) This means that “loves Mary” is a that, when combined with a term
that represents a person (the person doing the loving), yields a
complete logical sentence. Using the $\lambda$-notation (see ), we can
represent “loves Mary” as the predicate

 .

Now we need a rule that says “an with semantics followed by a with
semantics yields a sentence whose semantics is the result of applying to
:”

S(()) () ()  .

The rule tells us that the semantic interpretation of “John loves Mary”
is

()() ,

which is equivalent to ${Loves}({John},{Mary})$.

The rest of the semantics follows in a straightforward way from the
choices we have made so far. Because s are represented as predicates, it
is a good idea to be consistent and represent verbs as predicates as
well. The verb “loves” is represented as
$\Lam{y}{\Lam{x}{{Loves}(x,y)}}$, the predicate that, when given the
argument ${Mary}$, returns the predicate
$\Lam{x}{{Loves}(x,{Mary})}$. We end up with the grammar shown in
and the parse tree shown in . We could just as easily have added
semantics to ; we chose to work with so that the reader can focus on one
type of augmentation at a time.

Adding semantic augmentations to a grammar by hand is laborious and
error prone. Therefore, there have been several projects to learn
semantic augmentations from examples. @Zelle+Mooney:1996 is an inductive
logic programming (ILP) program that learns a grammar and a specialized
parser for that grammar from examples. The target domain is natural
language database queries. The training examples consist of pairs of
word strings and corresponding semantic forms—for example;

> What is the capital of the state with the largest population?\
> ${Answer}(c, {Capital}(s, c) \land {Largest}(p, {State}(s) \land {Population}(s, p)))$

’s task is to learn a predicate ${Parse}({words},
{semantics})$ that is consistent with the examples and, hopefully,
generalizes well to other examples. Applying ILP directly to learn this
predicate results in poor performance: the induced parser has only about
20% accuracy. Fortunately, ILP learners can improve by adding knowledge.
In this case, most of the ${Parse}$ predicate was defined as a logic
program, and ’s task was reduced to inducing the control rules that
guide the parser to select one parse over another. With this additional
background knowledge, can learn to achieve 70% to 85% accuracy on
various database query tasks.

[john-mary-grammar-figure]

[john-mary-semantics-figure]

### Complications

The grammar of real English is endlessly complex. We will briefly
mention some examples.

: Suppose we want to represent the difference between “John loves Mary”
and “John loved Mary.” English uses verb tenses (past, present, and
future) to indicate the relative time of an event. One good choice to
represent the time of events is the event calculus notation of . In
event calculus we have

E~1~ (, ) (, (E~1~))\
 E~2~ (, ) (, (E~2~))  .

This suggests that our two lexical rules for the words “loves” and
“loved” should be these:

()\
(e (x, y) (, e))  .

Other than this change, everything else about the grammar remains the
same, which is encouraging news; it suggests we are on the right track
if we can so easily add a complication like the tense of verbs (although
we have just scratched the surface of a complete grammar for time and
tense). It is also encouraging that the distinction between processes
and discrete events that we made in our discussion of knowledge
representation in is actually reflected in language use. We can say
“John slept a lot last night,” where ${Sleeping}$ is a process
category, but it is odd to say “John found a unicorn a lot last night,”
where ${Finding}$ is a discrete event category. A grammar would
reflect that fact by having a low probability for adding the adverbial
phrase “a lot” to discrete events.

: Consider the sentence “Every agent feels a breeze.” The sentence has
only one syntactic parse under , but it is actually semantically
ambiguous; the preferred meaning is “For every agent there exists a
breeze that the agent feels,” but an acceptable alternative meaning is
“There exists a breeze that every agent feels.”[^5] The two
interpretations can be represented as

a\
 b e (a,b) (,e) ;\
 b a\
 e (a,b) (,e) .

The standard approach to quantification is for the grammar to define not
an actual logical semantic sentence, but rather a that is then turned
into a logical sentence by algorithms outside of the parsing process.
Those algorithms can have preference rules for preferring one quantifier
scope over another—preferences that need not be reflected directly in
the grammar.

: We have shown how an agent can perceive a string of words and use a
grammar to derive a set of possible semantic interpretations. Now we
address the problem of completing the interpretation by adding
context-dependent information about the current situation. The most
obvious need for pragmatic information is in resolving the meaning of ,
which are phrases that refer directly to the current situation. For
example, in the sentence “I am in Boston today,” both “I” and “today”
are indexicals. The word “I” would be represented by the fluent
${Speaker}$, and it would be up to the hearer to resolve the meaning
of the fluent—that is not considered part of the grammar but rather an
issue of pragmatics; of using the context of the current situation to
interpret fluents.

Another part of pragmatics is interpreting the speaker’s intent. The
speaker’s action is considered a , and it is up to the hearer to
decipher what type of action it is—a question, a statement, a promise, a
warning, a command, and so on. A command such as “go to 2 2” implicitly
refers to the hearer. So far, our grammar for $\bnf{S}$ covers only
declarative sentences. We can easily extend it to cover commands. A
command can be formed from a , where the subject is implicitly the
hearer. We need to distinguish commands from statements, so we alter the
rules for to include the type of speech act:

S((, ())) () ()\
S((, ())) ()  .\

: Questions introduce a new grammatical complexity. In “Who did the
agent tell you to give the gold to?” the final word “to” should be
parsed as [ to ], where the “” denotes a gap or where an is missing; the
missing is licensed by the first word of the sentence, “who.” A complex
system of augmentations is used to make sure that the missing s match up
with the licensing words in just the right way, and prohibit gaps in the
wrong places. For example, you can’t have a gap in one branch of an
conjunction: “What did he play [ Dungeons and ]?” is ungrammatical. But
you can have the same gap in both branches of a conjunction: “What did
you [ [ smell ] and [ shoot an arrow at ]]?”

[ambiguity-section] : In some cases, hearers are consciously aware of
ambiguity in an utterance. Here are some examples taken from newspaper
headlines:

> Squad helps dog bite victim.\
> Police begin campaign to run down jaywalkers.\
> Helicopter powered by human flies.\
> Once-sagging cloth diaper industry saved by full dumps.\
> Portable toilet bombed; police have nothing to go on.\
> Teacher strikes idle kids.\
> Include your children when baking cookies.\
> Hospitals are sued by 7 foot doctors.\
> Milk drinkers are turning to powder.\
> Safety experts say school bus passengers should be belted.

But most of the time the language we hear seems unambiguous. Thus, when
researchers first began to use computers to analyze language in the
1960s, they were quite surprised to learn that

almost every utterance is highly ambiguous, even though the alternative
interpretations might not be apparent to a native speaker.

A system with a large grammar and lexicon might find thousands of
interpretations for a perfectly ordinary sentence. , in which a word has
more than one meaning, is quite common; “back” can be an adverb (go
back), an adjective (back door), a noun (the back of the room) or a verb
(back up your files). “Jack” can be a name, a noun (a playing card, a
six-pointed metal game piece, a nautical flag, a fish, a socket, or a
device for raising heavy objects), or a verb (to jack up a car, to hunt
with a light, or to hit a baseball hard). refers to a phrase that has
multiple parses: “I smelled a wumpus in 2,2” has two parses: one where
the prepositional phrase “in 2,2” modifies the noun and one where it
modifies the verb. The syntactic ambiguity leads to a , because one
parse means that the wumpus is in 2,2 and the other means that a stench
is in 2,2. In this case, getting the wrong interpretation could be a
deadly mistake for the agent.

Finally, there can be ambiguity between literal and figurative meanings.
Figures of speech are important in poetry, but are surprisingly common
in everyday speech as well. A is a figure of speech in which one object
is used to stand for another. When we hear “Chrysler announced a new
model,” we do not interpret it as saying that companies can talk; rather
we understand that a spokesperson representing the company made the
announcement. Metonymy is common and is often interpreted unconsciously
by human hearers. Unfortunately, our grammar as it is written is not so
facile. To handle the semantics of metonymy properly, we need to
introduce a whole new level of ambiguity. We do this by providing
*two* objects for the semantic interpretation of every
phrase in the sentence: one for the object that the phrase literally
refers to (Chrysler) and one for the metonymic reference (the
spokesperson). We then have to say that there is a relation between the
two. In our current grammar, “Chrysler announced” gets interpreted as

x = e (x) (, (e)) .

We need to change that to

x = e (m) (, (e))\
 (m,x) .

This says that there is one entity $x$ that is equal to Chrysler, and
another entity $m$ that did the announcing, and that the two are in a
metonymy relation. The next step is to define what kinds of metonymy
relations can occur. The simplest case is when there is no metonymy at
all—the literal object $x$ and the metonymic object $m$ are identical:

(m = x) (m,x) .

For the Chrysler example, a reasonable generalization is that an
organization can be used to stand for a spokesperson of that
organization:

x (m,x) (m,x) .

Other metonymies include the author for the works (I read
*Shakespeare*) or more generally the producer for the
product (I drive a *Honda*) and the part for the whole (The
Red Sox need a strong *arm*). Some examples of metonymy,
such as “The *ham sandwich* on Table 4 wants another beer,”
are more novel and are interpreted with respect to a situation.

A is another figure of speech, in which a phrase with one literal
meaning is used to suggest a different meaning by way of an analogy.
Thus, metaphor can be seen as a kind of metonymy where the relation is
one of similarity.

is the process of recovering the most probable intended meaning of an
utterance. In one sense we already have a framework for solving this
problem: each rule has a probability associated with it, so the
probability of an interpretation is the product of the probabilities of
the rules that led to the interpretation. Unfortunately, the
probabilities reflect how common the phrases are in the corpus from
which the grammar was learned, and thus reflect general knowledge, not
specific knowledge of the current situation. To do disambiguation
properly, we need to combine four models:

1.  The : the likelihood that a proposition occurs in the world. Given
    what we know about the world, it is more likely that a speaker who
    says “I’m dead” means “I am in big trouble” rather than “My life
    ended, and yet I can still talk.”

2.  The : the likelihood that the speaker forms the intention of
    communicating a certain fact to the hearer. This approach combines
    models of what the speaker believes, what the speaker believes the
    hearer believes, and so on. For example, when a politician says, “I
    am not a crook,” the world model might assign a probability of only
    50% to the proposition that the politician is not a criminal, and
    99.999% to the proposition that he is not a hooked shepherd’s staff.
    Nevertheless, we select the former interpretation because it is a
    more likely thing to say.

3.  The : the likelihood that a certain string of words will be chosen,
    given that the speaker has the intention of communicating a certain
    fact.

4.  The : for spoken communication, the likelihood that a particular
    sequence of sounds will be generated, given that the speaker has
    chosen a given string of words. covers speech recognition.

Machine Translation {#mt-section}
-------------------

Machine translation is the automatic translation of text from one
natural language (the source) to another (the target). It was one of the
first application areas envisioned for computers @Weaver:1949, but it is
only in the past decade that the technology has seen widespread usage.
Here is a passage from of this book:

AI is one of the newest fields in science and engineering. Work started
in earnest soon after World War II, and the name itself was coined in
1956. Along with molecular biology, AI is regularly cited as the “field
I would most like to be in” by scientists in other disciplines.

And here it is translated from English to by an online tool, :

AI er en af de nyeste områder inden for videnskab og
teknik. Arbejde startede for alvor lige efter Anden Verdenskrig, og
navnet i sig selv var opfundet i 1956. Sammen med molekylær
biologi, er AI jævnligt nævnt som “feltet Jeg
ville de fleste gerne være i” af forskere i andre
discipliner.

For those who don’t read Danish, here is the Danish translated back to
English. The words that came out different are in italics:

AI is one of the newest fields *of* science and
engineering. Work *began* in earnest *just*
after the *Second* World War, and the name itself was
*invented* in 1956. *Together* with molecular
biology, AI is *frequently mentioned* as
$\textvisiblespace$ “field I would most like to be in” by
*researchers* in other disciplines.

The differences are all reasonable paraphrases, such as
*frequently mentioned* for *regularly cited*.
The only real error is the omission of the article *the*,
denoted by the $\textvisiblespace$ symbol. This is typical accuracy: of
the two sentences, one has an error that would not be made by a native
speaker, yet the meaning is clearly conveyed.

Historically, there have been three main applications of machine
translation. *Rough translation*, as provided by free
online services, gives the “gist” of a foreign sentence or document, but
contains errors. *Pre-edited translation* is used by
companies to publish their documentation and sales materials in multiple
languages. The original source text is written in a constrained language
that is easier to translate automatically, and the results are usually
edited by a human to correct any errors. *Restricted-source
translation* works fully automatically, but only on highly
stereotypical language, such as a weather report.

Translation is difficult because, in the fully general case, it requires
in-depth understanding of the text. This is true even for very simple
texts—even “texts” of one word. Consider the word “Open” on the door of
a store.[^6] It communicates the idea that the store is accepting
customers at the moment. Now consider the same word “Open” on a large
banner outside a newly constructed store. It means that the store is now
in daily operation, but readers of this sign would not feel misled if
the store closed at night without removing the banner. The two signs use
the identical word to convey different meanings. In German the sign on
the door would be “Offen” while the banner would read “Neu Eröffnet.”

The problem is that different languages categorize the world
differently. For example, the French word “doux” covers a wide range of
meanings corresponding approximately to the English words “soft,”
“sweet,” and “gentle.” Similarly, the English word “hard” covers
virtually all uses of the German word “hart” (physically recalcitrant,
cruel) and some uses of the word “schwierig” (difficult). Therefore,
representing the meaning of a sentence is more difficult for translation
than it is for single-language understanding. An English parsing system
could use predicates like ${Open}(x)$, but for translation, the
representation language would have to make more distinctions, perhaps
with ${Open}{}_{1}(x)$ representing the “Offen” sense and
${Open}{}_{2}(x)$ representing the “Neu Eröffnet” sense. A
representation language that makes all the distinctions necessary for a
set of languages is called an .

A translator (human or machine) often needs to understand the actual
situation described in the source, not just the individual words. For
example, to translate the English word “him,” into Korean, a choice must
be made between the humble and honorific form, a choice that depends on
the social relationship between the speaker and the referent of “him.”
In Japanese, the honorifics are relative, so the choice depends on the
social relationships between the speaker, the referent, and the
listener. Translators (both machine and human) sometimes find it
difficult to make this choice. As another example, to translate “The
baseball hit the window. It broke.” into French, we must choose the
feminine “elle” or the masculine “il” for “it,” so we must decide
whether “it” refers to the baseball or the window. To get the
translation right, one must understand physics as well as language.

Sometimes there is *no choice* that can yield a completely
satisfactory translation. For example, an Italian love poem that uses
the masculine “il sole” (sun) and feminine “la luna” (moon) to symbolize
two lovers will necessarily be altered when translated into German,
where the genders are reversed, and further altered when translated into
a language where the genders are the same.[^7]

### Machine translation systems

All translation systems must model the source and target languages, but
systems vary in the type of models they use. Some systems attempt to
analyze the source language text all the way into an interlingua
knowledge representation and then generate sentences in the target
language from that representation. This is difficult because it involves
three unsolved problems: creating a complete knowledge representation of
everything; parsing into that representation; and generating sentences
from that representation.

Other systems are based on a . They keep a database of translation rules
(or examples), and whenever the rule (or example) matches, they
translate directly. Transfer can occur at the lexical, syntactic, or
semantic level. For example, a strictly syntactic rule maps English
[*Adjective Noun*] to French [*Noun
Adjective*]. A mixed syntactic and lexical rule maps French
[$S_1$ “et puis” $S_2$] to English [$S_1$ “and then” $S_2$]. diagrams
the various transfer points.

[mt-interlingua-figure]

### Statistical machine translation

Now that we have seen how complex the translation task can be, it should
come as no surprise that the most successful machine translation systems
are built by training a probabilistic model using statistics gathered
from a large corpus of text. This approach does not need a complex
ontology of interlingua concepts, nor does it need handcrafted grammars
of the source and target languages, nor a hand-labeled treebank. All it
needs is data—sample translations from which a translation model can be
learned. To translate a sentence in, say, English $(e)$ into French
$(f)$, we find the string of words $f^*$ that maximizes
$$f^* = \argmax_f P(f\given e) = \argmax P(e \given f)\, P(f) \ .$$ Here
the factor $P(f)$ is the target for French; it says how probable a given
sentence is in French. $P(e|f)$ is the ; it says how probable an English
sentence is as a translation for a given French sentence. Similarly,
$P(f\given e)$ is a translation model from English to French.

Should we work directly on $P(f\given e)$, or apply Bayes’ rule and work
on $P(e \given f)\, P(f)$? In applications like medicine, it is easier
to model the domain in the causal direction:
$P({symptoms}\given {disease})$ rather than
$P({disease}\given {symptoms})$. But in translation both directions
are equally easy. The earliest work in statistical machine translation
did apply Bayes’ rule—in part because the researchers had a good
language model, $P(f)$, and wanted to make use of it, and in part
because they came from a background in speech recognition, which
*is* a diagnostic problem. We follow their lead in this
chapter, but we note that recent work in statistical machine translation
often optimizes $P(f\given e)$ directly, using a more sophisticated
model that takes into account many of the features from the language
model.

The language model, $P(f)$, could address any level(s) on the right-hand
side of , but the easiest and most common approach is to build an
$n$-gram model from a French corpus, as we have seen before. This
captures only a partial, local idea of French sentences; however, that
is often sufficient for rough translation.[^8]

The translation model is learned from a —a collection of parallel texts,
each an English/French pair. Now, if we had an infinitely large corpus,
then translating a sentence would just be a lookup task: we would have
seen the English sentence before in the corpus, so we could just return
the paired French sentence. But of course our resources are finite, and
most of the sentences we will be asked to translate will be novel.
However, they will be composed of that we have seen before (even if some
phrases are as short as one word). For example, in this book, common
phrases include “in this exercise we will,” “size of the state space,”
“as a function of the” and “notes at the end of the chapter.” If asked
to translate the novel sentence “In this exercise we will compute the
size of the state space as a function of the number of actions.” into
French, we should be able to break the sentence into phrases, find the
phrases in the English corpus (this book), find the corresponding French
phrases (from the French translation of the book), and then reassemble
the French phrases into an order that makes sense in French. In other
words, given a source English sentence, $e$, finding a French
translation $f$ is a matter of three steps:

1.  Break the English sentence into phrases $e_1,\ldots,e_n$.

2.  For each phrase $e_i$, choose a corresponding French phrase $f_i$.
    We use the notation $P(f_i\given e_i)$ for the phrasal probability
    that $f_i$ is a translation of $e_i$.

3.  Choose a permutation of the phrases $f_1,\ldots,f_n$. We will
    specify this permutation in a way that seems a little complicated,
    but is designed to have a simple probability distribution: For each
    $f_i$, we choose a $d_i$, which is the number of words that phrase
    $f_i$ has moved with respect to $f_{i-1}$; positive for moving to
    the right, negative for moving to the left, and zero if $f_i$
    immediately follows $f_{i-1}$.

shows an example of the process. At the top, the sentence “There is a
smelly wumpus sleeping in 2 2” is broken into five phrases,
$e_1,\ldots,e_5$. Each of them is translated into a corresponding phrase
$f_i$, and then these are permuted into the order
$f_1, f_3, f_4, f_2, f_5$. We specify the permutation in terms of the
distortions $d_i$ of each French phrase, defined as
$$d_i = \noprog{Start}(f_i) - \noprog{End}(f_{i-1}) - 1 \ ,$$ where
$\noprog{Start}(f_i)$ is the ordinal number of the first word of phrase
$f_i$ in the French sentence, and $\noprog{End}(f_{i-1})$ is the ordinal
number of the last word of phrase $f_{i-1}$. In we see that $f_5$, “à 2
2,” immediately follows $f_4$, “qui dort,” and thus $d_5\eq 0$. Phrase
$f_2$, however, has moved one words to the right of $f_1$, so
$d_2\eq 1$. As a special case we have $d_1\eq  0$, because $f_1$ starts
at position 1 and $\noprog{End}(f_0)$ is defined to be 0 (even though
$f_0$ does not exist).

Now that we have defined the distortion, $d_i$, we can define the
probability distribution for distortion, $\pv(d_i)$. Note that for
sentences bounded by length $n$ we have $|d_i| \le n$ , and so the full
probability distribution $\pv(d_i)$ has only $2n+1$ elements, far fewer
numbers to learn than the number of permutations, $n!$. That is why we
defined the permutation in this circuitous way. Of course, this is a
rather impoverished model of distortion. It doesn’t say that adjectives
are usually distorted to appear after the noun when we are translating
from English to French—that fact is represented in the French language
model, $P(f)$. The distortion probability is completely independent of
the words in the phrases—it depends only on the integer value $d_i$. The
probability distribution provides a summary of the volatility of the
permutations; how likely a distortion of $P(d\eq2)$ is, compared to
$P(d\eq0)$, for example.

We’re ready now to put it all together: we can define $P(f,d \given e)$,
the probability that the sequence of phrases $f$ with distortions $d$ is
a translation of the sequence of phrases $e$. We make the assumption
that each phrase translation and each distortion is independent of the
others, and thus we can factor the expression as
$$P(f,d \given e) = \prod_i P(f_i\given e_i) \, P(d_i) \$$

[mt-alignment-figure]

That gives us a way to compute the probability $P(f, d \given e)$ for a
candidate translation $f$ and distortion $d$. But to find the best $f$
and $d$ we can’t just enumerate sentences; with maybe $100$ French
phrases for each English phrase in the corpus, there are $100^5$
different 5-phrase translations, and $5!$ reorderings for each of those.
We will have to search for a good solution. A local beam search (see )
with a heuristic that estimates probability has proven effective at
finding a nearly-most-probable translation.

All that remains is to learn the phrasal and distortion probabilities.
We sketch the procedure; see the notes at the end of the chapter for
details.

1.  **Find parallel texts**: First, gather a parallel
    bilingual corpus. For example, a [^9] is a record of parliamentary
    debate. Canada, Hong Kong, and other countries produce bilingual
    Hansards, the European Union publishes its official documents in 11
    languages, and the United Nations publishes multilingual documents.
    Bilingual text is also available online; some Web sites publish
    parallel content with parallel URLs, for example, /en/
    for the English page and /fr/ for the corresponding
    French page. The leading statistical translation systems train on
    hundreds of millions of words of parallel text and billions of words
    of monolingual text.

2.  **Segment into sentences**: The unit of translation is
    a sentence, so we will have to break the corpus into sentences.
    Periods are strong indicators of the end of a sentence, but consider
    “Dr. J. R. Smith of Rodeo Dr. paid \$29.99 on 9.9.09.”; only the
    final period ends a sentence. One way to decide if a period ends a
    sentence is to train a model that takes as features the surrounding
    words and their parts of speech. This approach achieves about 98%
    accuracy.

3.  **Align sentences**: For each sentence in the English
    version, determine what sentence(s) it corresponds to in the French
    version. Usually, the next sentence of English corresponds to the
    next sentence of French in a 1:1 match, but sometimes there is
    variation: one sentence in one language will be split into a 2:1
    match, or the order of two sentences will be swapped, resulting in a
    2:2 match. By looking at the sentence lengths alone (i.e. short
    sentences should align with short sentences), it is possible to
    align them (1:1, 1:2, or 2:2, etc.) with accuracy in the 90% to 99%
    range using a variation on the Viterbi algorithm. Even better
    alignment can be achieved by using landmarks that are common to both
    languages, such as numbers, dates, proper names, or words that we
    know from a bilingual dictionary have an unambiguous translation.
    For example, if the 3rd English and 4th French sentences contain the
    string “1989” and neighboring sentences do not, that is good
    evidence that the sentences should be aligned together.

4.  **Align phrases**: Within a sentence, phrases can be
    aligned by a process that is similar to that used for sentence
    alignment, but requiring iterative improvement. When we start, we
    have no way of knowing that “qui dort” aligns with “sleeping,” but
    we can arrive at that alignment by a process of aggregation of
    evidence. Over all the example sentences we have seen, we notice
    that “qui dort” and “sleeping” co-occur with high frequency, and
    that in the pair of aligned sentences, no phrase other than “qui
    dort” co-occurs so frequently in other sentences with “sleeping.” A
    complete phrase alignment over our corpus gives us the phrasal
    probabilities (after appropriate smoothing).

5.  **Extract distortions**: Once we have an alignment of
    phrases we can define distortion probabilities. Simply count how
    often distortion occurs in the corpus for each distance
    $d = 0, \pm 1, \pm 2,
    \ldots$, and apply smoothing.

6.  **Improve estimates with EM**: Use
    expectation–maximization to improve the estimates of $P(f\given e)$
    and $P(d)$ values. We compute the best alignments with the current
    values of these parameters in the E step, then update the estimates
    in the M step and iterate the process until convergence.

Speech Recognition {#speech-recognition-section}
------------------

is the task of identifying a sequence of words uttered by a speaker,
given the acoustic signal. It has become one of the mainstream
applications of AI—millions of people interact with speech recognition
systems every day to navigate voice mail systems, search the Web from
mobile phones, and other applications. Speech is an attractive option
when hands-free operation is necessary, as when operating machinery.

Speech recognition is difficult because the sounds made by a speaker are
ambiguous and, well, noisy. As a well-known example, the phrase
“recognize speech” sounds almost the same as “wreck a nice beach” when
spoken quickly. Even this short example shows several of the issues that
make speech problematic. First, : written words in English have spaces
between them, but in fast speech there are no pauses in “wreck a nice”
that would distinguish it as a multiword phrase as opposed to the single
word “recognize.” Second, : when speaking quickly the “s” sound at the
end of “nice” merges with the “b” sound at the beginning of “beach,”
yielding something that is close to a “sp.” Another problem that does
not show up in this example is —words like “to,” “too,” and “two” that
sound the same but differ in meaning.

We can view speech recognition as a problem in most-likely-sequence
explanation. As we saw in , this is the problem of computing the most
likely sequence of state variables, $\x_{1:t}$, given a sequence of
observations $\e_{1:t}$. In this case the state variables are the words,
and the observations are sounds. More precisely, an observation is a
vector of features extracted from the audio signal. As usual, the most
likely sequence can be computed with the help of Bayes’ rule to be:
$$\argmax_{{word}_{1:t}} P({word}_{1:t}\given {sound}_{1:t}) 
   = \argmax_{{word}_{1:t}} P({sound}_{1:t}\given{word}_{1:t}) P({word}_{1:t})\ .$$
Here $P({sound}_{1:t}|{word}_{1:t})$ is the . It describes the
sounds of words—that “ceiling” begins with a soft “c” and sounds the
same as “sealing.” $P({word}_{1:t})$ is known as the . It specifies
the prior probability of each utterance—for example, that “ceiling fan”
is about 500 times more likely as a word sequence than “sealing fan.”

This approach was named the by Claude . He described a situation in
which an original message (the *words* in our example) is
transmitted over a noisy channel (such as a telephone line) such that a
corrupted message (the *sounds* in our example) are
received at the other end. Shannon showed that no matter how noisy the
channel, it is possible to recover the original message with arbitrarily
small error, if we encode the original message in a redundant enough
way. The noisy channel approach has been applied to speech recognition,
machine translation, spelling correction, and other tasks.

Once we define the acoustic and language models, we can solve for the
most likely sequence of words using the Viterbi algorithm ( on ). Most
speech recognition systems use a language model that makes the Markov
assumption—that the current state ${Word}_t$ depends only on a fixed
number $n$ of previous states—and represent ${Word}_t$ as a single
random variable taking on a finite set of values, which makes it a
Hidden Markov Model (HMM). Thus, speech recognition becomes a simple
application of the HMM methodology, as described in —simple that is,
once we define the acoustic and language models. We cover them next.

[tbp] [darpabet-table]

### Acoustic model {#acoustic model}

Sound waves are periodic changes in pressure that propagate through the
air. When these waves strike the diaphragm of a microphone, the
back-and-forth movement generates an electric current. An
analog-to-digital converter measures the size of the current—which
approximates the amplitude of the sound wave—at discrete intervals
called the . Speech sounds, which are mostly in the range of 100 Hz (100
cycles per second) to 1000 Hz, are typically sampled at a rate of 8 kHz.
(CDs and mp3 files are sampled at 44.1 kHz.) The precision of each
measurement is determined by the ; speech recognizers typically keep 8
to 12 bits. That means that a low-end system, sampling at 8 kHz with
8-bit quantization, would require nearly half a megabyte per minute of
speech.

Since we only want to know what words were spoken, not exactly what they
sounded like, we don’t need to keep all that information. We only need
to distinguish between different speech sounds. Linguists have
identified about 100 speech sounds, or , that can be composed to form
all the words in all known human languages. Roughly speaking, a phone is
the sound that corresponds to a single vowel or consonant, but there are
some complications: combinations of letters, such as “th” and “ng”
produce single phones, and some letters produce different phones in
different contexts (e.g., the “a” in *rat* and
*rate*. lists all the phones that are used in English, with
an example of each. A is the smallest unit of sound that has a distinct
meaning to speakers of a particular language. For example, the “t” in
“stick” sounds similar enough to the “t” in “tick” that speakers of
English consider them the same phoneme. But the difference is
significant in the Thai language, so there they are two phonemes. To
represent spoken English we want a representation that can distinguish
between different phonemes, but one that need not distinguish the
nonphonemic variations in sound: loud or soft, fast or slow, male or
female voice, etc.

First, we observe that although the sound frequencies in speech may be
several kHz, the *changes* in the content of the signal
occur much less often, perhaps at no more than 100 Hz. Therefore, speech
systems summarize the properties of the signal over time slices called .
A frame length of about 10 milliseconds (i.e., 80 samples at 8 kHz) is
short enough to ensure that few short-duration phenomena will be missed.
Overlapping frames are used to make sure that we don’t miss a signal
because it happens to fall on a frame boundary.

Each frame is summarized by a vector of . Picking out features from a
speech signal is like listening to an orchestra and saying “here the
French horns are playing loudly and the violins are playing softly.”
We’ll give a brief overview of the features in a typical system. First,
a Fourier transform is used to determine the amount of acoustic energy
at about a dozen frequencies. Then we compute a measure called the or
MFCC for each frequency. We also compute the total energy in the frame.
That gives thirteen features; for each one we compute the difference
between this frame and the previous frame, and the difference between
differences, for a total of 39 features. These are continuous-valued;
the easiest way to fit them into the HMM framework is to discretize the
values. (It is also possible to extend the HMM model to handle
continuous mixtures of Gaussians.) shows the sequence of transformations
from the raw sound to a sequence of frames with discrete features.

[sr-acoustic-frames-figure]

[sr-hmm-figure]

[sr-tomato-figure]

We have seen how to go from the raw acoustic signal to a series of
observations, $\e_t$. Now we have to describe the (unobservable) states
of the HMM and define the transition model, $\pv(\X_t\given
\X_{t-1})$, and the sensor model, $\pv(\E_t\given \X_t)$. The transition
model can be broken into two levels: word and phone. We’ll start from
the bottom: the describes a phone as three states, the onset, middle,
and end. For example, the [t] phone has a silent beginning, a small
explosive burst of sound in the middle, and (usually) a hissing at the
end. shows an example for the phone [m]. Note that in normal speech, an
average phone has a duration of 50–100 milliseconds, or 5–10 frames. The
self-loops in each state allows for variation in this duration. By
taking many self-loops (especially in the mid state), we can represent a
long “mmmmmmmmmmm” sound. Bypassing the self-loops yields a short “m”
sound.

In the phone models are strung together to form a for a word. According
to , you say [t ow m ey t ow] and I say [t ow m aa t ow]. (a) shows a
transition model that provides for this dialect variation. Each of the
circles in this diagram represents a phone model like the one in .

In addition to dialect variation, words can have variation. For example,
the [t] phone is produced with the tongue at the top of the mouth,
whereas the [ow] has the tongue near the bottom. When speaking quickly,
the tongue doesn’t have time to get into position for the [ow], and we
end up with [t ah] rather than [t ow]. (b) gives a model for “tomato”
that takes this coarticulation effect into account. More sophisticated
phone models take into account the context of the surrounding phones.

There can be substantial variation in pronunciation for a word. The most
common pronunciation of “because” is [b iy k ah z], but that only
accounts for about a quarter of uses. Another quarter (approximately)
substitutes [ix], [ih] or [ax] for the first vowel, and the remainder
substitute [ax] or [aa] for the second vowel, [zh] or [s] for the final
[z], or drop “be” entirely, leaving “cuz.”

### Language model

For general-purpose speech recognition, the language model can be an
$n$-gram model of text learned from a corpus of written sentences.
However, spoken language has different characteristics than written
language, so it is better to get a corpus of transcripts of spoken
language. For task-specific speech recognition, the corpus should be
task-specific: to build your airline reservation system, get transcripts
of prior calls. It also helps to have task-specific vocabulary, such as
a list of all the airports and cities served, and all the flight
numbers.

Part of the design of a voice user interface is to coerce the user into
saying things from a limited set of options, so that the speech
recognizer will have a tighter probability distribution to deal with.
For example, asking “What city do you want to go to?” elicits a response
with a highly constrained language model, while asking “How can I help
you?” does not.

### Building a speech recognizer

The quality of a speech recognition system depends on the quality of all
of its components—the language model, the word-pronunciation models, the
phone models, and the signal-processing algorithms used to extract
spectral features from the acoustic signal. We have discussed how the
language model can be constructed from a corpus of written text, and we
leave the details of signal processing to other textbooks. We are left
with the pronunciation and phone models. The *structure* of
the pronunciation models—such as the tomato models in —is usually
developed by hand. Large pronunciation dictionaries are now available
for English and other languages, although their accuracy varies greatly.
The structure of the three-state phone models is the same for all
phones, as shown in . That leaves the probabilities themselves.

As usual, we will acquire the probabilities from a corpus, this time a
corpus of speech. The most common type of corpus to obtain is one that
includes the speech signal for each sentence paired with a transcript of
the words. Building a model from this corpus is more difficult than
building an $n$-gram model of text, because we have to build a hidden
Markov model—the phone sequence for each word and the phone state for
each time frame are hidden variables. In the early days of speech
recognition, the hidden variables were provided by laborious
hand-labeling of spectrograms. Recent systems use
expectation–maximization to automatically supply the missing data. The
idea is simple: given an HMM and an observation sequence, we can use the
smoothing algorithms from Sections [markov-inference-section]
and [hmm-section] to compute the probability of each state at each time
step and, by a simple extension, the probability of each state–state
pair at consecutive time steps. These probabilities can be viewed as
*uncertain labels*. From the uncertain labels, we can
estimate new transition and sensor probabilities, and the EM procedure
repeats. The method is guaranteed to increase the fit between model and
data on each iteration, and it generally converges to a much better set
of parameter values than those provided by the initial, hand-labeled
estimates.

The systems with the highest accuracy work by training a different model
for each speaker, thereby capturing differences in dialect as well as
male/female and other variations. This training can require several
hours of interaction with the speaker, so the systems with the most
widespread adoption do not create speaker-specific models.

The accuracy of a system depends on a number of factors. First, the
quality of the signal matters: a high-quality directional microphone
aimed at a stationary mouth in a padded room will do much better than a
cheap microphone transmitting a signal over phone lines from a car in
traffic with the radio playing. The vocabulary size matters: when
recognizing digit strings with a vocabulary of 11 words (1-9 plus “oh”
and “zero”), the word error rate will be below 0.5%, whereas it rises to
about 10% on news stories with a 20,000-word vocabulary, and 20% on a
corpus with a 64,000-word vocabulary. The task matters too: when the
system is trying to accomplish a specific task—book a flight or give
directions to a restaurant—the task can often be accomplished perfectly
even with a word error rate of 10% or more.

Natural language understanding is one of the most important subfields of
AI. Unlike most other areas of AI, natural language understanding
requires an empirical investigation of actual human behavior—which turns
out to be complex and interesting.

-   Formal language theory and grammars (and in particular, grammar) are
    useful tools for dealing with some aspects of natural language. The
    probabilistic context-free grammar (PCFG) formalism is widely used.

-   Sentences in a context-free language can be parsed in $O(n^3)$ time
    by a such as the , which requires grammar rules to be in .

-   A can be used to learn a grammar. It is also possible to learn a
    grammar from an unparsed corpus of sentences, but this is less
    successful.

-   A allows us to represent that some relationships between words are
    more common than others.

-   It is convenient to a grammar to handle such problems as
    subject–verb agreement and pronoun case. (DCG) is a formalism that
    allows for augmentations. With DCG, parsing and semantic
    interpretation (and even generation) can be done using logical
    inference.

-   can also be handled by an augmented grammar.

-   is a very important problem in natural language understanding; most
    sentences have many possible interpretations, but usually only one
    is appropriate. Disambiguation relies on knowledge about the world,
    about the current situation, and about language use.

-   systems have been implemented using a range of techniques, from full
    syntactic and semantic analysis to statistical techniques based on
    phrase frequencies. Currently the statistical models are most
    popular and most successful.

-   systems are also primarily based on statistical principles. Speech
    systems are popular and useful, albeit imperfect.

-   Together, machine translation and speech recognition are two of the
    big successes of natural language technology. One reason that the
    models perform well is that large corpora are available—both
    translation and speech are tasks that are performed “in the wild” by
    people every day. In contrast, tasks like parsing sentences have
    been less successful, in part because no large corpora of parsed
    sentences are available “in the wild” and in part because parsing is
    not useful in and of itself.

Like semantic networks, context-free grammars (also known as phrase
structure grammars) are a reinvention of a technique first used by
ancient Indian grammarians (especially Panini, ca. 350
b.c.) studying Shastric Sanskrit @Ingerman:1967. They were
reinvented by Noam Chomsky [-@Chomsky:1956] for the analysis of English
syntax and independently by John Backus for the analysis of Algol-58
syntax. Peter Naur extended Backus’s notation and is now credited
@Backus:1996 with the “N” in BNF, which originally stood for “Backus
**N**ormal Form.” Knuth [-@Knuth:1968] defined a kind of
augmented grammar called that is useful for programming languages.
Definite clause grammars were introduced by
Colmerauer [-@Colmerauer:1975] and developed and popularized by .

were investigated by and . Other algorithms for PCFGs are presented in
the excellent short monograph by and the excellent long textbooks by and
. introduces the inside–outside algorithm for learning a PCFG, and
describe its uses and limitations. show how to learn grammar rules with
Bayesian model merging; describe a learning system based on prototypes.

@Charniak:1997 [@Hwa:1998] combine the best aspects of PCFGs and
$n$-gram models. describes PCFG parsing that is lexicalized with head
features. show how to get the advantages of lexicalization without
actual lexical augmentations by learning specific syntactic categories
from a treebank that has general categories; for example, the treebank
has the category , from which more specific categories such as
$\bnf{NP}_O$ and $\bnf{NP}_S$ can be learned.

There have been many attempts to write formal grammars of natural
languages, both in “pure” linguistics and in computational linguistics.
There are several comprehensive but informal grammars of English
@Quirk+al:1985 [@McCawley:1988; @Huddleston+Pullum:2002]. Since the
mid-1980s, there has been a trend toward putting more information in the
lexicon and less in the grammar. Lexical-functional grammar, or LFG
@Bresnan:1982 was the first major grammar formalism to be highly
lexicalized. If we carry lexicalization to an extreme, we end up with
@Clark+Curran:2004, in which there can be as few as two grammar rules,
or with @Smith+Eisner:2008 [@Kubler+al:2009] in which there are no
syntactic categories, only relations between words. Sleator and
Temperley [-@Sleator+Temperley:1993] describe a dependency parser. shows
that a version of dependency grammar is easier to learn than PCFGs.

The first computerized parsing algorithms were demonstrated by
Yngve [-@Yngve:1955]. Efficient algorithms were developed in the late
1960s, with a few twists since then @Kasami:1965
[@Younger:1967; @Earley:1970; @Graham+al:1980]. Maxwell and
Kaplan [-@Maxwell+Kaplan:1993] show how chart parsing with augmentations
can be made efficient in the average case. Church and
Patil [-@Church+Patil:1982] address the resolution of syntactic
ambiguity. describe A parsing, and extend that to
*K*-best A parsing, in which the result is not
a single parse but the *K* best.

Leading parsers today include those by , which achieved 90.6% accuracy
on the Wall Street Journal corpus, , which achieved 92.0%, and , which
achieved 93.2% on the Penn treebank. These numbers are not directly
comparable, and there is some criticism of the field that it is focusing
too narrowly on a few select corpora, and perhaps overfitting on them.

Formal semantic interpretation of natural languages originates within
philosophy and formal logic, particularly Alfred Tarski’s
[-@Tarski:1935] work on the semantics of formal languages. Bar-Hillel
[-@Bar-Hillel:1954] was the first to consider the problems of pragmatics
and propose that they could be handled by formal logic. For example, he
introduced C. S. Peirce’s [-@Peirce:1902] term *indexical*
into linguistics. Richard Montague’s essay “English as a formal
language” [-@Montague:1970] is a kind of manifesto for the logical
analysis of language, but the books by and are more readable.

The first NLP system to solve an actual task was probably the question
answering system @Green+al:1961, which handled questions about a
database of baseball statistics. Close after that was Woods’s
[-@Woods:1973] , which answered questions about the rocks brought back
from the moon by the Apollo program. Roger Schank and his students built
a series of programs @Schank+Abelson:1977 [@Schank+Riesbeck:1981] that
all had the task of understanding language. Modern approaches to
semantic interpretation usually assume that the mapping from syntax to
semantics will be learned from examples @Zelle+Mooney:1996
[@Zettlemoyer+Collins:2005].

describes a quantitative nonprobabilistic framework for interpretation.
More recent work follows an explicitly probabilistic framework
@Charniak+Goldman:1992 [@Wu:1993; @Franz:1996]. In linguistics,
optimality theory @Kager:1999 is based on the idea of building soft
constraints into the grammar, giving a natural ranking to
interpretations (similar to a probability distribution), rather than
having the grammar generate all possibilities with equal rank. discusses
the problems of considering multiple simultaneous interpretations,
rather than settling for a single maximum-likelihood interpretation.
Literary critics @Empson:1953 [@Hobbs:1990] have been ambiguous about
whether ambiguity is something to be resolved or cherished.

Nunberg [-@Nunberg:1979] outlines a formal model of metonymy. Lakoff and
Johnson [-@Lakoff+Johnson:1980] give an engaging analysis and catalog of
common metaphors in English. Martin [-@Martin:1990] and offer
computational models of metaphor interpretation.

The first important result on was a negative one: showed that it is not
possible to reliably learn a correct context-free grammar, given a set
of strings from that grammar. Prominent linguists, such as Chomsky
[-@Chomsky:1957] and Pinker [-@Pinker:2003], have used Gold’s result to
argue that there must be an innate that all children have from birth.
The so-called *Poverty of the Stimulus* argument says that
children aren’t given enough input to learn a CFG, so they must already
“know” the grammar and be merely tuning some of its parameters. While
this argument continues to hold sway throughout much of Chomskyan
linguistics, it has been dismissed by some other linguists @Pullum:1996
[@Elman+al:1997] and most computer scientists. As early as 1969, Horning
showed that it *is* possible to learn, in the sense of PAC
learning, a *probabilistic* context-free grammar. Since
then, there have been many convincing empirical demonstrations of
learning from positive examples alone, such as the ILP work of and , the
sequence learning of , and the remarkable Ph.D. theses of and . There is
an annual International Conference on Grammatical Inference (ICGI). It
is possible to learn other grammar formalisms, such as regular languages
@Denis:2001 and finite state automata @Parekh+Honavar:2001. is a
textbook introduction to semi-supervised learning for language models.

Wordnet @Fellbaum:2001 is a publicly available dictionary of about
100,000 words and phrases, categorized into parts of speech and linked
by semantic relations such as synonym, antonym, and part-of. The Penn
Treebank @Marcus+al:1993 provides parse trees for a 3-million-word
corpus of English. and discuss parsing with treebank grammars. The
British National Corpus @Leech+al:2001 contains 100 million words, and
the World Wide Web contains several trillion words; @Brants+al:2007
describe $n$-gram models over a 2-trillion-word Web corpus.

In the 1930s Petr Troyanskii applied for a patent for a “translating
machine,” but there were no computers available to implement his ideas.
In March 1947, the ’s Warren Weaver wrote to Norbert Wiener, suggesting
that machine translation might be possible. Drawing on work in
cryptography and information theory, Weaver wrote, “When I look at an
article in Russian, I say: ‘This is really written in English, but it
has been coded in strange symbols. I will now proceed to decode.’” For
the next decade, the community tried to decode in this way. IBM
exhibited a rudimentary system in 1954. Bar-Hillel [-@Bar-Hillel:1960]
describes the enthusiasm of this period. However, the U.S. government
subsequently reported @ALPAC:1966 that “there is no immediate or
predictable prospect of useful machine translation.” However, limited
work continued, and starting in the 1980s, computer power had increased
to the point where the ALPAC findings were no longer correct.

The basic statistical approach we describe in the chapter is based on
early work by the IBM group @Brown+al:1988 [@Brown+al:1993] and the
recent work by the ISI and Google research groups @Och+Ney:2004
[@Zollmann+al:2008]. A textbook introduction on statistical machine
translation is given by , and a short tutorial by Kevin has been
influential. Early work on sentence segmentation was done by Palmer and
Hearst [-@Palmer+Hearst:1994]. and cover bilingual sentence alignment.

The prehistory of began in the 1920s with , a voice-activated toy dog.
Rex jumped out of his doghouse in response to the word “Rex!” (or
actually almost any sufficiently loud word). Somewhat more serious work
began after World War II. At AT&T , a system was built for recognizing
isolated digits @Davis+al:1952 by means of simple pattern matching of
acoustic features. Starting in 1971, the Defense Advanced Research
Projects Agency () of the Department of Defense funded four competing
five-year projects to develop high-performance speech recognition
systems. The winner, and the only system to meet the goal of 90%
accuracy with a 1000-word vocabulary, was the system at
@Lowerre+Reddy:1980. The final version of was derived from a system
called built by CMU graduate student James ; was the first to use HMMs
for speech. Almost simultaneously, at had developed another -based
system. Recent years have been characterized by steady incremental
progress, larger data sets and models, and more rigorous competitions on
more realistic speech tasks. In 1997, Bill Gates predicted, “The PC five
years from now—you won’t recognize it, because speech will come into the
interface.” That didn’t quite happen, but in 2008 he predicted “In five
years, Microsoft expects more Internet searches to be done through
speech than through typing on a keyboard.” History will tell if he is
right this time around.

Several good textbooks on are available @Rabiner+Juang:1993
[@Jelinek:1997; @Gold+Morgan:2000; @Huang+al:2001]. The presentation in
this chapter drew on the survey by Kay, Gawron, and
Norvig [-@Kay+al:1994] and on the textbook by . Speech recognition
research is published in *Computer Speech and Language*,
*Speech Communications*, and the IEEE *Transactions
on Acoustics, Speech, and Signal Processing* and at the DARPA
Workshops on Speech and Natural Language Processing and the Eurospeech,
ICSLP, and ASRU conferences.

Ken shows that natural language research has cycled between
concentrating on the data () and concentrating on theories (). The
linguist John proclaimed “You shall know a word by the company it
keeps,” and linguistics of the 1940s and early 1950s was based largely
on word frequencies, although without the computational power we have
available today. Then Noam @Chomsky:1956 showed the limitations of
finite-state models, and sparked an interest in theoretical studies of
syntax, disregarding frequency counts. This approach dominated for
twenty years, until empiricism made a comeback based on the success of
work in statistical speech recognition @Jelinek:1976. Today, most work
accepts the statistical framework, but there is great interest in
building statistical models that consider higher-level models, such as
syntactic trees and semantic relations, not just sequences of words.

Work on applications of language processing is presented at the biennial
Applied Natural Language Processing conference (ANLP), the conference on
Empirical Methods in Natural Language Processing (), and the journal
*Natural Language Engineering*. A broad range of NLP work
appears in the journal *Computational Linguistics* and its
conference, ACL, and in the Computational Linguistics (COLING)
conference.

[washing-clothes-exercise]Read the following text once for
understanding, and remember as much of it as you can. There will be a
test later.

The procedure is actually quite simple. First you arrange things into
different groups. Of course, one pile may be sufficient depending on how
much there is to do. If you have to go somewhere else due to lack of
facilities that is the next step, otherwise you are pretty well set. It
is important not to overdo things. That is, it is better to do too few
things at once than too many. In the short run this may not seem
important but complications can easily arise. A mistake is expensive as
well. At first the whole procedure will seem complicated. Soon, however,
it will become just another facet of life. It is difficult to foresee
any end to the necessity for this task in the immediate future, but then
one can never tell. After the procedure is completed one arranges the
material into different groups again. Then they can be put into their
appropriate places. Eventually they will be used once more and the whole
cycle will have to be repeated. However, this is part of life.

An *HMM grammar* is essentially a standard HMM whose state
variable is $N$ (nonterminal, with values such as $Det$, $Adjective$,
$Noun$ and so on) and whose evidence variable is $W$ (word, with values
such as $is$, $duck$, and so on). The HMM model includes a prior
$\pv(N_0)$, a transition model $\pv(N_{t+1}|N_t)$, and a sensor model
$\pv(W_t|N_t)$. Show that every HMM grammar can be written as a PCFG.
[Hint: start by thinking about how the HMM prior can be represented by
PCFG rules for the sentence symbol. You may find it helpful to
illustrate for the particular HMM with values $A$, $B$ for $N$ and
values $x$, $y$ for $W$.]

Consider the following PCFG for simple verb phrases:

0.1: VP Verb\
0.2: VP Copula Adjective\
0.5: VP Verb the Noun\
0.2: VP VP Adverb\
0.5: Verb is\
0.5: Verb shoots\
0.8: Copula is\
0.2: Copula seems\
0.5: Adjective\
0.5: Adjective\
0.5: Adverb\
0.5: Adverb\
0.6: Noun\
0.4: Noun

1.  Which of the following have a nonzero probability as a VP? (i)
    shoots the duck well well well(ii) seems the well well(iii) shoots
    the unwell well badly

2.  What is the probability of generating “is well well”?

3.  What types of ambiguity are exhibited by the phrase in (b)?

4.  Given any PCFG, is it possible to calculate the probability that the
    PCFG generates a string of exactly 10 words?

Consider the following simple PCFG for noun phrases:

0.6: NP Det AdjString Noun\
0.4: NP Det NounNounCompound\
0.5: AdjString Adj AdjString\
0.5: AdjString\
1.0: NounNounCompound Noun Noun\
0.8: Det\
0.2: Det\
0.5: Adj\
0.5: Adj\
0.6: Noun\
0.4: Noun

where $\Lambda$ denotes the empty string.

1.  What is the longest NP that can be generated by this grammar? (i)
    three words(ii) four words(iii) infinitely many words

2.  Which of the following have a nonzero probability of being generated
    as complete NPs? (i) a small green village(ii) a green green
    green(iii) a small village green

3.  What is the probability of generating “the green green”?

4.  What types of ambiguity are exhibited by the phrase in (c)?

5.  Given any PCFG and any finite word sequence, is it possible to
    calculate the probability that the sequence was generated by the
    PCFG?

Outline the major differences between Java (or any other computer
language with which you are familiar) and English, commenting on the
“understanding” problem in each case. Think about such things as
grammar, syntax, semantics, pragmatics, compositionality,
context-dependence, lexical ambiguity, syntactic ambiguity, reference
finding (including pronouns), background knowledge, and what it means to
“understand” in the first place.

This exercise concerns grammars for very simple languages.

1.  Write a context-free grammar for the language $a^n b^n$.

2.  Write a context-free grammar for the palindrome language: the set of
    all strings whose second half is the reverse of the first half.

3.  Write a context-sensitive grammar for the duplicate language: the
    set of all strings whose second half is the same as the first half.

Consider the sentence “Someone walked slowly to the supermarket” and a
lexicon consisting of the following words:

@l@l &\
 &\
 &

Which of the following three grammars, combined with the lexicon,
generates the given sentence? Show the corresponding parse tree(s).

l@l@l

(A): & (B): & (C):\
 & &\
 & &\
 & &\
 & &\
 & &\
 & &\
 & &\
 & &\
 & &

For each of the preceding three grammars, write down three sentences of
English and three sentences of non-English generated by the grammar.
Each sentence should be significantly different, should be at least six
words long, and should include some new lexical entries (which you
should define). Suggest ways to improve each grammar to avoid generating
the non-English sentences.

Collect some examples of time expressions, such as “two o’clock,”
“midnight,” and “12:46.” Also think up some examples that are
ungrammatical, such as “thirteen o’clock” or “half past two fifteen.”
Write a grammar for the time language.

Some linguists have argued as follows:

> Children learning a language hear only *positive
> examples* of the language and no *negative
> examples*. Therefore, the hypothesis that “every possible
> sentence is in the language” is consistent with all the observed
> examples. Moreover, this is the simplest consistent hypothesis.
> Furthermore, all grammars for languages that are supersets of the true
> language are also consistent with the observed data. Yet children do
> induce (more or less) the right grammar. It follows that they begin
> with very strong innate grammatical constraints that rule out all of
> these more general hypotheses *a priori*.

Comment briefly on the weak point(s) in this argument, given what you
know about statistical learning.

[chomsky-form-exercise] In this exercise you will transform into Chomsky
Normal Form (CNF). There are five steps: (a) Add a new start symbol, (b)
Eliminate $\epsilon$ rules, (c) Eliminate multiple words on right-hand
sides, (d) Eliminate rules of the form ( ), (e) Convert long right-hand
sides into binary rules.

1.  The start symbol, $S$, can occur only on the left-hand side in CNF.
    Add a new rule of the form , using a new symbol .

2.  The empty string, $\epsilon$ cannot appear on the right-hand side in
    CNF. does not have any rules with $\epsilon$, so this is not an
    issue.

3.  A word can appear on the right-hand side in a rule only of the form
    ( ). Replace each rule of the form ( …*word* …) with (
    … …) and ( ), using a new symbol .

4.  A rule ( ) is not allowed in CNF; it must be ( ) or ( ). Replace
    each rule of the form ( ) with a set of rules of the form ( …), one
    for each rule ( …), where (…) indicates one or more symbols.

5.  Replace each rule of the form ( …) with two rules, ( ) and ( …),
    where is a new symbol.

Show each step of the process and the final set of rules.

Consider the following toy grammar:

\
\
\
\
\
\
\
\
\
\
\

1.  Show all the parse trees in this grammar for the sentence “Sally
    swims in streams and pools.”

2.  Show all the table entries that would be made by a
    (non-probabalistic) CYK parser on this sentence.

[exercise-subj-verb-agree] Using DCG notation, write a grammar for a
language that is just like , except that it enforces agreement between
the subject and verb of a sentence and thus does not generate
ungrammatical sentences such as “I smells the wumpus.”

Consider the following PCFG:

[1.0]\
 [0.6] [0.4]\
 [0.8] [0.2]\
\
 [0.1] [0.3] …\
 [0.4] …\
 [0.01] [0.1] …\
 [0.3] …

The sentence “I can fish” has two parse trees with this grammar. Show
the two trees, their prior probabilities, and their conditional
probabilities, given the sentence.

An augmented context-free grammar can represent languages that a regular
context-free grammar cannot. Show an augmented context-free grammar for
the language $a^nb^nc^n$. The allowable values for augmentation
variables are 1 and $(n)$, where $n$ is a value. The rule for a sentence
in this language is $$S(n) \bnfeq A(n) \bl B(n) \bl C(n) \ .$$ Show the
rule(s) for each of , , and .

Augment the grammar so that it handles article–noun agreement. That is,
make sure that “agents” and “an agent” are s, but “agent” and “an
agents” are not.

Consider the following sentence (from *The New York Times,*
July 28, 2008):

> Banks struggling to recover from multibillion-dollar loans on real
> estate are curtailing loans to American businesses, depriving even
> healthy companies of money for expansion and hiring.

1.  Which of the words in this sentence are lexically ambiguous?

2.  Find two cases of syntactic ambiguity in this sentence (there are
    more than two.)

3.  Give an instance of metaphor in this sentence.

4.  Can you find semantic ambiguity?

[washing-clothes2-exercise] Without looking back at , answer the
following questions:

1.  What are the four steps that are mentioned?

2.  What step is left out?

3.  What is “the material” that is mentioned in the text?

4.  What kind of mistake would be expensive?

5.  Is it better to do too few things or too many? Why?

Select five sentences and submit them to an online translation service.
Translate them from English to another language and back to English.
Rate the resulting sentences for grammaticality and preservation of
meaning. Repeat the process; does the second round of iteration give
worse results or the same results? Does the choice of intermediate
language make a difference to the quality of the results? If you know a
foreign language, look at the translation of one paragraph into that
language. Count and describe the errors made, and conjecture why these
errors were made.

The $D_i$ values for the sentence in sum to 0. Will that be true of
every translation pair? Prove it or give a counterexample.

(Adapted from .) Our translation model assumes that, after the phrase
translation model selects phrases and the distortion model permutes
them, the language model can unscramble the permutation. This exercise
investigates how sensible that assumption is. Try to unscramble these
proposed lists of phrases into the correct order:

1.  have, programming, a, seen, never, I, language, better

2.  loves, john, mary

3.  is the, communication, exchange of, intentional, information
    brought, by, about, the production, perception of, and signs, from,
    drawn, a, of, system, signs, conventional, shared

4.  created, that, we hold these, to be, all men, truths, are, equal,
    self-evident

Which ones could you do? What type of knowledge did you draw upon? Train
a bigram model from a training corpus, and use it to find the
highest-probability permutation of some sentences from a test corpus.
Report on the accuracy of this model.

Calculate the most probable path through the HMM in for the output
sequence $[C_1,C_2,C_3,C_4,C_4,C_6,C_7]$. Also give its probability.

We forgot to mention that the text in is entitled “Washing Clothes.”
Reread the text and answer the questions in . Did you do better this
time? Bransford and Johnson [-@Bransford+Johnson:1973] used this text in
a controlled experiment and found that the title helped significantly.
What does this tell you about how language and memory works?

[^1]: PCFGs are also known as stochastic context-free grammars, or
    SCFGs.

[^2]: A relative clause follows and modifies a noun phrase. It consists
    of a relative pronoun (such as “who” or “that”) followed by a verb
    phrase. An example of a relative clause is *that
    stinks* in “The wumpus *that stinks* is in 2 2.”
    Another kind of relative clause has no relative pronoun, e.g.,
    *I know* in “the man *I know*.”

[^3]: There also would be $O(c!)$ ambiguity in the way the components
    conjoin—for example, ($X$ and ($Y$ and $Z$)) versus (($X$ and $Y$)
    and $Z$). But that is another story, one told well by Church and
    Patil [-@Church+Patil:1982].

[^4]: The subjective case is also sometimes called the nominative case
    and the objective case is sometimes called the accusative case. Many
    languages also have a dative case for words in the indirect object
    position.

[^5]: If this interpretation seems unlikely, consider “Every Protestant
    believes in a just God.”

[^6]: This example is due to Martin Kay.

[^7]: Warren reports that Max Zeldner points out that the great Hebrew
    poet H. N. Bialik once said that translation “is like kissing the
    bride through a veil.”

[^8]: For the finer points of translation, $n$-grams are clearly not
    enough. Marcel Proust’s 4000-page novel *A la récherche du
    temps perdu* begins and ends with the same word
    (*longtemps*), so some translators have decided to do
    the same, thus basing the translation of the final word on one that
    appeared roughly 2 million words earlier.

[^9]: Named after William Hansard, who first published the British
    parliamentary debates in 1811.
[final-part]

Philosophical Foundations {#philosophy-chapter}
=========================

Philosophers have been around far longer than computers and have been
trying to resolve some questions that relate to AI: How do minds work?
Is it possible for machines to act intelligently in the way that people
do, and if they did, would they have real, conscious minds? What are the
ethical implications of intelligent machines?

First, some terminology: the assertion that machines could act *as
if* they were intelligent is called the hypothesis by
philosophers, and the assertion that machines that do so are
*actually* thinking (not just *simulating*
thinking) is called the hypothesis.

Most AI researchers take the weak AI hypothesis for granted, and don’t
care about the strong AI hypothesis—as long as their program works, they
don’t care whether you call it a simulation of intelligence or real
intelligence. All AI researchers should be concerned with the ethical
implications of their work.

Weak AI: Can Machines Act Intelligently? {#intelligence-section}
----------------------------------------

The proposal for the 1956 summer workshop that defined the field of
Artificial Intelligence @McCarthy+al:1955 made the assertion that “Every
aspect of learning or any other feature of intelligence can be so
precisely described that a machine can be made to simulate it.” Thus, AI
was founded on the assumption that weak AI is possible. Others have
asserted that weak AI is impossible: “Artificial intelligence
*pursued within the cult of computationalism* stands not
even a ghost of a chance of producing durable results” @Sayre:1993.

Clearly, whether AI is impossible depends on how it is defined. In , we
defined AI as the quest for the best agent program on a given
architecture. With this formulation, AI is by definition possible: for
any digital architecture with $k$ bits of program storage there are
exactly $2^k$ agent programs, and all we have to do to find the best one
is enumerate and test them all. This might not be feasible for large
$k$, but philosophers deal with the theoretical, not the practical.

Our definition of AI works well for the engineering problem of finding a
good agent, given an architecture. Therefore, we’re tempted to end this
section right now, answering the title question in the affirmative. But
philosophers are interested in the problem of comparing two
architectures—human and machine. Furthermore, they have traditionally
posed the question not in terms of maximizing expected utility but
rather as, “?”

The computer scientist Edsger Dijkstra [-@Dijkstra:1984] said that “The
question of whether *Machines Can Think* …is about as
relevant as the question of whether *Submarines Can Swim*.”
The American Heritage Dictionary’s first definition of
*swim* is “To move through water by means of the limbs,
fins, or tail,” and most people agree that submarines, being limbless,
cannot swim. The dictionary also defines *fly* as “To move
through the air by means of wings or winglike parts,” and most people
agree that airplanes, having winglike parts, can fly. However, neither
the questions nor the answers have any relevance to the design or
capabilities of airplanes and submarines; rather they are about the
usage of words in English. (The fact that ships *do* swim
in Russian only amplifies this point.). The practical possibility of
“thinking machines” has been with us for only 50 years or so, not long
enough for speakers of English to settle on a meaning for the word
“think”—does it require “a brain” or just “brain-like parts.”

Alan Turing, in his famous paper “Computing Machinery and
Intelligence” [-@Turing:1950], suggested that instead of asking whether
machines can think, we should ask whether machines can pass a behavioral
intelligence test, which has come to be called the . The test is for a
program to have a conversation (via online typed messages) with an
interrogator for five minutes. The interrogator then has to guess if the
conversation is with a program or a person; the program passes the test
if it fools the interrogator 30% of the time. Turing conjectured that,
by the year 2000, a computer with a storage of ${10}^9$ units could be
programmed well enough to pass the test. He was wrong—programs have yet
to fool a sophisticated judge.

On the other hand, many people *have* been fooled when they
didn’t know they might be chatting with a computer. The program and
Internet chatbots such as @Humphrys:2008 and have fooled their
correspondents repeatedly, and the chatbot has attracted the attention
of law enforcement because of its penchant for tricking fellow chatters
into divulging enough personal information that their identity can be
stolen. The competition, held annually since 1991, is the
longest-running Turing Test-like contest. The competitions have led to
better models of human typing errors.

Turing himself examined a wide variety of possible objections to the
possibility of intelligent machines, including virtually all of those
that have been raised in the half-century since his paper appeared. We
will look at some of them.

### The argument from disability

The “argument from disability” makes the claim that “a machine can never
do *X*.” As examples of *X*, Turing lists the
following:

Be kind, resourceful, beautiful, friendly, have initiative, have a sense
of humor, tell right from wrong, make mistakes, fall in love, enjoy
strawberries and cream, make someone fall in love with it, learn from
experience, use words properly, be the subject of its own thought, have
as much diversity of behavior as man, do something really new.

In retrospect, some of these are rather easy—we’re all familiar with
computers that “make mistakes.” We are also familiar with a century-old
technology that has had a proven ability to “make someone fall in love
with it”—the teddy bear. Computer chess expert David Levy predicts that
by 2050 people will routinely fall in love with humanoid robots
@Levy:2007. As for a robot falling in love, that is a common theme in
fiction,[^1] but there has been only limited speculation about whether
it is in fact likely @Kim+al:2007. Programs do play chess, checkers and
other games; inspect parts on assembly lines, steer cars and
helicopters; diagnose diseases; and do hundreds of other tasks as well
as or better than humans. Computers have made small but significant
discoveries in astronomy, mathematics, chemistry, mineralogy, biology,
computer science, and other fields. Each of these required performance
at the level of a human expert.

Given what we now know about computers, it is not surprising that they
do well at combinatorial problems such as playing chess. But algorithms
also perform at human levels on tasks that seemingly involve human
judgment, or as Turing put it, “learning from experience” and the
ability to “tell right from wrong.” As far back as 1955, Paul Meehl
\<see also\>Grove+Meehl:1996 studied the decision-making
processes of trained experts at subjective tasks such as predicting the
success of a student in a training program or the recidivism of a
criminal. In 19 out of the 20 studies he looked at, Meehl found that
simple statistical learning algorithms (such as linear regression or
naive Bayes) predict better than the experts. The Educational Testing
Service has used an automated program to grade millions of essay
questions on the GMAT exam since 1999. The program agrees with human
graders 97% of the time, about the same level that two human graders
agree @Burstein+al:2001.

It is clear that computers can do many things as well as or better than
humans, including things that people believe require great human insight
and understanding. This does not mean, of course, that computers use
insight and understanding in performing these tasks—those are not part
of *behavior*, and we address such questions elsewhere—but
the point is that one’s first guess about the mental processes required
to produce a given behavior is often wrong. It is also true, of course,
that there are many tasks at which computers do not yet excel (to put it
mildly), including Turing’s task of carrying on an open-ended
conversation.

### The mathematical objection {#undecidability-section}

It is well known, through the work of Turing [-@Turing:1936] and
Gödel [-@Goedel:1931], that certain mathematical questions are in
principle unanswerable by particular formal systems. Gödel’s
incompleteness theorem (see ) is the most famous example of this.
Briefly, for any formal axiomatic system $F$ powerful enough to do
arithmetic, it is possible to construct a so-called Gödel sentence
$G(F)$ with the following properties:

-   $G(F)$ is a sentence of $F$, but cannot be proved within $F$.

-   If $F$ is consistent, then $G(F)$ is true.

Philosophers such as J. R. Lucas [-@Lucas:1961] have claimed that this
theorem shows that machines are mentally inferior to humans, because
machines are formal systems that are limited by the incompleteness
theorem—they cannot establish the truth of their own Gödel
sentence—while humans have no such limitation. This claim has caused
decades of controversy, spawning a vast literature, including two books
by the mathematician Sir Roger Penrose [-@Penrose:1989; -@Penrose:1994]
that repeat the claim with some fresh twists (such as the hypothesis
that humans are different because their brains operate by quantum
gravity). We will examine only three of the problems with the claim.

First, Gödel’s incompleteness theorem applies only to formal systems
that are powerful enough to do arithmetic. This includes Turing
machines, and Lucas’s claim is in part based on the assertion that
computers are Turing machines. This is a good approximation, but is not
quite true. Turing machines are infinite, whereas computers are finite,
and any computer can therefore be described as a (very large) system in
propositional logic, which is not subject to Gödel’s incompleteness
theorem. Second, an agent should not be too ashamed that it cannot
establish the truth of some sentence while other agents can. Consider
the sentence

J. R. Lucas cannot consistently assert that this sentence is true.

If Lucas asserted this sentence, then he would be contradicting himself,
so therefore Lucas cannot consistently assert it, and hence it must be
true. We have thus demonstrated that there is a sentence that Lucas
cannot consistently assert while other people (and machines) can. But
that does not make us think less of Lucas. To take another example, no
human could compute the sum of a billion 10 digit numbers in his or her
lifetime, but a computer could do it in seconds. Still, we do not see
this as a fundamental limitation in the human’s ability to think. Humans
were behaving intelligently for thousands of years before they invented
mathematics, so it is unlikely that formal mathematical reasoning plays
more than a peripheral role in what it means to be intelligent.

Third, and most important, even if we grant that computers have
limitations on what they can prove, there is no evidence that humans are
immune from those limitations. It is all too easy to show rigorously
that a formal system cannot do $X$, and then claim that humans
*can* do $X$ using their own informal method, without
giving any evidence for this claim. Indeed, it is impossible to
*prove* that humans are not subject to Gödel’s
incompleteness theorem, because any rigorous proof would require a
formalization of the claimed unformalizable human talent, and hence
refute itself. So we are left with an appeal to intuition that humans
can somehow perform superhuman feats of mathematical insight. This
appeal is expressed with arguments such as “we must assume our own
consistency, if thought is to be possible at all” @Lucas:1976. But if
anything, humans are known to be inconsistent. This is certainly true
for everyday reasoning, but it is also true for careful mathematical
thought. A famous example is the four-color map problem. Alfred Kempe
published a proof in 1879 that was widely accepted and contributed to
his election as a Fellow of the Royal Society. In 1890, however, Percy
Heawood pointed out a flaw and the theorem remained unproved until 1977.

### The argument from informality {#informality-section}

One of the most influential and persistent criticisms of AI as an
enterprise was raised by Turing as the “argument from informality of
behavior.” Essentially, this is the claim that human behavior is far too
complex to be captured by any simple set of rules and that because
computers can do no more than follow a set of rules, they cannot
generate behavior as intelligent as that of humans. The inability to
capture everything in a set of logical rules is called the in AI.

The principal proponent of this view has been the philosopher Hubert
Dreyfus, who has produced a series of influential critiques of
artificial intelligence: *What Computers Can’t
Do* [-@Dreyfus:1972], the sequel *What Computers Still
Can’t Do* [-@Dreyfus:1992], and, with his brother Stuart,
*Mind Over Machine* [-@Dreyfus+Dreyfus:1986].

The position they criticize came to be called “Good Old-Fashioned AI,”
or gofai, a term coined by philosopher John
Haugeland [-@Haugeland:1985]. gofai is supposed to claim
that all intelligent behavior can be captured by a system that reasons
logically from a set of facts and rules describing the domain. It
therefore corresponds to the simplest logical agent described in .
Dreyfus is correct in saying that logical agents are vulnerable to the
qualification problem. As we saw in , probabilistic reasoning systems
are more appropriate for open-ended domains. The Dreyfus critique
therefore is not addressed against computers *per se*, but
rather against one particular way of programming them. It is reasonable
to suppose, however, that a book called *What First-Order Logical
Rule-Based Systems Without Learning Can’t Do* might have had less
impact.

Under Dreyfus’s view, human expertise does include knowledge of some
rules, but only as a “holistic context” or “background” within which
humans operate. He gives the example of appropriate social behavior in
giving and receiving gifts: “Normally one simply responds in the
appropriate circumstances by giving an appropriate gift.” One apparently
has “a direct sense of how things are done and what to expect.” The same
claim is made in the context of chess playing: “A mere chess master
might need to figure out what to do, but a grandmaster just sees the
board as demanding a certain move …the right response just pops into his
or her head.” It is certainly true that much of the thought processes of
a present-giver or grandmaster is done at a level that is not open to
introspection by the conscious mind. But that does not mean that the
thought processes do not exist. The important question that Dreyfus does
not answer is *how* the right move gets into the
grandmaster’s head. One is reminded of Daniel Dennett’s [-@Dennett:1984]
comment,

It is rather as if philosophers were to proclaim themselves expert
explainers of the methods of stage magicians, and then, when we ask how
the magician does the sawing-the-lady-in-half trick, they explain that
it is really quite obvious: the magician doesn’t really saw her in half;
he simply makes it appear that he does. “But how does he do
*that*?” we ask. “Not our department,” say the
philosophers.

propose a five-stage process of acquiring expertise, beginning with
rule-based processing (of the sort proposed in gofai) and
ending with the ability to select correct responses instantaneously. In
making this proposal, Dreyfus and Dreyfus in effect move from being AI
critics to AI theorists—they propose a neural network architecture
organized into a vast “case library,” but point out several problems.
Fortunately, all of their problems have been addressed, some with
partial success and some with total success. Their problems include the
following:

1.  Good generalization from examples cannot be achieved without
    background knowledge. They claim no one has any idea how to
    incorporate background knowledge into the neural network learning
    process. In fact, we saw in that there are techniques for using
    prior knowledge in learning algorithms. Those techniques, however,
    rely on the availability of knowledge in explicit form, something
    that Dreyfus and Dreyfus strenuously deny. In our view, this is a
    good reason for a serious redesign of current models of neural
    processing so that they *can* take advantage of
    previously learned knowledge in the way that other learning
    algorithms do.

2.  Neural network learning is a form of supervised learning (see ),
    requiring the prior identification of relevant inputs and correct
    outputs. Therefore, they claim, it cannot operate autonomously
    without the help of a human trainer. In fact, learning without a
    teacher can be accomplished by () and ().

3.  Learning algorithms do not perform well with many features, and if
    we pick a subset of features, “there is no known way of adding new
    features should the current set prove inadequate to account for the
    learned facts.” In fact, new methods such as support vector machines
    handle large feature sets very well. With the introduction of large
    Web-based data sets, many applications in areas such as language
    processing @Sha+Pereira:2003 and computer vision @Viola+Jones:2001
    routinely handle millions of features. We saw in that there are also
    principled ways to generate new features, although much more work is
    needed.

4.  The brain is able to direct its sensors to seek relevant information
    and to process it to extract aspects relevant to the current
    situation. But, Dreyfus and Dreyfus claim, “Currently, no details of
    this mechanism are understood or even hypothesized in a way that
    could guide AI research.” In fact, the field of active vision,
    underpinned by the theory of information value (), is concerned with
    exactly the problem of directing sensors, and already some robots
    have incorporated the theoretical results obtained. ’s 132-mile trip
    through the desert () was made possible in large part by an active
    sensing system of this kind.

In sum, many of the issues Dreyfus has focused on—background commonsense
knowledge, the qualification problem, uncertainty, learning, compiled
forms of decision making—are indeed important issues, and have by now
been incorporated into standard intelligent agent design. In our view,
this is evidence of AI’s progress, not of its impossibility.

One of Dreyfus’ strongest arguments is for situated agents rather than
disembodied logical inference engines. An agent whose understanding of
“dog” comes only from a limited set of logical sentences such as
“${Dog}(x) \implies {Mammal}(x)$” is at a disadvantage compared to
an agent that has watched dogs run, has played fetch with them, and has
been licked by one. As philosopher Andy Clark [-@Clark:1998] says,
“Biological brains are first and foremost the control systems for
biological bodies. Biological bodies move and act in rich real-world
surroundings.” To understand how human (or other animal) agents work, we
have to consider the whole agent, not just the agent program. Indeed,
the approach claims that it makes no sense to consider the brain
separately: cognition takes place within a body, which is embedded in an
environment. We need to study the system as a whole; the brain augments
its reasoning by referring to the environment, as the reader does in
perceiving (and creating) marks on paper to transfer knowledge. Under
the embodied cognition program, robotics, vision, and other sensors
become central, not peripheral.

Strong AI: Can Machines Really Think? {#consciousness-section}
-------------------------------------

Many philosophers have claimed that a machine that passes the Turing
Test would still not be *actually* thinking, but would be
only a *simulation* of thinking. Again, the objection was
foreseen by Turing. He cites a speech by Professor Geoffrey
Jefferson [-@Jefferson:1949]:

Not until a machine could write a sonnet or compose a concerto because
of thoughts and emotions felt, and not by the chance fall of symbols,
could we agree that machine equals brain—that is, not only write it but
know that it had written it.

Turing calls this the argument from —the machine has to be aware of its
own mental states and actions. While consciousness is an important
subject, Jefferson’s key point actually relates to , or the study of
direct experience: the machine has to actually feel emotions. Others
focus on —that is, the question of whether the machine’s purported
beliefs, desires, and other representations are actually “about”
something in the real world.

Turing’s response to the objection is interesting. He could have
presented reasons that machines can in fact be conscious (or have
phenomenology, or have intentions). Instead, he maintains that the
question is just as ill-defined as asking, “Can machines think?”
Besides, why should we insist on a higher standard for machines than we
do for humans? After all, in ordinary life we never have
*any* direct evidence about the internal mental states of
other humans. Nevertheless, Turing says, “Instead of arguing continually
over this point, it is usual to have the polite convention that everyone
thinks.”

Turing argues that Jefferson would be willing to extend the polite
convention to machines if only he had experience with ones that act
intelligently. He cites the following dialog, which has become such a
part of AI’s oral tradition that we simply have to include it:

human: In the first line of your sonnet which reads “shall
I compare thee to a summer’s\
day,” would not a “spring day” do as well or better?\
machine: It wouldn’t scan.\
human: How about “a winter’s day.” That would scan all
right.\
machine: Yes, but nobody wants to be compared to a winter’s
day.\
human: Would you say Mr. Pickwick reminded you of
Christmas?\
machine: In a way.\
human: Yet Christmas is a winter’s day, and I do not think
Mr. Pickwick would mind\
the comparison.\
machine: I don’t think you’re serious. By a winter’s day
one means a typical winter’s\
day, rather than a special one like Christmas.

One can easily imagine some future time in which such conversations with
machines are commonplace, and it becomes customary to make no linguistic
distinction between “real” and “artificial” thinking. A similar
transition occurred in the years after 1848, when artificial urea was
synthesized for the first time by Frederick Wöhler. Prior to this event,
organic and inorganic chemistry were essentially disjoint enterprises
and many thought that no process could exist that would convert
inorganic chemicals into organic material. Once the synthesis was
accomplished, chemists agreed that artificial urea *was*
urea, because it had all the right physical properties. Those who had
posited an intrinsic property possessed by organic material that
inorganic material could never have were faced with the impossibility of
devising any test that could reveal the supposed deficiency of
artificial urea.

For thinking, we have not yet reached our 1848 and there are those who
believe that artificial thinking, no matter how impressive, will never
be real. For example, the philosopher John Searle [-@Searle:1980] argues
as follows:

No one supposes that a computer simulation of a storm will leave us all
wet $\ldots$ Why on earth would anyone in his right mind suppose a
computer simulation of mental processes actually had mental processes?
(pp. 37–38)

While it is easy to agree that computer simulations of storms do not
make us wet, it is not clear how to carry this analogy over to computer
simulations of mental processes. After all, a Hollywood simulation of a
storm using sprinklers and wind machines *does* make the
actors wet, and a video game simulation of a storm *does*
make the simulated characters wet. Most people are comfortable saying
that a computer simulation of addition is addition, and of chess is
chess. In fact, we typically speak of an *implementation*
of addition or chess, not a *simulation*. Are mental
processes more like storms, or more like addition?

Turing’s answer—the polite convention—suggests that the issue will
eventually go away by itself once machines reach a certain level of
sophistication. This would have the effect of *dissolving*
the difference between weak and strong AI. Against this, one may insist
that there is a *factual* issue at stake: humans do have
real minds, and machines might or might not. To address this factual
issue, we need to understand how it is that humans have real minds, not
just bodies that generate neurophysiological processes. Philosophical
efforts to solve this are directly relevant to the question of whether
machines could have real minds.

The mind–body problem was considered by the ancient Greek philosophers
and by various schools of Hindu thought, but was first analyzed in depth
by the 17th-century French philosopher and mathematician René Descartes.
His *Meditations on First Philosophy* [-@Descartes:1641]
considered the mind’s activity of thinking (a process with no spatial
extent or material properties) and the physical processes of the body,
concluding that the two must exist in separate realms—what we would now
call a theory. The mind–body problem faced by dualists is the question
of how the mind can control the body if the two are really separate.
Descartes speculated that the two might interact through the pineal
gland, which simply begs the question of how the mind controls the
pineal gland.

The theory of mind, often called , avoids this problem by asserting the
mind is not separate from the body—that mental states *are*
physical states. Most modern philosophers of mind are physicalists of
one form or another, and physicalism allows, at least in principle, for
the possibility of strong AI. The problem for physicalists is to explain
how physical states—in particular, the molecular configurations and
electrochemical processes of the brain—can simultaneously be , such as
being in pain, enjoying a hamburger, knowing that one is riding a horse,
or believing that Vienna is the capital of Austria.

### Mental states and the brain in a vat

Physicalist philosophers have attempted to explicate what it means to
say that a person—and, by extension, a computer—is in a particular
mental state. They have focused in particular on . These are states,
such as believing, knowing, desiring, fearing, and so on, that refer to
some aspect of the external world. For example, the knowledge that one
is eating a hamburger is a belief *about* the hamburger and
what is happening to it.

If physicalism is correct, it must be the case that the proper
description of a person’s mental state is *determined* by
that person’s brain state. Thus, if I am currently focused on eating a
hamburger in a mindful way, my instantaneous brain state is an instance
of the class of mental states “knowing that one is eating a hamburger.”
Of course, the specific configurations of all the atoms of my brain are
not essential: there are many configurations of my brain, or of other
people’s brain, that would belong to the same class of mental states.
The key point is that the same brain state could not correspond to a
fundamentally distinct mental state, such as the knowledge that one is
eating a banana.

The simplicity of this view is challenged by some simple thought
experiments. Imagine, if you will, that your brain was removed from your
body at birth and placed in a marvelously engineered vat. The vat
sustains your brain, allowing it to grow and develop. At the same time,
electronic signals are fed to your brain from a computer simulation of
an entirely fictitious world, and motor signals from your brain are
intercepted and used to modify the simulation as appropriate.[^2] In
fact, the simulated life you live replicates exactly the life you would
have lived, had your brain not been placed in the vat, including
simulated eating of simulated hamburgers. Thus, you could have a brain
state identical to that of someone who is really eating a real
hamburger, but it would be literally false to say that you have the
mental state “knowing that one is eating a hamburger.” You aren’t eating
a hamburger, you have never even experienced a hamburger, and you could
not, therefore, have such a mental state.

This example seems to contradict the view that brain states determine
mental states. One way to resolve the dilemma is to say that the content
of mental states can be interpreted from two different points of view.
The “” view interprets it from the point of view of an omniscient
outside observer with access to the whole situation, who can distinguish
differences in the world. Under this view, the content of mental states
involves both the brain state and the environment history. , on the
other hand, considers only the brain state. The narrow content of the
brain states of a real hamburger-eater and a brain-in-a-vat
“hamburger”-“eater” is the same in both cases.

Wide content is entirely appropriate if one’s goals are to ascribe
mental states to others who share one’s world, to predict their likely
behavior and its effects, and so on. This is the setting in which our
ordinary language about mental content has evolved. On the other hand,
if one is concerned with the question of whether AI systems are really
thinking and really do have mental states, then narrow content is
appropriate; it simply doesn’t make sense to say that whether or not an
AI system is really thinking depends on conditions outside that system.
Narrow content is also relevant if we are thinking about designing AI
systems or understanding their operation, because it is the narrow
content of a brain state that determines what will be the (narrow
content of the) next brain state. This leads naturally to the idea that
what matters about a brain state—what makes it have one kind of mental
content and not another—is its functional role within the mental
operation of the entity involved.

### Functionalism and the brain replacement experiment

The theory of says that a mental state is any intermediate causal
condition between input and output. Under functionalist theory, any two
systems with isomorphic causal processes would have the same mental
states. Therefore, a computer program could have the same mental states
as a person. Of course, we have not yet said what “isomorphic” really
means, but the assumption is that there is some level of abstraction
below which the specific implementation does not matter.

The claims of functionalism are illustrated most clearly by the brain
replacement experiment. This thought experiment was introduced by the
philosopher Clark Glymour and was touched on by John , but is most
commonly associated with roboticist Hans . It goes like this: Suppose
neurophysiology has developed to the point where the input–output
behavior and connectivity of all the neurons in the human brain are
perfectly understood. Suppose further that we can build microscopic
electronic devices that mimic this behavior and can be smoothly
interfaced to neural tissue. Lastly, suppose that some miraculous
surgical technique can replace individual neurons with the corresponding
electronic devices without interrupting the operation of the brain as a
whole. The experiment consists of gradually replacing all the neurons in
someone’s head with electronic devices.

We are concerned with both the external behavior and the internal
experience of the subject, during and after the operation. By the
definition of the experiment, the subject’s external behavior must
remain unchanged compared with what would be observed if the operation
were not carried out.[^3] Now although the presence or absence of
consciousness cannot easily be ascertained by a third party, the subject
of the experiment ought at least to be able to record any changes in his
or her own conscious experience. Apparently, there is a direct clash of
intuitions as to what would happen. Moravec, a robotics researcher and
functionalist, is convinced his consciousness would remain unaffected.
Searle, a philosopher and biological naturalist, is equally convinced
his consciousness would vanish:

You find, to your total amazement, that you are indeed losing control of
your external behavior. You find, for example, that when doctors test
your vision, you hear them say “We are holding up a red object in front
of you; please tell us what you see.” You want to cry out “I can’t see
anything. I’m going totally blind.” But you hear your voice saying in a
way that is completely out of your control, “I see a red object in front
of me.” $\ldots$ your conscious experience slowly shrinks to nothing,
while your externally observable behavior remains the same. @Searle:1992

One can do more than argue from intuition. First, note that, for the
external behavior to remain the same while the subject gradually becomes
unconscious, it must be the case that the subject’s volition is removed
instantaneously and totally; otherwise the shrinking of awareness would
be reflected in external behavior—“Help, I’m shrinking!” or words to
that effect. This instantaneous removal of volition as a result of
gradual neuron-at-a-time replacement seems an unlikely claim to have to
make.

Second, consider what happens if we do ask the subject questions
concerning his or her conscious experience during the period when no
real neurons remain. By the conditions of the experiment, we will get
responses such as “I feel fine. I must say I’m a bit surprised because I
believed Searle’s argument.” Or we might poke the subject with a pointed
stick and observe the response, “Ouch, that hurt.” Now, in the normal
course of affairs, the skeptic can dismiss such outputs from AI programs
as mere contrivances. Certainly, it is easy enough to use a rule such as
“If sensor 12 reads ‘High’ then output ‘Ouch.’” But the point here is
that, because we have replicated the functional properties of a normal
human brain, we assume that the electronic brain contains no such
contrivances. Then we must have an explanation of the manifestations of
consciousness produced by the electronic brain that appeals only to the
functional properties of the neurons. *And this explanation must
also apply to the real brain, which has the same functional
properties.* There are three possible conclusions:

1.  The causal mechanisms of consciousness that generate these kinds of
    outputs in normal brains are still operating in the electronic
    version, which is therefore conscious.

2.  The conscious mental events in the normal brain have no causal
    connection to behavior, and are missing from the electronic brain,
    which is therefore not conscious.

3.  The experiment is impossible, and therefore speculation about it is
    meaningless.

Although we cannot rule out the second possibility, it reduces
consciousness to what philosophers call an role—something that happens,
but casts no shadow, as it were, on the observable world. Furthermore,
if consciousness is indeed epiphenomenal, then it cannot be the case
that the subject says “Ouch” *because it hurts*—that is,
because of the conscious experience of pain. Instead, the brain must
contain a second, unconscious mechanism that is responsible for the
“Ouch.”

Patricia Churchland [-@Churchland:1986] points out that the
functionalist arguments that operate at the level of the neuron can also
operate at the level of any larger functional unit—a clump of neurons, a
mental module, a lobe, a hemisphere, or the whole brain. That means that
if you accept the notion that the brain replacement experiment shows
that the replacement brain is conscious, then you should also believe
that consciousness is maintained when the entire brain is replaced by a
circuit that updates its state and maps from inputs to outputs via a
huge lookup table. This is disconcerting to many people (including
Turing himself), who have the intuition that lookup tables are not
conscious—or at least, that the conscious experiences generated during
table lookup are not the same as those generated during the operation of
a system that might be described (even in a simple-minded, computational
sense) as accessing and generating beliefs, introspections, goals, and
so on.

### Biological naturalism and the Chinese Room

A strong challenge to functionalism has been mounted by John
Searle’s [-@Searle:1980] , according to which mental states are
high-level emergent features that are caused by low-level physical
processes *in the neurons*, and it is the (unspecified)
properties of the neurons that matter. Thus, mental states cannot be
duplicated just on the basis of some program having the same functional
structure with the same input–output behavior; we would require that the
program be running on an architecture with the same causal power as
neurons. To support his view, Searle describes a hypothetical system
that is clearly running a program and passes the Turing Test, but that
equally clearly (according to Searle) does not *understand*
anything of its inputs and outputs. His conclusion is that running the
appropriate program (i.e., having the right outputs) is not a
*sufficient* condition for being a mind.

The system consists of a human, who understands only English, equipped
with a rule book, written in English, and various stacks of paper, some
blank, some with indecipherable inscriptions. (The human therefore plays
the role of the CPU, the rule book is the program, and the stacks of
paper are the storage device.) The system is inside a room with a small
opening to the outside. Through the opening appear slips of paper with
indecipherable symbols. The human finds matching symbols in the rule
book, and follows the instructions. The instructions may include writing
symbols on new slips of paper, finding symbols in the stacks,
rearranging the stacks, and so on. Eventually, the instructions will
cause one or more symbols to be transcribed onto a piece of paper that
is passed back to the outside world.

So far, so good. But from the outside, we see a system that is taking
input in the form of Chinese sentences and generating answers in Chinese
that are as “intelligent” as those in the conversation imagined by
Turing.[^4] Searle then argues: the person in the room does not
understand Chinese (given). The rule book and the stacks of paper, being
just pieces of paper, do not understand Chinese. Therefore, there is no
understanding of Chinese.

Hence, according to Searle, running the right program does not
necessarily generate understanding.

Like Turing, Searle considered and attempted to rebuff a number of
replies to his argument. Several commentators, including John McCarthy
and Robert Wilensky, proposed what Searle calls the systems reply. The
objection is that asking if the human in the room understands Chinese is
analogous to asking if the CPU can take cube roots. In both cases, the
answer is no, and in both cases, according to the systems reply, the
entire system *does* have the capacity in question.
Certainly, if one asks the Chinese Room whether it understands Chinese,
the answer would be affirmative (in fluent Chinese). By Turing’s polite
convention, this should be enough. Searle’s response is to reiterate the
point that the understanding is not in the human and cannot be in the
paper, so there cannot be any understanding. He seems to be relying on
the argument that a property of the whole must reside in one of the
parts. Yet water is wet, even though neither H nor $\mbox{O}_2$ is. The
real claim made by Searle rests upon the following four
axioms @Searle:1990:

1.  Computer programs are formal (syntactic).

2.  Human minds have mental contents (semantics).

3.  Syntax by itself is neither constitutive of nor sufficient for
    semantics.

4.  Brains cause minds.

From the first three axioms Searle concludes that programs are not
sufficient for minds. In other words, an agent running a program
*might* be a mind, but it is not *necessarily*
a mind just by virtue of running the program. From the fourth axiom he
concludes “Any other system capable of causing minds would have to have
causal powers (at least) equivalent to those of brains.” From there he
infers that any artificial brain would have to duplicate the causal
powers of brains, not just run a particular program, and that human
brains do not produce mental phenomena solely by virtue of running a
program.

The axioms are controversial. For example, axioms 1 and 2 rely on an
unspecified distinction between syntax and semantics that seems to be
closely related to the distinction between narrow and wide content. On
the one hand, we can view computers as manipulating syntactic symbols;
on the other, we can view them as manipulating electric current, which
happens to be what brains mostly do (according to our current
understanding). So it seems we could equally say that brains are
syntactic.

Assuming we are generous in interpreting the axioms, then the
conclusion—that programs are not sufficient for
minds—*does* follow. But the conclusion is
unsatisfactory—all Searle has shown is that if you explicitly deny
functionalism (that is what his axiom 3 does), then you can’t
necessarily conclude that non-brains are minds. This is reasonable
enough—almost tautological—so the whole argument comes down to whether
axiom 3 can be accepted. According to Searle, the point of the Chinese
Room argument is to provide intuitions for axiom 3. The public reaction
shows that the argument is acting as what Daniel calls an : it amplifies
one’s prior intuitions, so biological naturalists are more convinced of
their positions, and functionalists are convinced only that axiom 3 is
unsupported, or that in general Searle’s argument is unconvincing. The
argument stirs up combatants, but has done little to change anyone’s
opinion. Searle remains undeterred, and has recently started calling the
Chinese Room a “refutation” of strong AI rather than just an “argument”
@Snell:2008.

Even those who accept axiom 3, and thus accept Searle’s argument, have
only their intuitions to fall back on when deciding what entities are
minds. The argument purports to show that the Chinese Room is not a mind
*by virtue of running the program*, but the argument says
nothing about how to decide whether the room (or a computer, some other
type of machine, or an alien) is a mind *by virtue of some other
reason*. Searle himself says that some machines do have minds:
humans are biological machines with minds. According to Searle, human
brains may or may not be running something like an AI program, but if
they are, that is not the reason they are minds. It takes more to make a
mind—according to Searle, something equivalent to the causal powers of
individual neurons. What these powers are is left unspecified. It should
be noted, however, that neurons evolved to fulfill
*functional* roles—creatures with neurons were learning and
deciding long before consciousness appeared on the scene. It would be a
remarkable coincidence if such neurons just happened to generate
consciousness because of some causal powers that are irrelevant to their
functional capabilities; after all, it is the functional capabilities
that dictate survival of the organism.

In the case of the Chinese Room, Searle relies on intuition, not proof:
just look at the room; what’s there to be a mind? But one could make the
same argument about the brain: just look at this collection of cells (or
of atoms), blindly operating according to the laws of biochemistry (or
of physics)—what’s there to be a mind? Why can a hunk of brain be a mind
while a hunk of liver cannot? That remains the great mystery.

### Consciousness, qualia, and the explanatory gap

Running through all the debates about strong AI—the elephant in the
debating room, so to speak—is the issue of . Consciousness is often
broken down into aspects such as understanding and self-awareness. The
aspect we will focus on is that of *subjective experience*:
why it is that it *feels* like something to have certain
brain states (e.g., while eating a hamburger), whereas it presumably
does not feel like anything to have other physical states (e.g., while
being a rock). The technical term for the intrinsic nature of
experiences is (from the Latin word meaning, roughly, “such things”).

Qualia present a challenge for functionalist accounts of the mind
because different qualia could be involved in what are otherwise
isomorphic causal processes. Consider, for example, the thought
experiment, which the subjective experience of person $X$ when seeing
red objects is the same experience that the rest of us experience when
seeing green objects, and vice versa. $X$ still calls red objects “red,”
stops for red traffic lights, and agrees that the redness of red traffic
lights is a more intense red than the redness of the setting sun. Yet,
$X$’s subjective experience is just different.

Qualia are challenging not just for functionalism but for all of
science. Suppose, for the sake of argument, that we have completed the
process of scientific research on the brain—we have found that neural
process $P_{12}$ in neuron $N_{177}$ transforms molecule $A$ into
molecule $B$, and so on, and on. There is simply no currently accepted
form of reasoning that would lead from such findings to the conclusion
that the entity owning those neurons has any particular subjective
experience. This has led some philosophers to conclude that humans are
simply incapable of forming a proper understanding of their own
consciousness. Others, notably Daniel  , avoid the gap by denying the
existence of qualia, attributing them to a philosophical confusion.

Turing himself concedes that the question of consciousness is a
difficult one, but denies that it has much relevance to the practice of
AI: “I do not wish to give the impression that I think there is no
mystery about consciousness $\ldots$ But I do not think these mysteries
necessarily need to be solved before we can answer the question with
which we are concerned in this paper.” We agree with Turing—we are
interested in creating programs that behave intelligently. The
additional project of making them conscious is not one that we are
equipped to take on, nor one whose success we would be able to
determine.

The Ethics and Risks of Developing Artificial Intelligence {#ethics-section}
----------------------------------------------------------

So far, we have concentrated on whether we *can* develop
AI, but we must also consider whether we *should*. If the
effects of AI technology are more likely to be negative than positive,
then it would be the moral responsibility of workers in the field to
redirect their research. Many new technologies have had unintended
negative side effects: nuclear fission brought Chernobyl and the threat
of global destruction; the internal combustion engine brought air
pollution, global warming, and the paving-over of paradise. In a sense,
automobiles are robots that have conquered the world by making
themselves indispensable.

All scientists and engineers face ethical considerations of how they
should act on the job, what projects should or should not be done, and
how they should be handled. See the handbook on the *Ethics of
Computing* @Berleur+Brunnstein:2001. AI, however, seems to pose
some fresh problems beyond that of, say, building bridges that don’t
fall down:

-   People might lose their jobs to automation.

-   People might have too much (or too little) leisure time.

-   People might lose their sense of being unique.

-   AI systems might be used toward undesirable ends.

-   The use of AI systems might result in a loss of accountability.

-   The success of AI might mean the end of the human race.

We will look at each issue in turn.

People might lose their jobs to automation. The modern
industrial economy has become dependent on computers in general, and
select AI programs in particular. For example, much of the economy,
especially in the , depends on the availability of consumer credit.
Credit card applications, charge approvals, and fraud detection are now
done by AI programs. One could say that thousands of workers have been
displaced by these AI programs, but in fact if you took away the AI
programs these jobs would not exist, because human labor would add an
unacceptable cost to the transactions. So far, automation through
information technology in general and AI in particular has created more
jobs than it has eliminated, and has created more interesting,
higher-paying jobs. Now that the canonical AI program is an “intelligent
agent” designed to assist a human, loss of jobs is less of a concern
than it was when AI focused on “expert systems” designed to replace
humans. But some researchers think that doing the complete job is the
right goal for AI. In reflecting on the 25th Anniversary of the AAAI,
Nils Nilsson [-@Nilsson:2005] set as a challenge the creation of that
could pass the employment test rather than the Turing Test—a robot that
could learn to do any one of a range of jobs. We may end up in a future
where unemployment is high, but even the unemployed serve as managers of
their own cadre of robot workers.

People might have too much (or too little) leisure time.
Alvin Toffler wrote in *Future Shock* [-@Toffler:1970],
“The work week has been cut by 50 percent since the turn of the century.
It is not out of the way to predict that it will be slashed in half
again by 2000.” Arthur C. Clarke [-@Clarke:1968] wrote that people in
2001 might be “faced with a future of utter boredom, where the main
problem in life is deciding which of several hundred TV channels to
select.” The only one of these predictions that has come close to
panning out is the number of TV channels. Instead, people working in
knowledge-intensive industries have found themselves part of an
integrated computerized system that operates 24 hours a day; to keep up,
they have been forced to work *longer* hours. In an
industrial economy, rewards are roughly proportional to the time
invested; working 10% more would tend to mean a 10% increase in income.
In an information economy marked by high-bandwidth communication and
easy replication of intellectual property (what Frank and
Cook [-@Frank+Cook:1996] call the “Winner-Take-All Society”), there is a
large reward for being slightly better than the competition; working 10%
more could mean a 100% increase in income. So there is increasing
pressure on everyone to work harder. AI increases the pace of
technological innovation and thus contributes to this overall trend, but
AI also holds the promise of allowing us to take some time off and let
our automated agents handle things for a while. Tim recommends using
automation and outsourcing to achieve a four-hour work week.

People might lose their sense of being unique. In
*Computer Power and Human Reason*, , the author of the
program, points out some of the potential threats that AI poses to
society. One of Weizenbaum’s principal arguments is that AI research
makes possible the idea that humans are automata—an idea that results in
a loss of autonomy or even of humanity. We note that the idea has been
around much longer than AI, going back at least to *L’Homme
Machine* @LaMettrie:1748. Humanity has survived other setbacks to
our sense of uniqueness: *De Revolutionibus Orbium
Coelestium* @Copernicus:1543 moved the Earth away from the center
of the solar system, and *Descent of Man* @Darwin:1871 put
*Homo sapiens* at the same level as other species. AI, if
widely successful, may be at least as threatening to the moral
assumptions of 21st-century society as Darwin’s theory of evolution was
to those of the 19th century.

AI systems might be used toward undesirable ends. Advanced
technologies have often been used by the powerful to suppress their
rivals. As the number theorist G. H. Hardy wrote @Hardy:1940, “A science
is said to be useful if its development tends to accentuate the existing
inequalities in the distribution of wealth, or more directly promotes
the destruction of human life.” This holds for all sciences, AI being no
exception. Autonomous AI systems are now commonplace on the battlefield;
the U.S. military deployed over 5,000 autonomous aircraft and 12,000
autonomous ground vehicles in Iraq @Singer:2009. One moral theory holds
that military robots are like medieval armor taken to its logical
extreme: no one would have moral objections to a soldier wanting to wear
a helmet when being attacked by large, angry, axe-wielding enemies, and
a teleoperated robot is like a very safe form of armor. On the other
hand, robotic weapons pose additional risks. To the extent that human
decision making is taken out of the firing loop, robots may end up
making decisions that lead to the killing of innocent civilians. At a
larger scale, the possession of powerful robots (like the possession of
sturdy helmets) may give a nation overconfidence, causing it to go to
war more recklessly than necessary. In most wars, at least one party is
overconfident in its military abilities—otherwise the conflict would
have been resolved peacefully.

Weizenbaum [-@Weizenbaum:1976] also pointed out that speech recognition
technology could lead to widespread wiretapping, and hence to a loss of
civil liberties. He didn’t foresee a world with terrorist threats that
would change the balance of how much surveillance people are willing to
accept, but he did correctly recognize that AI has the potential to
mass-produce . His prediction has in part come true: the
U.K. now has an extensive network of surveillance cameras,
and other countries routinely monitor Web traffic and telephone calls.
Some accept that computerization leads to a loss of privacy—Sun
Microsystems CEO Scott McNealy has said “You have zero privacy anyway.
Get over it.” David argues that loss of privacy is inevitable, and the
way to combat the asymmetry of power of the state over the individual is
to make the surveillance accessible to all citizens. argues for a
balancing of privacy and security; individual rights and community.

The use of AI systems might result in a loss of
accountability. In the litigious atmosphere that prevails in the
, legal liability becomes an important issue. When a physician relies on
the judgment of a medical expert system for a diagnosis, who is at fault
if the diagnosis is wrong? Fortunately, due in part to the growing
influence of decision-theoretic methods in medicine, it is now accepted
that negligence cannot be shown if the physician performs medical
procedures that have high *expected* utility, even if the
*actual* result is catastrophic for the patient. The
question should therefore be “Who is at fault if the diagnosis is
unreasonable?” So far, courts have held that medical expert systems play
the same role as medical textbooks and reference books; physicians are
responsible for understanding the reasoning behind any decision and for
using their own judgment in deciding whether to accept the system’s
recommendations. In designing medical expert systems as agents,
therefore, the actions should be thought of not as directly affecting
the patient but as influencing the physician’s behavior. If expert
systems become reliably more accurate than human diagnosticians, doctors
might become legally liable if they *don’t* use the
recommendations of an expert system. Atul explores this premise.

Similar issues are beginning to arise regarding the use of intelligent
agents on the Internet. Some progress has been made in incorporating
constraints into intelligent agents so that they cannot, for example,
damage the files of other users @Weld+Etzioni:1994. The problem is
magnified when money changes hands. If monetary transactions are made
“on one’s behalf” by an intelligent agent, is one liable for the debts
incurred? Would it be possible for an intelligent agent to have assets
itself and to perform electronic trades on its own behalf? So far, these
questions do not seem to be well understood. To our knowledge, no
program has been granted legal status as an individual for the purposes
of financial transactions; at present, it seems unreasonable to do so.
Programs are also not considered to be “drivers” for the purposes of
enforcing traffic regulations on real highways. In California law, at
least, there do not seem to be any legal sanctions to prevent an
automated vehicle from exceeding the speed limits, although the designer
of the vehicle’s control mechanism would be liable in the case of an
accident. As with human reproductive technology, the law has yet to
catch up with the new developments.

The success of AI might mean the end of the human race.
Almost any technology has the potential to cause harm in the wrong
hands, but with AI and robotics, we have the new problem that the wrong
hands might belong to the technology itself. Countless science fiction
stories have warned about robots or robot–human cyborgs running amok.
Early examples include Mary Shelley’s *Frankenstein, or the Modern
Prometheus* [-@Shelley:1818][^5] and Karel Capek’s play
*R.U.R.* (1921), in which robots conquer the world. In
movies, we have *The Terminator* (1984), which combines the
cliches of robots-conquer-the-world with time travel, and *The
Matrix* (1999), which combines robots-conquer-the-world with
brain-in-a-vat.

It seems that robots are the protagonists of so many conquer-the-world
stories because they represent the unknown, just like the witches and
ghosts of tales from earlier eras, or the Martians from *The War
of the Worlds* @Wells:1898. The question is whether an AI system
poses a bigger risk than traditional software. We will look at three
sources of risk.

First, the AI system’s state estimation may be incorrect, causing it to
do the wrong thing. For example, an autonomous car might incorrectly
estimate the position of a car in the adjacent lane, leading to an
accident that might kill the occupants. More seriously, a missile
defense system might erroneously detect an attack and launch a
counterattack, leading to the death of billions. These risks are not
really risks of AI systems—in both cases the same mistake could just as
easily be made by a human as by a computer. The correct way to mitigate
these risks is to design a system with checks and balances so that a
single state-estimation error does not propagate through the system
unchecked.

Second, specifying the right utility function for an AI system to
maximize is not so easy. For example, we might propose a utility
function designed to *minimize human suffering*, expressed
as an additive reward function over time as in . Given the way humans
are, however, we’ll always find a way to suffer even in paradise; so the
optimal decision for the AI system is to terminate the human race as
soon as possible—no humans, no suffering. With AI systems, then, we need
to be very careful what we ask for, whereas humans would have no trouble
realizing that the proposed utility function cannot be taken literally.
On the other hand, computers need not be tainted by the irrational
behaviors described in . Humans sometimes use their intelligence in
aggressive ways because humans have some innately aggressive tendencies,
due to natural selection. The machines we build need not be innately
aggressive, unless we decide to build them that way (or unless they
emerge as the end product of a mechanism design that encourages
aggressive behavior). Fortunately, there are techniques, such as , that
allows us to specify a utility function by example. One can hope that a
robot that is smart enough to figure out how to terminate the human race
is also smart enough to figure out that that was not the intended
utility function.

Third, the AI system’s learning function may cause it to evolve into a
system with unintended behavior. This scenario is the most serious, and
is unique to AI systems, so we will cover it in more depth. I. J. Good
wrote [-@Good:1965],

Let an be defined as a machine that can far surpass all the intellectual
activities of any man however clever. Since the design of machines is
one of these intellectual activities, an ultraintelligent machine could
design even better machines; there would then unquestionably be an
“intelligence explosion,” and the intelligence of man would be left far
behind. Thus the first ultraintelligent machine is the
*last* invention that man need ever make, provided that the
machine is docile enough to tell us how to keep it under control.

The “intelligence explosion” has also been called the by mathematics
professor and science fiction author Vernor Vinge, who
writes [-@Vinge:1993], “Within thirty years, we will have the
technological means to create superhuman intelligence. Shortly after,
the human era will be ended.” Good and Vinge (and many others) correctly
note that the curve of technological progress (on many measures) is
growing exponentially at present (consider Moore’s Law). However, it is
a leap to extrapolate that the curve will continue to a singularity of
near-infinite growth. So far, every other technology has followed an
S-shaped curve, where the exponential growth eventually tapers off.
Sometimes new technologies step in when the old ones plateau; sometimes
we hit hard limits. With less than a century of high-technology history
to go on, it is difficult to extrapolate hundreds of years ahead.

Note that the concept of ultraintelligent machines assumes that
intelligence is an especially important attribute, and if you have
enough of it, all problems can be solved. But we know there are limits
on computability and computational complexity. If the problem of
defining ultraintelligent machines (or even approximations to them)
happens to fall in the class of, say, NEXPTIME-complete problems, and if
there are no heuristic shortcuts, then even exponential progress in
technology won’t help—the speed of light puts a strict upper bound on
how much computing can be done; problems beyond that limit will not be
solved. We still don’t know where those upper bounds are.

Vinge is concerned about the coming singularity, but some computer
scientists and futurists relish it. Hans Moravec [-@Moravec:2000]
encourages us to give every advantage to our “mind children,” the robots
we create, which may surpass us in intelligence. There is even a new
word——for the active social movement that looks forward to this future
in which humans are merged with—or replaced by—robotic and biotech
inventions. Suffice it to say that such issues present a challenge for
most moral theorists, who take the preservation of human life and the
human species to be a good thing. Ray Kurzweil is currently the most
visible advocate for the singularity view, writing in *The
Singularity is Near* [-@Kurzweil:2005]:

The Singularity will allow us to transcend these limitations of our
biological bodies and brain. We will gain power over our fates. Our
mortality will be in our own hands. We will be able to live as long as
we want (a subtly different statement from saying we will live forever).
We will fully understand human thinking and will vastly extend and
expand its reach. By the end of this century, the nonbiological portion
of our intelligence will be trillions of trillions of times more
powerful than unaided human intelligence.

Kurzweil also notes the potential dangers, writing “But the Singularity
will also amplify the ability to act on our destructive inclinations, so
its full story has not yet been written.”

If ultraintelligent machines are a possibility, we humans would do well
to make sure that we design their predecessors in such a way that they
design themselves to treat us well. Science fiction writer Isaac was the
first to address this issue, with his three laws of robotics:

1.  A robot may not injure a human being or, through inaction, allow a
    human being to come to harm.

2.  A robot must obey orders given to it by human beings, except where
    such orders would conflict with the First Law.

3.  A robot must protect its own existence as long as such protection
    does not conflict with the First or Second Law.

These laws seem reasonable, at least to us humans.[^6] But the trick is
how to implement these laws. In the Asimov story
*Roundabout* a robot is sent to fetch some selenium. Later
the robot is found wandering in a circle around the selenium source.
Every time it heads toward the source, it senses a danger, and the third
law causes it to veer away. But every time it veers away, the danger
recedes, and the power of the second law takes over, causing it to veer
back towards the selenium. The set of points that define the balancing
point between the two laws defines a circle. This suggests that the laws
are not logical absolutes, but rather are weighed against each other,
with a higher weighting for the earlier laws. Asimov was probably
thinking of an architecture based on control theory—perhaps a linear
combination of factors—while today the most likely architecture would be
a probabilistic reasoning agent that reasons over probability
distributions of outcomes, and maximizes utility as defined by the three
laws. But presumably we don’t want our robots to prevent a human from
crossing the street because of the nonzero chance of harm. That means
that the negative utility for harm to a human must be much greater than
for disobeying, but that each of the utilities is finite, not infinite.

goes into more detail about how to design a . He asserts that
friendliness (a desire not to harm humans) should be designed in from
the start, but that the designers should recognize both that their own
designs may be flawed, and that the robot will learn and evolve over
time. Thus the challenge is one of mechanism design—to define a
mechanism for evolving AI systems under a system of checks and balances,
and to give the systems utility functions that will remain friendly in
the face of such changes.

We can’t just give a program a static utility function, because
circumstances, and our desired responses to circumstances, change over
time. For example, if technology had allowed us to design a
super-powerful AI agent in 1800 and endow it with the prevailing morals
of the time, it would be fighting today to reestablish slavery and
abolish women’s right to vote. On the other hand, if we build an AI
agent today and tell it to evolve its utility function, how can we
assure that it won’t reason that “Humans think it is moral to kill
annoying insects, in part because insect brains are so primitive. But
human brains are primitive compared to my powers, so it must be moral
for me to kill humans.”

hypothesizes that even an innocuous chess program could pose a risk to
society. Similarly, Marvin Minsky once suggested that an AI program
designed to solve the Riemann Hypothesis might end up taking over all
the resources of Earth to build more powerful supercomputers to help
achieve its goal. The moral is that even if you only want your program
to play chess or prove theorems, if you give it the capability to learn
and alter itself, you need safeguards. Omohundro concludes that “Social
structures which cause individuals to bear the cost of their negative
externalities would go a long way toward ensuring a stable and positive
future,” This seems to be an excellent idea for society in general,
regardless of the possibility of ultraintelligent machines.

We should note that the idea of safeguards against change in utility
function is not a new one. In the *Odyssey*, Homer (ca. 700
b.c.) described ’ encounter with the sirens, whose song was
so alluring it compelled sailors to cast themselves into the sea.
Knowing it would have that effect on him, Ulysses ordered his crew to
bind him to the mast so that he could not perform the self-destructive
act. It is interesting to think how similar safeguards could be built
into AI systems.

Finally, let us consider the robot’s point of view. If robots become
conscious, then to treat them as mere “machines” (e.g., to take them
apart) might be immoral. Science fiction writers have addressed the
issue of robot rights. The movie
*A.I.* @Spielberg:2001 was based on a story by
Brian Aldiss about an intelligent robot who was programmed to believe
that he was human and fails to understand his eventual abandonment by
his owner–mother. The story (and the movie) argue for the need for a
civil rights movement for robots.

This chapter has addressed the following issues:

-   Philosophers use the term for the hypothesis that machines could
    possibly behave intelligently, and for the hypothesis that such
    machines would count as having actual minds (as opposed to simulated
    minds).

-   Alan Turing rejected the question “Can machines think?” and replaced
    it with a behavioral test. He anticipated many objections to the
    possibility of thinking machines. Few AI researchers pay attention
    to the Turing Test, preferring to concentrate on their systems’
    performance on practical tasks, rather than the ability to imitate
    humans.

-   There is general agreement in modern times that mental states are
    brain states.

-   Arguments for and against strong AI are inconclusive. Few mainstream
    AI researchers believe that anything significant hinges on the
    outcome of the debate.

-   Consciousness remains a mystery.

-   We identified six potential threats to society posed by AI and
    related technology. We concluded that some of the threats are either
    unlikely or differ little from threats posed by “unintelligent”
    technologies. One threat in particular is worthy of further
    consideration: that ultraintelligent machines might lead to a future
    that is very different from today—we may not like it, and at that
    point we may not have a choice. Such considerations lead inevitably
    to the conclusion that we must weigh carefully, and soon, the
    possible consequences of AI research.

Sources for the various responses to Turing’s 1950 paper and for the
main critics of weak AI were given in the chapter. Although it became
fashionable in the post-neural-network era to deride symbolic
approaches, not all philosophers are critical of gofai.
Some are, in fact, ardent advocates and even practitioners. Zenon
Pylyshyn [-@Pylyshyn:1984] has argued that cognition can best be
understood through a computational model, not only in principle but also
as a way of conducting research at present, and has specifically
rebutted Dreyfus’s criticisms of the computational model of human
cognition @Pylyshyn:1974. Gilbert Harman [-@Harman:1983], in analyzing
belief revision, makes connections with AI research on truth maintenance
systems. Michael Bratman has applied his “belief-desire-intention” model
of human psychology @Bratman:1987 to AI research on planning
@Bratman:1992. At the extreme end of strong AI, Aaron
Sloman [-@Sloman:1978 p. xiii] has even described as “racialist” the
claim by Joseph that intelligent machines can never be regarded as
persons.

Proponents of the importance of embodiment in cognition include the
philosophers Merleau-Ponty, whose *Phenomenology of
Perception* [-@Merleau-Ponty:1945] stressed the importance of the
body and the subjective interpretation of reality afforded by our
senses, and Heidegger, whose *Being and Time*
[-@Heidegger:1927] asked what it means to actually be an agent, and
criticized all of the history of philosophy for taking this notion for
granted. In the computer age, Alva Noe [-@Noe:2009] and Andy Clark
[-@Clark:1998; -@Clark:2008] propose that our brains form a rather
minimal representation of the world, use the world itself in a
just-in-time basis to maintain the illusion of a detailed internal
model, use props in the world (such as paper and pencil as well as
computers) to increase the capabilities of the mind. and present
arguments for how the body helps shape cognition.

The nature of the mind has been a standard topic of philosophical
theorizing from ancient times to the present. In the
*Phaedo*, Plato specifically considered and rejected the
idea that the mind could be an “attunement” or pattern of organization
of the parts of the body, a viewpoint that approximates the
functionalist viewpoint in modern philosophy of mind. He decided instead
that the mind had to be an immortal, immaterial soul, separable from the
body and different in substance—the viewpoint of dualism. Aristotle
distinguished a variety of souls (Greek $\psi\upsilon\chi\eta$) in
living things, some of which, at least, he described in a functionalist
manner. (See Nussbaum [-@Nussbaum:1978] for more on ’s .)

Descartes is notorious for his dualistic view of the human mind, but
ironically his historical influence was toward mechanism and
physicalism. He explicitly conceived of animals as automata, and he
anticipated the Turing Test, writing “it is not conceivable [that a
machine] should produce different arrangements of words so as to give an
appropriately meaningful answer to whatever is said in its presence, as
even the dullest of men can do” @Descartes:1637. Descartes’s spirited
defense of the animals-as-automata viewpoint actually had the effect of
making it easier to conceive of humans as automata as well, even though
he himself did not take this step. The book *L’Homme
Machine* @LaMettrie:1748 did explicitly argue that humans are
automata.

Modern analytic philosophy has typically accepted physicalism, but the
variety of views on the content of mental states is bewildering. The
identification of mental states with brain states is usually attributed
to and . The debate between narrow-content and wide-content views of
mental states was triggered by Hilary , who introduced so-called (rather
than brain-in-a-vat, as we did in the chapter) as a device to generate
identical brain states with different (wide) content.

Functionalism is the philosophy of mind most naturally suggested by AI.
The idea that mental states correspond to classes of brain states
defined functionally is due to and . Perhaps the most forceful proponent
of functionalism is Daniel Dennett, whose ambitiously titled work
*Consciousness Explained* @Dennett:1991 has attracted many
attempted rebuttals. argues there is no such thing as an objective
*self*, that consciousness is the subjective appearance of
a world. The inverted spectrum argument concerning qualia was introduced
by John . Frank  designed an influential thought experiment involving
Mary, a color scientist who has been brought up in an entirely
black-and-white world. *There’s Something About Mary*
@Ludlow+al:2004 collects several papers on this topic.

Functionalism has come under attack from authors who claim that they do
not account for the *qualia* or “what it’s like” aspect of
mental states @Nagel:1974. Searle has focused instead on the alleged
inability of functionalism to account for intentionality @Searle:1980
[@Searle:1984; @Searle:1992]. Churchland and
Churchland [-@Churchland+Churchland:1982] rebut both these types of
criticism. The Chinese Room has been debated endlessly @Searle:1980
[@Searle:1990; @Preston+Bishop:2002]. We’ll just mention here a related
work: Terry Bisson’s [-@Bisson:1990] science fiction story
*They’re Made out of Meat*, in which alien robotic
explorers who visit earth are incredulous to find thinking human beings
whose minds are made of meat. Presumably, the robotic alien equivalent
of Searle believes that he can think due to the special causal powers of
robotic circuits; causal powers that mere meat-brains do not possess.

Ethical issues in AI predate the existence of the field itself. I. J.
Good’s [-@Good:1965] ultraintelligent machine idea was foreseen a
hundred years earlier by Samuel . Written four years after the
publication of Darwin’s *On the Origins of Species* and at
a time when the most sophisticated machines were steam engines, Butler’s
article on *Darwin Among the Machines* envisioned “the
ultimate development of mechanical consciousness” by natural selection.
The theme was reiterated by George in a book of the same title.

The philosophical literature on minds, brains, and related topics is
large and difficult to read without training in the terminology and
methods of argument employed. The *Encyclopedia of
Philosophy* @Edwards:1967 is an impressively authoritative and
very useful aid in this process. *The Cambridge Dictionary of
Philosophy* @Audi:1999 is a shorter and more accessible work, and
the online *Stanford Encyclopedia of Philosophy* offers
many excellent articles and up-to-date references. The *MIT
Encyclopedia of Cognitive Science* @Wilson+Keil:1999 covers the
philosophy of mind as well as the biology and psychology of mind. There
are several general introductions to the philosophical “AI question”
@Boden:1990
[@Haugeland:1985; @Copeland:1993; @McCorduck:2004; @Minsky:2007].
*The Behavioral and Brain Sciences*, abbreviated
*BBS*, is a major journal devoted to philosophical and
scientific debates about AI and neuroscience. Topics of ethics and
responsibility in AI are covered in the journals *AI and
Society* and *Journal of Artificial Intelligence and
Law*.

[disability-exercise]Go through Turing’s list of alleged “disabilities”
of machines, identifying which have been achieved, which are achievable
in principle by a program, and which are still problematic because they
require conscious mental states.

Find and analyze an account in the popular media of one or more of the
arguments to the effect that AI is impossible.

Attempt to write definitions of the terms “intelligence,” “thinking,”
and “consciousness.” Suggest some possible objections to your
definitions.

Does a refutation of the Chinese room argument necessarily prove that
appropriately programmed computers have mental states? Does an
acceptance of the argument necessarily mean that computers cannot have
mental states?

[brain-prosthesis-exercise]In the brain replacement argument, it is
important to be able to restore the subject’s brain to normal, such that
its external behavior is as it would have been if the operation had not
taken place. Can the skeptic reasonably object that this would require
updating those neurophysiological properties of the neurons relating to
conscious experience, as distinct from those involved in the functional
behavior of the neurons?

Suppose that a Prolog program containing many clauses about the rules of
British citizenship is compiled and run on an ordinary computer. Analyze
the “brain states” of the computer under wide and narrow content.

Alan Perlis [-@Perlis:1982] wrote, “A year spent in artificial
intelligence is enough to make one believe in God”. He also wrote, in a
letter to Philip Davis, that one of the central dreams of computer
science is that “through the performance of computers and their programs
we will remove all doubt that there is only a chemical distinction
between the living and nonliving world.” To what extent does the
progress made so far in artificial intelligence shed light on these
issues? Suppose that at some future date, the AI endeavor has been
completely successful; that is, we have build intelligent agents capable
of carrying out any human cognitive task at human levels of ability. To
what extent would that shed light on these issues?

Compare the social impact of artificial intelligence in the last fifty
years with the social impact of the introduction of electric appliances
and the internal combustion engine in the fifty years between 1890 and
1940.

I. J. Good claims that intelligence is the most important quality, and
that building ultraintelligent machines will change everything. A
sentient cheetah counters that “Actually speed is more important; if we
could build ultrafast machines, that would change everything,” and a
sentient elephant claims “You’re both wrong; what we need is ultrastrong
machines.” What do you think of these arguments?

Analyze the potential threats from AI technology to society. What
threats are most serious, and how might they be combated? How do they
compare to the potential benefits?

How do the potential threats from AI technology compare with those from
other computer science technologies, and to bio-, nano-, and nuclear
technologies?

Some critics object that AI is impossible, while others object that it
is *too* possible and that ultraintelligent machines pose a
threat. Which of these objections do you think is more likely? Would it
be a contradiction for someone to hold both positions?

[^1]: For example, the opera Coppélia (1870), the novel *Do
    Androids Dream of Electric Sheep?* (1968), the movies
    *AI* (2001) and *Wall-E* (2008), and in
    song, Noel Coward’s 1955 version of *Let’s Do It: Let’s Fall
    in Love* predicted “probably we’ll live to see machines do
    it.” He didn’t.

[^2]: This situation may be familiar to those who have seen the 1999
    film *The Matrix*.

[^3]: One can imagine using an identical “control” subject who is given
    a placebo operation, for comparison.

[^4]: The fact that the stacks of paper might contain trillions of pages
    and the generation of answers would take millions of years has no
    bearing on the *logical* structure of the argument. One
    aim of philosophical training is to develop a finely honed sense of
    which objections are germane and which are not.

[^5]: As a young man, Charles Babbage was influenced by reading
    *Frankenstein*.

[^6]: A robot might notice the inequity that a human is allowed to kill
    another in self-defense, but a robot is required to sacrifice its
    own life to save a human.
Classical Planning {#planning-chapter}
==================

We have defined AI as the study of rational action, which means that
—devising a plan of action to achieve one’s goals—is a critical part of
AI. We have seen two examples of planning agents so far: the
search-based problem-solving agent of and the hybrid logical agent of .
In this chapter we introduce a representation for planning problems that
scales up to problems that could not be handled by those earlier
approaches.

develops an expressive yet carefully constrained language for
representing planning problems. shows how forward and backward search
algorithms can take advantage of this representation, primarily through
accurate heuristics that can be derived automatically from the structure
of the representation. (This is analogous to the way in which effective
domain-independent heuristics were constructed for constraint
satisfaction problems in .) Section [graphplan-section] shows how a data
structure called the planning graph can make the search for a plan more
efficient. We then describe a few of the other approaches to planning,
and conclude by comparing the various approaches.

This chapter covers fully observable, deterministic, static environments
with single agents. Chapters [advanced-planning-chapter]
and [complex-decisions-chapter] cover partially observable, stochastic,
dynamic environments with multiple agents.

Definition of Classical Planning {#planning-vs-search-section}
--------------------------------

[strips-operator-section]

The problem-solving agent of can find sequences of actions that result
in a goal state. But it deals with atomic representations of states and
thus needs good domain-specific heuristics to perform well. The hybrid
propositional logical agent of can find plans without domain-specific
heuristics because it uses domain-independent heuristics based on the
logical structure of the problem. But it relies on ground
(variable-free) propositional inference, which means that it may be
swamped when there are many actions and states. For example, in the
wumpus world, the simple action of moving a step forward had to be
repeated for all four agent orientations, $T$ time steps, and $n^2$
current locations.

In response to this, planning researchers have settled on a —one in
which a state of the world is represented by a collection of variables.
We use a language called , the Planning Domain Definition Language, that
allows us to express all $4
T n^2$ actions with one action schema. There have been several versions
of PDDL; we select a simple version and alter its syntax to be
consistent with the rest of the book.[^1] We now show how describes the
four things we need to define a search problem: the initial state, the
actions that are available in a state, the result of applying an action,
and the goal test.

Each is represented as a conjunction of fluents that are ground,
functionless atoms. For example, ${Poor} \land
{Unknown}$ might represent the state of a hapless agent, and a state
in a package delivery problem might be
${At}({Truck}{}_{1}, {Melbourne}) \land {At}({Truck}{}_{2},
{Sydney})$. is used: the closed-world assumption means that any
fluents that are not mentioned are false, and the unique names
assumption means that ${Truck}_1$ and ${Truck}_2$ are distinct. The
following fluents are *not* allowed in a state:
${At}(x,y)$ (because it is non-ground), $\lnot {Poor}$ (because it
is a negation), and ${At}({Father}({Fred}), {Sydney})$ (because
it uses a function symbol). The representation of states is carefully
designed so that a state can be treated either as a conjunction of
fluents, which can be manipulated by logical inference, or as a
*set* of fluents, which can be manipulated with set
operations. The is sometimes easier to deal with.

are described by a set of action schemas that implicitly define the
$\noprog{Actions}(s)$ and $\result{s}{a}$ functions needed to do a
problem-solving search. We saw in that any system for action description
needs to solve the frame problem—to say what changes and what stays the
same as the result of the action. Classical planning concentrates on
problems where most actions leave most things unchanged. Think of a
world consisting of a bunch of objects on a flat surface. The action of
nudging an object causes that object to change its location by a vector
$\Delta$. A concise description of the action should mention only
$\Delta$; it shouldn’t have to mention all the objects that stay in
place. does that by specifying the result of an action in terms of what
changes; everything that stays the same is left unmentioned.

A set of ground (variable-free) actions can be represented by a single .
The schema is a representation—it lifts the level of reasoning from
propositional logic to a restricted subset of first-order logic. For
example, here is an action schema for flying a plane from one location
to another:

((p, , ),\
\
 )

The schema consists of the action name, a list of all the variables used
in the schema, a and an . Although we haven’t said yet how the action
schema converts into logical sentences, think of the variables as being
universally quantified. We are free to choose whatever values we want to
instantiate the variables. For example, here is one ground action that
results from substituting values for all the variables:

((~1~, , ),\
\
 )

The precondition and effect of an action are each conjunctions of
literals (positive or negated atomic sentences). The precondition
defines the states in which the action can be executed, and the effect
defines the result of executing the action. An action $a$ can be
executed in state $s$ if $s$ entails the precondition of $a$. Entailment
can also be expressed with the set semantics: $s \entails q$ iff every
positive literal in $q$ is in $s$ and every negated literal in $q$ is
not. In formal notation we say

$$\begin{aligned}
(a \in  \noprog{Actions}(s)) \lequiv  s \entails \noprog{Precond}(a) \,,\end{aligned}$$

where any variables in $a$ are universally quantified. For example,

((, , ) (s))\
 s ((p, ) (p) () ())

We say that action $a$ is in state $s$ if the preconditions are
satisfied by $s$. When an action schema $a$ contains variables, it may
have multiple applicable instantiations. For example, with the initial
state defined in , the ${Fly}$ action can be instantiated as
${Fly}(P_1, {SFO},
{JFK})$ or as ${Fly}(P_2, {JFK}, {SFO})$, both of which are
applicable in the initial state. If an action $a$ has $v$ variables,
then, in a domain with $k$ unique names of objects, it takes $O(v^k)$
time in the worst case to find the applicable ground actions.

Sometimes we want to [propositionalize-page] a PDDL problem—replace each
action schema with a set of ground actions and then use a propositional
solver such as to find a solution. However, this is impractical when $v$
and $k$ are large.

The of executing action $a$ in state $s$ is defined as a state $s'$
which is represented by the set of fluents formed by starting with $s$,
removing the fluents that appear as negative literals in the action’s
effects (what we call the or $\prog{Del}(a)$), and adding the fluents
that are positive literals in the action’s effects (what we call the or
$\prog{Add}(a)$):

$$\result{s}{a} = (s - \noprog{Del}(a)) \union \noprog{Add}(a) \ .
\label{strips-update-equation}$$

For example, with the action ${Fly}({P}_1, {SFO}, {JFK})$, we
would remove ${At}({P}_1, {SFO})$ and add ${At}({P}_1,
{JFK})$. It is a requirement of action schemas that any variable in
the effect must also appear in the precondition. That way, when the
precondition is matched against the state $s$, all the variables will be
bound, and $\result{s}{a}$ will therefore have only ground atoms. In
other words, ground states are closed under the operation.

Also note that the fluents do not explicitly refer to time, as they did
in . There we needed superscripts for time, and successor-state axioms
of the form $$F^{t+1} \lequiv 
  {ActionCausesF}^{t} \lor (F^{t} \land \lnot {ActionCausesNotF}^{t}) \ .$$
In the times and states are implicit in the action schemas: the
precondition always refers to time $t$ and the effect to time $t+1$.

A set of action schemas serves as a definition of a planning
*domain*. A specific *problem* within the
domain is defined with the addition of an initial state and a goal. The
is a conjunction of ground atoms. (As with all states, the closed-world
assumption is used, which means that any atoms that are not mentioned
are false.) The is just like a precondition: a conjunction of literals
(positive or negative) that may contain variables, such as
${At}(p,{SFO}) \land {Plane}(p)$. Any variables are treated as
existentially quantified, so this goal is to have *any*
plane at SFO. The problem is solved when we can find a sequence of
actions that end in a state $s$ that entails the goal. For example, the
state ${Rich} \land {Famous} \land
{Miserable}$ entails the goal ${Rich} \land {Famous}$, and the
state ${Plane}({Plane}_1) \land {At}({Plane}_1,{SFO})$ entails
the goal ${At}(p,{SFO}) \land {Plane}(p)$.

Now we have defined planning as a search problem: we have an initial
state, an function, a function, and a goal test. We’ll look at some
example problems before investigating efficient search algorithms.

### Example: Air cargo transport

[airport-pddl-algorithm]

shows an air cargo transport problem involving loading and unloading
cargo and flying it from place to place. The problem can be defined with
three actions: ${Load}$, ${Unload}$, and ${Fly}$. The actions
affect two predicates: ${In}(c, p)$ means that cargo $c$ is inside
plane $p$, and ${At}(x, a)$ means that object $x$ (either plane or
cargo) is at airport $a$. Note that some care must be taken to make sure
the ${At}$ predicates are maintained properly. When a plane flies from
one airport to another, all the cargo inside the plane goes with it. In
first-order logic it would be easy to quantify over all objects that are
inside the plane. But basic does not have a universal quantifier, so we
need a different solution. The approach we use is to say that a piece of
cargo ceases to be $At$ anywhere when it is $In$ a plane; the cargo only
becomes $At$ the new airport when it is unloaded. So ${At}$ really
means “available for use at a given location.” The following plan is a
solution to the problem: $$\begin{array}{l}
 [{Load}(C_{1},P_{1},{SFO}), {Fly}(P_{1}, {SFO}, {JFK}), {Unload}(C_{1},P_{1},{JFK}), \\
 ~~ {Load}(C_{2},P_{2},{JFK}), {Fly}(P_{2},{JFK},{SFO}), {Unload}(C_{2},P_{2},{SFO})]\ .
  \end{array}$$ Finally, there is the problem of spurious actions such
as ${Fly}(P_1,{JFK},{JFK})$, which should be a no-op, but which
has contradictory effects (according to the definition, the effect would
include ${At}(P_1,{JFK}) \land \lnot {At}(P_1, {JFK})$). It is
common to ignore such problems, because they seldom cause incorrect
plans to be produced. The correct approach is to add inequality
preconditions saying that the ${from}$ and ${to}$ airports must be
different; see another example of this in .

### Example: The spare tire problem {#spare-tire-section}

Consider the problem of changing a flat tire (). The goal is to have a
good spare tire properly mounted onto the car’s axle, where the initial
state has a flat tire on the axle and a good spare tire in the trunk. To
keep it simple, our version of the problem is an abstract one, with no
sticky lug nuts or other complications. There are just four actions:
removing the spare from the trunk, removing the flat tire from the axle,
putting the spare on the axle, and leaving the car unattended overnight.
We assume that the car is parked in a particularly bad neighborhood, so
that the effect of leaving it overnight is that the tires disappear. A
solution to the problem is
$ [{Remove}({Flat}, {Axle}), {Remove}({Spare}, {Trunk}), {PutOn}({Spare}, {Axle})]$.

[tire-pddl-algorithm]

### Example: The blocks world {#blocks-world-section}

One of the most famous planning domains is known as the . This domain
consists of a set of cube-shaped blocks sitting on a table.[^2] The
blocks can be stacked, but only one block can fit directly on top of
another. A robot arm can pick up a block and move it to another
position, either on the table or on top of another block. The arm can
pick up only one block at a time, so it cannot pick up a block that has
another one on it. The goal will always be to build one or more stacks
of blocks, specified in terms of what blocks are on top of what other
blocks. For example, a goal might be to get block $A$ on $B$ and block
$B$ on $C$ (see ).

[blocks-pddl-algorithm]

[sussman-anomaly-figure]

We use ${On}(b,x)$ to indicate that block $b$ is on $x$, where $x$ is
either another block or the table. The action for moving block $b$ from
the top of $x$ to the top of $y$ will be ${Move}(b,x,y)$. Now, one of
the preconditions on moving $b$ is that no other block be on it. In
first-order logic, this would be $\lnot \exists\,x\ {On}(x,b)$ or,
alternatively, $\forall\,x\
\lnot {On}(x,b)$. Basic does not allow quantifiers, so instead we
introduce a predicate ${Clear}(x)$ that is true when nothing is on
$x$. (The complete problem description is in .)

The action ${Move}$ moves a block $b$ from $x$ to $y$ if both $b$ and
$y$ are clear. After the move is made, $b$ is still clear but $y$ is
not. A first attempt at the ${Move}$ schema is

((b,x,y),\
 ,\
 )  .

Unfortunately, this does not maintain ${Clear}$ properly when $x$ or
$y$ is the table. When $x$ is the ${Table}$, this action has the
effect ${Clear}({Table})$, but the table should not become clear;
and when $y \eq 
{Table}$, it has the precondition ${Clear}({Table})$, but the
table does not have to be clear for us to move a block onto it. To fix
this, we do two things. First, we introduce another action to move a
block $b$ from $x$ to the table:

((b,x),\
 ,\
 ) .

Second, we take the interpretation of ${Clear}(x)$ to be “there is a
clear space on $x$ to hold a block.” Under this interpretation,
${Clear}({Table})$ will always be true. The only problem is that
nothing prevents the planner from using ${Move}(b,x,{Table})$
instead of ${MoveToTable}(b,x)$. We could live with this problem—it
will lead to a larger-than-necessary search space, but will not lead to
incorrect answers—or we could introduce the predicate ${Block}$ and
add ${Block}(b) \land 
{Block}(y)$ to the precondition of ${Move}$.

### The complexity of classical planning

In this subsection we consider the theoretical complexity of planning
and distinguish two decision problems. is the question of whether there
exists any plan that solves a planning problem. asks whether there is a
solution of length $k$ or less; this can be used to find an optimal
plan.

The first result is that both decision problems are decidable for
classical planning. The proof follows from the fact that the number of
states is finite. But if we add function symbols to the language, then
the number of states becomes infinite, and PlanSAT becomes only
semidecidable: an algorithm exists that will terminate with the correct
answer for any solvable problem, but may not terminate on unsolvable
problems. The Bounded PlanSAT problem remains decidable even in the
presence of function symbols. For proofs of the assertions in this
section, see .

Both PlanSAT and Bounded PlanSAT are in the complexity class PSPACE, a
class that is larger (and hence more difficult) than NP and refers to
problems that can be solved by a deterministic Turing machine with a
polynomial amount of space. Even if we make some rather severe
restrictions, the problems remain quite difficult. For example, if we
disallow negative effects, both problems are still NP-hard. However, if
we also disallow negative preconditions, PlanSAT reduces to the class P.

These worst-case results may seem discouraging. We can take solace in
the fact that agents are usually not asked to find plans for arbitrary
worst-case problem instances, but rather are asked for plans in specific
domains (such as blocks-world problems with $n$ blocks), which can be
much easier than the theoretical worst case. For many domains (including
the blocks world and the air cargo world), Bounded PlanSAT is
NP-complete while PlanSAT is in P; in other words, optimal planning is
usually hard, but sub-optimal planning is sometimes easy. To do well on
easier-than-worst-case problems, we will need good search heuristics.
That’s the true advantage of the classical planning formalism: it has
facilitated the development of very accurate domain-independent
heuristics, whereas systems based on successor-state axioms in
first-order logic have had less success in coming up with good
heuristics.

Algorithms for Planning as State-Space Search {#planning-search-section}
---------------------------------------------

Now we turn our attention to planning algorithms. We saw how the
description of a planning problem defines a search problem: we can
search from the initial state through the space of states, looking for a
goal. One of the nice advantages of the declarative representation of
action schemas is that we can also search backward from the goal,
looking for the initial state. compares forward and backward searches.

[two-plan-searches-figure]

### Forward (progression) state-space search

Now that we have shown how a planning problem maps into a search
problem, we can solve planning problems with any of the heuristic search
algorithms from or a local search algorithm from (provided we keep track
of the actions used to reach the goal). From the earliest days of
planning research (around 1961) until around 1998 it was assumed that
forward state-space search was too inefficient to be practical. It is
not hard to come up with reasons why.

First, forward search is prone to exploring irrelevant actions. Consider
the noble task of buying a copy of *AI: A Modern Approach*
from an online bookseller. Suppose there is an action schema
${Buy}({isbn})$ with effect ${Own}({isbn})$. ISBNs are 10
digits, so this action schema represents 10 billion ground actions. An
uninformed forward-search algorithm would have to start enumerating
these 10 billion actions to find one that leads to the goal.

Second, planning problems often have large state spaces. Consider an air
cargo problem with 10 airports, where each airport has 5 planes and 20
pieces of cargo. The goal is to move all the cargo at airport $A$ to
airport $B$. There is a simple solution to the problem: load the 20
pieces of cargo into one of the planes at $A$, fly the plane to $B$, and
unload the cargo. Finding the solution can be difficult because the
average branching factor is huge: each of the 50 planes can fly to 9
other airports, and each of the 200 packages can be either unloaded (if
it is loaded) or loaded into any plane at its airport (if it is
unloaded). So in any state there is a minimum of 450 actions (when all
the packages are at airports with no planes) and a maximum of 10,450
(when all packages and planes are at the same airport). On average,
let’s say there are about 2000 possible actions per state, so the search
graph up to the depth of the obvious solution has about ${2000}^{{41}}$
nodes.

Clearly, even this relatively small problem instance is hopeless without
an accurate heuristic. Although many real-world applications of planning
have relied on domain-specific heuristics, it turns out (as we see in )
that strong domain-independent heuristics can be derived automatically;
that is what makes forward search feasible.

### Backward (regression) relevant-states search {#regression-planning-section}

In regression search we start at the goal and apply the actions backward
until we find a sequence of steps that reaches the initial state. It is
called search because we only consider actions that are relevant to the
goal (or current state). As in belief-state search (), there is a
*set* of relevant states to consider at each step, not just
a single state.

We start with the goal, which is a conjunction of literals forming a
description of a set of states—for example, the goal $\lnot {Poor}
\land {Famous}$ describes those states in which ${Poor}$ is false,
${Famous}$ is true, and any other fluent can have any value. If there
are $n$ ground fluents in a domain, then there are $2^n$ ground states
(each fluent can be true or false), but $3^n$ descriptions of sets of
goal states (each fluent can be positive, negative, or not mentioned).

In general, backward search works only when we know how to regress from
a state description to the predecessor state description. For example,
it is hard to search backwards for a solution to the $n$-queens problem
because there is no easy way to describe the states that are one move
away from the goal. Happily, the representation was designed to make it
easy to regress actions—if a domain can be expressed in , then we can do
regression search on it. Given a ground goal description $g$ and a
ground action $a$, the regression from $g$ over $a$ gives us a state
description $g'$ defined by
$$g' = (g - \noprog{Add}(a)) \union {Precond}(a) \ .$$ That is, the
effects that were added by the action need not have been true before,
and also the preconditions must have held before, or else the action
could not have been executed. Note that $\noprog{Del}(a)$ does not
appear in the formula; that’s because while we know the fluents in
$\noprog{Del}(a)$ are no longer true after the action, we don’t know
whether or not they were true before, so there’s nothing to be said
about them.

To get the full advantage of backward search, we need to deal with
partially uninstantiated actions and states, not just ground ones. For
example, suppose the goal is to deliver a specific piece of cargo to
SFO: ${At}(C_2, {SFO})$. That suggests the action
${Unload}(C_2, p', {SFO})$:

((C~2~, p’, ),\
\
 .

(Note that we have variable names (changing $p$ to $p'$ in this case) so
that there will be no confusion between variable names if we happen to
use the same action schema twice in a plan. The same approach was used
in for first-order logical inference.) This represents unloading the
package from an *unspecified* plane at SFO; any plane will
do, but we need not say which one now. We can take advantage of the
power of first-order representations: a single description summarizes
the possibility of using *any* of the planes by implicitly
quantifying over $p'$. The regressed state description is
$$g' = {In}(C_2, p') \land {At}(p', {SFO}) \land {Cargo}(C_2) \land {Plane}(p') \land {Airport}({SFO}) \,.$$
The final issue is deciding which actions are candidates to regress
over. In the forward direction we chose actions that were —those actions
that could be the next step in the plan. In backward search we want
actions that are —those actions that could be the *last*
step in a plan leading up to the current goal state.

For an action to be relevant to a goal it obviously must contribute to
the goal: at least one of the action’s effects (either positive or
negative) must unify with an element of the goal. What is less obvious
is that the action must not have any effect (positive or negative) that
negates an element of the goal. Now, if the goal is $A
\land B \land C$ and an action has the effect $A \land B \land
\lnot C$ then there is a colloquial sense in which that action is very
relevant to the goal—it gets us two-thirds of the way there. But it is
not relevant in the technical sense defined here, because this action
could not be the *final* step of a solution—we would always
need at least one more step to achieve $C$.

Given the goal ${At}(C_2, {SFO})$, several instantiations of
${Unload}$ are relevant: we could chose any specific plane to unload
from, or we could leave the plane unspecified by using the action
${Unload}(C_2, p', {SFO})$. We can reduce the branching factor
without ruling out any solutions by always using the action formed by
substituting the most general unifier into the (standardized) action
schema.

As another example, consider the goal ${Own}({0136042597})$, given an
initial state with 10 billion ISBNs, and the single action schema
$$A = {Action}({Buy}(i), \Pre{{ISBN}(i)}, \Eff{{Own}(i)}) \,.$$
As we mentioned before, forward search without a heuristic would have to
start enumerating the 10 billion ground ${Buy}$ actions. But with
backward search, we would unify the goal ${Own}({0136042597})$ with
the (standardized) effect ${Own}(i')$, yielding the substitution
$\theta =
\{i'/0136042597\}$. Then we would regress over the action
${Subst}(\theta, A')$ to yield the predecessor state description
${ISBN}(0136042597)$. This is part of, and thus entailed by, the
initial state, so we are done.

We can make this more formal. Assume a goal description $g$ which
contains a goal literal $g_i$ and an action schema $A$ that is
standardized to produce $A'$. If $A'$ has an effect literal $e_j'$ where
${Unify}(g_i, e_j')\eq\theta$ and where we define
$a' = \noprog{Subst}(\theta, A')$ and if there is no effect in $a'$ that
is the negation of a literal in $g$, then $a'$ is a relevant action
towards $g$.

Backward search keeps the branching factor lower than forward search,
for most problem domains. However, the fact that backward search uses
state sets rather than individual states makes it harder to come up with
good heuristics. That is the main reason why the majority of current
systems favor forward search.

### Heuristics for planning {#planning-h-section}

Neither forward nor backward search is efficient without a good
heuristic function. Recall from that a heuristic function $h(s)$
estimates the distance from a state $s$ to the goal and that if we can
derive an heuristic for this distance—one that does not
overestimate—then we can use A search to find optimal solutions. An
admissible heuristic can be derived by defining a that is easier to
solve. The exact cost of a solution to this easier problem then becomes
the heuristic for the original problem.

By definition, there is no way to analyze an atomic state, and thus it
it requires some ingenuity by a human analyst to define good
domain-specific heuristics for search problems with atomic states.
Planning uses a factored representation for states and action schemas.
That makes it possible to define good domain-independent heuristics and
for programs to automatically apply a good domain-independent heuristic
for a given problem.

Think of a search problem as a graph where the nodes are states and the
edges are actions. The problem is to find a path connecting the initial
state to a goal state. There are two ways we can relax this problem to
make it easier: by adding more edges to the graph, making it strictly
easier to find a path, or by grouping multiple nodes together, forming
an abstraction of the state space that has fewer states, and thus is
easier to search.

We look first at heuristics that add edges to the graph. For example,
the drops all preconditions from actions. Every action becomes
applicable in every state, and any single goal fluent can be achieved in
one step (if there is an applicable action—if not, the problem is
impossible). This almost implies that the number of steps required to
solve the relaxed problem is the number of unsatisfied goals—almost but
not quite, because (1) some action may achieve multiple goals and (2)
some actions may undo the effects of others. For many problems an
accurate heuristic is obtained by considering (1) and ignoring (2).
First, we relax the actions by removing all preconditions and all
effects except those that are literals in the goal. Then, we count the
minimum number of actions required such that the union of those actions’
effects satisfies the goal. This is an instance of the . There is one
minor irritation: the set-cover problem is NP-hard. Fortunately a simple
greedy algorithm is guaranteed to return a set covering whose size is
within a factor of $\log{n}$ of the true minimum covering, where $n$ is
the number of literals in the goal. Unfortunately, the greedy algorithm
loses the guarantee of admissibility.

It is also possible to ignore only *selected* preconditions
of actions. Consider the (8-puzzle or 15-puzzle) from . We could encode
this as a planning problem involving tiles with a single schema :

((t, s~1~, s~2~),\
\
 )

As we saw in , if we remove the preconditions
${Blank}(s_2) \land {Adjacent}(s_1, s_2)$ then any tile can move in
one action to any space and we get the number-of-misplaced-tiles
heuristic. If we remove ${Blank}(s_2)$ then we get the
Manhattan-distance heuristic. It is easy to see how these heuristics
could be derived automatically from the action schema description. The
ease of manipulating the schemas is the great advantage of the factored
representation of planning problems, as compared with the atomic
representation of search problems.

Another possibility is the heuristic[ignore-delete-lists-page]. Assume
for a moment that all goals and preconditions contain only positive
literals[^3] We want to create a relaxed version of the original problem
that will be easier to solve, and where the length of the solution will
serve as a good heuristic. We can do that by removing the delete lists
from all actions (i.e., removing all negative literals from effects).
That makes it possible to make monotonic progress towards the goal—no
action will ever undo progress made by another action. It turns out it
is still NP-hard to find the optimal solution to this relaxed problem,
but an approximate solution can be found in polynomial time by
hill-climbing. diagrams part of the state space for two planning
problems using the ignore-delete-lists heuristic. The dots represent
states and the edges actions, and the height of each dot above the
bottom plane represents the heuristic value. States on the bottom plane
are solutions. In both these problems, there is a wide path to the goal.
There are no dead ends, so no need for backtracking; a simple
hillclimbing search will easily find a solution to these problems
(although it may not be an optimal solution).

[ignore-del-figure]

The relaxed problems leave us with a simplified—but still
expensive—planning problem just to calculate the value of the heuristic
function. Many planning problems have $10^{100}$ states or more, and
relaxing the *actions* does nothing to reduce the number of
states. Therefore, we now look at relaxations that decrease the number
of states by forming a —a many-to-one mapping from states in the ground
representation of the problem to the abstract representation.

The easiest form of state abstraction is to ignore some fluents. For
example, consider an air cargo problem with 10 airports, 50 planes, and
200 pieces of cargo. Each plane can be at one of 10 airports and each
package can be either in one of the planes or unloaded at one of the
airports. So there are $50^{10} \times 200^{50+10} \approx
10^{155}$ states. Now consider a particular problem in that domain in
which it happens that all the packages are at just 5 of the airports,
and all packages at a given airport have the same destination. Then a
useful abstraction of the problem is to drop all the $At$ fluents except
for the ones involving one plane and one package at each of the 5
airports. Now there are only $5^{10} \times 5^{5+10} \approx
10^{17}$ states. A solution in this abstract state space will be shorter
than a solution in the original space (and thus will be an admissible
heuristic), and the abstract solution is easy to extend to a solution to
the original problem (by adding additional ${Load}$ and ${Unload}$
actions).

A key idea in defining heuristics is : dividing a problem into parts,
solving each part independently, and then combining the parts. The
[subgoal-independence-page] assumption is that the cost of solving a
conjunction of subgoals is approximated by the sum of the costs of
solving each subgoal *independently*. The subgoal
independence assumption can be optimistic or pessimistic. It is
optimistic when there are negative interactions between the subplans for
each subgoal—for example, when an action in one subplan deletes a goal
achieved by another subplan. It is pessimistic, and therefore
inadmissible, when subplans contain redundant actions—for instance, two
actions that could be replaced by a single action in the merged plan.

Suppose the goal is a set of fluents $G$, which we divide into disjoint
subsets $G_1, \ldots, G_n$. We then find plans $P_1, \ldots, P_n$ that
solve the respective subgoals. What is an estimate of the cost of the
plan for achieving all of $G$? We can think of each ${Cost}(P_i)$ as a
heuristic estimate, and we know that if we combine estimates by taking
their maximum value, we always get an admissible heuristic. So
$\max_i \noprog{Cost}(P_i)$ is admissible, and sometimes it is exactly
correct: it could be that $P_1$ serendipitously achieves all the $G_i$.
But in most cases, in practice the estimate is too low. Could we sum the
costs instead? For many problems that is a reasonable estimate, but it
is not admissible. The best case is when we can determine that $G_i$ and
$G_j$ are . If the effects of $P_i$ leave all the preconditions and
goals of $P_j$ unchanged, then the estimate
$\noprog{Cost}(P_i) + \noprog{Cost}(P_j)$ is admissible, and more
accurate than the max estimate. We show in that planning graphs can help
provide better heuristic estimates.

It is clear that there is great potential for cutting down the search
space by forming abstractions. The trick is choosing the right
abstractions and using them in a way that makes the total cost—defining
an abstraction, doing an abstract search, and mapping the abstraction
back to the original problem—less than the cost of solving the original
problem. The techniques of from can be useful, because the cost of
creating the pattern database can be amortized over multiple problem
instances.

An example of a system that makes use of effective heuristics is , or
@Hoffmann:2005, a forward state-space searcher that uses the
ignore-delete-lists heuristic, estimating the heuristic with the help of
a planning graph (see ). then uses hill-climbing search (modified to
keep track of the plan) with the heuristic to find a solution. When it
hits a plateau or local maximum—when no action leads to a state with
better heuristic score—then uses iterative deepening search until it
finds a state that is better, or it gives up and restarts hill-climbing.

Planning Graphs {#graphplan-section}
---------------

All of the heuristics we have suggested can suffer from inaccuracies.
This section shows how a special data structure called a can be used to
give better heuristic estimates. These heuristics can be applied to any
of the search techniques we have seen so far. Alternatively, we can
search for a solution over the space formed by the planning graph, using
an algorithm called .

A planning problem asks if we can reach a goal state from the initial
state. Suppose we are given a tree of all possible actions from the
initial state to successor states, and their successors, and so on. If
we indexed this tree appropriately, we could answer the planning
question “can we reach state $G$ from state $S_0$” immediately, just by
looking it up. Of course, the tree is of exponential size, so this
approach is impractical. A planning graph is polynomial-size
approximation to this tree that can be constructed quickly. The planning
graph can’t answer definitively whether $G$ is reachable from $S_0$, but
it can *estimate* how many steps it takes to reach $G$. The
estimate is always correct when it reports the goal is not reachable,
and it never overestimates the number of steps, so it is an admissible
heuristic.

A planning graph is a directed graph organized into : first a level
$S_0$ for the initial state, consisting of nodes representing each
fluent that holds in $S_0$; then a level $A_0$ consisting of nodes for
each ground action that might be applicable in $S_0$; then alternating
levels $S_i$ followed by $A_i$; until we reach a termination condition
(to be discussed later).

Roughly speaking, $S_i$ contains all the literals that
*could* hold at time $i$, depending on the actions executed
at preceding time steps. If it is possible that either $P$ or $\lnot P$
could hold, then both will be represented in $S_i$. Also roughly
speaking, $A_i$ contains all the actions that *could* have
their preconditions satisfied at time $i$. We say “roughly speaking”
because the planning graph records only a restricted subset of the
possible negative interactions among actions; therefore, a literal might
show up at level $S_j$ when actually it could not be true until a later
level, if at all. (A literal will never show up too late.) Despite the
possible error, the level $j$ at which a literal first appears is a good
estimate of how difficult it is to achieve the literal from the initial
state.

Planning graphs work only for propositional planning problems—ones with
no variables. As we mentioned on , it is straightforward to
propositionalize a set of action schemas. Despite the resulting increase
in the size of the problem description, planning graphs have proved to
be effective tools for solving hard planning problems.

[cake-algorithm]

[eatcake-graphplan2-figure]

shows a simple planning problem, and shows its planning graph. Each
action at level $A_i$ is connected to its preconditions at $S_i$ and its
effects at $S_{i+1}$. So a literal appears because an action caused it,
but we also want to say that a literal can persist if no action negates
it. This is represented by a (sometimes called a *no-op*).
For every literal $C$, we add to the problem a persistence action with
precondition $C$ and effect $C$. Level $A_0$ in shows one “real” action,
${Eat}({Cake})$, along with two persistence actions drawn as small
square boxes.

Level $A_{0}$ contains all the actions that *could* occur
in state $S_{0}$, but just as important it records conflicts between
actions that would prevent them from occurring together. The gray lines
in indicate (or ) links. For example, ${Eat}({Cake})$ is mutually
exclusive with the persistence of either ${Have}({Cake})$ or
$\lnot {Eaten}({Cake})$. We shall see shortly how mutex links are
computed.

Level $S_{1}$ contains all the literals that could result from picking
any subset of the actions in $A_{0}$, as well as mutex links (gray
lines) indicating literals that could not appear together, regardless of
the choice of actions. For example, ${Have}({Cake})$ and
${Eaten}({Cake})$ are mutex: depending on the choice of actions in
$A_{0}$, either, but not both, could be the result. In other words,
$S_{1}$ represents a belief state: a set of possible states. The members
of this set are all subsets of the literals such that there is no mutex
link between any members of the subset.

We continue in this way, alternating between state level $S_{i}$ and
action level $A_{i}$ until we reach a point where two consecutive levels
are identical. At this point, we say that the graph has . The graph in
levels off at $S_2$.

What we end up with is a structure where every $A_{i}$ level contains
all the actions that are applicable in $S_{i}$, along with constraints
saying that two actions cannot both be executed at the same level. Every
$S_{i}$ level contains all the literals that could result from any
possible choice of actions in $A_{i-1}$, along with constraints saying
which pairs of literals are not possible. It is important to note that
the process of constructing the planning graph does *not*
require choosing among actions, which would entail combinatorial search.
Instead, it just records the impossibility of certain choices using
mutex links.

We now define mutex links for both actions and literals. A mutex
relation holds between two *actions* at a given level if
any of the following three conditions holds:

-   *Inconsistent effects:* one action negates an effect of
    the other. For example, ${Eat}({Cake})$ and the persistence of
    ${Have}({Cake})$ have inconsistent effects because they disagree
    on the effect ${Have}({Cake})$.

-   *Interference:* one of the effects of one action is the
    negation of a precondition of the other. For example
    ${Eat}({Cake})$ interferes with the persistence of
    ${Have}({Cake})$ by negating its precondition.

-   *Competing needs:* one of the preconditions of one
    action is mutually exclusive with a precondition of the other. For
    example, ${Bake}({Cake})$ and ${Eat}({Cake})$ are mutex
    because they compete on the value of the ${Have}({Cake})$
    precondition.

A mutex relation holds between two *literals* at the same
level if one is the negation of the other or if each possible pair of
actions that could achieve the two literals is mutually exclusive. This
condition is called *inconsistent support*. For example,
${Have}({Cake})$ and ${Eaten}({Cake})$ are mutex in $S_{1}$
because the only way of achieving ${Have}({Cake})$, the persistence
action, is mutex with the only way of achieving ${Eaten}({Cake})$,
namely ${Eat}({Cake})$. In $S_{2}$ the two literals are not mutex,
because there are new ways of achieving them, such as
${Bake}({Cake})$ and the persistence of ${Eaten}({Cake})$, that
are not mutex.

A planning graph is polynomial in the size of the planning problem. For
a planning problem with $l$ literals and $a$ actions, each $S_i$ has no
more than $l$ nodes and $l^2$ mutex links, and each $A_i$ has no more
than $a + l$ nodes (including the no-ops), $(a+l)^2$ mutex links, and
$2(al + l)$ precondition and effect links. Thus, an entire graph with
$n$ levels has a size of $O(n(a+l)^2)$. The time to build the graph has
the same complexity.

### Planning graphs for heuristic estimation {#graphplan-heuristic-section}

A planning graph, once constructed, is a rich source of information
about the problem. First, if any goal literal fails to appear in the
final level of the graph, then the problem is unsolvable. Second, we can
estimate the cost of achieving any goal literal $g_i$ from state $s$ as
the level at which $g_i$ first appears in the planning graph constructed
from initial state $s$. We call this the of $g_i$. In ,
${Have}({Cake})$ has level cost 0 and ${Eaten}({Cake})$ has
level cost 1. It is easy to show () that these estimates are admissible
for the individual goals. The estimate might not always be accurate,
however, because planning graphs allow several actions at each level,
whereas the heuristic counts just the level and not the number of
actions. For this reason, it is common to use a for computing
heuristics. A serial graph insists that only one action can actually
occur at any given time step; this is done by adding mutex links between
every pair of nonpersistence actions. Level costs extracted from serial
graphs are often quite reasonable estimates of actual costs.

To estimate the cost of a *conjunction* of goals, there are
three simple approaches. The heuristic simply takes the maximum level
cost of any of the goals; this is admissible, but not necessarily
accurate.

The heuristic, following the subgoal independence assumption, returns
the sum of the level costs of the goals; this can be inadmissible but
works well in practice for problems that are largely decomposable. It is
much more accurate than the number-of-unsatisfied-goals heuristic from .
For our problem, the level-sum heuristic estimate for the conjunctive
goal ${Have}({Cake}) \land
{Eaten}({Cake})$ will be $0 + 1 = 1$, whereas the correct answer is
2, achieved by the plan $[{Eat}({Cake}),
{Bake}({Cake})]$. That doesn’t seem so bad. A more serious error is
that if ${Bake}({Cake})$ were not in the set of actions, then the
estimate would still be 1, when in fact the conjunctive goal would be
impossible.

Finally, the [set-level-page] heuristic finds the level at which all the
literals in the conjunctive goal appear in the planning graph without
any pair of them being mutually exclusive. This heuristic gives the
correct values of 2 for our original problem and infinity for the
problem without ${Bake}({Cake})$. It is admissible, it dominates the
max-level heuristic, and it works extremely well on tasks in which there
is a good deal of interaction among subplans. It is not perfect, of
course; for example, it ignores interactions among three or more
literals.

As a tool for generating accurate heuristics, we can view the planning
graph as a relaxed problem that is efficiently solvable. To understand
the nature of the relaxed problem, we need to understand exactly what it
means for a literal $g$ to appear at level $S_{i}$ in the planning
graph. Ideally, we would like it to be a guarantee that there exists a
plan with $i$ action levels that achieves $g$, and also that if $g$ does
not appear, there is no such plan. Unfortunately, making that guarantee
is as difficult as solving the original planning problem. So the
planning graph makes the second half of the guarantee (if $g$ does not
appear, there is no plan), but if $g$ does appear, then all the planning
graph promises is that there is a plan that *possibly*
achieves $g$ and has no “obvious” flaws. An obvious flaw is defined as a
flaw that can be detected by considering two actions or two literals at
a time—in other words, by looking at the mutex relations. There could be
more subtle flaws involving three, four, or more actions, but experience
has shown that it is not worth the computational effort to keep track of
these possible flaws. This is similar to a lesson learned from
constraint satisfaction problems—that it is often worthwhile to compute
2-consistency before searching for a solution, but less often worthwhile
to compute 3-consistency or higher. (See .)

One example of an unsolvable problem that cannot be recognized as such
by a planning graph is the blocks-world problem where the goal is to get
block $A$ on $B$, $B$ on $C$, and $C$ on $A$. This is an impossible
goal; a tower with the bottom on top of the top. But a planning graph
cannot detect the impossibility, because any two of the three subgoals
are achievable. There are no mutexes between any pair of literals, only
between the three as a whole. To detect that this problem is impossible,
we would have to search over the planning graph.

### The algorithm

This subsection shows how to extract a plan directly from the planning
graph, rather than just using the graph to provide a heuristic. The
algorithm () repeatedly adds a level to a planning graph with . Once all
the goals show up as non-mutex in the graph, calls to search for a plan
that solves the problem. If that fails, it expands another level and
tries again, terminating with failure when there is no reason to go on.

[graphplan-algorithm]

Let us now trace the operation of on the spare tire problem from . The
graph is shown in . The first line of initializes the planning graph to
a one-level ($S_{0}$) graph representing the initial state. The positive
fluents from the problem description’s initial state are shown, as are
the relevant negative fluents. Not shown are the unchanging positive
literals (such as ${Tire}({Spare})$) and the irrelevant negative
literals. The goal ${At}({Spare},{Axle})$ is not present in
$S_{0}$, so we need not call —we are certain that there is no solution
yet. Instead, adds into $A_0$ the three actions whose preconditions
exist at level $S_{0}$ (i.e., all the actions except
${PutOn}({Spare}, {Axle})$), along with persistence actions for
all the literals in $S_{0}$. The effects of the actions are added at
level $S_{1}$. then looks for mutex relations and adds them to the
graph.

[tire-graphplan2-figure]

${At}({Spare},{Axle})$ is still not present in $S_{1}$, so again
we do not call . We call again, adding $A_1$ and $S_1$ and giving us the
planning graph shown in . Now that we have the full complement of
actions, it is worthwhile to look at some of the examples of mutex
relations and their causes:

-   *Inconsistent effects:*
    ${Remove}({Spare},{Trunk})$ is mutex with ${LeaveOvernight}$
    because one has the effect ${At}({Spare},{Ground})$ and the
    other has its negation.

-   *Interference:* ${Remove}({Flat}, {Axle})$ is
    mutex with ${LeaveOvernight}$ because one has the precondition
    ${At}({Flat},{Axle})$ and the other has its negation as an
    effect.

-   *Competing needs:* ${PutOn}({Spare}, {Axle})$ is
    mutex with ${Remove}({Flat}, {Axle})$ because one has
    ${At}({Flat},{Axle})$ as a precondition and the other has its
    negation.

-   *Inconsistent support:* ${At}({Spare},{Axle})$ is
    mutex with ${At}({Flat},{Axle})$ in $S_{2}$ because the only
    way of achieving ${At}({Spare},{Axle})$ is by
    ${PutOn}({Spare}, {Axle})$, and that is mutex with the
    persistence action that is the only way of achieving
    ${At}({Flat},{Axle})$. Thus, the mutex relations detect the
    immediate conflict that arises from trying to put two objects in the
    same place at the same time.

This time, when we go back to the start of the loop, all the literals
from the goal are present in $S_{2}$, and none of them is mutex with any
other. That means that a solution might exist, and will try to find it.
We can formulate as a Boolean constraint satisfaction problem (CSP)
where the variables are the actions at each level, the values for each
variable are *in* or *out* of the plan, and
the constraints are the mutexes and the need to satisfy each goal and
precondition.

Alternatively, we can define as a backward search problem, where each
state in the search contains a pointer to a level in the planning graph
and a set of unsatisfied goals. We define this search problem as
follows:

-   The initial state is the last level of the planning graph, $S_{n}$,
    along with the set of goals from the planning problem.

-   The actions available in a state at level $S_{i}$ are to select any
    conflict-free subset of the actions in $A_{i-1}$ whose effects cover
    the goals in the state. The resulting state has level $S_{i-1}$ and
    has as its set of goals the preconditions for the selected set of
    actions. By “conflict free,” we mean a set of actions such that no
    two of them are mutex and no two of their preconditions are mutex.

-   The goal is to reach a state at level $S_{0}$ such that all the
    goals are satisfied.

-   The cost of each action is 1.

For this particular problem, we start at $S_{2}$ with the goal
${At}({Spare},{Axle})$. The only choice we have for achieving the
goal set is ${PutOn}({Spare}, {Axle})$. That brings us to a search
state at $S_{1}$ with goals ${At}({Spare},{Ground})$ and
$\lnot {At}({Flat},{Axle})$. The former can be achieved only by
${Remove}({Spare},{Trunk})$, and the latter by either
${Remove}({Flat}, {Axle})$ or ${LeaveOvernight}$. But
${LeaveOvernight}$ is mutex with ${Remove}({Spare},{Trunk})$, so
the only solution is to choose ${Remove}({Spare},{Trunk})$ and
${Remove}({Flat},{Axle})$. That brings us to a search state at
$S_{0}$ with the goals ${At}({Spare}, {Trunk})$ and
${At}({Flat}, {Axle})$. Both of these are present in the state, so
we have a solution: the actions ${Remove}({Spare},$ ${Trunk})$ and
${Remove}({Flat},$ ${Axle})$ in level $A_{0}$, followed by
${PutOn}({Spare},$ ${Axle})$ in $A_{1}$.

In the case where fails to find a solution for a set of goals at a given
level, we record the $(\v{level},
\v{goals})$ pair as a , just as we did in constraint learning for CSPs
(). Whenever is called again with the same level and goals, we can find
the recorded no-good and immediately return failure rather than
searching again. We see shortly that no-goods are also used in the
termination test.

We know that planning is and that constructing the planning graph takes
polynomial time, so it must be the case that solution extraction is
intractable in the worst case. Therefore, we will need some heuristic
guidance for choosing among actions during the backward search. One
approach that works well in practice is a greedy algorithm based on the
level cost of the literals. For any set of goals, we proceed in the
following order:

1.  Pick first the literal with the highest level cost.

2.  To achieve that literal, prefer actions with easier preconditions.
    That is, choose an action such that the sum (or maximum) of the
    level costs of its preconditions is smallest.

### Termination of 

So far, we have skated over the question of termination. Here we show
that will in fact terminate and return failure when there is no
solution.

The first thing to understand is why we can’t stop expanding the graph
as soon as it has leveled off. Consider an air cargo domain with one
plane and $n$ pieces of cargo at airport $A$, all of which have airport
$B$ as their destination. In this version of the problem, only one piece
of cargo can fit in the plane at a time. The graph will level off at
level 4, reflecting the fact that for any single piece of cargo, we can
load it, fly it, and unload it at the destination in three steps. But
that does not mean that a solution can be extracted from the graph at
level 4; in fact a solution will require $4n - 1$ steps: for each piece
of cargo we load, fly, and unload, and for all but the last piece we
need to fly back to airport $A$ to get the next piece.

How long do we have to keep expanding after the graph has leveled off?
If the function fails to find a solution, then there must have been at
least one set of goals that were not achievable and were marked as a
no-good. So if it is possible that there might be fewer no-goods in the
next level, then we should continue. As soon as the graph itself and the
no-goods have both leveled off, with no solution found, we can terminate
with failure because there is no possibility of a subsequent change that
could add a solution.

Now all we have to do is prove that the graph and the no-goods will
always level off. The key to this proof is that certain properties of
planning graphs are monotonically increasing or decreasing. “X increases
monotonically” means that the set of Xs at level $i+1$ is a superset
(not necessarily proper) of the set at level $i$. The properties are as
follows:

-   *Literals increase monotonically:* Once a literal
    appears at a given level, it will appear at all subsequent levels.
    This is because of the persistence actions; once a literal shows up,
    persistence actions cause it to stay forever.

-   *Actions increase monotonically:* Once an action
    appears at a given level, it will appear at all subsequent levels.
    This is a consequence of the monotonic increase of literals; if the
    preconditions of an action appear at one level, they will appear at
    subsequent levels, and thus so will the action.

-   *Mutexes decrease monotonically:* If two actions are
    mutex at a given level $A_{i}$, then they will also be mutex for all
    *previous* levels at which they both appear. The same
    holds for mutexes between literals. It might not always appear that
    way in the figures, because the figures have a simplification: they
    display neither literals that cannot hold at level $S_{i}$ nor
    actions that cannot be executed at level $A_{i}$. We can see that
    “mutexes decrease monotonically” is true if you consider that these
    invisible literals and actions are mutex with everything.

    The proof can be handled by cases: if actions $A$ and $B$ are mutex
    at level $A_{i}$, it must be because of one of the three types of
    mutex. The first two, inconsistent effects and interference, are
    properties of the actions themselves, so if the actions are mutex at
    $A_{i}$, they will be mutex at every level. The third case,
    competing needs, depends on conditions at level $S_{i}$: that level
    must contain a precondition of $A$ that is mutex with a precondition
    of $B$. Now, these two preconditions can be mutex if they are
    negations of each other (in which case they would be mutex in every
    level) or if all actions for achieving one are mutex with all
    actions for achieving the other. But we already know that the
    available actions are increasing monotonically, so, by induction,
    the mutexes must be decreasing.

-   *No-goods decrease monotonically:* If a set of goals is
    not achievable at a given level, then they are not achievable in any
    *previous* level. The proof is by contradiction: if
    they were achievable at some previous level, then we could just add
    persistence actions to make them achievable at a subsequent level.

Because the actions and literals increase monotonically and because
there are only a finite number of actions and literals, there must come
a level that has the same number of actions and literals as the previous
level. Because mutexes and no-goods decrease, and because there can
never be fewer than zero mutexes or no-goods, there must come a level
that has the same number of mutexes and no-goods as the previous level.
Once a graph has reached this state, then if one of the goals is missing
or is mutex with another goal, then we can stop the algorithm and return
failure. That concludes a sketch of the proof; for more details see .

[htbp]

[ipc-table]

Other Classical Planning Approaches
-----------------------------------

Currently the most popular and effective approaches to fully automated
planning are:

-   Translating to a Boolean satisfiability (SAT) problem

-   Forward state-space search with carefully crafted heuristics ()

-   Search using a planning graph ()

These three approaches are not the only ones tried in the 40-year
history of automated planning. shows some of the top systems in the
International Planning Competitions, which have been held every even
year since 1998. In this section we first describe the translation to a
satisfiability problem and then describe three other influential
approaches: planning as first-order logical deduction; as constraint
satisfaction; and as plan refinement.

### Classical planning as Boolean satisfiability {#FOL-satplan-section}

In we saw how solves planning problems that are expressed in
propositional logic. Here we show how to translate a description into a
form that can be processed by . The translation is a series of
straightforward steps:

-   Propositionalize the actions: replace each action schema with a set
    of ground actions formed by substituting constants for each of the
    variables. These ground actions are not part of the translation, but
    will be used in subsequent steps.

-   Define the initial state: assert $F^0$ for every fluent $F$ in the
    problem’s initial state, and $\lnot F$ for every fluent not
    mentioned in the initial state.

-   Propositionalize the goal: for every variable in the goal, replace
    the literals that contain the variable with a disjunction over
    constants. For example, the goal of having block $A$ on another
    block, $On(A,x) \land {Block}(x)$ in a world with objects $A, B$
    and $C$, would be replaced by the goal
    $$(On(A,A) \land {Block}(A)) \lor (On(A,B) \land {Block}(B)) \lor (On(A,C) \land {Block}(C)) \,.$$

-   Add successor-state axioms: For each fluent $F$, add an axiom of the
    form $$F^{t+1} \lequiv 
      {ActionCausesF}^{t} \lor (F^{t} \land \lnot {ActionCausesNotF}^{t}) \ ,$$
    where ${ActionCausesF}$ is a disjunction of all the ground actions
    that have $F$ in their add list, and ${ActionCausesNotF}$ is a
    disjunction of all the ground actions that have $F$ in their delete
    list.

-   Add precondition axioms: For each ground action $A$, add the axiom
    $A^t \implies \noprog{Pre}(A)^t$, that is, if an action is taken at
    time $t$, then the preconditions must have been true.

-   Add action exclusion axioms: say that every action is distinct from
    every other action.

The resulting translation is in the form that we can hand to to find a
solution.

### Planning as first-order logical deduction: Situation calculus {#planning-as-deduction-section}

PDDL is a language that carefully balances the expressiveness of the
language with the complexity of the algorithms that operate on it. But
some problems remain difficult to express in PDDL. For example, we can’t
express the goal “move all the cargo from $A$ to $B$ regardless of how
many pieces of cargo there are” in PDDL, but we can do it in first-order
logic, using a universal quantifier. Likewise, first-order logic can
concisely express global constraints such as “no more than four robots
can be in the same place at the same time.” PDDL can only say this with
repetitious preconditions on every possible action that involves a move.

[wumpus-situations-figure]

The propositional logic representation of planning problems also has
limitations, such as the fact that the notion of time is tied directly
to fluents. For example, ${South}^2$ means “the agent is facing south
at time 2.” With that representation, there is no way to say “the agent
would be facing south at time 2 if it executed a right turn at time 1;
otherwise it would be facing east.” First-order logic lets us get around
this limitation by replacing the notion of linear time with a notion of
branching *situations*, using a representation called that
works like this:

-   The initial state is called a . If $s$ is a situation and $a$ is an
    action, then $\result{s}{a}$ is also a situation. There are no other
    situations. Thus, a situation corresponds to a sequence, or history,
    of actions. You can also think of a situation as the result of
    applying the actions, but note that two situations are the same only
    if their start and actions are the same:
    $(\result{s}{a} = \result{s'}{a'}) \lequiv (s = s' \land a=a')$.
    Some examples of actions and situations are shown in .

-   A function or relation that can vary from one situation to the next
    is a . By convention, the situation $s$ is always the last argument
    to the fluent, for example ${At}(x,l,s)$ is a relational fluent
    that is true when object $x$ is at location $l$ in situation $s$,
    and ${Location}$ is a functional fluent such that
    ${Location}(x, s) = l$ holds in the same situations as
    ${At}(x,l,s)$.

-   Each action’s preconditions are described with a that says when the
    action can be taken. It has the form
    $\Phi(s)  \implies {Poss}(a, s)$ where $\Phi(s)$ is some formula
    involving $s$ that describes the preconditions. An example from the
    wumpus world says that it is possible to shoot if the agent is alive
    and has an arrow:
    $${Alive}({Agent}, s) \land {Have}({Agent}, {Arrow}, s) \implies {Poss}({Shoot}, s)$$

-   Each fluent is described with a that says what happens to the
    fluent, depending on what action is taken. This is similar to the
    approach we took for propositional logic. The axiom has the form

    \
     ( &&\
    & &  ) .

    For example, the axiom for the relational fluent ${Holding}$ says
    that the agent is holding some gold $g$ after executing a possible
    action if and only if the action was a ${Grab}$ of $g$ or if the
    agent was already holding $g$ and the action was not releasing it:

    (a, s)\
    ((, g, (a, s))\
    a (g) ((, g, s) a (g))) .

-   We need so that the agent can deduce that, for example,
    $a \neq {Release}(g)$. For each distinct pair of action names
    $A_i$ and $A_j$ we have an axiom that says the actions are
    different: $$A_i(x,\ldots) \neq A_j(y, \ldots)$$ and for each action
    name $A_i$ we have an axiom that says two uses of that action name
    are equal if and only if all their arguments are equal:
    $$A_i(x_1,\ldots,x_n) \eq A_i(y_1,\ldots,y_n) \lequiv x_1 \eq y_1 \land \ldots \land x_n \eq y_n\ .$$

-   A solution is a situation (and hence a sequence of actions) that
    satisfies the goal.

Work in situation calculus has done a lot to define the formal semantics
of planning and to open up new areas of investigation. But so far there
have not been any practical large-scale planning programs based on
logical deduction over the situation calculus. This is in part because
of the difficulty of doing efficient inference in FOL, but is mainly
because the field has not yet developed effective heuristics for
planning with situation calculus.

### Planning as constraint satisfaction

We have seen that constraint satisfaction has a lot in common with
Boolean satisfiability, and we have seen that CSP techniques are
effective for scheduling problems, so it is not surprising that it is
possible to encode a bounded planning problem (i.e., the problem of
finding a plan of length $k$) as a constraint satisfaction problem
(CSP). The encoding is similar to the encoding to a SAT problem (), with
one important simplification: at each time step we need only a single
variable, ${Action}^t$, whose domain is the set of possible actions.
We no longer need one variable for every action, and we don’t need the
action exclusion axioms. It is also possible to encode a planning graph
into a CSP. This is the approach taken by @Do+Kambhampati:2003.

### Planning as refinement of partially ordered plans {#pop-section}

All the approaches we have seen so far construct *totally
ordered* plans consisting of a strictly linear sequences of
actions. This representation ignores the fact that many subproblems are
independent. A solution to an air cargo problem consists of a totally
ordered sequence of actions, yet if 30 packages are being loaded onto
one plane in one airport and 50 packages are being loaded onto another
at another airport, it seems pointless to come up with a strict linear
ordering of 80 load actions; the two subsets of actions should be
thought of independently.

An alternative is to represent plans as *partially ordered*
structures: a plan is a set of actions and a set of constraints of the
form ${Before}(a_i, a_j)$ saying that one action occurs before
another. In the bottom of , we see a partially ordered plan that is a
solution to the spare tire problem. Actions are boxes and ordering
constraints are arrows. Note that ${Remove}({Spare},{Trunk})$ and
${Remove}({Flat},
{Axle})$ can be done in either order as long as they are both
completed before the ${PutOn}({Spare}, {Axle})$ action.

[tire-figure]

Partially ordered plans are created by a *search through the space
of plans* rather than through the state space. We start with the
empty plan consisting of just the initial state and the goal, with no
actions in between, as in the top of . The search procedure then looks
for a in the plan, and makes an addition to the plan to correct the flaw
(or if no correction can be made, the search backtracks and tries
something else). A flaw is anything that keeps the partial plan from
being a solution. For example, one flaw in the empty plan is that no
action achieves ${At}({Spare}, {Axle})$. One way to correct the
flaw is to insert into the plan the action
${PutOn}({Spare}, {Axle})$. Of course that introduces some new
flaws: the preconditions of the new action are not achieved. The search
keeps adding to the plan (backtracking if necessary) until all flaws are
resolved, as in the bottom of . At every step, we make the possible to
fix the flaw. For example, in adding the action ${Remove}({Spare},
{Trunk})$ we need to commit to having it occur before
${PutOn}({Spare}, {Axle})$, but we make no other commitment that
places it before or after other actions. If there were a variable in the
action schema that could be left unbound, we would do so.

In the 1980s and 90s, partial-order planning was seen as the best way to
handle planning problems with independent subproblems—after all, it was
the only approach that explicitly represents independent branches of a
plan. On the other hand, it has the disadvantage of not having an
explicit representation of states in the state-transition model. That
makes some computations cumbersome. By 2000, forward-search planners had
developed excellent heuristics that allowed them to efficiently discover
the independent subproblems that partial-order planning was designed
for. As a result, partial-order planners are not competitive on fully
automated classical planning problems.

However, partial-order planning remains an important part of the field.
For some specific tasks, such as operations scheduling, partial-order
planning with domain specific heuristics is the technology of choice.
Many of these systems use libraries of high-level plans, as described in
. Partial-order planning is also often used in domains where it is
important for humans to understand the plans. Operational plans for
spacecraft and Mars rovers are generated by partial-order planners and
are then checked by human operators before being uploaded to the
vehicles for execution. The plan refinement approach makes it easier for
the humans to understand what the planning algorithms are doing and
verify that they are correct.

Analysis of Planning Approaches
-------------------------------

Planning combines the two major areas of AI we have covered so far:
*search* and *logic*. A planner can be seen
either as a program that searches for a solution or as one that
(constructively) proves the existence of a solution. The
cross-fertilization of ideas from the two areas has led both to
improvements in performance amounting to several orders of magnitude in
the last decade and to an increased use of planners in industrial
applications. Unfortunately, we do not yet have a clear understanding of
which techniques work best on which kinds of problems. Quite possibly,
new techniques will emerge that dominate existing methods.

Planning is foremost an exercise in controlling combinatorial explosion.
If there are $n$ propositions in a domain, then there are $2^{n}$
states. As we have seen, planning is PSPACE-hard. Against such
pessimism, the identification of independent subproblems can be a
powerful weapon. In the best case—full decomposability of the problem—we
get an exponential speedup. Decomposability is destroyed, however, by
negative interactions between actions. records mutexes to point out
where the difficult interactions are. represents a similar range of
mutex relations, but does so by using the general CNF form rather than a
specific data structure. Forward search addresses the problem
heuristically by trying to find patterns (subsets of propositions) that
cover the independent subproblems. Since this approach is heuristic, it
can work even when the subproblems are not completely independent.

Sometimes it is possible to solve a problem efficiently by recognizing
that negative interactions can be ruled out. We say that a problem has
if there exists an order of subgoals such that the planner can achieve
them in that order without having to undo any of the previously achieved
subgoals. For example, in the blocks world, if the goal is to build a
tower (e.g., $A$ on $B$, which in turn is on $C$, which in turn is on
the ${Table}$, as in on ), then the subgoals are serializable bottom
to top: if we first achieve $C$ on ${Table}$, we will never have to
undo it while we are achieving the other subgoals. A planner that uses
the bottom-to-top trick can solve any problem in the blocks world
without backtracking (although it might not always find the shortest
plan).

As a more complex example, for the planner that commanded ’s spacecraft,
it was determined that the propositions involved in commanding a
spacecraft are serializable. This is perhaps not too surprising, because
a spacecraft is *designed* by its engineers to be as easy
as possible to control (subject to other constraints). Taking advantage
of the serialized ordering of goals, the Remote Agent planner was able
to eliminate most of the search. This meant that it was fast enough to
control the spacecraft in real time, something previously considered
impossible.

Planners such as , , and have moved the field of planning forward, by
raising the level of performance of planning systems, by clarifying the
representational and combinatorial issues involved, and by the
development of useful heuristics. However, there is a question of how
far these techniques will scale. It seems likely that further progress
on larger problems cannot rely only on factored and propositional
representations, and will require some kind of synthesis of first-order
and hierarchical representations with the efficient heuristics currently
in use.

In this chapter, we defined the problem of planning in deterministic,
fully observable, static environments. We described the PDDL
representation for planning problems and several algorithmic approaches
for solving them. The points to remember:

-   Planning systems are problem-solving algorithms that operate on
    explicit propositional or relational representations of states and
    actions. These representations make possible the derivation of
    effective heuristics and the development of powerful and flexible
    algorithms for solving problems.

-   , the Planning Domain Definition Language, describes the initial and
    goal states as conjunctions of literals, and actions in terms of
    their preconditions and effects.

-   State-space search can operate in the forward direction () or the
    backward direction (). Effective heuristics can be derived by
    subgoal independence assumptions and by various relaxations of the
    planning problem.

-   A can be constructed incrementally, starting from the initial state.
    Each layer contains a superset of all the literals or actions that
    could occur at that time step and encodes mutual exclusion (mutex)
    relations among literals or actions that cannot co-occur. Planning
    graphs yield useful heuristics for state-space and partial-order
    planners and can be used directly in the algorithm.

-   Other approaches include first-order deduction over situation
    calculus axioms; encoding a planning problem as a Boolean
    satisfiability problem or as a constraint satisfaction problem; and
    explicitly searching through the space of partially ordered plans.

-   Each of the major approaches to planning has its adherents, and
    there is as yet no consensus on which is best. Competition and
    cross-fertilization among the approaches have resulted in
    significant gains in efficiency for planning systems.

AI planning arose from investigations into state-space search, , and and
from the practical needs of robotics, scheduling, and other domains.
 @Fikes+Nilsson:1971, the first major planning system, illustrates the
interaction of these influences. was designed as the planning component
of the software for the Shakey robot project at SRI. Its overall control
structure was modeled on that of , the General Problem Solver
@Newell+Simon:1961, a state-space search system that used means–ends
analysis. Bylander [-@Bylander:1992] shows simple planning to be . Fikes
and Nilsson [-@Fikes+Nilsson:1993] give a historical retrospective on
the project and its relationship to more recent planning efforts.

The representation language used by has been far more influential than
its algorithmic approach; what we call the “classical” language is close
to what used. The Action Description Language, or ADL @Pednault:1986,
relaxed some of the restrictions and made it possible to encode more
realistic problems. explores schemes for compiling ADL into . The
Problem Domain Description Language, or @Ghallab+al:1998, was introduced
as a computer-parsable, standardized syntax for representing planning
problems and has been used as the standard language for the
International Planning Competition since 1998. There have been several
extensions; the most recent version, 3.0, includes plan constraints and
preferences @Gerevini+Long:2005.

Planners in the early 1970s generally considered totally ordered action
sequences. Problem decomposition was achieved by computing a subplan for
each subgoal and then stringing the subplans together in some order.
This approach, called by , was soon discovered to be incomplete. It
cannot solve some very simple problems, such as the (see ), found by
Allen Brown during experimentation with the system @Sussman:1975. A
complete planner must allow for of actions from different subplans
within a single sequence. The notion of serializable subgoals @Korf:1987
corresponds exactly to the set of problems for which noninterleaved
planners are complete.

One solution to the interleaving problem was goal-regression planning, a
technique in which steps in a totally ordered plan are reordered so as
to avoid conflict between subgoals. This was introduced by
Waldinger [-@Waldinger:1975] and also used by Warren’s [-@Warren:1974] .
is also notable in that it was the first planner to be written in a
logic programming language (Prolog) and is one of the best examples of
the remarkable economy that can sometimes be gained with logic
programming: is only 100 lines of code, a small fraction of the size of
comparable planners of the time.

The ideas underlying partial-order planning include the detection of
conflicts @Tate:1975a and the protection of achieved conditions from
interference @Sussman:1975. The construction of partially ordered plans
(then called ) was pioneered by the planner @Sacerdoti:1975
[@Sacerdoti:1977] and by Tate’s [-@Tate:1975; -@Tate:1977] system.

Partial-order planning dominated the next 20 years of research, yet the
first clear formal exposition was @Chapman:1987, a planner that was
simple enough to allow proofs of completeness and intractability
(NP-hardness and undecidability) of various planning problems. Chapman’s
work led to a straightforward description of a complete partial-order
planner @McAllester+Rosenblitt:1991, then to the widely distributed
implementations  @Soderland+Weld:1991 and  @Penberthy+Weld:1992.
Partial-order planning fell out of favor in the late 1990s as faster
methods emerged. suggest that a reconsideration is merited: with
accurate heuristics derived from a planning graph, their planner scales
up much better than in parallelizable domains and is competitive with
the fastest state-space planners.

The resurgence of interest in state-space planning was pioneered by Drew
McDermott’s program [-@McDermott:1996], which was the first to suggest
the ignore-delete-list heuristic, The name was a reaction to the
overwhelming concentration on partial-order planning at the time;
McDermott suspected that other approaches were not getting the attention
they deserved. Bonet and Geffner’s Heuristic Search Planner () and its
later derivatives @Bonet+Geffner:1999 [@Haslum+al:2005; @Haslum:2006]
were the first to make state-space search practical for large planning
problems. searches in the forward direction while @Bonet+Geffner:1999
searches backward. The most successful state-space searcher to date is
@Hoffmann:2001 [@Hoffmann+Nebel:2001; @Hoffmann:2005], winner of the
AIPS 2000 planning competition. @Helmert:2006 is a forward state-space
search planner that preprocesses the action schemas into an alternative
representation which makes some of the constraints more explicit.
@Helmert+Richter:2004 [@Helmert:2006] won the 2004 planning competition,
and @Richter+Westphal:2008, a planner based on with improved heuristics,
won the 2008 competition.

and discuss the computational complexity of several variants of the
planning problem. proves complexity bounds for many of the standard
benchmark problems, and analyzes the search space of the
ignore-delete-list heuristic. Heuristics for the set-covering problem
are discussed by for scheduling operations of the Italian railway. and
describe how to construct pattern databases for planning heuristics. As
we mentioned in , show encouraging results using pattern databases for
sliding blocks puzzles, which can be thought of as a planning domain,
but show some limitations of abstraction for classical planning
problems.

Avrim Blum and Merrick Furst [-@Blum+Furst:1995; -@Blum+Furst:1997]
revitalized the field of planning with their system, which was orders of
magnitude faster than the partial-order planners of the time. Other
graph-planning systems, such as @Koehler+al:1997, @Fox+Long:1998, and
@Weld+al:1998, soon followed. A data structure closely resembling the
planning graph had been developed slightly earlier by , whose
partial-order planner used it to derive accurate heuristics to guide
searches. thoroughly analyze heuristics derived from planning graphs.
Our discussion of planning graphs is based partly on this work and on
lecture notes and articles by Subbarao Kambhampati
@Bryce+Kambhampati:2007. As mentioned in the chapter, a planning graph
can be used in many different ways to guide the search for a solution.
The winner of the 2002 AIPS planning competition, @Gerevini+Serina:2002
[@Gerevini+Serina:2003], searched planning graphs using a local search
technique inspired by .

The situation calculus approach to planning was introduced by John
McCarthy [-@McCarthy:1963]. The version we show here was proposed by Ray
Reiter [-@Reiter:1991; -@Reiter:2001a].

investigated various ways to propositionalize action schemas, finding
that the most compact forms did not necessarily lead to the fastest
solution times. A systematic analysis was carried out by , who also
developed an automatic “compiler” for generating propositional
representations from PDDL problems. The planner, which combines ideas
from and , was developed by . , a planner based on constraint
satisfaction, was described by .

Most recently, there has been interest in the representation of plans as
, compact data structures for Boolean expressions widely studied in the
hardware verification community @Clarke+Grumberg:1987 [@McMillan:1993].
There are techniques for proving properties of binary decision diagrams,
including the property of being a solution to a planning problem.
present a planner based on this approach. Other representations have
also been used; for example, survey the use of integer programming for
planning.

The jury is still out, but there are now some interesting comparisons of
the various approaches to planning. analyzes several classes of planning
problems, and shows that constraint-based approaches such as and are
best for NP-hard domains, while search-based approaches do better in
domains where feasible solutions can be found without backtracking. and
have trouble in domains with many objects because that means they must
create many actions. In some cases the problem can be delayed or avoided
by generating the propositionalized actions dynamically, only as needed,
rather than instantiating them all before the search begins.

*Readings in Planning* @Allen+al:1990 is a comprehensive
anthology of early work in the field. Weld [-@Weld:1994; -@Weld:1999]
provides two excellent surveys of planning algorithms of the 1990s. It
is interesting to see the change in the five years between the two
surveys: the first concentrates on partial-order planning, and the
second introduces and . *Automated Planning*
@Ghallab+al:2004 is an excellent textbook on all aspects of planning.
LaValle’s text *Planning Algorithms* [-@LaValle:2006]
covers both classical and stochastic planning, with extensive coverage
of robot motion planning.

Planning research has been central to AI since its inception, and papers
on planning are a staple of mainstream AI journals and conferences.
There are also specialized conferences such as the International
Conference on AI Planning Systems, the International Workshop on
Planning and Scheduling for Space, and the European Conference on
Planning.

Consider a robot whose operation is described by the following PDDL
operators:

Op(,,)\
Op(,,)\
Op(,,)

1.  The operators allow the robot to hold more than one object. Show how
    to modify them with an $EmptyHand$ predicate for a robot that can
    hold only one object.

2.  Assuming that these are the only actions in the world, write a
    successor-state axiom for $EmptyHand$.

Describe the differences and similarities between problem solving and
planning.

[strips-airport-exercise]Given the action schemas and initial state from
, what are all the applicable concrete instances of
${Fly}(p,{from},{to})$ in the state described by

(P~1~, ) (P~2~, ) (P~1~) (P~2~)\
 () () ?

The monkey-and-bananas problem is faced by a monkey in a laboratory with
some bananas hanging out of reach from the ceiling. A box is available
that will enable the monkey to reach the bananas if he climbs on it.
Initially, the monkey is at $A$, the bananas at $B$, and the box at $C$.
The monkey and box have height ${Low}$, but if the monkey climbs onto
the box he will have height ${High}$, the same as the bananas. The
actions available to the monkey include ${Go}$ from one place to
another, ${Push}$ an object from one place to another, ${ClimbUp}$
onto or ${ClimbDown}$ from an object, and ${Grasp}$ or ${Ungrasp}$
an object. The result of a ${Grasp}$ is that the monkey holds the
object if the monkey and object are in the same place at the same
height.

1.  Write down the initial state description.

2.  Write the six action schemas.

3.  Suppose the monkey wants to fool the scientists, who are off to tea,
    by grabbing the bananas, but leaving the box in its original place.
    Write this as a general goal (i.e., not assuming that the box is
    necessarily at C) in the language of situation calculus. Can this
    goal be solved by a classical planning system?

4.  Your schema for pushing is probably incorrect, because if the object
    is too heavy, its position will remain the same when the ${Push}$
    schema is applied. Fix your action schema to account for heavy
    objects.

The original planner was designed to control Shakey the robot. shows a
version of Shakey’s world consisting of four rooms lined up along a
corridor, where each room has a door and a light switch.

[shakey-figure]

The actions in Shakey’s world include moving from place to place,
pushing movable objects (such as boxes), climbing onto and down from
rigid objects (such as boxes), and turning light switches on and off.
The robot itself could not climb on a box or toggle a switch, but the
planner was capable of finding and printing out plans that were beyond
the robot’s abilities. Shakey’s six actions are the following:

-   ${Go}(x,y,r)$, which requires that Shakey be ${At}$ $x$ and that
    $x$ and $y$ are locations ${In}$ the same room $r$. By convention
    a door between two rooms is in both of them.

-   Push a box $b$ from location $x$ to location $y$ within the same
    room: ${Push}(b,x,y,r)$. You will need the predicate ${Box}$ and
    constants for the boxes.

-   Climb onto a box from position $x$: ${ClimbUp}(x, b)$; climb down
    from a box to position $x$: ${ClimbDown}(b, x)$. We will need the
    predicate ${On}$ and the constant ${Floor}$.

-   Turn a light switch on or off: ${TurnOn}(s,b)$;
    ${TurnOff}(s,b)$. To turn a light on or off, Shakey must be on top
    of a box at the light switch’s location.

Write PDDL sentences for Shakey’s six actions and the initial state from
. Construct a plan for Shakey to get ${Box}{}_2$ into ${Room}{}_2$.

A finite Turing machine has a finite one-dimensional tape of cells, each
cell containing one of a finite number of symbols. One cell has a read
and write head above it. There is a finite set of states the machine can
be in, one of which is the accept state. At each time step, depending on
the symbol on the cell under the head and the machine’s current state,
there are a set of actions we can choose from. Each action involves
writing a symbol to the cell under the head, transitioning the machine
to a state, and optionally moving the head left or right. The mapping
that determines which actions are allowed is the Turing machine’s
program. Your goal is to control the machine into the accept state.

Represent the Turing machine acceptance problem as a planning problem.
If you can do this, it demonstrates that determining whether a planning
problem has a solution is at least as hard as the Turing acceptance
problem, which is PSPACE-hard.

[negative-effects-exercise]Explain why dropping negative effects from
every action schema in a planning problem results in a relaxed problem.

[sussman-anomaly-exercise] () shows a blocks-world problem that is known
as the . The problem was considered anomalous because the noninterleaved
planners of the early 1970s could not solve it. Write a definition of
the problem and solve it, either by hand or with a planning program. A
noninterleaved planner is a planner that, when given two subgoals
$G_{1}$ and $G_{2}$, produces either a plan for $G_{1}$ concatenated
with a plan for $G_{2}$, or vice versa. Explain why a noninterleaved
planner cannot solve this problem.

Prove that backward search with PDDL problems is complete.

Construct levels 0, 1, and 2 of the planning graph for the problem in .

[graphplan-proof-exercise] Prove the following assertions about planning
graphs:

1.  A literal that does not appear in the final level of the graph
    cannot be achieved.

2.  The level cost of a literal in a serial graph is no greater than the
    actual cost of an optimal plan for achieving it.

We saw that planning graphs can handle only propositional actions. What
if we want to use planning graphs for a problem with variables in the
goal, such as ${At}(P_{1}, x) 
    \land {At}(P_{2}, x)$, where $x$ is assumed to be bound by an
existential quantifier that ranges over a finite domain of locations?
How could you encode such a problem to work with planning graphs?

The set-level heuristic (see ) uses a planning graph to estimate the
cost of achieving a conjunctive goal from the current state. What
relaxed problem is the set-level heuristic the solution to?

Examine the definition of in .

1.  Would bidirectional state-space search be a good idea for planning?

2.  What about bidirectional search in the space of partial-order plans?

3.  Devise a version of partial-order planning in which an action can be
    added to a plan if its preconditions can be achieved by the effects
    of actions already in the plan. Explain how to deal with conflicts
    and ordering constraints. Is the algorithm essentially identical to
    forward state-space search?

We contrasted forward and backward state-space searchers with
partial-order planners, saying that the latter is a plan-space searcher.
Explain how forward and backward state-space search can also be
considered plan-space searchers, and say what the plan refinement
operators are.

[satplan-preconditions-exercise] Up to now we have assumed that the
plans we create always make sure that an action’s preconditions are
satisfied. Let us now investigate what propositional successor-state
axioms such as ${HaveArrow}^{t+1} \lequiv {}$ $({HaveArrow}^t
\land \lnot {Shoot}^t)$ have to say about actions whose preconditions
are not satisfied.

1.  Show that the axioms predict that nothing will happen when an action
    is executed in a state where its preconditions are not satisfied.

2.  Consider a plan $p$ that contains the actions required to achieve a
    goal but also includes illegal actions. Is it the case that
    $$\mbox{{\em initial state}}\land \mbox{{\em successor-state axioms}} \land
    p \entails \mbox{{\em goal}\ ?}$$

3.  With first-order successor-state axioms in situation calculus, is it
    possible to prove that a plan containing illegal actions will
    achieve the goal?

[strips-translation-exercise]Consider how to translate a set of action
schemas into the successor-state axioms of situation calculus.

1.  Consider the schema for ${Fly}(p,{from},{to})$. Write a
    logical definition for the predicate
    ${Poss}({Fly}(p,{from},{to}),s)$, which is true if the
    preconditions for ${Fly}(p,{from},{to})$ are satisfied in
    situation $s$.

2.  Next, assuming that ${Fly}(p,{from},{to})$ is the only action
    schema available to the agent, write down a successor-state axiom
    for ${At}(p,x,s)$ that captures the same information as the action
    schema.

3.  Now suppose there is an additional method of travel:
    ${Teleport}(p,{from},{to})$. It has the additional
    precondition $\lnot {Warped}(p)$ and the additional effect
    ${Warped}(p)$. Explain how the situation calculus knowledge base
    must be modified.

4.  Finally, develop a general and precisely specified procedure for
    carrying out the translation from a set of action schemas to a set
    of successor-state axioms.

[disjunctive-satplan-exercise] In the algorithm in (), each call to the
satisfiability algorithm asserts a goal $g^T$, where $T$ ranges from 0
to $T_{{\rm max}}$. Suppose instead that the satisfiability algorithm is
called only once, with the goal
$g^0 \lor g^1 \lor \cdots \lor g^{T_{{\rm max}}}$.

1.  Will this always return a plan if one exists with length less than
    or equal to $T_{{\rm max}}$?

2.  Does this approach introduce any new spurious “solutions”?

3.  Discuss how one might modify a satisfiability algorithm such as so
    that it finds short solutions (if they exist) when given a
    disjunctive goal of this form.

[^1]: PDDL was derived from the original planning
    language@Fikes+Nilsson:1971. which is slightly more restricted than
    PDDL: preconditions and goals cannot contain negative literals.

[^2]: The blocks world used in planning research is much simpler than ’s
    version, shown on .

[^3]: Many problems are written with this convention. For problems that
    aren’t, replace every negative literal $\lnot P$ in a goal or
    precondition with a new positive literal, $P'$.
[uncertainty-part]

Quantifying Uncertainty {#probability-chapter}
=======================

Acting under Uncertainty
------------------------

Agents may need to handle , whether due to partial observability,
nondeterminism, or a combination of the two. An agent may never know for
certain what state it’s in or where it will end up after a sequence of
actions.

We have seen problem-solving agents () and logical agents () designed to
handle uncertainty by keeping track of a —a representation of the set of
all possible world states that it might be in—and generating a
contingency plan that handles every possible eventuality that its
sensors may report during execution. Despite its many virtues, however,
this approach has significant drawbacks when taken literally as a recipe
for creating agent programs:

-   When interpreting partial sensor information, a logical agent must
    consider *every logically possible* explanation for the
    observations, no matter how unlikely. This leads to impossible large
    and complex belief-state representations.

-   A correct contingent plan that handles every eventuality can grow
    arbitrarily large and must consider arbitrarily unlikely
    contingencies.

-   Sometimes there is no plan that is guaranteed to achieve the
    goal—yet the agent must act. It must have some way to compare the
    merits of plans that are not guaranteed.

Suppose, for example, that an automated has the goal of delivering a
passenger to the airport on time. The agent forms a plan, $A_{{90}}$,
that involves leaving home 90 minutes before the flight departs and
driving at a reasonable speed. Even though the airport is only about 5
miles away, a logical taxi agent will not be able to conclude with
certainty that “Plan $A_{{90}}$ will get us to the airport in time.”
Instead, it reaches the weaker conclusion “Plan $A_{{90}}$ will get us
to the airport in time, as long as the car doesn’t break down or run out
of gas, and I don’t get into an accident, and there are no accidents on
the bridge, and the plane doesn’t leave early, and no meteorite hits the
car, and $\ldots$ .” None of these conditions can be deduced for sure,
so the plan’s success cannot be inferred. This is the (), for which we
so far have seen no real solution.

Nonetheless, in some sense $A_{{90}}$ *is* in fact the
right thing to do. What do we mean by this? As we discussed in , we mean
that out of all the plans that could be executed, $A_{{90}}$ is expected
to maximize the agent’s performance measure (where the expectation is
relative to the agent’s knowledge about the environment). The
performance measure includes getting to the airport in time for the
flight, avoiding a long, unproductive wait at the airport, and avoiding
speeding tickets along the way. The agent’s knowledge cannot guarantee
any of these outcomes for $A_{{90}}$, but it can provide some degree of
belief that they will be achieved. Other plans, such as $A_{{180}}$,
might increase the agent’s belief that it will get to the airport on
time, but also increase the likelihood of a long wait.

The right thing to do—the —therefore depends on both the relative
importance of various goals and the likelihood that, and degree to
which, they will be achieved.

The remainder of this section hones these ideas, in preparation for the
development of the general theories of uncertain reasoning and rational
decisions that we present in this and subsequent chapters.

### Summarizing uncertainty

Let’s consider an example of uncertain reasoning: diagnosing a dental
patient’s toothache. Diagnosis—whether for medicine, automobile repair,
or whatever—almost always involves uncertainty. Let us try to write
rules for dental diagnosis using propositional logic, so that we can see
how the logical approach breaks down. Consider the following simple
rule: $${Toothache} \implies {Cavity} \ .\index{toothache}$$ The
problem is that this rule is wrong. Not all patients with toothaches
have cavities; some of them have gum disease, an abscess, or one of
several other problems:
$${Toothache} \implies {Cavity}\lor {GumProblem} \lor {Abscess} \ldots$$
Unfortunately, in order to make the rule true, we have to add an almost
unlimited list of possible problems. We could try turning the rule into
a causal rule: $${Cavity} \implies {Toothache}\ .$$ But this rule is
not right either; not all cavities cause pain. The only way to fix the
rule is to make it logically exhaustive: to augment the left-hand side
with all the qualifications required for a cavity to cause a toothache.
Trying to use logic to cope with a domain like medical diagnosis thus
fails for three main reasons:

It is too much work to list the complete set of antecedents or
consequents needed to ensure an exceptionless rule and too hard to use
such rules. Medical science has no complete theory for the domain. Even
if we know all the rules, we might be uncertain about a particular
patient because not all the necessary tests have been or can be run.

The connection between toothaches and cavities is just not a logical
consequence in either direction. This is typical of the medical domain,
as well as most other judgmental domains: law, business, design,
automobile repair, gardening, dating, and so on. The agent’s knowledge
can at best provide only a in the relevant sentences. Our main tool for
dealing with degrees of belief is . In the terminology of , the of logic
and probability theory are the same—that the world is composed of facts
that do or do not hold in any particular case—but the are different: a
logical agent believes each sentence to be true or false or has no
opinion, whereas a probabilistic agent may have a numerical degree of
belief between 0 (for sentences that are certainly false) and 1
(certainly true).

*Probability provides a way of **summarizing** the
uncertainty that comes from our laziness and ignorance,* thereby
solving the qualification problem. We might not know for sure what
afflicts a particular patient, but we believe that there is, say, an 80%
chance—that is, a probability of 0.8—that the patient who has a
toothache has a cavity. That is, we expect that out of all the
situations that are indistinguishable from the current situation as far
as our knowledge goes, the patient will have a cavity in 80% of them.
This belief could be derived from statistical data—80% of the toothache
patients seen so far have had cavities—or from some general dental
knowledge, or from a combination of evidence sources.

One confusing point is that at the time of our diagnosis, there is no
uncertainty in the actual world: the patient either has a cavity or
doesn’t. So what does it mean to say the probability of a cavity is 0.8?
Shouldn’t it be either 0 or 1? The answer is that probability statements
are made with respect to a knowledge state, not with respect to the real
world. We say “The probability that the patient has a cavity,
*given that she has a toothache*, is 0.8.” If we later
learn that the patient has a history of gum disease, we can make a
different statement: “The probability that the patient has a cavity,
given that she has a toothache and a history of gum disease, is 0.4.” If
we gather further conclusive evidence against a cavity, we can say “The
probability that the patient has a cavity, given all we now know, is
almost 0.” Note that these statements do not contradict each other; each
is a separate assertion about a different knowledge state.

### Uncertainty and rational decisions

Consider again the $A_{{90}}$ plan for getting to the airport. Suppose
it gives us a 97% chance of catching our flight. Does this mean it is a
rational choice? Not necessarily: there might be other plans, such as
$A_{{180}}$, with higher probabilities. If it is vital not to miss the
flight, then it is worth risking the longer wait at the airport. What
about $A_{{1440}}$, a plan that involves leaving home 24 hours in
advance? In most circumstances, this is not a good choice, because
although it almost guarantees getting there on time, it involves an
intolerable wait—not to mention a possibly unpleasant diet of airport
food.

To make such choices, an agent must first have between the different
possible of the various plans. An outcome is a completely specified
state, including such factors as whether the agent arrives on time and
the length of the wait at the airport. We use to represent and reason
with preferences. (The term is used here in the sense of “the quality of
being useful,” not in the sense of the electric company or water works.)
Utility theory says that every state has a degree of usefulness, or
utility, to an agent and that the agent will prefer states with higher
utility.

The utility of a state is relative to an agent. For example, the utility
of a state in which White has checkmated Black in a game of chess is
obviously high for the agent playing White, but low for the agent
playing Black. But we can’t go strictly by the scores of 1, 1/2, and 0
that are dictated by the rules of tournament chess—some players
(including the authors) might be thrilled with a draw against the world
champion, whereas other players (including the former world champion)
might not. There is no accounting for taste or preferences: you might
think that an agent who prefers jalapeño bubble-gum ice cream to
chocolate chocolate chip is odd or even misguided, but you could not say
the agent is irrational. A utility function can account for any set of
preferences—quirky or typical, noble or perverse. Note that utilities
can account for altruism, simply by including the welfare of others as
one of the factors.

Preferences, as expressed by utilities, are combined with probabilities
in the general theory of rational decisions called :
$$\mbox{\it Decision theory} = \mbox{\it probability theory} + \mbox{\it utility
theory}\ .$$ The fundamental idea of decision theory is that

an agent is rational if and only if it chooses the action that yields
the highest expected utility, averaged over all the possible outcomes of
the action.

This is called the principle of (MEU). Note that “expected” might seem
like a vague, hypothetical term, but as it is used here it has a precise
meaning: it means the “average,” or “statistical mean” of the outcomes,
weighted by the probability of the outcome. We saw this principle in
action in when we touched briefly on optimal decisions in backgammon; it
is in fact a completely general principle.

sketches the structure of an agent that uses decision theory to select
actions. The agent is identical, at an abstract level, to the agents
described in that maintain a belief state reflecting the history of
percepts to date. The primary difference is that the decision-theoretic
agent’s belief state represents not just the
*possibilities* for world states but also their
*probabilities*. Given the belief state, the agent can make
probabilistic predictions of action outcomes and hence select the action
with highest expected utility. This chapter and the next concentrate on
the task of representing and computing with probabilistic information in
general. deals with methods for the specific tasks of representing and
updating the belief state over time and predicting the environment.
covers utility theory in more depth, and develops algorithms for
planning sequences of actions in uncertain environments.

[dt-agent-algorithm]

Basic Probability Notation
--------------------------

[probability-notation-section]

For our agent to represent and use probabilistic information, we need a
formal language. The language of probability theory has traditionally
been informal, written by human mathematicians to other human
mathematicians. includes a standard introduction to elementary
probability theory; here, we take an approach more suited to the needs
of AI and more consistent with the concepts of formal logic.

### What probabilities are about

Like logical assertions, probabilistic assertions are about possible
worlds. Whereas logical assertions say which possible worlds are
strictly ruled out (all those in which the assertion is false),
probabilistic assertions talk about how probable the various worlds are.
In probability theory, the set of all possible worlds is called the .
The possible worlds are *mutually exclusive* and
*exhaustive*—two possible worlds cannot both be the case,
and one possible world must be the case. For example, if we are about to
roll two (distinguishable) dice, there are 36 possible worlds to
consider: (1,1), (1,2), $\ldots$, (6,6). The Greek letter $\Omega$
(uppercase omega) is used to refer to the sample space, and $\omega$
(lowercase omega) refers to elements of the space, that is, particular
possible worlds.

A fully specified associates a numerical probability $P(\omega)$ with
each possible world.[^1] The basic axioms of probability theory say that
every possible world has a probability between 0 and 1 and that the
total probability of the set of possible worlds is 1:

$$0 \leq P(\omega) \leq 1 \mbox{ for every } \omega \mbox{    and    } \sum_{\omega\in\Omega} P(\omega) = 1\ .
\label{basic-probability-axiom-equation}$$

For example, if we assume that each die is fair and the rolls don’t
interfere with each other, then each of the possible worlds (1,1),
(1,2), $\ldots$, (6,6) has probability 1/36. On the other hand, if the
dice conspire to produce the same number, then the worlds (1,1), (2,2),
(3,3), etc., might have higher probabilities, leaving the others with
lower probabilities.

Probabilistic assertions and queries are not usually about particular
possible worlds, but about sets of them. For example, we might be
interested in the cases where the two dice add up to 11, the cases where
doubles are rolled, and so on. In probability theory, these sets are
called —a term already used extensively in for a different concept. In
AI, the sets are always described by in a formal language. (One such
language is described in .) For each proposition, the corresponding set
contains just those possible worlds in which the proposition holds. The
probability associated with a proposition is defined to be the sum of
the probabilities of the worlds in which it holds:

$$\mbox{For any proposition }\phi,\ P(\phi) = \sum_{\omega\in\phi} P(\omega)\ .
\label{proposition-probability-equation}$$

For example, when rolling fair dice, we have $P({Total}\eq 11) =
P((5,6)) + P((6,5)) = 1/36 + 1/36 = 1/18$. Note that probability theory
does not require complete knowledge of the probabilities of each
possible world. For example, if we believe the dice conspire to produce
the same number, we might *assert* that $P({doubles}) =
1/4$ without knowing whether the dice prefer double 6 to double 2. Just
as with logical assertions, this assertion *constrains* the
underlying probability model without fully determining it.

Probabilities such as $P({Total}\eq 11)$ and $P({doubles})$ are
called or (and sometimes just “priors” for short); they refer to degrees
of belief in propositions *in the absence of any other
information*. Most of the time, however, we have
*some* information, usually called , that has already been
revealed. For example, the first die may already be showing a 5 and we
are waiting with bated breath for the other one to stop spinning. In
that case, we are interested not in the unconditional probability of
rolling doubles, but the or probability (or just “posterior” for short)
of rolling doubles *given that the first die is a 5*. This
probability is written
$P({doubles}\given\index{1given@$\given$ (given)}
{Die}_1\eq 5)$, where the “$\given $” is pronounced “given.”
Similarly, if I am going to the dentist for a regular checkup, the
probability $P({cavity})\eq 0.2$ might be of interest; but if I go to
the dentist because I have a toothache, it’s $P({cavity}\given
{toothache})\eq 0.6$ that matters. Note that the precedence of
“$\given$” is such that any expression of the form $P(\ldots |
\ldots)$ always means $P((\ldots) | (\ldots))$.

It is important to understand that $P({cavity})\eq
0.2$ is still *valid* after ${toothache}$ is observed; it
just isn’t especially useful. When making decisions, an agent needs to
condition on *all* the evidence it has observed. It is also
important to understand the difference between conditioning and logical
implication. The assertion that $P({cavity}\given
{toothache})\eq 0.6$ does not mean “Whenever ${toothache}$ is true,
conclude that ${cavity}$ is true with probability 0.6” rather it means
“Whenever ${toothache}$ is true *and we have no further
information*, conclude that ${cavity}$ is true with probability
0.6.” The extra condition is important; for example, if we had the
further information that the dentist found no cavities, we definitely
would not want to conclude that ${cavity}$ is true with probability
0.6; instead we need to use
$P({cavity} | {toothache} \land \lnot {cavity}) \eq 0$.

Mathematically speaking, conditional probabilities are defined in terms
of unconditional probabilities as follows: for any propositions $a$ and
$b$, we have

$$P(a\given b) = \frac{P(a \land b)}{P(b)} \ ,
\label{conditional-probability-equation}$$

which holds whenever $P(b) > 0$. For example,
$$P({doubles}\given {Die}_1\eq 5) = \frac{P({doubles} \land
 {Die}_1\eq 5)}{P({Die}_1\eq 5)}\ .$$ The definition makes sense if
you remember that observing $b$ rules out all those possible worlds
where $b$ is false, leaving a set whose total probability is just
$P(b)$. Within that set, the $a$-worlds satisfy $a \land b$ and
constitute a fraction $P(a
\land b)/P(b)$.

The definition of conditional probability, , can be written in a
different form called the [product-rule-page]:
$$P(a \land b) = P(a\given b)P(b) \ ,$$ The product rule is perhaps
easier to remember: it comes from the fact that, for $a$ and $b$ to be
true, we need $b$ to be true, and we also need $a$ to be true given $b$.

### The language of propositions in probability assertions {#probability-language-section}

In this chapter and the next, propositions describing sets of possible
worlds are written in a notation that combines elements of propositional
logic and constraint satisfaction notation. In the terminology of , it
is a , in which a possible world is represented by a set of
variable/value pairs.

Variables in probability theory are called and their names begin with an
uppercase letter. Thus, in the dice example, ${Total}$ and ${Die}_1$
are random variables. Every random variable has a —the set of possible
values it can take on. The domain of ${Total}$ for two dice is the set
$\{2,\ldots,12\}$ and the domain of ${Die}_1$ is $\{1,\ldots,6\}$. A
Boolean random variable has the domain $\{{true},{false}\}$ (notice
that values are always lowercase); for example, the proposition that
doubles are rolled can be written as ${Doubles}\eq {true}$. By
convention, propositions of the form ${A}\eq {true}$ are abbreviated
simply as ${a}$, while ${A}\eq {false}$ is abbreviated as
$\lnot a$. (The uses of ${doubles}$, ${cavity}$, and ${toothache}$
in the preceding section are abbreviations of this kind.) As in CSPs,
domains can be sets of arbitrary tokens; we might choose the domain of
${Age}$ to be $\{{juvenile},{teen},{adult}\}$ and the domain of
${Weather}$ might be $\{{sunny},{rain},{cloudy},{snow}\}$.
When no ambiguity is possible, it is common to use a value by itself to
stand for the proposition that a particular variable has that value;
thus, ${sunny}$ can stand for ${Weather}\eq {sunny}$.

The preceding examples all have finite domains. Variables can have
infinite domains, too—either discrete (like the integers) or continuous
(like the reals). For any variable with an ordered domain, inequalities
are also allowed, such as ${NumberOfAtomsInUniverse}\geq 10^{70}$.

Finally, we can combine these sorts of elementary propositions
(including the abbreviated forms for Boolean variables) by using the
connectives of propositional logic. For example, we can express “The
probability that the patient has a cavity, given that she is a teenager
with no toothache, is 0.1” as follows:
$$P({cavity} \given \lnot {toothache} \land {teen}) = 0.1 \ .$$

Sometimes we will want to talk about the probabilities of
*all* the possible values of a random variable. We could
write: $$\begin{array}{l}
P({Weather}\eq {sunny}) = {0.6}\\
P({Weather}\eq {rain}) = {0.1}\\
P({Weather}\eq {cloudy}) = {0.29}\\
P({Weather}\eq {snow}) = {0.01}\ ,
\end{array}$$ but as an abbreviation we will allow
$$\pv({Weather}) \eq \langle {0.6},{0.1},{0.29},{0.01} \rangle\ ,$$
where the bold $\pv$ indicates that the result is a vector of numbers,
and where we assume a predefined ordering
$\langle{sunny},{rain},{cloudy},{snow}\rangle$ on the domain of
${Weather}$. We say that the $\pv$ statement defines a for the random
variable ${Weather}$. The $\pv$ notation is also used for conditional
distributions: $\pv(X\given Y)$ gives the values of $P(X\eq
x_i\given Y\eq y_j)$ for each possible $i$, $j$ pair.

For continuous variables, it is not possible to write out the entire
distribution as a vector, because there are infinitely many values.
Instead, we can define the probability that a random variable takes on
some value $x$ as a parameterized function of $x$. For example, the
sentence $$P({NoonTemp}\eq x) = {Uniform}_{[{18C},{26C}]}(x)$$
expresses the belief that the temperature at noon is distributed
uniformly between 18 and 26 degrees Celsius. We call this a .

Probability density functions (sometimes called ) differ in meaning from
discrete distributions. Saying that the probability density is uniform
from $18C$ to $26C$ means that there is a 100% chance that the
temperature will fall somewhere in that $8C$-wide region and a 50%
chance that it will fall in any $4C$-wide region, and so on. We write
the probability density for a continuous random variable $X$ at value
$x$ as $P(X\eq x)$ or just $P(x)$; the intuitive definition of $P(x)$ is
the probability that $X$ falls within an arbitrarily small region
beginning at $x$, divided by the width of the region:
$$P(x) = \lim_{dx \to 0} P(x \leq X \leq x+dx)/dx\ .$$ For
${NoonTemp}$ we have
$$P({NoonTemp}\eq x) = {Uniform}_{[{18C},{26C}]}(x) = \left\{\begin{array}{l}
  \frac{1}{8C} \mbox{~if~} 18C \le x \le 26C \\
  0 \mbox{~otherwise~} 
\end{array}\right. \ ,$$ where $C$ stands for centigrade (not for a
constant). In $P({NoonTemp}\eq 20.18C) \eq \frac{1}{8C}$, note that
$\frac{1}{8C}$ is not a probability, it is a probability density. The
probability that ${NoonTemp}$ is *exactly* $20.18C$ is
zero, because $20.18C$ is a region of width 0. Some authors use
different symbols for discrete distributions and density functions; we
use $P$ in both cases, since confusion seldom arises and the equations
are usually identical. Note that probabilities are unitless numbers,
whereas density functions are measured with a unit, in this case
reciprocal degrees.

In addition to distributions on single variables, we need notation for
distributions on multiple variables. Commas are used for this. For
example, $\pv({Weather},{Cavity})$ denotes the probabilities of all
combinations of the values of ${Weather}$ and ${Cavity}$. This is a
$4
\stimes 2$ table of probabilities called the of ${Weather}$ and
${Cavity}$. We can also mix variables with and without values;
$\pv({sunny},{Cavity})$ would be a two-element vector giving the
probabilities of a sunny day with a cavity and a sunny day with no
cavity. The $\pv$ notation makes certain expressions much more concise
than they might otherwise be. For example, the product rules for all
possible values of ${Weather}$ and ${Cavity}$ can be written as a
single equation:
$$\pv({Weather},{Cavity}) = \pv({Weather}\given {Cavity}) \pv({Cavity}) \ ,$$
instead of as these $4 \stimes 2 \eq 8$ equations (using abbreviations
$W$ and $C$):

P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)\
P(W C) = P(W | C) P(C)  .

As a degenerate case, $\pv({sunny},{cavity})$ has no variables and
thus is a one-element vector that is the probability of a sunny day with
a cavity, which could also be written as $P({sunny},{cavity})$ or
$P({sunny} \land {cavity})$. We will sometimes use $\pv$ notation to
derive results about individual $P$ values, and when we say
“$\pv({sunny})\eq 0.6$” it is really an abbreviation for
“$\pv({sunny})$ is the one-element vector $\<0.6\>$, which means that
$P({sunny})\eq 0.6$.”

Now we have defined a syntax for propositions and probability assertions
and we have given part of the semantics: defines the probability of a
proposition as the sum of the probabilities of worlds in which it holds.
To complete the semantics, we need to say what the worlds are and how to
determine whether a proposition holds in a world. We borrow this part
directly from the semantics of propositional logic, as
follows[possible-worlds-page].

A possible world is defined to be an assignment of values to all of the
random variables under consideration.

It is easy to see that this definition satisfies the basic requirement
that possible worlds be mutually exclusive and exhaustive (). For
example, if the random variables are ${Cavity}$, ${Toothache}$, and
${Weather}$, then there are $2\stimes 2\stimes 4\eq 16$ possible
worlds. Furthermore, the truth of any given proposition, no matter how
complex, can be determined easily in such worlds using the same
recursive definition of truth as for formulas in propositional logic.

From the preceding definition of possible worlds, it follows that a
probability model is completely determined by the joint distribution for
all of the random variables—the so-called . For example, if the
variables are ${Cavity}$, ${Toothache}$, and ${Weather}$, then the
full joint distribution is given by
$\pv({Cavity},{Toothache},{Weather})$. This joint distribution can
be represented as a $2\stimes 2 \stimes 4$ table with 16 entries.
Because every proposition’s probability is a sum over possible worlds, a
full joint distribution suffices, in principle, for calculating the
probability of any proposition.

### Probability axioms and their reasonableness {#probability-axiom-section}

[probability-axioms-section]

The basic axioms of probability () imply certain relationships among the
degrees of belief that can be accorded to logically related
propositions. For example, we can derive the familiar relationship
between the probability of a proposition and the probability of its
negation: $$\begin{array}{rcll}
P(\lnot a) &=& \sum_{\omega\in\lnot a} P(\omega) & 
                        \mbox{by \eqref{proposition-probability-equation}} \\
           &=& \sum_{\omega\in\lnot a} P(\omega) + \sum_{\omega\in a} P(\omega)  - \sum_{\omega\in a} P(\omega) & \\
           &=& \sum_{\omega\in\Omega} P(\omega) - \sum_{\omega\in a} P(\omega) & 
                        \mbox{grouping the first two terms} \\
           &=& 1-P(a) &
                        \mbox{by (\ref{basic-probability-axiom-equation}) and (\ref{proposition-probability-equation}).}
\end{array}$$ We can also derive the well-known formula for the
probability of a disjunction, sometimes called the :

$$P(a \lor b) = P(a) + P(b) - P(a \land b)\ .
\label{kolmogorov-disjunction-equation}$$

This rule is easily remembered by noting that the cases where $a$ holds,
together with the cases where $b$ holds, certainly cover all the cases
where $a\lor b$ holds; but summing the two sets of cases counts their
intersection twice, so we need to subtract $P(a \land b)$. The proof is
left as an exercise ().

are often called in honor of the Russian mathematician Andrei
Kolmogorov, who showed how to build up the rest of probability theory
from this simple foundation and how to handle the difficulties caused by
continuous variables.[^2] While has a definitional flavor, reveals that
the axioms really do constrain the degrees of belief an agent can have
concerning logically related propositions. This is analogous to the fact
that a logical agent cannot simultaneously believe $A$, $B$, and
$\lnot (A\land B)$, because there is no possible world in which all
three are true. With probabilities, however, statements refer not to the
world directly, but to the agent’s own state of knowledge. Why, then,
can an agent not hold the following set of beliefs (even though they
violate Kolmogorov’s axioms)?

$$\begin{array}{lcl}
P(a) =  {0.4} & \qquad\qquad & P(a\land b) = {0.0}\\
P(b) =  {0.3} & \qquad\qquad & P(a\lor b)  = {0.8}\ .
\end{array}
\label{dumb-beliefs-equation}$$

This kind of question has been the subject of decades of intense debate
between those who advocate the use of probabilities as the only
legitimate form for degrees of belief and those who advocate alternative
approaches.

One argument for the axioms of probability, first stated in 1931 by
Bruno de Finetti (and translated into English in ), is as follows: If an
agent has some degree of belief in a proposition $a$, then the agent
should be able to state odds at which it is indifferent to a bet for or
against $a$.[^3] Think of it as a game between two agents: Agent 1
states, “my degree of belief in event $a$ is 0.4.” Agent 2 is then free
to choose whether to wager for or against $a$ at stakes that are
consistent with the stated degree of belief. That is, Agent 2 could
choose to accept Agent 1’s bet that $a$ will occur, offering 6 against
Agent 1’s 4. Or Agent 2 could accept Agent 1’s bet that $\lnot
a$ will occur, offering 4 against Agent 1’s 6. Then we observe the
outcome of $a$, and whoever is right collects the money. If an agent’s
degrees of belief do not accurately reflect the world, then you would
expect that it would tend to lose money over the long run to an opposing
agent whose beliefs more accurately reflect the state of the world.

But de Finetti proved something much stronger:

If Agent 1 expresses a set of degrees of belief that violate the axioms
of probability theory then there is a combination of bets by Agent 2
that guarantees that Agent 1 will lose money every time.

For example, suppose that Agent 1 has the set of degrees of belief from
. shows that if Agent 2 chooses to bet 4 on $a$, 3 on $b$, and 2 on
$\lnot(a \lor b)$, then Agent 1 always loses money, regardless of the
outcomes for $a$ and $b$. De Finetti’s theorem implies that no rational
agent can have beliefs that violate the axioms of probability.

[ht] [de-finetti-table]

One common objection to is that this betting game is rather contrived.
For example, what if one refuses to bet? Does that end the argument? The
answer is that the betting game is an abstract model for the
decision-making situation in which every agent is
*unavoidably* involved at every moment. Every action
(including inaction) is a kind of bet, and every outcome can be seen as
a payoff of the bet. Refusing to bet is like refusing to allow time to
pass.[de-Finetti-page]

Other strong philosophical arguments have been put forward for the use
of probabilities, most notably those of Cox [-@Cox:1946],
Carnap [-@Carnap:1950], and . They each construct a set of axioms for
reasoning with degrees of beliefs: no contradictions, correspondence
with ordinary logic (for example, if belief in $A$ goes up, then belief
in $\lnot A$ must go down), and so on. The only controversial axiom is
that degrees of belief must be numbers, or at least act like numbers in
that they must be transitive (if belief in $A$ is greater than belief in
$B$, which is greater than belief in $C$, then belief in $A$ must be
greater than $C$) and comparable (the belief in $A$ must be one of equal
to, greater than, or less than belief in $B$). It can then be proved
that probability is the only approach that satisfies these axioms.

The world being the way it is, however, practical demonstrations
sometimes speak louder than proofs. The success of reasoning systems
based on probability theory has been much more effective in making
converts. We now look at how the axioms can be deployed to make
inferences.

Inference Using Full Joint Distributions {#full-joint-section}
----------------------------------------

In this section we describe a simple method for —that is, the
computation of posterior probabilities for query propositions given
observed evidence. We use the full joint distribution as the “knowledge
base” from which answers to all questions may be derived. Along the way
we also introduce several useful techniques for manipulating equations
involving probabilities.

We begin with a simple example: a domain consisting of just the three
Boolean variables ${Toothache}$, ${Cavity}$, and ${Catch}$ (the
dentist’s nasty steel probe catches in my tooth). The full joint
distribution is a $2\stimes 2\stimes 2$ table as shown in .

[tbp] [dentist-joint-table]

Notice that the probabilities in the joint distribution sum to 1, as
required by the axioms of probability. Notice also that gives us a
direct way to calculate the probability of any proposition, simple or
complex: simply identify those possible worlds in which the proposition
is true and add up their probabilities. For example, there are six
possible worlds in which ${cavity} \lor {toothache}$ holds:
$$P({cavity} \lor {toothache}) = {0.108} + {0.012} + {0.072} + {0.008} + {0.016} + {0.064} 
                         = {0.28}\ .$$ One particularly common task is
to extract the distribution over some subset of variables or a single
variable. For example, adding the entries in the first row gives the
unconditional or [^4] of ${cavity}$:
$$P({cavity}) = {0.108} + {0.012} + {0.072} + {0.008} = {0.2}\ .$$
This process is called , or —because we sum up the probabilities for
each possible value of the other variables, thereby taking them out of
the equation. We can write the following general marginalization rule
for any sets of variables $\Y$ and $\Z$:

$$\pv(\Y) = \sum_{\sz \in \Z} \pv(\Y,\z)\ ,
\label{marginalization-equation}$$

where $\sum_{\sz \in \Z}$ means to sum over all the possible
combinations of values of the set of variables $\Z$. We sometimes
abbreviate this as $\sum_{\sz}$, leaving $\Z$ implicit. We just used the
rule as

$$\pv({Cavity}) = \sum_{\sz \in \{{Catch},{Toothache}\}} \pv({Cavity},\z)\ .$$

A variant of this rule involves conditional probabilities instead of
joint probabilities, using the product rule:

$$\pv(\Y) = \sum_{\sz} \pv(\Y\given \z)P(\z)\ .
\label{conditioning-equation}$$

This rule is called . Marginalization and conditioning turn out to be
useful rules for all kinds of derivations involving probability
expressions.

In most cases, we are interested in computing *conditional*
probabilities of some variables, given evidence about others.
Conditional probabilities can be found by first using to obtain an
expression in terms of unconditional probabilities and then evaluating
the expression from the full joint distribution. For example, we can
compute the probability of a cavity, given evidence of a toothache, as
follows:

$$\begin{aligned}
P({cavity}\given {toothache}) 
     &=&  \frac{P({cavity} \land {toothache})}{P({toothache})}\\
     &=&  \frac{{0.108} + {0.012}}{{0.108} + {0.012} + {0.016} + {0.064}}
          = {0.6}\ .\end{aligned}$$

Just to check, we can also compute the probability that there is no
cavity, given a toothache:

$$\begin{aligned}
P(\lnot {cavity}\given {toothache}) 
     &=& \frac{P(\lnot {cavity} \land {toothache})}{P({toothache})}\\
     &=& \frac{{0.016}+{0.064}}{{0.108} + {0.012} + {0.016} + {0.064}} = {0.4}\ .\end{aligned}$$

The two values sum to 1.0, as they should. Notice that in these two
calculations the term $1/P({toothache})$ remains constant, no matter
which value of ${Cavity}$ we calculate. In fact, it can be viewed as a
constant for the distribution $\pv({Cavity}\given {toothache})$,
ensuring that it adds up to 1. Throughout the chapters dealing with
probability, we use $\alpha$ to denote such constants. With this
notation, we can write the two preceding equations in one:

$$\begin{aligned}
 \lefteqn{\pv({Cavity}\given {toothache}) = \alpha\, \pv({Cavity},{toothache})} \\
  &=& \alpha\, [\pv({Cavity},{toothache},{catch})+\pv({Cavity},{toothache},\lnot {catch})]\\
  &=& \alpha\, [\<{0.108},{0.016}\> + \<{0.012},{0.064}\>] 
      = \alpha\, \<{0.12},{0.08}\> = \<{0.6},{0.4}\>\ .\end{aligned}$$

In other words, we can calculate $\pv({Cavity}\given
{toothache})$ even if we don’t know the value of $P({toothache})$!
We temporarily forget about the factor $1/P({toothache})$ and add up
the values for ${cavity}$ and $\lnot {cavity}$, getting 0.12 and
0.08. Those are the correct relative proportions, but they don’t sum to
1, so we normalize them by dividing each one by $0.12+0.08$, getting the
true probabilities of 0.6 and 0.4. Normalization turns out to be a
useful shortcut in many probability calculations, both to make the
computation easier and to allow us to proceed when some probability
assessment (such as $P({toothache})$) is not available.

From the example, we can extract a general inference procedure. We begin
with the case in which the query involves a single variable, $X$
(${Cavity}$ in the example). Let $\E$ be the list of evidence
variables (just ${Toothache}$ in the example), let $\e$ be the list of
observed values for them, and let $\Y$ be the remaining unobserved
variables (just ${Catch}$ in the example). The query is
$\pv(X\given \e)$ and can be evaluated as

$$\pv(X\given \e) = \alpha\, \pv(X,\e) = \alpha \sum_{\sy} \pv(X,\e,\y) \ ,
\label{enumeration-equation}$$

where the summation is over all possible $\y$s (i.e., all possible
combinations of values of the unobserved variables $\Y$). Notice that
together the variables $X$, $\E$, and $\Y$ constitute the complete set
of variables for the domain, so $\pv(X,\e,\y)$ is simply a subset of
probabilities from the full joint distribution.

Given the full joint distribution to work with, can answer probabilistic
queries for discrete variables. It does not scale well, however: for a
domain described by $n$ Boolean variables, it requires an input table of
size $O(2^n)$ and takes $O(2^n)$ time to process the table. In a
realistic problem we could easily have $n > 100$, making $O(2^n)$
impractical. The full joint distribution in tabular form is just not a
practical tool for building reasoning systems. Instead, it should be
viewed as the theoretical foundation on which more effective approaches
may be built, just as truth tables formed a theoretical foundation for
more practical algorithms like . The remainder of this chapter
introduces some of the basic ideas required in preparation for the
development of realistic systems in .

Independence {#independence-section}
------------

Let us expand the full joint distribution in by adding a fourth
variable, ${Weather}$. The full joint distribution then becomes
$\pv({Toothache},{Catch},{Cavity},{Weather})$, which has $2
\times 2 \times 2 \times 4 = 32$ entries. It contains four “editions” of
the table shown in , one for each kind of weather. What relationship do
these editions have to each other and to the original three-variable
table? For example, how are
$P({toothache},{catch},{cavity},{cloudy})$ and
$P({toothache},{catch},{cavity})$ related? We can use the product
rule:

$$\begin{aligned}
\lefteqn{P({toothache},{catch},{cavity},{cloudy})}\\
 & = & P({cloudy}\given {toothache},{catch},{cavity})P({toothache},{catch},{cavity})\ .\end{aligned}$$

Now, unless one is in the deity business, one should not imagine that
one’s dental problems influence the weather. And for indoor dentistry,
at least, it seems safe to say that the weather does not influence the
dental variables. Therefore, the following assertion seems reasonable:

$$P({cloudy}\given {toothache},{catch},{cavity}) = P({cloudy})\ .
\label{weather-independence-equation}$$

From this, we can deduce
$$P({toothache},{catch},{cavity},{cloudy})\\
    =  P({cloudy})P({toothache},{catch},{cavity}) \ .$$ A
similar equation exists for *every entry* in
$\pv({Toothache},{Catch},{Cavity},{Weather})$. In fact, we can
write the general equation

$$\pv({Toothache},{Catch},{Cavity},{Weather})
    = \pv({Toothache},{Catch},{Cavity}) \pv({Weather})\ .$$

Thus, the 32-element table for four variables can be constructed from
one 8-element table and one 4-element table. This decomposition is
illustrated schematically in (a).

[independence-figure]

The property we used in is called (also and ). In particular, the
weather is independent of one’s dental problems. Independence between
propositions $a$ and $b$ can be written as

$$P(a\given b) \eq P(a)\quad\mbox{or}\quad P(b\given a) \eq P(b)
  \quad\mbox{or}\quad P(a\land b) \eq P(a)P(b)\ .
\label{independence-equation}$$

All these forms are equivalent (). Independence between variables $X$
and $Y$ can be written as follows (again, these are all equivalent):
$$\pv(X\given Y) \eq \pv(X)\quad\mbox{or}\quad \pv(Y\given X) \eq \pv(Y)
  \quad\mbox{or}\quad \pv(X,Y) \eq \pv(X)\pv(Y)\ .$$ Independence
assertions are usually based on knowledge of the domain. As the
toothache–weather example illustrates, they can dramatically reduce the
amount of information necessary to specify the full joint distribution.
If the complete set of variables can be divided into independent
subsets, then the full joint distribution can be *factored*
into separate joint distributions on those subsets. For example, the
full joint distribution on the outcome of $n$ independent coin flips,
$\pv(C_1,\ldots,C_n)$, has $2^n$ entries, but it can be represented as
the product of $n$ single-variable distributions $\pv(C_i)$. In a more
practical vein, the independence of dentistry and meteorology is a good
thing, because otherwise the practice of dentistry might require
intimate knowledge of meteorology, and vice versa.

When they are available, then, independence assertions can help in
reducing the size of the domain representation and the complexity of the
inference problem. Unfortunately, clean separation of entire sets of
variables by independence is quite rare. Whenever a connection, however
indirect, exists between two variables, independence will fail to hold.
Moreover, even independent subsets can be quite large—for example,
dentistry might involve dozens of diseases and hundreds of symptoms, all
of which are interrelated. To handle such problems, we need more subtle
methods than the straightforward concept of independence.

Bayes’ Rule and Its Use {#bayes-rule-section}
-----------------------

On , we defined the . It can actually be written in two forms:
$$P(a \land b) = P(a\given b)P(b) \qquad \mbox{and} \qquad P(a \land b) = P(b\given a)P(a)\ .$$
Equating the two right-hand sides and dividing by $P(a)$, we get

$$P(b\given a) = \frac{P(a\given b)P(b)}{P(a)}\ .
\label{bayes-equation}$$

This equation is known as (also Bayes’ law or Bayes’ theorem). This
simple equation underlies most modern AI systems for probabilistic
inference.

The more general case of Bayes’ rule for multivalued variables can be
written in the $\pv$ notation as follows:
$$\pv(Y\given X) = \frac{\pv(X\given Y)\pv(Y)}{\pv(X)} \ ,$$ As before,
this is to be taken as representing a set of equations, each dealing
with specific values of the variables. We will also have occasion to use
a more general version conditionalized on some background evidence $\e$:

$$\pv(Y\given X,\e) = \frac{\pv(X\given Y,\e) \pv(Y\given \e)}{\pv(X\given \e)}\ .
\label{conditional-bayes-equation}$$

### Applying Bayes’ rule: The simple case {#meningitis-section}

On the surface, Bayes’ rule does not seem very useful. It allows us to
compute the single term $P(b\given a)$ in terms of three terms:
$P(a\given b)$, $P(b)$, and $P(a)$. That seems like two steps backwards,
but Bayes’ rule is useful in practice because there are many cases where
we do have good probability estimates for these three numbers and need
to compute the fourth. Often, we perceive as evidence the
*effect* of some unknown *cause* and we would
like to determine that cause. In that case, Bayes’ rule becomes
$$P({cause}\given \effect) = \frac{P(\effect\given {cause}) P({cause})}{P(\effect)}\ .$$
The conditional probability $P(\effect\given {cause})$ quantifies the
relationship in the direction, whereas $P({cause}\given \effect)$
describes the direction. In a task such as medical diagnosis, we often
have conditional probabilities on causal relationships (that is, the
doctor knows $P({symptoms}\given {disease})$) and want to derive a
diagnosis, $P({disease}\given
{symptoms})$. For example, a doctor knows that the disease meningitis
causes the patient to have a , say, 70% of the time. The doctor also
knows some unconditional facts: the prior probability that a patient has
meningitis is 1/50,000, and the prior probability that any patient has a
stiff neck is 1%. Letting $s$ be the proposition that the patient has a
stiff neck and $m$ be the proposition that the patient has meningitis,
we have

$$\begin{aligned}
P(s\given m) &=& {0.7} \nonumber\\
P(m) &=& 1/{50000} \nonumber \\
P(s) &=& {0.01} \nonumber \\
P(m\given s) &=& \frac{P(s\given m) P(m)}{P(s)} = \frac{{0.7} \times 1/{50000}}{0.01}
= {0.0014}\ . \label{meningitis-bayes-equation}\end{aligned}$$

That is, we expect less than 1 in 700 patients with a stiff neck to have
meningitis. Notice that even though a stiff neck is quite strongly
indicated by meningitis (with probability 0.7), the probability of
meningitis in the patient remains small. This is because the prior
probability of stiff necks is much higher than that of meningitis.

illustrated a process by which one can avoid assessing the prior
probability of the evidence (here, $P(s)$) by instead computing a
posterior probability for each value of the query variable (here, $m$
and $\lnot m$) and then normalizing the results. The same process can be
applied when using Bayes’ rule. We have
$$\pv(M\given s) = \alpha\, \<P(s\given m)P(m),P(s\given \lnot m)P(\lnot m)\>\ .$$
Thus, to use this approach we need to estimate $P(s\given \lnot m)$
instead of $P(s)$. There is no free lunch—sometimes this is easier,
sometimes it is harder. The general form of Bayes’ rule with
normalization is

$$\pv(Y\given X) = \alpha\, \pv(X\given Y)\pv(Y)\ ,
\label{normalized-bayes-equation}$$

where $\alpha$ is the normalization constant needed to make the entries
in $\pv(Y\given X)$ sum to 1.

One obvious question to ask about Bayes’ rule is why one might have
available the conditional probability in one direction, but not the
other. In the meningitis domain, perhaps the doctor knows that a stiff
neck implies meningitis in 1 out of 5000 cases; that is, the doctor has
quantitative information in the direction from symptoms to causes. Such
a doctor has no need to use Bayes’ rule. Unfortunately,

diagnostic knowledge is often more fragile than causal knowledge.

If there is a sudden epidemic of meningitis, the unconditional
probability of meningitis, $P(m)$, will go up. The doctor who derived
the diagnostic probability $P(m\given s)$ directly from statistical
observation of patients before the epidemic will have no idea how to
update the value, but the doctor who computes $P(m\given s)$ from the
other three values will see that $P(m\given s)$ should go up
proportionately with $P(m)$. Most important, the causal information
$P(s\given m)$ is *unaffected* by the epidemic, because it
simply reflects the way meningitis works. The use of this kind of direct
causal or model-based knowledge provides the crucial robustness needed
to make probabilistic systems feasible in the real world.

### Using Bayes’ rule: Combining evidence

We have seen that can be useful for answering probabilistic queries
conditioned on one piece of evidence—for example, the stiff neck. In
particular, we have argued that probabilistic information is often
available in the form $P(\effect\given {cause})$. What happens when we
have two or more pieces of evidence? For example, what can a dentist
conclude if her nasty steel probe catches in the aching tooth of a
patient? If we know the full joint distribution (), we can read off the
answer:
$$\pv({Cavity}\given {toothache}\land {catch})=\alpha\, \<{0.108},{0.016}\> \approx \<{0.871},{0.129}\>\ .$$
We know, however, that such an approach does not scale up to larger
numbers of variables. We can try using Bayes’ rule to reformulate the
problem:

$$\begin{aligned}
  \lefteqn{\pv({Cavity}\given {toothache} \land {catch})} \nonumber \\
&&      = \alpha\, \pv({toothache}\land {catch}\given {Cavity})\, \pv({Cavity})\ .
\label{cavity-equation}\end{aligned}$$

For this reformulation to work, we need to know the conditional
probabilities of the conjunction ${toothache} \land {catch}$ for
each value of ${Cavity}$. That might be feasible for just two evidence
variables, but again it does not scale up. If there are $n$ possible
evidence variables (X rays, diet, oral hygiene, etc.), then there are
$2^n$ possible combinations of observed values for which we would need
to know conditional probabilities. We might as well go back to using the
full joint distribution. This is what first led researchers away from
probability theory toward approximate methods for evidence combination
that, while giving incorrect answers, require fewer numbers to give any
answer at all.

Rather than taking this route, we need to find some additional
assertions about the domain that will enable us to simplify the
expressions. The notion of in provides a clue, but needs refining. It
would be nice if ${Toothache}$ and ${Catch}$ were independent, but
they are not: if the probe catches in the tooth, then it is likely that
the tooth has a cavity and that the cavity causes a toothache. These
variables *are* independent, however, *given the
presence or the absence of a cavity*. Each is directly caused by
the cavity, but neither has a direct effect on the other: toothache
depends on the state of the nerves in the tooth, whereas the probe’s
accuracy depends on the dentist’s skill, to which the toothache is
irrelevant.[^5] Mathematically, this property is written as

$$\pv({toothache} \land  {catch}\given {Cavity}) = 
   \pv({toothache}\given {Cavity}) \pv({catch}\given {Cavity})\ .
\label{dentist-propci-equation}$$

This equation expresses the of ${toothache}$ and ${catch}$ given
${Cavity}$. We can plug it into to obtain the probability of a cavity:

$$\begin{aligned}
\lefteqn{\pv({Cavity}\given {toothache} \land {catch})} \nonumber \\
&&     = \alpha\, \pv({toothache}\given {Cavity}) \,\pv({catch}\given {Cavity})\, \pv({Cavity}) \ . \end{aligned}$$

Now the information requirements are the same as for inference, using
each piece of evidence separately: the prior probability
$\pv({Cavity})$ for the query variable and the conditional probability
of each effect, given its cause.

The general definition of of two variables $X$ and $Y$, given a third
variable $Z$, is $$\pv(X,Y\given Z) = \pv(X\given Z)\pv(Y\given Z) \ .$$
In the dentist domain, for example, it seems reasonable to assert
conditional independence of the variables ${Toothache}$ and
${Catch}$, given ${Cavity}$:

$$\pv({Toothache},{Catch}\given {Cavity}) = 
   \pv({Toothache}\given {Cavity}) \pv({Catch}\given {Cavity})\ .
\label{dentist-varci-equation}$$

Notice that this assertion is somewhat stronger than , which asserts
independence only for specific values of ${Toothache}$ and
${Catch}$. As with absolute independence in , the equivalent forms
$$\pv(X\given Y,Z) \eq \pv(X\given Z) \quad\mbox{and}\quad \pv(Y\given X,Z) \eq \pv(Y\given Z)$$
can also be used (see ). showed that absolute independence assertions
allow a decomposition of the full joint distribution into much smaller
pieces. It turns out that the same is true for conditional independence
assertions. For example, given the assertion in , we can derive a
decomposition as follows:

$$\begin{aligned}
\lefteqn{\pv({Toothache},{Catch},{Cavity})}\\
   &=& \pv({Toothache},{Catch}\given {Cavity}) \pv({Cavity})
                                \quad\mbox{(product rule)}\\
   &=& \pv({Toothache}\given {Cavity}) \pv({Catch}\given {Cavity}) \pv({Cavity})
                           \quad\mbox{(using \ref{dentist-varci-equation}).}\end{aligned}$$

(The reader can easily check that this equation does in fact hold in .)
In this way, the original large table is decomposed into three smaller
tables. The original table has seven independent numbers ($2^3\eq 8$
entries in the table, but they must sum to 1, so 7 are independent). The
smaller tables contain five independent numbers (for a conditional
probability distributions such as $\pv(T|C$ there are two rows of two
numbers, and each row sums to 1, so that’s two independent numbers; for
a prior distribution like $\pv(C)$ there is only one independent
number). Going from seven to five might not seem like a major triumph,
but the point is that, for $n$ symptoms that are all conditionally
independent given ${Cavity}$, the size of the representation grows as
$O(n)$ instead of $O(2^n)$. That means that

conditional independence assertions can allow probabilistic systems to
scale up; moreover, they are much more commonly available than absolute
independence assertions.

Conceptually, ${Cavity}$ ${Toothache}$ and ${Catch}$ because it is
a direct cause of both of them. The decomposition of large probabilistic
domains into weakly connected subsets through conditional independence
is one of the most important developments in the recent history of AI.

The dentistry example illustrates a commonly occurring pattern in which
a single cause directly influences a number of effects, all of which are
conditionally independent, given the cause. The full joint distribution
can be written as $$\pv({Cause},\Effect_1,\ldots,\Effect_n) =
       \pv({Cause})\prod_i \pv(\Effect_i\given {Cause})\ .$$ Such a
probability distribution is called a [naive-bayes-intro-page]
model—“naive” because it is often used (as a simplifying assumption) in
cases where the “effect” variables are *not* actually
conditionally independent given the cause variable. (The naive Bayes
model is sometimes called a , a somewhat careless usage that has
prompted true Bayesians to call it the model.) In practice, naive Bayes
systems can work surprisingly well, even when the conditional
independence assumption is not true. describes methods for learning
naive Bayes distributions from observations.

The Wumpus World Revisited {#wumpus-prob-section}
--------------------------

We can combine of the ideas in this chapter to solve probabilistic
reasoning problems in the wumpus world. (See for a complete description
of the wumpus world.) Uncertainty arises in the wumpus world because the
agent’s sensors give only partial information about the world. For
example, shows a situation in which each of the three reachable
squares—[1,3], [2,2], and [3,1]—might contain a pit. Pure logical
inference can conclude nothing about which square is most likely to be
safe, so a logical agent might have to choose randomly. We will see that
a probabilistic agent can do much better than the logical agent.

[wumpus-stuck-figure]

Our aim is to calculate the probability that each of the three squares
contains a pit. (For this example we ignore the wumpus and the gold.)
The relevant properties of the wumpus world are that (1) a pit causes
breezes in all neighboring squares, and (2) each square other than [1,1]
contains a pit with probability 0.2. The first step is to identify the
set of random variables we need:

-   As in the propositional logic case, we want one Boolean variable
    $P_{ij}$ for each square, which is true iff square [$i,j$] actually
    contains a pit.

-   We also have Boolean variables $B_{ij}$ that are true iff square
    [$i,j$] is breezy; we include these variables only for the observed
    squares—in this case, [1,1], [1,2], and [2,1].

The next step is to specify the full joint distribution,
$\pv(P_{1,1},\ldots,P_{4,4},B_{1,1},B_{1,2},B_{2,1})$. Applying the
product rule, we have

(P~1,1~,…,P~4,4~,B~1,1~,B~1,2~,B~2,1~) =\
(B~1,1~,B~1,2~,B~2,1~P~1,1~,…,P~4,4~) (P~1,1~,…,P~4,4~) .

This decomposition makes it easy to see what the joint probability
values should be. The first term is the conditional probability
distribution of a breeze configuration, given a pit configuration; its
values are 1 if the breezes are adjacent to the pits and 0 otherwise.
The second term is the prior probability of a pit configuration. Each
square contains a pit with probability 0.2, independently of the other
squares; hence,

$$\pv(P_{1,1},\ldots,P_{4,4}) = \prod_{i,j\eq 1,1}^{4,4} \pv(P_{i,j})\ .
\label{wumpus-independent-prior-equation}$$

For a particular configuration with exactly $n$ pits,
$P(P_{1,1},\ldots,P_{4,4})\eq {0.2}^n \stimes
{0.8}^{{16}-n}$.

In the situation in (a), the evidence consists of the observed breeze
(or its absence) in each square that is visited, combined with the fact
that each such square contains no pit. We abbreviate these facts as
$b\eq \lnot b_{1,1} \land
b_{1,2}
\land b_{2,1}$ and ${known} \eq \lnot p_{1,1} \land \lnot p_{1,2}\land
\lnot p_{2,1}$. We are interested in answering queries such as
$\pv(P_{1,3}\given {known},b)$: how likely is it that [1,3] contains a
pit, given the observations so far?

To answer this query, we can follow the standard approach of , namely,
summing over entries from the full joint distribution. Let ${Unknown}$
be the set of $P_{i,j}$ variables for squares other than the ${Known}$
squares and the query square [1,3]. Then, by , we have
$$\pv(P_{1,3}\given {known},b) = \alpha \sum_{{unknown}}\pv(P_{1,3},{unknown},{known},b)  \ .$$
The full joint probabilities have already been specified, so we are
done—that is, unless we care about computation. There are 12 unknown
squares; hence the summation contains $2^{{12}} \eq {4096}$ terms. In
general, the summation grows exponentially with the number of squares.

Surely, one might ask, aren’t the other squares irrelevant? How could
[4,4] affect whether [1,3] has a pit? Indeed, this intuition is correct.
Let ${Frontier}$ be the pit variables (other than the query variable)
that are adjacent to visited squares, in this case just [2,2] and [3,1].
Also, let ${Other}$ be the pit variables for the other unknown
squares; in this case, there are 10 other squares, as shown in (b). The
key insight is that the observed breezes are *conditionally
independent* of the other variables, given the known, frontier,
and query variables. To use the insight, we manipulate the query formula
into a form in which the breezes are conditioned on all the other
variables, and then we apply conditional independence:

$$\begin{aligned}
  \lefteqn{\pv(P_{1,3}\given {known},b) }\\
  &=& \alpha \sum_{{unknown}}\pv(P_{1,3},{known},b,{unknown}) \qquad \mbox{(by \eqref{enumeration-equation})} \\
  &=& \alpha \sum_{{unknown}}\pv(b\given P_{1,3},{known},{unknown})\pv(P_{1,3},{known},{unknown})\\
  &&\qquad \qquad \qquad \mbox{(by the product rule)}\\%  &=& \alpha \sum_{{frontier}}\sum_{{other}} \pv(b\given {known},P_{1,3},{frontier},{other})\pv(P_{1,3},{known},{frontier},{other})\\
  &=& \alpha \sum_{{frontier}}\sum_{{other}} \pv(b\given {known},P_{1,3},{frontier})\pv(P_{1,3},{known},{frontier},{other})\ ,\end{aligned}$$

where the final step uses conditional independence: $b$ is independent
of ${other}$ given ${known}$, $P_{1,3}$, and ${frontier}$. Now,
the first term in this expression does not depend on the ${Other}$
variables, so we can move the summation inward:

$$\begin{aligned}
\lefteqn{\pv(P_{1,3}\given {known},b)}\\
&=& \alpha \!\sum_{{frontier}} \!\pv(b\given {known},P_{1,3},{frontier}) \sum_{{other}}\! \pv(P_{1,3},{known},{frontier},{other})\ .\end{aligned}$$

By independence, as in , the prior term can be factored, and then the
terms can be reordered:

$$\begin{aligned}
\lefteqn{\pv(P_{1,3}\given {known},b) }\\
&=&  \alpha \sum_{{frontier}} \pv(b\given {known},P_{1,3},{frontier})
       \sum_{{other}} \pv(P_{1,3}) P({known})P({frontier})P({other})\\
&=&  \alpha\, P({known}) \pv(P_{1,3}) \sum_{{frontier}}
       \pv(b\given {known},P_{1,3},{frontier}) P({frontier}) \sum_{{other}} P({other})\\
&=&  \alpha'\, \pv(P_{1,3}) \sum_{{frontier}} \pv(b\given {known},P_{1,3},{frontier}) P({frontier})\ ,\end{aligned}$$

where the last step folds $P({known})$ into the normalizing constant
and uses the fact that $\sum_{{other}} P({other})$ equals 1.

Now, there are just four terms in the summation over the frontier
variables $P_{2,2}$ and $P_{3,1}$. The use of independence and has
completely eliminated the other squares from consideration.

Notice that the expression $\pv(b\given {known},P_{1,3},{frontier})$
is 1 when the frontier is consistent with the breeze observations, and 0
otherwise. Thus, for each value of $P_{1,3}$, we sum over the
*logical models* for the frontier variables that are
consistent with the known facts. (Compare with the enumeration over
models in on .) The models and their associated prior
probabilities—$P({frontier})$—are shown in . We have
$$\pv(P_{1,3}\given {known},b) 
   =  \alpha'\, \< {0.2}({0.04} + {0.16} + {0.16}),\ {0.8}({0.04} + {0.16}) \> \approx
   \<{0.31},{0.69}\>\ .$$ That is, [1,3] (and [3,1] by symmetry)
contains a pit with roughly 31% probability. A similar calculation,
which the reader might wish to perform, shows that [2,2] contains a pit
with roughly 86% probability. The wumpus agent should definitely avoid
[2,2]! Note that our logical agent from did not know that [2,2] was
worse than the other squares. Logic can tell us that it is unknown
whether there is a pit in [2, 2], but we need probability to tell us how
likely it is.

[wumpus-fringe-models-figure]

What this section has shown is that even seemingly complicated problems
can be formulated precisely in probability theory and solved with simple
algorithms. To get *efficient* solutions, independence and
conditional independence relationships can be used to simplify the
summations required. These relationships often correspond to our natural
understanding of how the problem should be decomposed. In the next
chapter, we develop formal representations for such relationships as
well as algorithms that operate on those representations to perform
probabilistic inference efficiently.

This chapter has suggested probability theory as a suitable foundation
for uncertain reasoning and provided a gentle introduction to its use.

-   Uncertainty arises because of both laziness and ignorance. It is
    inescapable in complex, nondeterministic, or partially observable
    environments.

-   Probabilities express the agent’s inability to reach a definite
    decision regarding the truth of a sentence. Probabilities summarize
    the agent’s beliefs relative to the evidence.

-   Decision theory combines the agent’s beliefs and desires, defining
    the best action as the one that maximizes expected utility.

-   Basic probability statements include and over simple and complex
    propositions.

-   The axioms of probability constrain the possible assignments of
    probabilities to propositions. An agent that violates the axioms
    must behave irrationally in some cases.

-   The specifies the probability of each complete assignment of values
    to random variables. It is usually too large to create or use in its
    explicit form, but when it is available it can be used to answer
    queries simply by adding up entries for the possible worlds
    corresponding to the query propositions.

-   between subsets of random variables allows the full joint
    distribution to be factored into smaller joint distributions,
    greatly reducing its complexity. Absolute independence seldom occurs
    in practice.

-   allows unknown probabilities to be computed from known conditional
    probabilities, usually in the causal direction. Applying Bayes’ rule
    with many pieces of evidence runs into the same scaling problems as
    does the full joint distribution.

-   brought about by direct causal relationships in the domain might
    allow the full joint distribution to be factored into smaller,
    conditional distributions. The model assumes the conditional
    independence of all effect variables, given a single cause variable,
    and grows linearly with the number of effects.

-   A wumpus-world agent can calculate probabilities for unobserved
    aspects of the world, thereby improving on the decisions of a purely
    logical agent. Conditional independence makes these calculations
    tractable.

Probability theory was invented as a way of analyzing games of chance.
In about 850 a.d. the Indian mathematician Mahaviracarya
described how to arrange a set of bets that can’t lose (what we now call
a Dutch book). In Europe, the first significant systematic analyses were
produced by Girolamo Cardano around 1565, although publication was
posthumous [-@Cardano:1663]. By that time, probability had been
established as a mathematical discipline due to a series of results
established in a famous correspondence between Blaise Pascal and Pierre
de Fermat in 1654. As with probability itself, the results were
initially motivated by gambling problems (see ). The first published
textbook on probability was *De Ratiociniis in Ludo Aleae*
@Huygens:1657. The “laziness and ignorance” view of uncertainty was
described by John Arbuthnot in the preface of his translation of
Huygens @Arbuthnot:1692: “It is impossible for a Die, with such
determin’d force and direction, not to fall on such determin’d side,
only I don’t know the force and direction which makes it fall on such
determin’d side, and therefore I call it Chance, which is nothing but
the want of art...”

gave an exceptionally accurate and modern overview of probability; he
was the first to use the example “take two urns, A and B, the first
containing four white and two black balls, …” The Rev. Thomas Bayes
(1702–1761) introduced the rule for reasoning about conditional
probabilities that was named after him @Bayes:1763. Bayes only
considered the case of uniform priors; it was Laplace who independently
developed the general case. Kolmogorov [-@Kolmogorov:1950 first
published in German in 1933] presented probability theory in a
rigorously axiomatic framework for the first time. Rényi
[-@Renyi:1970] later gave an axiomatic presentation that took
conditional probability, rather than absolute probability, as primitive.

Pascal used probability in ways that required both the objective
interpretation, as a property of the world based on symmetry or relative
frequency, and the subjective interpretation, based on degree of
belief—the former in his analyses of probabilities in games of chance,
the latter in the famous “Pascal’s wager” argument about the possible
existence of God. However, Pascal did not clearly realize the
distinction between these two interpretations. The distinction was first
drawn clearly by James Bernoulli (1654–1705).

Leibniz introduced the “classical” notion of probability as a proportion
of enumerated, equally probable cases, which was also used by Bernoulli,
although it was brought to prominence by Laplace (1749–1827). This
notion is ambiguous between the frequency interpretation and the
subjective interpretation. The cases can be thought to be equally
probable either because of a natural, physical symmetry between them, or
simply because we do not have any knowledge that would lead us to
consider one more probable than another. The use of this latter,
subjective consideration to justify assigning equal probabilities is
known as the . The principle is often attributed to Laplace, but he
never isolated the principle explicitly. George Boole and John Venn both
referred to it as the ; the modern name is due to .

The debate between objectivists and subjectivists became sharper in the
20th century. Kolmogorov [-@Kolmogorov:1963],
R. A. Fisher [-@Fisher:1922], and Richard von Mises [-@VonMises:1928]
were advocates of the relative frequency interpretation. Karl Popper’s
[-@Popper:1959 first published in German in 1934] “propensity”
interpretation traces relative frequencies to an underlying physical
symmetry. Frank Ramsey [-@Ramsey:1931], Bruno de Finetti
[-@DeFinetti:1937], R. T. Cox [-@Cox:1946], Leonard Savage
[-@Savage:1954], Richard Jeffrey [-@Jeffrey:1983], and E. T. 
interpreted probabilities as the degrees of belief of specific
individuals. Their analyses of degree of belief were closely tied to
utilities and to behavior—specifically, to the willingness to place
bets. Rudolf Carnap, following Leibniz and Laplace, offered a different
kind of subjective interpretation of probability—not as any actual
individual’s degree of belief, but as the degree of belief that an
idealized individual *should* have in a particular
proposition $a$, given a particular body of evidence $\e$. Carnap
attempted to go further than Leibniz or Laplace by making this notion of
degree of mathematically precise, as a logical relation between $a$ and
$\e$. The study of this relation was intended to constitute a
mathematical discipline called , analogous to ordinary deductive logic
@Carnap:1948 [@Carnap:1950]. Carnap was not able to extend his inductive
logic much beyond the propositional case, and Putnam [-@Putnam:1963]
showed by adversarial arguments that some fundamental difficulties would
prevent a strict extension to languages capable of expressing
arithmetic.

Cox’s theorem [-@Cox:1946] shows that any system for uncertain reasoning
that meets his set of assumptions is equivalent to probability theory.
This gave renewed confidence to those who already favored probability,
but others were not convinced, pointing to the assumptions (primarily
that belief must be represented by a single number, and thus the belief
in $\lnot p$ must be a function of the belief in $p$). describes the
assumptions and shows some gaps in Cox’s original formulation. shows how
to patch up the difficulties. has a similar argument that is easier to
read.

The question of reference classes is closely tied to the attempt to find
an inductive logic. The approach of choosing the “most specific”
reference class of sufficient size was formally proposed by
Reichenbach [-@Reichenbach:1949]. Various attempts have been made,
notably by Henry Kyburg [-@Kyburg:1977; -@Kyburg:1983], to formulate
more sophisticated policies in order to avoid some obvious fallacies
that arise with Reichenbach’s rule, but such approaches remain somewhat
*ad hoc*. More recent work by Bacchus, Grove, Halpern, and
Koller [-@Bacchus+al:1992] extends Carnap’s methods to first-order
theories, thereby avoiding many of the difficulties associated with the
straightforward reference-class method. contrast probabilistic inference
with nonmonotonic logic.

Bayesian probabilistic reasoning has been used in AI since the 1960s,
especially in . It was used not only to make a diagnosis from available
evidence, but also to select further questions and tests by using the
theory of information value () when available evidence was inconclusive
@Gorry:1968 [@Gorry+al:1973]. One system outperformed human experts in
the diagnosis of acute abdominal illnesses @DeDombal+al:1974. gives an
overview. These early Bayesian systems suffered from a number of
problems, however. Because they lacked any theoretical model of the
conditions they were diagnosing, they were vulnerable to
unrepresentative data occurring in situations for which only a small
sample was available @DeDombal+al:1981. Even more fundamentally, because
they lacked a concise formalism (such as the one to be described in )
for representing and using conditional independence information, they
depended on the acquisition, storage, and processing of enormous tables
of probabilistic data. Because of these difficulties, probabilistic
methods for coping with uncertainty fell out of favor in AI from the
1970s to the mid-1980s. Developments since the late 1980s are described
in the next chapter.

The model for joint distributions has been studied extensively in the
pattern recognition literature since the 1950s @Duda+Hart:1973. It has
also been used, often unwittingly, in information retrieval, beginning
with the work of Maron [-@Maron:1961]. The probabilistic foundations of
this technique, described further in , were elucidated by . Domingos and
Pazzani [-@Domingos+Pazzani:1997] provide an explanation for the
surprising success of naive Bayesian reasoning even in domains where the
independence assumptions are clearly violated.

There are many good introductory textbooks on probability theory,
including those by and . offer a combined introduction to probability
and statistics from a Bayesian standpoint. Richard Hamming’s
[-@Hamming:1991] textbook gives a mathematically sophisticated
introduction to probability theory from the standpoint of a propensity
interpretation based on physical symmetry. Hacking [-@Hacking:1975] and
Hald [-@Hald:1990] cover the early history of the concept of
probability. Bernstein [-@Bernstein:1996] gives an entertaining popular
account of the story of risk.

Show from first principles that $P(a\given b\land a) = 1$.

[sum-to-1-exercise]Using the axioms of probability, prove that any
probability distribution on a discrete random variable must sum to 1.

For each of the following statements, either prove it is true or give a
counterexample.

1.  If $P(a \given b, c) = P(b \given a, c)$, then
    $P(a \given c) = P(b \given c)$

2.  If $P(a \given b, c) = P(a)$, then $P(b \given c) = P(b)$

3.  If $P(a \given b) = P(a)$, then $P(a \given b, c) = P(a \given c)$

Would it be rational for an agent to hold the three beliefs
$P(A) \eq {0.4}$, $P(B) \eq {0.3}$, and $P(A \lor B) \eq {0.5}$? If so,
what range of probabilities would be rational for the agent to hold for
$A \land B$? Make up a table like the one in , and show how it supports
your argument about rationality. Then draw another version of the table
where $P(A \lor B)
\eq {0.7}$. Explain why it is rational to have this probability, even
though the table shows one case that is a loss and three that just break
even. (*Hint:* what is Agent 1 committed to about the
probability of each of the four cases, especially the case that is a
loss?)

[exclusive-exhaustive-exercise]This question deals with the properties
of possible worlds, defined on as assignments to all random variables.
We will work with propositions that correspond to exactly one possible
world because they pin down the assignments of all the variables. In
probability theory, such propositions are called . For example, with
Boolean variables $X_1$, $X_2$, $X_3$, the proposition
$x_1\land \lnot x_2 \land \lnot x_3$ fixes the assignment of the
variables; in the language of propositional logic, we would say it has
exactly one model.

1.  Prove, for the case of $n$ Boolean variables, that any two distinct
    atomic events are mutually exclusive; that is, their conjunction is
    equivalent to ${false}$.

2.  Prove that the disjunction of all possible atomic events is
    logically equivalent to ${true}$.

3.  Prove that any proposition is logically equivalent to the
    disjunction of the atomic events that entail its truth.

[inclusion-exclusion-exercise]Prove from .

Consider the set of all possible five-card poker hands dealt fairly from
a standard deck of fifty-two cards.

1.  How many atomic events are there in the joint probability
    distribution (i.e., how many five-card hands are there)?

2.  What is the probability of each atomic event?

3.  What is the probability of being dealt a royal straight flush? Four
    of a kind?

Given the full joint distribution shown in , calculate the following:

1.  $\pv({toothache})$.

2.  $\pv({Cavity})$.

3.  $\pv({Toothache}\given{cavity})$.

4.  $\pv({Cavity}\given{toothache}\lor {catch})$.

Given the full joint distribution shown in , calculate the following:

1.  $\pv({toothache})$.

2.  $\pv({Catch})$.

3.  $\pv({Cavity}\given{catch})$.

4.  $\pv({Cavity}\given{toothache}\lor {catch})$.

[unfinished-game-exercise] In his letter of August 24, 1654, Pascal was
trying to show how a pot of money should be allocated when a gambling
game must end prematurely. Imagine a game where each turn consists of
the roll of a die, player *E* gets a point when the die is
even, and player *O* gets a point when the die is odd. The
first player to get 7 points wins the pot. Suppose the game is
interrupted with *E* leading 4–2. How should the money be
fairly split in this case? What is the general formula? (Fermat and
Pascal made several errors before solving the problem, but you should be
able to get it right the first time.)

Deciding to put probability theory to good use, we encounter a slot
machine with three independent wheels, each producing one of the four
symbols bar, bell, lemon, or
cherry with equal probability. The slot machine has the
following payout scheme for a bet of 1 coin (where “?” denotes that we
don’t care what comes up for that wheel):

> bar/bar/bar pays 20 coins\
> bell/bell/bell pays 15 coins\
> lemon/lemon/lemon pays 5 coins\
> cherry/cherry/cherry pays 3
> coins\
> cherry/cherry/? pays 2 coins\
> cherry/?/? pays 1 coin

1.  Compute the expected “payback” percentage of the machine. In other
    words, for each coin played, what is the expected coin return?

2.  Compute the probability that playing the slot machine once will
    result in a win.

3.  Estimate the mean and median number of plays you can expect to make
    until you go broke, if you start with 10 coins. You can run a
    simulation to estimate this, rather than trying to compute an exact
    answer.

Deciding to put our knowledge of probability to good use, we encounter a
slot machine with three independently turning reels, each producing one
of the four symbols bar, bell,
lemon, or cherry with equal probability. The
slot machine has the following payout scheme for a bet of 1 coin (where
“?” denotes that we don’t care what comes up for that wheel):

> bar/bar/bar pays 21 coins\
> bell/bell/bell pays 16 coins\
> lemon/lemon/lemon pays 5 coins\
> cherry/cherry/cherry pays 3
> coins\
> cherry/cherry/? pays 2 coins\
> cherry/?/? pays 1 coin

1.  Compute the expected “payback” percentage of the machine. In other
    words, for each coin played, what is the expected coin return?

2.  Compute the probability that playing the slot machine once will
    result in a win.

3.  Estimate the mean and median number of plays you can expect to make
    until you go broke, if you start with 8 coins. You can run a
    simulation to estimate this, rather than trying to compute an exact
    answer.

We wish to transmit an $n$-bit message to a receiving agent. The bits in
the message are independently corrupted (flipped) during transmission
with $\epsilon$ probability each. With an extra parity bit sent along
with the original information, a message can be corrected by the
receiver if at most one bit in the entire message (including the parity
bit) has been corrupted. Suppose we want to ensure that the correct
message is received with probability at least $1-\delta$. What is the
maximum feasible value of $n$? Calculate this value for the case
$\epsilon\eq 0.001$, $\delta\eq 0.01$.

We wish to transmit an $n$-bit message to a receiving agent. The bits in
the message are independently corrupted (flipped) during transmission
with $\epsilon$ probability each. With an extra parity bit sent along
with the original information, a message can be corrected by the
receiver if at most one bit in the entire message (including the parity
bit) has been corrupted. Suppose we want to ensure that the correct
message is received with probability at least $1-\delta$. What is the
maximum feasible value of $n$? Calculate this value for the case
$\epsilon\eq 0.002$, $\delta\eq 0.01$.

[independence-exercise]Show that the three forms of independence in are
equivalent.

Consider two medical tests, A and B, for a virus. Test A is 95%
effective at recognizing the virus when it is present, but has a 10%
false positive rate (indicating that the virus is present, when it is
not). Test B is 90% effective at recognizing the virus, but has a 5%
false positive rate. The two tests use independent methods of
identifying the virus. The virus is carried by 1% of all people. Say
that a person is tested for the virus using only one of the tests, and
that test comes back positive for carrying the virus. Which test
returning positive is more indicative of someone really carrying the
virus? Justify your answer mathematically.

Suppose you are given a coin that lands ${heads}$ with probability $x$
and ${tails}$ with probability $1 - x$. Are the outcomes of successive
flips of the coin independent of each other given that you know the
value of $x$? Are the outcomes of successive flips of the coin
independent of each other if you do *not* know the value of
$x$? Justify your answer.

After your yearly checkup, the doctor has bad news and good news. The
bad news is that you tested positive for a serious disease and that the
test is 99% accurate (i.e., the probability of testing positive when you
do have the disease is 0.99, as is the probability of testing negative
when you don’t have the disease). The good news is that this is a rare
disease, striking only 1 in 10,000 people of your age. Why is it good
news that the disease is rare? What are the chances that you actually
have the disease?

After your yearly checkup, the doctor has bad news and good news. The
bad news is that you tested positive for a serious disease and that the
test is 99% accurate (i.e., the probability of testing positive when you
do have the disease is 0.99, as is the probability of testing negative
when you don’t have the disease). The good news is that this is a rare
disease, striking only 1 in 100,000 people of your age. Why is it good
news that the disease is rare? What are the chances that you actually
have the disease?

[conditional-bayes-exercise]It is quite often useful to consider the
effect of some specific propositions in the context of some general
background evidence that remains fixed, rather than in the complete
absence of information. The following questions ask you to prove more
general versions of the product rule and Bayes’ rule, with respect to
some background evidence $\e$:

1.  Prove the conditionalized version of the general product rule:
    $$\pv(X,Y \given \e) = \pv(X\given Y,\e) \pv(Y\given\e)\ .$$

2.  Prove the conditionalized version of Bayes’ rule in .

[pv-xyz-exercise] Show that the statement of conditional independence
$$\pv(X,Y \given Z) = \pv(X\given Z) \pv(Y\given Z)$$ is equivalent to
each of the statements
$$\pv(X\given Y,Z) = \pv(X\given Z) \quad\mbox{and}\quad \pv(B\given X,Z) = \pv(Y\given Z)\ .$$

Suppose you are given a bag containing $n$ unbiased coins. You are told
that $n-1$ of these coins are normal, with heads on one side and tails
on the other, whereas one coin is a fake, with heads on both sides.

1.  Suppose you reach into the bag, pick out a coin at random, flip it,
    and get a head. What is the (conditional) probability that the coin
    you chose is the fake coin?

2.  Suppose you continue flipping the coin for a total of $k$ times
    after picking it and see $k$ heads. Now what is the conditional
    probability that you picked the fake coin?

3.  Suppose you wanted to decide whether the chosen coin was fake by
    flipping it $k$ times. The decision procedure returns ${fake}$ if
    all $k$ flips come up heads; otherwise it returns ${normal}$. What
    is the (unconditional) probability that this procedure makes an
    error?

[normalization-exercise]In this exercise, you will complete the
normalization calculation for the meningitis example. First, make up a
suitable value for $P(s\given\lnot m)$, and use it to calculate
unnormalized values for $P(m\given s)$ and $P(\lnot m \given s)$ (i.e.,
ignoring the $P(s)$ term in the Bayes’ rule expression, ). Now normalize
these values so that they add to 1.

This exercise investigates the way in which conditional independence
relationships affect the amount of information needed for probabilistic
calculations.

1.  Suppose we wish to calculate $P(h\given e_1,e_2)$ and we have no
    conditional independence information. Which of the following sets of
    numbers are sufficient for the calculation?

    1.  $\pv(E_1,E_2)$, $\pv(H)$, $\pv(E_1\given H)$, $\pv(E_2\given H)$

    2.  $\pv(E_1,E_2)$, $\pv(H)$, $\pv(E_1,E_2\given H)$

    3.  $\pv(H)$, $\pv(E_1\given H)$, $\pv(E_2\given H)$

2.  Suppose we know that $\pv(E_1\given H,E_2)=\pv(E_1\given H)$ for all
    values of $H$, $E_1$, $E_2$. Now which of the three sets are
    sufficient?

Let $X$, $Y$, $Z$ be Boolean random variables. Label the eight entries
in the joint distribution $\pv(X,Y,Z)$ as $a$ through $h$. Express the
statement that $X$ and $Y$ are conditionally independent given $Z$, as a
set of equations relating $a$ through $h$. How many
*nonredundant* equations are there?

(Adapted from Pearl [-@Pearl:1988].) Suppose you are a witness to a
nighttime hit-and-run accident involving a taxi in Athens. All taxis in
Athens are blue or green. You swear, under oath, that the taxi was blue.
Extensive testing shows that, under the dim lighting conditions,
discrimination between blue and green is 75% reliable.

1.  Is it possible to calculate the most likely color for the taxi?
    (*Hint:* distinguish carefully between the proposition
    that the taxi *is* blue and the proposition that it
    *appears* blue.)

2.  What if you know that 9 out of 10 Athenian taxis are green?

Write out a general algorithm for answering queries of the form
$\pv({Cause}\given \e)$, using a naive Bayes distribution. Assume that
the evidence $\e$ may assign values to *any subset* of the
effect variables.

[naive-bayes-retrieval-exercise] Text categorization is the task of
assigning a given document to one of a fixed set of categories on the
basis of the text it contains. Naive Bayes models are often used for
this task. In these models, the query variable is the document category,
and the “effect” variables are the presence or absence of each word in
the language; the assumption is that words occur independently in
documents, with frequencies determined by the document category.

1.  Explain precisely how such a model can be constructed, given as
    “training data” a set of documents that have been assigned to
    categories.

2.  Explain precisely how to categorize a new document.

3.  Is the conditional independence assumption reasonable? Discuss.

In our analysis of the , we used the fact that each square contains a
pit with probability 0.2, independently of the contents of the other
squares. Suppose instead that exactly $N/5$ pits are scattered at random
among the $N$ squares other than [1,1]. Are the variables $P_{i,j}$ and
$P_{k,l}$ still independent? What is the joint distribution
$\pv(P_{1,1},\ldots,P_{4,4})$ now? Redo the calculation for the
probabilities of pits in [1,3] and [2,2].

Redo the probability calculation for pits in [1,3] and [2,2], assuming
that each square contains a pit with probability 0.01, independent of
the other squares. What can you say about the relative performance of a
logical versus a probabilistic agent in this case?

Implement a hybrid probabilistic agent for the wumpus world, based on
the hybrid agent in and the probabilistic inference procedure outlined
in this chapter.

[^1]: For now, we assume a discrete, countable set of worlds. The proper
    treatment of the continuous case brings in certain complications
    that are less relevant for most purposes in AI.

[^2]: The difficulties include the , a well-defined subset of the
    interval $[0,1]$ with no well-defined size.

[^3]: One might argue that the agent’s preferences for different bank
    balances are such that the possibility of losing 1 is not
    counterbalanced by an equal possibility of winning 1. One possible
    response is to make the bet amounts small enough to avoid this
    problem. Savage’s analysis [-@Savage:1954] circumvents the issue
    altogether.

[^4]: So called because of a common practice among actuaries of writing
    the sums of observed frequencies in the margins of insurance tables.

[^5]: We assume that the patient and dentist are distinct individuals.
Reinforcement Learning {#reinforcement-learning-chapter}
======================

Introduction {#rl-intro-section}
------------

Chapters [concept-learning-chapter], [ilp-chapter],
and [bayesian-learning-chapter] covered methods that learn functions,
logical theories, and probability models from examples. In this chapter,
we will study how agents can learn *what to do* in the
absence of labeled examples of what to do.

Consider, for example, the problem of learning to play chess. A
supervised learning agent needs to be told the correct move for each
position it encounters, but such feedback is seldom available. In the
absence of feedback from a teacher, an agent can learn a transition
model for its own moves and can perhaps learn to predict the opponent’s
moves, but

without some feedback about what is good and what is bad, the agent will
have no grounds for deciding which move to make.

The agent needs to know that something good has happened when it
(accidentally) checkmates the opponent, and that something bad has
happened when it is checkmated—or vice versa, if the game is suicide
chess. This kind of feedback is called a , or . In games like chess, the
reinforcement is received only at the end of the game. In other
environments, the rewards come more frequently. In ping-pong, each point
scored can be considered a reward; when learning to crawl, any forward
motion is an achievement. Our framework for agents regards the reward as
*part* of the input percept, but the agent must be
“hardwired” to recognize that part as a reward rather than as just
another sensory input. Thus, animals seem to be hardwired to recognize
pain and hunger as negative rewards and pleasure and food intake as
positive rewards. Reinforcement has been carefully studied by animal
psychologists for over 60 years.

Rewards were introduced in , where they served to define optimal
policies in (MDPs). An optimal policy is a policy that maximizes the
expected total reward. The task of is to use observed rewards to learn
an optimal (or nearly optimal) policy for the environment. Whereas in
the agent has a complete model of the environment and knows the reward
function, here we assume no prior knowledge of either. Imagine playing a
new game whose rules you don’t know; after a hundred or so moves, your
opponent announces, “You lose.” This is reinforcement learning in a
nutshell.

In many complex domains, reinforcement learning is the only feasible way
to train a program to perform at high levels. For example, in game
playing, it is very hard for a human to provide accurate and consistent
evaluations of large numbers of positions, which would be needed to
train an evaluation function directly from examples. Instead, the
program can be told when it has won or lost, and it can use this
information to learn an evaluation function that gives reasonably
accurate estimates of the probability of winning from any given
position. Similarly, it is extremely difficult to program an agent to
fly a helicopter; yet given appropriate negative rewards for crashing,
wobbling, or deviating from a set course, an agent can learn to fly by
itself.

Reinforcement learning might be considered to encompass all of AI: an
agent is placed in an environment and must learn to behave successfully
therein. To keep the chapter manageable, we will concentrate on simple
environments and simple agent designs. For the most part, we will assume
a fully observable environment, so that the current state is supplied by
each percept. On the other hand, we will assume that the agent does not
know how the environment works or what its actions do, and we will allow
for probabilistic action outcomes. Thus, the agent faces an unknown
Markov decision process. We will consider three of the agent designs
first introduced in :

-   A learns a utility function on states and uses it to select actions
    that maximize the expected outcome utility.

-   A agent learns an , or , giving the expected utility of taking a
    given action in a given state.

-   A learns a policy that maps directly from states to actions.

A utility-based agent must also have a model of the environment in order
to make decisions, because it must know the states to which its actions
will lead. For example, in order to make use of a backgammon evaluation
function, a backgammon program must know what its legal moves are
*and how they affect the board position*. Only in this way
can it apply the utility function to the outcome states. A Q-learning
agent, on the other hand, can compare the expected utilities for its
available choices without needing to know their outcomes, so it does not
need a model of the environment. On the other hand, because they do not
know where their actions lead, Q-learning agents cannot look ahead; this
can seriously restrict their ability to learn, as we shall see.

We begin in with , where the agent’s policy is fixed and the task is to
learn the utilities of states (or state–action pairs); this could also
involve learning a model of the environment. covers , where the agent
must also learn what to do. The principal issue is : an agent must
experience as much as possible of its environment in order to learn how
to behave in it. discusses how an agent can use inductive learning to
learn much faster from its experiences. covers methods for learning
direct policy representations in reflex agents. An understanding of
Markov decision processes () is essential for this chapter.

Passive Reinforcement Learning {#passive-rl-section}
------------------------------

To keep things simple, we start with the case of a passive learning
agent using a state-based representation in a fully observable
environment. In passive learning, the agent’s policy $\pi$ is fixed: in
state $s$, it always executes the action $\pi(s)$. Its goal is simply to
learn how good the policy is—that is, to learn the utility function
$U^{\pi}(s)$. We will use as our example the $4\stimes 3$ world
introduced in . shows a policy for that world and the corresponding
utilities. Clearly, the passive learning task is similar to the task,
part of the algorithm described in . The main difference is that the
passive learning agent does not know the $\transprob{s}{a}{s'}$, which
specifies the probability of reaching state $s'$ from state $s$ after
doing action $a$; nor does it know the $R(s)$, which specifies the
reward for each state.

[rl-4x3-figure]

The agent executes a set of in the environment using its policy $\pi$.
In each trial, the agent starts in state (1,1) and experiences a
sequence of state transitions until it reaches one of the terminal
states, (4,2) or (4,3). Its percepts supply both the current state and
the reward received in that state. Typical trials might look like
this:[rl-trials-page] $$\begin{array}{l}
(1,1)_{\smbf{-.{04}}}{\leadsto}(1,2)_{\smbf{-.{04}}}{\leadsto}(1,3)_{\smbf{-.{04}}}{\leadsto}(1,2)_{\smbf{-.{04}}}{\leadsto}(1,3)_{\smbf{-.{04}}}{\leadsto}(2,3)_{\smbf{-.{04}}}{\leadsto}(3,3)_{\smbf{-.{04}}}{\leadsto}(4,3)_{\smbf{+1}}\\
(1,1)_{\smbf{-.{04}}}{\leadsto}(1,2)_{\smbf{-.{04}}}{\leadsto}(1,3)_{\smbf{-.{04}}}{\leadsto}(2,3)_{\smbf{-.{04}}}{\leadsto}(3,3)_{\smbf{-.{04}}}{\leadsto}(3,2)_{\smbf{-.{04}}}{\leadsto}(3,3)_{\smbf{-.{04}}}{\leadsto}(4,3)_{\smbf{+1}}\\
(1,1)_{\smbf{-.{04}}}{\leadsto}(2,1)_{\smbf{-.{04}}}{\leadsto}(3,1)_{\smbf{-.{04}}}{\leadsto}(3,2)_{\smbf{-.{04}}}{\leadsto}(4,2)_{\smbf{-1}} \ .
\end{array}$$ Note that each state percept is subscripted with the
reward received. The object is to use the information about rewards to
learn the expected utility $U^{\pi}(s)$ associated with each nonterminal
state $s$. The utility is defined to be the expected sum of (discounted)
rewards obtained if policy $\pi$ is followed. As in on , we write

$$U^{\pi}(s) = E\left[\sum_{t\eq 0}^\infty \gamma^t R(S_t) \right]
\label{utility-definition-repeat-equation}$$

where $R(s)$ is the reward for a state, $S_t$ (a random variable) is the
state reached at time $t$ when executing policy $\pi$, and $S_0\eq s$.
We will include a $\gamma$ in all of our equations, but for the
$4\stimes 3$ world we will set $\gamma\eq 1$.

### Direct utility estimation

A simple method for was invented in the late 1950s in the area of by
Widrow and Hoff [-@Widrow+Hoff:1960]. The idea is that the utility of a
state is the expected total reward from that state onward (called the
expected ), and each trial provides a *sample* of this
quantity for each state visited. For example, the first trial in the set
of three given earlier provides a sample total reward of 0.72 for state
(1,1), two samples of 0.76 and 0.84 for (1,2), two samples of 0.80 and
0.88 for (1,3), and so on. Thus, at the end of each sequence, the
algorithm calculates the observed reward-to-go for each state and
updates the estimated utility for that state accordingly, just by
keeping a running average for each state in a table. In the limit of
infinitely many trials, the sample average will converge to the true
expectation in .

It is clear that direct utility estimation is just an instance of
supervised learning where each example has the state as input and the
observed reward-to-go as output. This means that we have reduced
reinforcement learning to a standard inductive learning problem, as
discussed in . discusses the use of more powerful kinds of
representations for the utility function. Learning techniques for those
representations can be applied directly to the observed data.

Direct utility estimation succeeds in reducing the reinforcement
learning problem to an inductive learning problem, about which much is
known. Unfortunately, it misses a very important source of information,
namely, the fact that the utilities of states are not independent!

The utility of each state equals its own reward plus the expected
utility of its successor states.

That is, the utility values obey the Bellman equations for a fixed
policy (see also ):

$$U^{\pi}(s) = R(s) + \gamma\, \sum\limits_{s'} \transprob{s}{\pi(s)}{s'} U^{\pi}(s')\ .
\label{policy-evaluation-repeated-equation}$$

By ignoring the connections between states, direct utility estimation
misses opportunities for learning. For example, the second of the three
trials given earlier reaches the state (3,2), which has not previously
been visited. The next transition reaches (3,3), which is known from the
first trial to have a high utility. The Bellman equation suggests
immediately that (3,2) is also likely to have a high utility, because it
leads to (3,3), but direct utility estimation learns nothing until the
end of the trial. More broadly, we can view direct utility estimation as
searching for $U$ in a hypothesis space that is much larger than it
needs to be, in that it includes many functions that violate the Bellman
equations. For this reason, the algorithm often converges very slowly.

### Adaptive dynamic programming

An (or ADP) agent takes advantage of the constraints among
the utilities of states by learning the transition model that connects
them and solving the corresponding Markov decision process using a
dynamic programming method. For a passive learning agent, this means
plugging the learned transition model $\transprob{s}{\pi(s)}{s'}$ and
the observed rewards $R(s)$ into the Bellman equations
([policy-evaluation-repeated-equation]) to calculate the utilities of
the states. As we remarked in our discussion of policy iteration in ,
these equations are linear (no maximization involved) so they can be
solved using any linear algebra package. Alternatively, we can adopt the
approach of (see ), using a simplified value iteration process to update
the utility estimates after each change to the learned model. Because
the model usually changes only slightly with each observation, the value
iteration process can use the previous utility estimates as initial
values and should converge quite quickly.

The process of learning the model itself is easy, because the
environment is fully observable. This means that we have a supervised
learning task where the input is a state–action pair and the output is
the resulting state. In the simplest case, we can represent the
transition model as a table of probabilities. We keep track of how often
each action outcome occurs and estimate the transition probability
$\transprob{s}{a}{s'}$ from the frequency with which $s'$ is reached
when executing $a$ in $s$. For example, in the three trials given on ,
Řight is executed three times in (1,3) and two out of three times the
resulting state is (2,3), so $\transprob{(1,3)}{{Right}}{(2,3)}$ is
estimated to be 2/3.

[passive-adp-agent-algorithm]

[4x3-passive-adp-figure]

The full agent program for a passive ADP agent is shown in . Its
performance on the $4\stimes 3$ world is shown in . In terms of how
quickly its value estimates improve, the ADP agent is limited only by
its ability to learn the transition model. In this sense, it provides a
standard against which to measure other reinforcement learning
algorithms. It is, however, intractable for large state spaces. In
backgammon, for example, it would involve solving roughly ${10}^{{50}}$
equations in ${10}^{{50}}$ unknowns.

A reader familiar with the Bayesian learning ideas of will have noticed
that the algorithm in is using maximum-likelihood estimation to learn
the transition model; moreover, by choosing a policy based solely on the
*estimated* model it is acting *as if* the
model were correct. This is not necessarily a good idea! For example, a
taxi agent that didn’t know about how traffic lights might ignore a red
light once or twice without no ill effects and then formulate a policy
to ignore red lights from then on. Instead, it might be a good idea to
choose a policy that, while not optimal for the model estimated by
maximum likelihood, works reasonably well for the whole range of models
that have a reasonable chance of being the true model. There are two
mathematical approaches that have this flavor.

The first approach, , assumes a prior probability $P(h)$ for each
hypothesis $h$ about what the true model is; the posterior probability
$P(h\given \e)$ is obtained in the usual way by Bayes’ rule given the
observations to date. Then, if the agent has decided to stop learning,
the optimal policy is the one that gives the highest expected utility.
Let $u_h^{\pi}$ be the expected utility, averaged over all possible
start states, obtained by executing policy $\pi$ in model $h$. Then we
have $$\pistar = \argmax_{\pi} \sum_h P(h\given \e) u_h^{\pi}\ .$$ In
some special cases, this policy can even be computed! If the agent will
continue learning in the future, however, then finding an optimal policy
becomes considerably more difficult, because the agent must consider the
effects of future observations on its beliefs about the transition
model. The problem becomes a POMDP whose belief states are distributions
over models. This concept provides an analytical foundation for
understanding the exploration problem described in .

The second approach, derived from [robust-control-page], allows for a
*set* of possible models ${\cal H}$ and defines an optimal
robust policy as one that gives the best outcome in the *worst
case* over ${\cal H}$:
$$\pistar = \argmax_{\pi} \min_h u_h^{\pi}\ .$$ Often, the set
${\cal H}$ will be the set of models that exceed some likelihood
threshold on $P(h\given \e)$, so the robust and Bayesian approaches are
related. Sometimes, the robust solution can be computed efficiently.
There are, moreover, reinforcement learning algorithms that tend to
produce robust solutions, although we do not cover them here.

### Temporal-difference learning

Solving the underlying MDP as in the preceding section is not the only
way to bring the Bellman equations to bear on the learning problem.
Another way is to use the observed transitions to adjust the utilities
of the observed states so that they agree with the constraint equations.
Consider, for example, the transition from (1,3) to (2,3) in the second
trial on . Suppose that, as a result of the first trial, the utility
estimates are $U^{\pi}(1,3)\eq {0.84}$ and $U^{\pi}(2,3)\eq {0.92}$.
Now, if this transition occurred all the time, we would expect the
utilities to obey the equation
$$U^{\pi}(1,3) = -{0.04} + U^{\pi}(2,3)\ ,$$ so $U^{\pi}(1,3)$ would be
0.88. Thus, its current estimate of 0.84 might be a little low and
should be increased. More generally, when a transition occurs from state
$s$ to state $s'$, we apply the following update to $U^{\pi}(s)$:

$$U^{\pi}(s) \leftarrow U^{\pi}(s) + \alpha(R(s) + 
    \gamma\,U^{\pi}(s') - U^{\pi}(s)) \ .
\label{passive-td-equation}$$

Here, $\alpha$ is the parameter. Because this update rule uses the
difference in utilities between successive states, it is often called
the , or TD, equation.

All temporal-difference methods work by adjusting the utility estimates
towards the ideal equilibrium that holds locally when the utility
estimates are correct. In the case of passive learning, the equilibrium
is given by . Now does in fact cause the agent to reach the equilibrium
given by , but there is some subtlety involved. First, notice that the
update involves only the observed successor $s'$, whereas the actual
equilibrium conditions involve all possible next states. One might think
that this causes an improperly large change in $U^{\pi}(s)$ when a very
rare transition occurs; but, in fact, because rare transitions occur
only rarely, the *average value* of $U^{\pi}(s)$ will
converge to the correct value. Furthermore, if we change $\alpha$ from a
fixed parameter to a function that decreases as the number of times a
state has been visited increases, then $U^{\pi}(s)$ itself will converge
to the correct value.[^1] This gives us the agent program shown in .
illustrates the performance of the passive TD agent on the $4\stimes 3$
world. It does not learn quite as fast as the ADP agent and shows much
higher variability, but it is much simpler and requires much less
computation per observation. Notice that

TD does not need a transition model to perform its updates.

The environment supplies the connection between neighboring states in
the form of observed transitions.

[passive-td-agent-algorithm]

[4x3-passive-td-figure]

The ADP approach and the TD approach are actually closely related. Both
try to make local adjustments to the utility estimates in order to make
each state “agree” with its successors. One difference is that TD
adjusts a state to agree with its *observed* successor (),
whereas ADP adjusts the state to agree with *all* of the
successors that might occur, weighted by their probabilities (). This
difference disappears when the effects of TD adjustments are averaged
over a large number of transitions, because the frequency of each
successor in the set of transitions is approximately proportional to its
probability. A more important difference is that whereas TD makes a
single adjustment per observed transition, ADP makes as many as it needs
to restore consistency between the utility estimates and the environment
model $P$. Although the observed transition makes only a local change in
$P$, its effects might need to be propagated throughout . Thus, TD can
be viewed as a crude but efficient first approximation to ADP.

Each adjustment made by ADP could be seen, from the TD point of view, as
a result of a “pseudoexperience” generated by simulating the current
environment model. It is possible to extend the TD approach to use an
environment model to generate several pseudoexperiences—transitions that
the TD agent can imagine *might* happen, given its current
model. For each observed transition, the TD agent can generate a large
number of imaginary transitions. In this way, the resulting utility
estimates will approximate more and more closely those of ADP—of course,
at the expense of increased computation time.

In a similar vein, we can generate more efficient versions of ADP by
directly approximating the algorithms for value iteration or policy
iteration. Even though the value iteration algorithm is efficient, it is
intractable if we have, say, $10^{100}$ states. However, many of the
necessary adjustments to the state values on each iteration will be
extremely tiny. One possible approach to generating reasonably good
answers quickly is to bound the number of adjustments made after each
observed transition. One can also use a heuristic to rank the possible
adjustments so as to carry out only the most significant ones. The
heuristic prefers to make adjustments to states whose
*likely* successors have just undergone a
*large* adjustment in their own utility estimates. Using
heuristics like this, approximate ADP algorithms usually can learn
roughly as fast as full ADP, in terms of the number of training
sequences, but can be several orders of magnitude more efficient in
terms of computation. (See .) This enables them to handle state spaces
that are far too large for full ADP. Approximate ADP algorithms have an
additional advantage: in the early stages of learning a new environment,
the environment model $P$ often will be far from correct, so there is
little point in calculating an exact utility function to match it. An
approximation algorithm can use a minimum adjustment size that decreases
as the environment model becomes more accurate. This eliminates the very
long value iterations that can occur early in learning due to large
changes in the model.

Active Reinforcement Learning {#active-rl-section}
-----------------------------

A passive learning agent has a fixed policy that determines its
behavior. An active agent must decide what actions to take. Let us begin
with the adaptive dynamic programming agent and consider how it must be
modified to handle this new freedom.

First, the agent will need to learn a complete model with outcome
probabilities for all actions, rather than just the model for the fixed
policy. The simple learning mechanism used by will do just fine for
this. Next, we need to take into account the fact that the agent has a
choice of actions. The utilities it needs to learn are those defined by
the *optimal* policy; they obey the Bellman equations given
on , which we repeat here for convenience:

$$U(s) = R(s) + \gamma\;\max_a \sum\limits_{s'} \transprob{s}{a}{s'} U(s') \ .
\label{active-constraint-equation}$$

These equations can be solved to obtain the utility function $U$ using
the value iteration or policy iteration algorithms from . The final
issue is what to do at each step. Having obtained a utility function $U$
that is optimal for the learned model, the agent can extract an optimal
action by one-step look-ahead to maximize the expected utility;
alternatively, if it uses policy iteration, the optimal policy is
already available, so it should simply execute the action the optimal
policy recommends. Or should it?

### Exploration {#rl-exploration-section}

shows the results of one sequence of trials for an ADP agent that
follows the recommendation of the optimal policy for the learned model
at each step. The agent *does not* learn the true utilities
or the true optimal policy! What happens instead is that, in the 39th
trial, it finds a policy that reaches the +1 reward along the lower
route via (2,1), (3,1), (3,2), and (3,3). (See (b).) After experimenting
with minor variations, from the 276th trial onward it sticks to that
policy, never learning the utilities of the other states and never
finding the optimal route via (1,2), (1,3), and (2,3). We call this
agent the . Repeated experiments show that the greedy agent *very
seldom* converges to the optimal policy for this environment and
sometimes converges to really horrendous policies.

[4x3-greedy-adp-figure]

How can it be that choosing the optimal action leads to suboptimal
results? The answer is that the learned model is not the same as the
true environment; what is optimal in the learned model can therefore be
suboptimal in the true environment. Unfortunately, the agent does not
know what the true environment is, so it cannot compute the optimal
action for the true environment. What, then, is to be done?

What the greedy agent has overlooked is that actions do more than
provide rewards according to the current learned model; they also
contribute to learning the true model by affecting the percepts that are
received. By improving the model, the agent will receive greater rewards
in the future.[^2] An agent therefore must make a tradeoff between to
maximize its reward—as reflected in its current utility estimates—and to
maximize its long-term well-being. Pure exploitation risks getting stuck
in a rut. Pure exploration to improve one’s knowledge is of no use if
one never puts that knowledge into practice. In the real world, one
constantly has to decide between continuing in a comfortable existence
and striking out into the unknown in the hopes of discovering a new and
better life. With greater understanding, less exploration is necessary.

Can we be a little more precise than this? Is there an
*optimal* exploration policy? This question has been
studied in depth in the subfield of statistical decision theory that
deals with so-called . (See sidebar.)

Although bandit problems are extremely difficult to solve exactly to
obtain an *optimal* exploration method, it is nonetheless
possible to come up with a *reasonable* scheme that will
eventually lead to optimal behavior by the agent. Technically, any such
scheme needs to be greedy in the limit of infinite exploration, or . A
GLIE scheme must try each action in each state an unbounded number of
times to avoid having a finite probability that an optimal action is
missed because of an unusually bad series of outcomes. An ADP agent
using such a scheme will eventually learn the true environment model. A
GLIE scheme must also eventually become greedy, so that the agent’s
actions become optimal with respect to the learned (and hence the true)
model.

There are several GLIE schemes; one of the simplest is to have the agent
choose a random action a fraction $1/t$ of the time and to follow the
greedy policy otherwise. While this does eventually converge to an
optimal policy, it can be extremely slow. A more sensible approach would
give some weight to actions that the agent has not tried very often,
while tending to avoid actions that are believed to be of low utility.
This can be implemented by altering the constraint equation
([active-constraint-equation]) so that it assigns a higher utility
estimate to relatively unexplored state–action pairs. Essentially, this
amounts to an optimistic prior over the possible environments and causes
the agent to behave initially as if there were wonderful rewards
scattered all over the place. Let us use $U^+(s)$ to denote the
optimistic estimate of the utility (i.e., the expected reward-to-go) of
the state $s$, and let $N(s,a)$ be the number of times action $a$ has
been tried in state $s$. Suppose we are using value iteration in an ADP
learning agent; then we need to rewrite the update equation ( on ) to
incorporate the optimistic estimate. The following equation does this:

$$U^+(s) \leftarrow R(s) + \gamma\;\max_a\ f\left( \textstyle\sum\limits_{s'} \transprob{s}{a}{s'} U^+(s'),\ 
                                      N(s,a)\right)\ .
\label{exploratory-avi-update-equation}$$

Here, $f(u,n)$ is called the . It determines how greed (preference for
high values of $u$) is traded off against curiosity (preference for
actions that have not been tried often and have low $n$). The function
$f(u,n)$ should be increasing in $u$ and decreasing in $n$. Obviously,
there are many possible functions that fit these conditions. One
particularly simple definition is
$$f(u,n) = \left\{ \begin{array}{l} R^+ \quad \mbox{if}\ n< N_e \\
                                  u \quad \mbox{otherwise}
                 \end{array}\right.$$ where $R^+$ is an optimistic
estimate of the best possible reward obtainable in any state and $N_e$
is a fixed parameter. This will have the effect of making the agent try
each action–state pair at least $N_e$ times.

The fact that $U^+$ rather than $U$ appears on the right-hand side of is
very important. As exploration proceeds, the states and actions near the
start state might well be tried a large number of times. If we used $U$,
the more pessimistic utility estimate, then the agent would soon become
disinclined to explore further afield. The use of $U^+$ means that the
benefits of exploration are propagated back from the edges of unexplored
regions, so that actions that lead *toward* unexplored
regions are weighted more highly, rather than just actions that are
themselves unfamiliar. The effect of this exploration policy can be seen
clearly in , which shows a rapid convergence toward optimal performance,
unlike that of the greedy approach. A very nearly optimal policy is
found after just 18 trials. Notice that the utility estimates themselves
do not converge as quickly. This is because the agent stops exploring
the unrewarding parts of the state space fairly soon, visiting them only
“by accident” thereafter. However, it makes perfect sense for the agent
not to care about the exact utilities of states that it knows are
undesirable and can be avoided.

[4x3-exploring-adp-figure]

### Learning an action-utility function {#q-learning-section}

Now that we have an active ADP agent, let us consider how to construct
an active temporal-difference learning agent. The most obvious change
from the passive case is that the agent is no longer equipped with a
fixed policy, so, if it learns a utility function $U$, it will need to
learn a model in order to be able to choose an action based on $U$ via
one-step look-ahead. The model acquisition problem for the TD agent is
identical to that for the ADP agent. What of the TD update rule itself?
Perhaps surprisingly, the update rule ([passive-td-equation]) remains
unchanged. This might seem odd, for the following reason: Suppose the
agent takes a step that normally leads to a good destination, but
because of nondeterminism in the environment the agent ends up in a
catastrophic state. The TD update rule will take this as seriously as if
the outcome had been the normal result of the action, whereas one might
suppose that, because the outcome was a fluke, the agent should not
worry about it too much. In fact, of course, the unlikely outcome will
occur only infrequently in a large set of training sequences; hence in
the long run its effects will be weighted proportionally to its
probability, as we would hope. Once again, it can be shown that the TD
algorithm will converge to the same values as ADP as the number of
training sequences tends to infinity.

There is an alternative TD method, called , which learns an
action-utility representation instead of learning utilities. We will use
the notation $Q(s,a)$ to denote the value of doing action $a$ in state
$s$. Q-values are directly related to utility values as follows:

$$U(s) = \max_{a} Q(s,a)\ .
\label{u-q-equation}$$

Q-functions may seem like just another way of storing utility
information, but they have a very important property:

a TD agent that learns a Q-function does not need a model of the form
$P(s'\given s,a)$, either for learning or for action selection.

For this reason, Q-learning is called a method. As with utilities, we
can write a constraint equation that must hold at equilibrium when the
Q-values are correct:

$$Q(s,a) = R(s) + \gamma\;\sum\limits_{s'} \transprob{s}{a}{s'} \max_{a'} Q(s',a')\ .
\label{q-constraint-equation}$$

As in the ADP learning agent, we can use this equation directly as an
update equation for an iteration process that calculates exact Q-values,
given an estimated model. This does, however, require that a model also
be learned, because the equation uses $\transprob{s}{a}{s'}$. The
temporal-difference approach, on the other hand, requires no model of
state transitions—all it needs are the $Q$ values. The update equation
for TD Q-learning is

$$Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma\;\max_{a'} Q(s',a') - Q(s,a))\ ,
\label{q-update-equation}$$

which is calculated whenever action $a$ is executed in state $s$ leading
to state $s'$.

The complete agent design for an exploratory Q-learning agent using TD
is shown in . Notice that it uses exactly the same exploration function
$\,f$ as that used by the exploratory ADP agent—hence the need to keep
statistics on actions taken (the table $N$). If a simpler exploration
policy is used—say, acting randomly on some fraction of steps, where the
fraction decreases over time—then we can dispense with the statistics.

[q-learning-agent-algorithm]

Q-learning has a close relative called (for
State-Action-Reward-State-Action). The update rule for SARSA is very
similar to :

$$Q(s,a) \leftarrow Q(s,a) + \alpha(R(s) + \gamma\;Q(s',a') - Q(s,a))\ ,
\label{SARSA-update-equation}$$

where $a'$ is the action *actually taken* in state $s'$.
The rule is applied at the end of each $s,\ a,\ r,\ s',\ a'$
quintuplet—hence the name. The difference from Q-learning is quite
subtle: whereas Q-learning backs up the *best* Q-value from
the state reached in the observed transition, SARSA waits until an
action is actually taken and backs up the Q-value for that action. Now,
for a greedy agent that always takes the action with best Q-value, the
two algorithms are identical. When exploration is happening, however,
they differ significantly. Because Q-learning uses the best Q-value, it
pays no attention to the actual policy being followed—it is an learning
algorithm, whereas SARSA is an algorithm. Q-learning is more flexible
than SARSA, in the sense that a Q-learning agent can learn how to behave
well even when guided by a random or adversarial exploration policy. On
the other hand, SARSA is more realistic: for example, if the overall
policy is even partly controlled by other agents, it is better to learn
a Q-function for what will actually happen rather than what the agent
would like to happen.

Both Q-learning and SARSA learn the optimal policy for the $4\stimes 3$
world, but do so at a much slower rate than the ADP agent. This is
because the local updates do not enforce consistency among all the
Q-values via the model. The comparison raises a general question: is it
better to learn a model and a utility function or to learn an
action-utility function with no model? In other words, what is the best
way to represent the agent function? This is an issue at the foundations
of artificial intelligence. As we stated in , one of the key historical
characteristics of much of AI research is its (often unstated) adherence
to the approach. This amounts to an assumption that the best way to
represent the agent function is to build a representation of some
aspects of the environment in which the agent is situated.

Some researchers, both inside and outside AI, have claimed that the
availability of model-free methods such as Q-learning means that the
knowledge-based approach is unnecessary. There is, however, little to go
on but intuition. Our intuition, for what it’s worth, is that as the
environment becomes more complex, the advantages of a knowledge-based
approach become more apparent. This is borne out even in games such as
chess, checkers (draughts), and backgammon (see next section), where
efforts to learn an evaluation function by means of a model have met
with more success than Q-learning methods.

Generalization in Reinforcement Learning {#function-approximation-section}
----------------------------------------

So far, we have assumed that the utility functions and Q-functions
learned by the agents are represented in tabular form with one output
value for each input tuple. Such an approach works reasonably well for
small state spaces, but the time to convergence and (for ADP) the time
per iteration increase rapidly as the space gets larger. With carefully
controlled, approximate ADP methods, it might be possible to handle
10,000 states or more. This suffices for two-dimensional maze-like
environments, but more realistic worlds are out of the question.
Backgammon and chess are tiny subsets of the real world, yet their state
spaces contain on the order of ${10}^{{20}}$ and ${10}^{{40}}$ states,
respectively. It would be absurd to suppose that one must visit all
these states many times in order to learn how to play the game!

One way to handle such problems is to use , which simply means using any
sort of representation for the Q-function other than a lookup table. The
representation is viewed as approximate because it might not be the case
that the *true* utility function or Q-function can be
represented in the chosen form. For example, in we described an for
chess that is represented as a weighted linear function of a set of (or
) $f_1, \ldots, f_n$:
$$\hat{U}_{\theta}(s) = \theta_1\,f_1(s) + \theta_2\,f_2(s) + \cdots + \theta_n\,f_n(s)\ .$$
A reinforcement learning algorithm can learn values for the parameters
$\theta\eq \theta_1,\ldots,\theta_n$ such that the evaluation function
$\hat{U}_{\theta}$ approximates the true utility function. Instead of,
say, ${10}^{{40}}$ values in a table, this function approximator is
characterized by, say, $n\eq {20}$ parameters—an *enormous*
compression. Although no one knows the true utility function for chess,
no one believes that it can be represented exactly in 20 numbers. If the
approximation is good enough, however, the agent might still play
excellent chess.[^3] Function approximation makes it practical to
represent utility functions for very large state spaces, but that is not
its principal benefit.

The compression achieved by a function approximator allows the learning
agent to generalize from states it has visited to states it has not
visited.

That is, the most important aspect of function approximation is not that
it requires less space, but that it allows for inductive generalization
over input states. To give you some idea of the power of this effect: by
examining only one in every ${10}^{{12}}$ of the possible backgammon
states, it is possible to learn a utility function that allows a program
to play as well as any human @Tesauro:1992.

On the flip side, of course, there is the problem that there could fail
to be any function in the chosen hypothesis space that approximates the
true utility function sufficiently well. As in all inductive learning,
there is a tradeoff between the size of the hypothesis space and the
time it takes to learn the function. A larger hypothesis space increases
the likelihood that a good approximation can be found, but also means
that convergence is likely to be delayed.

Let us begin with the simplest case, which is direct utility estimation.
(See .) With function approximation, this is an instance of . For
example, suppose we represent the utilities for the $4\stimes 3$ world
using a simple linear function. The features of the squares are just
their $x$ and $y$ coordinates, so we have

$$\hat{U}_{\theta}(x,y) = \theta_0 + \theta_1 x + \theta_2 y\ .
\label{4x3-linear-approx-equation}$$

Thus, if $(\theta_0,\theta_1,\theta_2)\eq ({0.5},{0.2},{0.1})$, then
$\hat{U}_{\theta}(1,1)\eq {0.8}$. Given a collection of trials, we
obtain a set of sample values of $\hat{U}_{\theta}(x,y)$, and we can
find the best fit, in the sense of minimizing the squared error, using
standard linear regression. (See .)

For reinforcement learning, it makes more sense to use an
*online* learning algorithm that updates the parameters
after each trial. Suppose we run a trial and the total reward obtained
starting at (1,1) is 0.4. This suggests that $\hat{U}_{\theta}(1,1)$,
currently 0.8, is too large and must be reduced. How should the
parameters be adjusted to achieve this? As with neural-network learning,
we write an error function and compute its gradient with respect to the
parameters. If $u_j(s)$ is the observed total reward from state $s$
onward in the $j$th trial, then the error is defined as (half) the
squared difference of the predicted total and the actual total:
$E_j(s) = (\hat{U}_{\theta}(s)-u_j(s))^2/2$. The rate of change of the
error with respect to each parameter $\theta_i$ is
$\partial E_j / \partial\theta_i$, so to move the parameter in the
direction of decreasing the error, we want

$$\theta_i \leftarrow 
  \theta_i - \alpha\,\frac{\partial E_j(s)}{\partial\theta_i} = 
  \theta_i + \alpha\,(u_j(s)-\hat{U}_{\theta}(s))\frac{\partial \hat{U}_{\theta}(s)}{\partial\theta_i} \ .
\label{delta-rule-equation}$$

This is called the , or the , for online least-squares. For the linear
function approximator $\hat{U}_{\theta}(s)$ in , we get three simple
update rules:

$$\begin{aligned}
  \theta_0 &\leftarrow & \theta_0 + \alpha\,(u_j(s)-\hat{U}_{\theta}(s)) \ ,\\
  \theta_1 &\leftarrow & \theta_1 + \alpha\,(u_j(s)-\hat{U}_{\theta}(s))x \ ,\\
  \theta_2 &\leftarrow & \theta_2 + \alpha\,(u_j(s)-\hat{U}_{\theta}(s))y \ .\end{aligned}$$

We can apply these rules to the example where $\hat{U}_{\theta}(1,1)$ is
0.8 and $u_j(1,1)$ is 0.4. $\theta_0$, $\theta_1$, and $\theta_2$ are
all decreased by ${0.4}\alpha$, which reduces the error for (1,1).
Notice that

changing the parameters $\theta$ in response to an observed transition
between two states also changes the values of $\hat{U}_{\theta}$ for
every other state!

This is what we mean by saying that function approximation allows a
reinforcement learner to generalize from its experiences.

We expect that the agent will learn faster if it uses a function
approximator, provided that the hypothesis space is not too
large, but includes some functions that are a reasonably good fit to the
true utility function. asks you to evaluate the performance of direct
utility estimation, both with and without function approximation. The
improvement in the $4\stimes 3$ world is noticeable but not dramatic,
because this is a very small state space to begin with. The improvement
is much greater in a ${10}\stimes {10}$ world with a +1 reward at
(10,10). This world is well suited for a linear utility function because
the true utility function is smooth and nearly linear. (See .) If we put
the +1 reward at (5,5), the true utility is more like a pyramid and the
function approximator in will fail miserably. All is not lost, however!
Remember that what matters for linear function approximation is that the
function be linear in the *parameters*—the features
themselves can be arbitrary nonlinear functions of the state variables.
Hence, we can include a term such as $\theta_3
f_3(x,y) = \theta_3 \sqrt{(x-x_g)^2 + (y-y_g)^2}$ that measures the
distance to the goal.

We can apply these ideas equally well to temporal-difference learners.
All we need do is adjust the parameters to try to reduce the temporal
difference between successive states. The new versions of the TD and
Q-learning equations ([passive-td-equation] on and [q-update-equation]
on ) are given by

$$\theta_i \leftarrow \theta_i + \alpha\, [R(s) + \gamma\,\hat{U}_{\theta}(s') - \hat{U}_{\theta}(s)] 
         \frac{\partial \hat{U}_{\theta}(s)}{\partial\theta_i}
\label{generalized-td-equation}$$

for utilities and

$$\theta_i \leftarrow \theta_i + \alpha\, [R(s) + \gamma\,\max_{a'}\hat{Q}_{\theta}(s',a') - \hat{Q}_{\theta}(s,a)] 
         \frac{\partial \hat{Q}_{\theta}(s,a)}{\partial\theta_i}
\label{generalized-q-learning-equation}$$

for Q-values. For passive TD learning, the update rule can be shown to
converge to the closest possible[^4] approximation to the true function
when the function approximator is *linear* in the
parameters. With active learning and *nonlinear* functions
such as neural networks, all bets are off: There are some very simple
cases in which the parameters can go off to infinity even though there
are good solutions in the hypothesis space. There are more sophisticated
algorithms that can avoid these problems, but at present reinforcement
learning with general function approximators remains a delicate art.

Function approximation can also be very helpful for learning a model of
the environment. Remember that learning a model for an
*observable* environment is a *supervised*
learning problem, because the next percept gives the outcome state. Any
of the supervised learning methods in can be used, with suitable
adjustments for the fact that we need to predict a complete state
description rather than just a Boolean classification or a single real
value. For a *partially observable* environment, the
learning problem is much more difficult. If we know what the hidden
variables are and how they are causally related to each other and to the
observable variables, then we can fix the structure of a dynamic
Bayesian network and use the EM algorithm to learn the parameters, as
was described in . Inventing the hidden variables and learning the model
structure are still open problems. Some practical examples are described
in .

Policy Search {#policy-search-section}
-------------

The final approach we will consider for reinforcement learning problems
is called . In some ways, policy search is the simplest of all the
methods in this chapter: the idea is to keep twiddling the policy as
long as its performance improves, then stop.

Let us begin with the policies themselves. Remember that a policy $\pi$
is a function that maps states to actions. We are interested primarily
in *parameterized* representations of $\pi$ that have far
fewer parameters than there are states in the state space (just as in
the preceding section). For example, we could represent $\pi$ by a
collection of parameterized Q-functions, one for each action, and take
the action with the highest predicted value:

$$\pi(s) = \max_a \hat{Q}_{\theta}(s,a)\ .
\label{policy-q-equation}$$

Each Q-function could be a linear function of the parameters $\theta$,
as in , or it could be a nonlinear function such as a neural network.
Policy search will then adjust the parameters $\theta$ to improve the
policy. Notice that if the policy is represented by Q-functions, then
policy search results in a process that learns Q-functions.

This process is not the same as Q-learning!

In Q-learning with function approximation, the algorithm finds a value
of $\theta$ such that $\hat{Q}_{\theta}$ is “close” to $\Qstar$, the
optimal Q-function. Policy search, on the other hand, finds a value of
$\theta$ that results in good performance; the values found by the two
methods may differ very substantially. (For example, the approximate
Q-function defined by $\hat{Q}_{\theta}(s,a)\eq
\Qstar(s,a)/{10}$ gives optimal performance, even though it is not at
all close to $\Qstar$.) Another clear instance of the difference is the
case where $\pi(s)$ is calculated using, say, depth-10 look-ahead search
with an approximate utility function $\hat{U}_{\theta}$. A value of
$\theta$ that gives good results may be a long way from making
$\hat{U}_{\theta}$ resemble the true utility function.

One problem with policy representations of the kind given in is that the
policy is a *discontinuous* function of the parameters when
the actions are discrete. (For a continuous action space, the policy can
be a smooth function of the parameters.) That is, there will be values
of $\theta$ such that an infinitesimal change in $\theta$ causes the
policy to switch from one action to another. This means that the value
of the policy may also change discontinuously, which makes
gradient-based search difficult. For this reason, policy search methods
often use a representation $\pi_{\theta}(s,a)$, which specifies the
*probability* of selecting action $a$ in state $s$. One
popular representation is the [softmax-page]:
$$\pi_{\theta}(s,a) = e^{\hat{Q}_{\theta}(s,a)} / \sum_{a'} e^{\hat{Q}_{\theta}(s,a')}\ .$$
Softmax becomes nearly deterministic if one action is much better than
the others, but it always gives a differentiable function of $\theta$;
hence, the value of the policy (which depends in a continuous fashion on
the action selection probabilities) is a differentiable function of
$\theta$. Softmax is a generalization of the logistic function () to
multiple variables.

Now let us look at methods for improving the policy. We start with the
simplest case: a deterministic policy and a deterministic environment.
Let $\plv(\theta)$ be the , i.e., the expected reward-to-go when
$\pi_{\theta}$ is executed. If we can derive an expression for
$\plv(\theta)$ in closed form, then we have a standard optimization
problem, as described in . We can follow the vector
$\nabla_{\theta}\plv(\theta)$ provided $\plv(\theta)$ is differentiable.
Alternatively, if $\plv(\theta)$ is not available in closed form, we can
evaluate $\pi_{\theta}$ simply by executing it and observing the
accumulated reward. We can follow the by hill climbing—i.e., evaluating
the change in policy value for small increments in each parameter. With
the usual caveats, this process will converge to a local optimum in
policy space.

When the environment (or the policy) is stochastic, things get more
difficult. Suppose we are trying to do hill climbing, which requires
comparing $\plv(\theta)$ and $\plv(\theta+\Delta\theta)$ for some small
$\Delta\theta$. The problem is that the total reward on each trial may
vary widely, so estimates of the policy value from a small number of
trials will be quite unreliable; trying to compare two such estimates
will be even more unreliable. One solution is simply to run lots of
trials, measuring the sample variance and using it to determine that
enough trials have been run to get a reliable indication of the
direction of improvement for $\plv(\theta)$. Unfortunately, this is
impractical for many real problems where each trial may be expensive,
time-consuming, and perhaps even dangerous.

For the case of a stochastic policy $\pi_{\theta}(s,a)$, it is possible
to obtain an unbiased estimate of the gradient at $\theta$,
$\nabla_{\theta}\plv(\theta)$, directly from the results of trials
executed at $\theta$. For simplicity, we will derive this estimate for
the simple case of a nonsequential environment in which the reward
$R(a)$ is obtained immediately after doing action $a$ in the start state
$s_0$. In this case, the policy value is just the expected value of the
reward, and we have $$\nabla_{\theta}\plv(\theta) 
   = \nabla_{\theta} \sum_a \pi_{\theta}(s_0,a) R(a) 
   = \sum_a (\nabla_{\theta}\pi_{\theta}(s_0,a)) R(a)\ .$$ Now we
perform a simple trick so that this summation can be approximated by
samples generated from the probability distribution defined by
$\pi_{\theta}(s_0,a)$. Suppose that we have $N$ trials in all and the
action taken on the $j$th trial is $a_j$. Then
$$\nabla_{\theta}\plv(\theta) 
   = \sum_a \pi_{\theta}(s_0,a) \cdot \frac{(\nabla_{\theta}\pi_{\theta}(s_0,a))R(a)}{\pi_{\theta}(s_0,a)} 
   \approx \frac{1}{N}\sum_{j\eq 1}^N \frac{(\nabla_{\theta}\pi_{\theta}(s_0,a_j))R(a_j)}{\pi_{\theta}(s_0,a_j)}\ .$$
Thus, the true gradient of the policy value is approximated by a sum of
terms involving the gradient of the action-selection probability in each
trial. For the sequential case, this generalizes to
$$\nabla_{\theta}\plv(\theta) \approx \frac{1}{N}\sum_{j\eq 1}^N \frac{(\nabla_{\theta}\pi_{\theta}(s,a_j))R_j(s)}{\pi_{\theta}(s,a_j)}$$
for each state $s$ visited, where $a_j$ is executed in $s$ on the $j$th
trial and $R_j(s)$ is the total reward received from state $s$ onwards
in the $j$th trial. The resulting algorithm is called  @Williams:1992;
it is usually much more effective than hill climbing using lots of
trials at each value of $\theta$. It is still much slower than
necessary, however.

Consider the following task: given two blackjack[^5] programs, determine
which is best. One way to do this is to have each play against a
standard “dealer” for a certain number of hands and then to measure
their respective winnings. The problem with this, as we have seen, is
that the winnings of each program fluctuate widely depending on whether
it receives good or bad cards. An obvious solution is to generate a
certain number of hands in advance and

have each program play the same set of hands.

In this way, we eliminate the measurement error due to differences in
the cards received. This idea, called , underlies a policy-search
algorithm called @Ng+Jordan:2000. The algorithm is applicable to domains
for which a simulator is available so that the “random” outcomes of
actions can be repeated. The algorithm works by generating in advance
$N$ sequences of random numbers, each of which can be used to run a
trial of any policy. Policy search is carried out by evaluating each
candidate policy using the *same* set of random sequences
to determine the action outcomes. It can be shown that the number of
random sequences required to ensure that the value of
*every* policy is well estimated depends only on the
complexity of the policy space, and not at all on the complexity of the
underlying domain.

Applications of Reinforcement Learning {#rl-application-section}
--------------------------------------

We now turn to examples of large-scale applications of reinforcement
learning. We consider applications in game playing, where the transition
model is known and the goal is to learn the utility function, and in
robotics, where the model is usually unknown.

### Applications to game playing

The first significant application of reinforcement learning was also the
first significant learning program of any kind—the checkers program
written by Arthur Samuel [-@Samuel:1959; -@Samuel:1967]. Samuel first
used a weighted linear function for the evaluation of positions, using
up to 16 terms at any one time. He applied a version of to update the
weights. There were some significant differences, however, between his
program and current methods. First, he updated the weights using the
difference between the current state and the backed-up value generated
by full look-ahead in the search tree. This works fine, because it
amounts to viewing the state space at a different granularity. A second
difference was that the program did *not* use any observed
rewards! That is, the values of terminal states reached in self-play
were ignored. This means that it is theoretically possible for Samuel’s
program not to converge, or to converge on a strategy designed to lose
rather than to win. He managed to avoid this fate by insisting that the
weight for material advantage should always be positive. Remarkably,
this was sufficient to direct the program into areas of weight space
corresponding to good checkers play.

Gerry Tesauro’s program  [-@Tesauro:1992] forcefully illustrates the
potential of reinforcement learning techniques. In earlier
work @Tesauro+Sejnowski:1989, Tesauro tried learning a neural network
representation of $Q(s,a)$ directly from examples of moves labeled with
relative values by a human expert. This approach proved extremely
tedious for the expert. It resulted in a program, called , that was
strong by computer standards, but not competitive with human experts.
The project was an attempt to learn from self-play alone. The only
reward signal was given at the end of each game. The evaluation function
was represented by a fully connected neural network with a single hidden
layer containing 40 nodes. Simply by repeated application of , learned
to play considerably better than , even though the input representation
contained just the raw board position with no computed features. This
took about 200,000 training games and two weeks of computer time.
Although that may seem like a lot of games, it is only a vanishingly
small fraction of the state space. When precomputed features were added
to the input representation, a network with 80 hidden nodes was able,
after 300,000 training games, to reach a standard of play comparable to
that of the top three human players worldwide. Kit Woolsey, a top player
and analyst, said that “There is no question in my mind that its
positional judgment is far better than mine.”

### Application to robot control

The setup for the famous balancing problem, also known as the , is shown
in . The problem is to control the position $x$ of the cart so that the
pole stays roughly upright ($\theta \approx
\pi/2$), while staying within the limits of the cart track as shown.
Several thousand papers in reinforcement learning and control theory
have been published on this seemingly simple problem. The cart–pole
problem differs from the problems described earlier in that the state
variables $x$, $\theta$, $\dot{x}$, and $\dot{\theta}$ are continuous.
The actions are usually discrete: jerk left or jerk right, the so-called
regime.

[cart-pole-figure]

The earliest work on learning for this problem was carried out by Michie
and Chambers [-@Michie+Chambers:1968]. Their algorithm was able to
balance the pole for over an hour after only about 30 trials. Moreover,
unlike many subsequent systems, was implemented with a real cart and
pole, not a simulation. The algorithm first discretized the
four-dimensional state space into boxes—hence the name. It then ran
trials until the pole fell over or the cart hit the end of the track.
Negative reinforcement was associated with the final action in the final
box and then propagated back through the sequence. It was found that the
discretization caused some problems when the apparatus was initialized
in a position different from those used in training, suggesting that
generalization was not perfect. Improved generalization and faster
learning can be obtained using an algorithm that
*adaptively* partitions the state space according to the
observed variation in the reward, or by using a continuous-state,
nonlinear function approximator such as a neural network. Nowadays,
balancing a *triple* inverted pendulum is a common
exercise—a feat far beyond the capabilities of most humans.

Still more impressive is the application of reinforcement learning to
helicopter flight (). This work has generally used policy
search @Bagnell+Schneider:2001 as well as the algorithm with simulation
based on a learned transition model @Ng+al:2004. Further details are
given in .

[helicopter-figure]

This chapter has examined the reinforcement learning problem: how an
agent can become proficient in an unknown environment, given only its
percepts and occasional rewards. Reinforcement learning can be viewed as
a microcosm for the entire AI problem, but it is studied in a number of
simplified settings to facilitate progress. The major points are:

-   The overall agent design dictates the kind of information that must
    be learned. The three main designs we covered were the model-based
    design, using a model and a utility function ; the model-free
    design, using an action-utility function ; and the reflex design,
    using a policy $\pi$.

-   Utilities can be learned using three approaches:

    1.  uses the total observed reward-to-go for a given state as direct
        evidence for learning its utility.

    2.  (ADP) learns a model and a reward function from observations and
        then uses value or policy iteration to obtain the utilities or
        an optimal policy. ADP makes optimal use of the local
        constraints on utilities of states imposed through the
        neighborhood structure of the environment.

    3.  (TD) methods update utility estimates to match those of
        successor states. They can be viewed as simple approximations to
        the ADP approach that can learn without requiring a transition
        model. Using a learned model to generate pseudoexperiences can,
        however, result in faster learning.

-   Action-utility functions, or Q-functions, can be learned by an ADP
    approach or a TD approach. With TD, Q-learning requires no model in
    either the learning or action-selection phase. This simplifies the
    learning problem but potentially restricts the ability to learn in
    complex environments, because the agent cannot simulate the results
    of possible courses of action.

-   When the learning agent is responsible for selecting actions while
    it learns, it must trade off the estimated value of those actions
    against the potential for learning useful new information. An exact
    solution of the exploration problem is infeasible, but some simple
    heuristics do a reasonable job.

-   In large state spaces, reinforcement learning algorithms must use an
    approximate functional representation in order to generalize over
    states. The temporal-difference signal can be used directly to
    update parameters in representations such as neural networks.

-   Policy-search methods operate directly on a
    representation of the policy, attempting to improve it based on
    observed performance. The variation in the performance in a
    stochastic domain is a serious problem; for simulated domains this
    can be overcome by fixing the randomness in advance.

Because of its potential for eliminating hand coding of control
strategies, reinforcement learning continues to be one of the most
active areas of machine learning research. Applications in robotics
promise to be particularly valuable; these will require methods for
handling continuous, high-dimensional,
partially observable environments in which successful
behaviors may consist of thousands or even millions of primitive
actions.

Turing [-@Turing:1948; -@Turing:1950] proposed the
reinforcement-learning approach, although he was not convinced of its
effectiveness, writing, “the use of punishments and rewards can at best
be a part of the teaching process.” Arthur Samuel’s work [-@Samuel:1959]
was probably the earliest successful machine learning research. Although
this work was informal and had a number of flaws, it contained most of
the modern ideas in reinforcement learning, including temporal
differencing and function approximation. Around the same time,
researchers in adaptive control theory @Widrow+Hoff:1960, building on
work by Hebb [-@Hebb:1949], were training simple networks using the
delta rule. (This early connection between neural networks and
reinforcement learning may have led to the persistent misperception that
the latter is a subfield of the former.) The cart–pole work of Michie
and Chambers [-@Michie+Chambers:1968] can also be seen as a
reinforcement learning method with a function approximator. The
psychological literature on reinforcement learning is much older;
provide a good survey. Direct evidence for the operation of
reinforcement learning in animals has been provided by investigations
into the foraging behavior of bees; there is a clear neural correlate of
the reward signal in the form of a large neuron mapping from the nectar
intake sensors directly to the motor cortex @Montague+al:1995. Research
using single-cell recording suggests that the dopamine system in primate
brains implements something resembling value function
learning @Schultz+al:1997. The neuroscience text by describes possible
neural implementations of temporal-difference learning, while survey the
latest evidence from neuroscientific and behavioral experiments.

The connection between reinforcement learning and Markov decision
processes was first made by , but the development of reinforcement
learning in AI stems from work at the University of Massachusetts in the
early 1980s @Barto+al:1981. The paper by Sutton [-@Sutton:1988] provides
a good historical overview. in this chapter is a special case for
$\lambda\eq 0$ of Sutton’s general TD$(\lambda)$ algorithm.
TD$(\lambda)$ updates the utility values of all states in a sequence
leading up to each transition by an amount that drops off as $\lambda^t$
for states $t$ steps in the past. TD$(1)$ is identical to the
Widrow–Hoff or delta rule. , building on work by , argues that
TD$(\lambda)$ and related algorithms make inefficient use of
experiences; essentially, they are online regression algorithms that
converge much more slowly than offline regression. His (least-squares
temporal differencing) algorithm is an online algorithm for passive
reinforcement learning that gives the same results as offline
regression. Least-squares policy iteration, or  @Lagoudakis+Parr:2003,
combines this idea with the policy iteration algorithm, yielding a
robust, statistically efficient, model-free algorithm for learning
policies.

The combination of temporal-difference learning with the model-based
generation of simulated experiences was proposed in Sutton’s
architecture @Sutton:1990. The idea of prioritized sweeping was
introduced independently by and . Q-learning was developed in Watkins’s
Ph.D. thesis [-@Watkins:1989], while SARSA appeared in a technical
report by .

Bandit problems, which model the problem of exploration for
nonsequential decisions, are analyzed in depth by Berry and
Fristedt [-@Berry+Fristedt:1985]. Optimal exploration strategies for
several settings are obtainable using the technique called
 @Gittins:1989. A variety of exploration methods for sequential decision
problems are discussed by . and describe algorithms that explore unknown
environments and are guaranteed to converge on near-optimal policies in
polynomial time. Bayesian reinforcement learning @Dearden+al:1998
[@Dearden+al:1999] provides another angle on both model uncertainty and
exploration.

Function approximation in reinforcement learning goes back to the work
of Samuel, who used both linear and nonlinear evaluation functions and
also used feature-selection methods to reduce the feature space. Later
methods include the (Cerebellar Model Articulation
Controller) @Albus:1975, which is essentially a sum of overlapping local
kernel functions, and the associative neural networks of . Neural
networks are currently the most popular form of function approximator.
The best-known application is TD-Gammon @Tesauro:1992 [@Tesauro:1995],
which was discussed in the chapter. One significant problem exhibited by
neural-network-based TD learners is that they tend to forget earlier
experiences, especially those in parts of the state space that are
avoided once competence is achieved. This can result in catastrophic
failure if such circumstances reappear. Function approximation based on
can avoid this problem @Ormoneit+Sen:2002 [@Forbes:2002].

The convergence of reinforcement learning algorithms using function
approximation is an extremely technical subject. Results for TD learning
have been progressively strengthened for the case of linear function
approximators @Sutton:1988 [@Dayan:1992; @Tsitsiklis+VanRoy:1997], but
several examples of divergence have been presented for nonlinear
functions \<see\>[for a discussion]Tsitsiklis+VanRoy:1997.
describe a new type of reinforcement learning that converges with any
form of function approximator, provided that a best-fit approximation
can be found for the observed data.

Policy search methods were brought to the fore by , who developed the
family of algorithms. Later work by , , and strengthened and generalized
the convergence results for policy search. The method of correlated
sampling for comparing different configurations of a system was
described formally by , but seems to have been known long before that.
Its use in reinforcement learning is due to and ; the latter paper also
introduced the algorithm and proved its formal properties.

As we mentioned in the chapter, the performance of a
*stochastic* policy is a continuous function of its
parameters, which helps with gradient-based search methods. This is not
the only benefit: argue that stochastic policies actually work better
than deterministic policies in partially observable environments, if
both are limited to acting based on the current percept. (One reason is
that the stochastic policy is less likely to get “stuck” because of some
unseen hindrance.) Now, in we pointed out that optimal policies in
partially observable MDPs are deterministic functions of the
*belief state* rather than the current percept, so we would
expect still better results by keeping track of the belief state using
the methods of . Unfortunately, belief-state space is high-dimensional
and continuous, and effective algorithms have not yet been developed for
reinforcement learning with belief states.

Real-world environments also exhibit enormous complexity in terms of the
number of primitive actions required to achieve significant reward. For
example, a robot playing soccer might make a hundred thousand individual
leg motions before scoring a goal. One common method, used originally in
animal training, is called . This involves supplying the agent with
additional rewards, called , for “making progress.” For example, in
soccer the real reward is for scoring a goal, but pseudorewards might be
given for making contact with the ball or for kicking it toward the
goal. Such rewards can speed up learning enormously and are simple to
provide, but there is a risk that the agent will learn to maximize the
pseudorewards rather than the true rewards; for example, standing next
to the ball and “vibrating” causes many contacts with the ball. show
that the agent will still learn the optimal policy provided that the
pseudoreward $F(s,a,s')$ satisfies
$F(s,a,s')\eq \gamma \Phi(s') - \Phi(s)$, where $\Phi$ is an arbitrary
function of the state. $\Phi$ can be constructed to reflect any
desirable aspects of the state, such as achievement of subgoals or
distance to a goal state.

The generation of complex behaviors can also be facilitated by methods,
which attempt to solve problems at multiple levels of abstraction—much
like the methods of . For example, “scoring a goal” can be broken down
into “obtain possession,” “dribble towards the goal,” and “shoot;” and
each of these can be broken down further into lower-level motor
behaviors. The fundamental result in this area is due to , who proved
that lower-level behaviors of arbitrary complexity can be treated just
like primitive actions (albeit ones that can take varying amounts of
time) from the point of view of the higher-level behavior that invokes
them. Current approaches @Parr+Russell:1998
[@Dietterich:2000; @Sutton+al:2000; @Andre+Russell:2002] build on this
result to develop methods for supplying an agent with a that constrains
the agent’s behavior to have a particular hierarchical structure. The
partial-programming language for agent programs extends an ordinary
programming language by adding primitives for unspecified choices that
must be filled in by learning. Reinforcement learning is then applied to
learn the best behavior consistent with the partial program. The
combination of function approximation, shaping, and hierarchical
reinforcement learning has been shown to solve large-scale problems—for
example, policies that execute for $10^4$ steps in state spaces of
$10^{100}$ states with branching factors of $10^{30}$ @Marthi+al:2005.
One key result @Dietterich:2000 is that the hierarchical structure
provides a natural *additive decomposition* of the overall
utility function into terms that depend on small subsets of the
variables defining the state space. This is somewhat analogous to the
representation theorems underlying the conciseness of Bayes nets ().

The topic of distributed and multiagent reinforcement learning was not
touched upon in the chapter but is of great current interest. In
distributed RL, the aim is to devise methods by which multiple,
coordinated agents learn to optimize a common utility function. For
example, can we devise methods whereby separate for robot navigation and
robot obstacle avoidance could cooperatively achieve a combined control
system that is globally optimal? Some basic results in this direction
have been obtained @Guestrin+al:2002 [@Russell+Zimdars:2003]. The basic
idea is that each subagent learns its own Q-function from its own stream
of rewards. For example, a robot-navigation component can receive
rewards for making progress towards the goal, while the
obstacle-avoidance component receives negative rewards for every
collision. Each global decision maximizes the sum of Q-functions and the
whole process converges to globally optimal solutions.

Multiagent RL is distinguished from distributed RL by the presence of
agents who cannot coordinate their actions (except by explicit
communicative acts) and who may not share the same utility function.
Thus, multiagent RL deals with sequential game-theoretic problems or ,
as defined in . The consequent requirement for randomized policies is
not a significant complication, as we saw on . What *does*
cause problems is the fact that, while an agent is learning to defeat
its opponent’s policy, the opponent is changing its policy to defeat the
agent. Thus, the environment is (see ). noted this difficulty when
introducing the first RL algorithms for zero-sum Markov games. present a
Q-learning algorithm for general-sum games that converges when the Nash
equilibrium is unique; when there are multiple equilibria, the notion of
convergence is not so easy to define @Shoham+al:2004.

Sometimes the reward function is not easy to define. Consider the task
of driving a car. There are extreme states (such as crashing the car)
that clearly should have a large penalty. But beyond that, it is
difficult to be precise about the reward function. However, it is easy
enough for a human to drive for a while and then tell a robot “do it
like that.” The robot then has the task of ; learning from an example of
the task done right, without explicit rewards. and show how this
technique works for learning to fly a helicopter; see on for an example
of the acrobatics the resulting policy is capable of. describes the task
of —figuring out what the reward function must be from an example path
through that state space. This is useful as a part of apprenticeship
learning, or as a part of doing science—we can understand an animal or
robot by working backwards from what it does to what its reward function
must be.

This chapter has dealt only with atomic states—all the agent knows about
a state is the set of available actions and the utilities of the
resulting states (or of state-action pairs). But it is also possible to
apply reinforcement learning to structured representations rather than
atomic ones; this is called @Tadepalli+al:2004.

The survey by provides a good entry point to the literature. The text by
, two of the field’s pioneers, focuses on architectures and algorithms,
showing how reinforcement learning weaves together the ideas of
learning, planning, and acting. The somewhat more technical work by
gives a rigorous grounding in the theory of dynamic programming and
stochastic convergence. Reinforcement learning papers are published
frequently in *Machine Learning*, in the *Journal of
Machine Learning Research*, and in the International Conferences
on Machine Learning and the Neural Information Processing Systems
meetings.

Implement a passive learning agent in a simple environment, such as the
$4\stimes 3$ world. For the case of an initially unknown environment
model, compare the learning performance of the direct utility
estimation, TD, and ADP algorithms. Do the comparison for the optimal
policy and for several random policies. For which do the utility
estimates converge faster? What happens when the size of the environment
is increased? (Try environments with and without obstacles.)

defined a for an MDP as one that is guaranteed to reach a terminal
state. Show that it is possible for a passive ADP agent to learn a
transition model for which its policy $\pi$ is improper even if $\pi$ is
proper for the true MDP; with such models, the step may fail if
$\gamma\eq 1$. Show that this problem cannot arise if is applied to the
learned model only at the end of a trial.

[prioritized-sweeping-exercise]Starting with the passive ADP agent,
modify it to use an approximate ADP algorithm as discussed in the text.
Do this in two steps:

1.  Implement a priority queue for adjustments to the utility estimates.
    Whenever a state is adjusted, all of its predecessors also become
    candidates for adjustment and should be added to the queue. The
    queue is initialized with the state from which the most recent
    transition took place. Allow only a fixed number of adjustments.

2.  Experiment with various heuristics for ordering the priority queue,
    examining their effect on learning rates and computation time.

The direct utility estimation method in uses distinguished terminal
states to indicate the end of a trial. How could it be modified for
environments with discounted rewards and no terminal states?

Write out the parameter update equations for TD learning with
$$\hat{U}(x,y) = \theta_0 + \theta_1 x + \theta_2 y + \theta_3\,\sqrt{(x-x_g)^2 + (y-y_g)^2}\ .$$

Adapt the vacuum world () for reinforcement learning by including
rewards for squares being clean. Make the world observable by providing
suitable percepts. Now experiment with different reinforcement learning
agents. Is function approximation necessary for success? What sort of
approximator works for this application?

[approx-LMS-exercise]Implement an exploring reinforcement learning agent
that uses direct utility estimation. Make two versions—one with a
tabular representation and one using the function approximator in .
Compare their performance in three environments:

1.  The $4\stimes 3$ world described in the chapter.

2.  A ${10}\stimes {10}$ world with no obstacles and a +1 reward at
    (10,10).

3.  A ${10}\stimes {10}$ world with no obstacles and a +1 reward at
    (5,5).

Devise suitable features for reinforcement learning in stochastic grid
worlds (generalizations of the $4\stimes 3$ world) that contain multiple
obstacles and multiple terminal states with rewards of $+1$ or $-1$.

Extend the standard game-playing environment () to incorporate a reward
signal. Put two reinforcement learning agents into the environment (they
may, of course, share the agent program) and have them play against each
other. Apply the generalized TD update rule () to update the evaluation
function. You might wish to start with a simple linear weighted
evaluation function and a simple game, such as tic-tac-toe.

[10x10-exercise] Compute the true utility function and the best linear
approximation in $x$ and $y$ (as in ) for the following environments:

1.  A ${10}\stimes {10}$ world with a single $+1$ terminal state at
    (10,10).

2.  As in (a), but add a $-1$ terminal state at (10,1).

3.  As in (b), but add obstacles in 10 randomly selected squares.

4.  As in (b), but place a wall stretching from (5,2) to (5,9).

5.  As in (a), but with the terminal state at (5,5).

The actions are deterministic moves in the four directions. In each
case, compare the results using three-dimensional plots. For each
environment, propose additional features (besides $x$ and $y$) that
would improve the approximation and show the results.

Implement the and algorithms and apply them to the $4\stimes 3$ world,
using a policy family of your own choosing. Comment on the results.

the application of reinforcement learning ideas to the modeling of human
and animal behavior.

reinforcement learning an appropriate abstract model for evolution? What
connection exists, if any, between hardwired reward signals and
evolutionary fitness?

[^1]: The technical conditions are given on . In we have used
    $\alpha(n)\eq {60}/({59}+n)$, which satisfies the conditions.

[^2]: Notice the direct analogy to the theory of information value in .

[^3]: We do know that the exact utility function can be represented in a
    page or two of Lisp, Java, or C++. That is, it can be represented by
    a program that solves the game exactly every time it is called. We
    are interested only in function approximators that use a
    *reasonable* amount of computation. It might in fact be
    better to learn a very simple function approximator and combine it
    with a certain amount of look-ahead search. The tradeoffs involved
    are currently not well understood.

[^4]: The definition of distance between utility functions is rather
    technical; see .

[^5]: Also known as twenty-one or pontoon.
Robotics {#robotics-chapter}
========

Introduction
------------

are physical agents that perform tasks by manipulating the physical
world. To do so, they are equipped with such as legs, wheels, joints,
and grippers. Effectors have a single purpose: to assert physical forces
on the environment.[^1] Robots are also equipped with , which allow them
to perceive their environment. Present day robotics employs a diverse
set of sensors, including cameras and lasers to measure the environment,
and gyroscopes and accelerometers to measure the robot’s own motion.

[rover+humanoid-figure]

[predator+sojourner-figure]

Most of today’s robots fall into one of three primary categories. , or
robot arms ((a)), are physically anchored to their workplace, for
example in a factory assembly line or on the International Space
Station. Manipulator motion usually involves a chain of controllable
joints, enabling such robots to place their effectors in any position
within the workplace. Manipulators are by far the most common type of
industrial robots, with approximately one million units installed
worldwide. Some mobile manipulators are used in hospitals to assist
surgeons. Few car manufacturers could survive without robotic
manipulators, and some manipulators have even been used to generate
original artwork.

The second category is the . Mobile robots move about their environment
using wheels, legs, or similar mechanisms. They have been put to use
delivering food in hospitals, moving containers at loading docks, and
similar tasks. , or UGVs, drive autonomously on streets, highways, and
off-road. The shown in (b) explored Mars for a period of 3 months in
1997. Subsequent NASA robots include the twin Mars Exploration Rovers
(one is depicted on the cover of this book), which landed in 2003 and
were still operating six years later. Other types of mobile robots
include (UAVs), commonly used for surveillance, crop-spraying, and
military operations. (a) shows a UAV commonly used by the U.S. military.
(AUVs) are used in deep sea exploration. Mobile robots deliver packages
in the workplace and vacuum the floors at home.

The third type of robot combines mobility with manipulation, and is
often called a . mimic the human torso. (b) shows two early humanoid
robots, both manufactured by Honda Corp. in Japan. Mobile manipulators
can apply their effectors further afield than anchored manipulators can,
but their task is made harder because they don’t have the rigidity that
the anchor provides.

The field of robotics also includes prosthetic devices (artificial
limbs, ears, and eyes for humans), intelligent environments (such as an
entire house that is equipped with sensors and effectors), and multibody
systems, wherein robotic action is achieved through swarms of small
cooperating robots.

Real robots must cope with environments that are partially observable,
stochastic, dynamic, and continuous. Many robot environments are
sequential and multiagent as well. Partial observability and
stochasticity are the result of dealing with a large, complex world.
Robot cameras cannot see around corners, and motion commands are subject
to uncertainty due to gears slipping, friction, etc. Also, the real
world stubbornly refuses to operate faster than real time. In a
simulated environment, it is possible to use simple algorithms (such as
the algorithm described in ) to learn in a few CPU hours from millions
of trials. In a real environment, it might take years to run these
trials. Furthermore, real crashes really hurt, unlike simulated ones.
Practical robotic systems need to embody prior knowledge about the
robot, its physical environment, and the tasks that the robot will
perform so that the robot can learn quickly and perform safely.

Robotics brings together many of the concepts we have seen earlier in
the book, including probabilistic state estimation, perception,
planning, unsupervised learning, and reinforcement learning. For some of
these concepts robotics serves as a challenging example application. For
other concepts this chapter breaks new ground in introducing the
continuous version of techniques that we previously saw only in the
discrete case.

Robot Hardware {#robot-hardware-section}
--------------

So far in this book, we have taken the agent architecture—sensors,
effectors, and processors—as given, and we have concentrated on the
agent program. The success of real robots depends at least as much on
the design of sensors and effectors that are appropriate for the task.

[toc-camera]

### Sensors

Sensors are the perceptual interface between robot and environment. ,
such as cameras, are true observers of the environment: they capture
signals that are generated by other sources in the environment. , such
as sonar, send energy into the environment. They rely on the fact that
this energy is reflected back to the sensor. Active sensors tend to
provide more information than passive sensors, but at the expense of
increased power consumption and with a danger of interference when
multiple active sensors are used at the same time. Whether active or
passive, sensors can be divided into three types, depending on whether
they sense the environment, the robot’s location, or the robot’s
internal configuration.

are sensors that measure the distance to nearby objects. In the early
days of robotics, robots were commonly equipped with . Sonar sensors
emit directional sound waves, which are reflected by objects, with some
of the sound making it back into the sensor. The time and intensity of
the returning signal indicates the distance to nearby objects. Sonar is
the technology of choice for autonomous underwater vehicles. (see )
relies on multiple cameras to image the environment from slightly
different viewpoints, analyzing the resulting parallax in these images
to compute the range of surrounding objects. For mobile ground robots,
sonar and stereo vision are now rarely used, because they are not
reliably accurate.

Most ground robots are now equipped with optical range finders. Just
like sonar sensors, optical range sensors emit active signals (light)
and measure the time until a reflection of this signal arrives back at
the sensor. (a) shows a . This camera acquires range images like the one
shown in (b) at up to 60 frames per second. Other range sensors use
laser beams and special 1-pixel cameras that can be directed using
complex arrangements of mirrors or rotating elements. These sensors are
called (short for *light detection and ranging*). Scanning lidars tend
to provide longer ranges than time of flight cameras, and tend to
perform better in bright daylight.

Other common range sensors include radar, which is often the sensor of
choice for UAVs. Radar sensors can measure distances of multiple
kilometers. On the other extreme end of range sensing are such as
whiskers, bump panels, and touch-sensitive skin. These sensors measure
range based on physical contact, and can be deployed only for sensing
objects very close to the robot.

A second important class of sensors is . Most location sensors use range
sensing as a primary component to determine location. Outdoors, the
(GPS) is the most common solution to the localization problem. GPS
measures the distance to satellites that emit pulsed signals. At
present, there are 31 satellites in orbit, transmitting signals on
multiple frequencies. GPS receivers can recover the distance to these
satellites by analyzing phase shifts. By triangulating signals from
multiple satellites, GPS receivers can determine their absolute location
on Earth to within a few meters. involves a second ground receiver with
known location, providing millimeter accuracy under ideal conditions.
Unfortunately, GPS does not work indoors or underwater. Indoors,
localization is often achieved by attaching beacons in the environment
at known locations. Many indoor environments are full of wireless base
stations, which can help robots localize through the analysis of the
wireless signal. Underwater, active sonar beacons can provide a sense of
location, using sound to inform AUVs of their relative distances to
those beacons.

The third important class is , which inform the robot of its own motion.
To measure the exact configuration of a robotic joint, motors are often
equipped with that count the revolution of motors in small increments.
On robot arms, shaft decoders can provide accurate information over any
period of time. On mobile robots, shaft decoders that report wheel
revolutions can be used for —the measurement of distance traveled.
Unfortunately, wheels tend to drift and slip, so odometry is accurate
only over short distances. External forces, such as the current for AUVs
and the wind for UAVs, increase positional uncertainty. , such as
gyroscopes, rely on the resistance of mass to the change of velocity.
They can help reduce uncertainty.

Other important aspects of robot state are measured by and . These are
indispensable when robots handle fragile objects or objects whose exact
shape and location is unknown. Imagine a one-ton robotic manipulator
screwing in a light bulb. It would be all too easy to apply too much
force and break the bulb. Force sensors allow the robot to sense how
hard it is gripping the bulb, and torque sensors allow it to sense how
hard it is turning. Good sensors can measure forces in all three
translational and three rotational directions. They do this at a
frequency of several hundred times a second, so that a robot can quickly
detect unexpected forces and correct its actions before it breaks a
light bulb.

### Effectors {#effectors-section}

Effectors are the means by which robots move and change the shape of
their bodies. To understand the design of effectors, it will help to
talk about motion and shape in the abstract, using the concept of a
(DOF) We count one degree of freedom for each independent direction in
which a robot, or one of its effectors, can move. For example, a rigid
mobile robot such as an AUV has six degrees of freedom, three for its
$\zt (x, y, z)$ location in space and three for its angular orientation,
known as *yaw*, *roll*, and
*pitch*. These six degrees define the [^2] or of the robot.
The of a robot includes these six plus an additional six dimensions for
the rate of change of each kinematic dimension, that is, their
velocities.

For nonrigid bodies, there are additional degrees of freedom within the
robot itself. For example, the elbow of a human arm possesses two degree
of freedom. It can flex the upper arm towards or away, and can rotate
right or left. The wrist has three degrees of freedom. It can move up
and down, side to side, and can also rotate. Robot joints also have one,
two, or three degrees of freedom each. Six degrees of freedom are
required to place an object, such as a hand, at a particular point in a
particular orientation. The arm in (a) has exactly six degrees of
freedom, created by five that generate rotational motion and one that
generates sliding motion. You can verify that the human arm as a whole
has more than six degrees of freedom by a simple experiment: put your
hand on the table and notice that you still have the freedom to rotate
your elbow without changing the configuration of your hand. Manipulators
that have extra degrees of freedom are easier to control than robots
with only the minimum number of DOFs. Many industrial manipulators
therefore have seven DOFs, not six.

[DOF-figure]

For mobile robots, the DOFs are not necessarily the same as the number
of actuated elements. Consider, for example, your average car: it can
move forward or backward, and it can turn, giving it two DOFs. In
contrast, a car’s kinematic configuration is three-dimensional: on an
open flat surface, one can easily maneuver a car to any $\zt (x, y)$
point, in any orientation. (See (b).) Thus, the car has three but two .
We say a robot is if it has more effective DOFs than controllable DOFs
and if the two numbers are the same. Holonomic robots are easier to
control—it would be much easier to park a car that could move sideways
as well as forward and backward—but holonomic robots are also
mechanically more complex. Most robot arms are holonomic, and most
mobile robots are nonholonomic.

[willow+raibert-figure]

Mobile robots have a range of mechanisms for locomotion, including
wheels, tracks, and legs. robots possess two independently actuated
wheels (or tracks), one on each side, as on a military tank. If both
wheels move at the same velocity, the robot moves on a straight line. If
they move in opposite directions, the robot turns on the spot. An
alternative is the , in which each wheel can move and turn around its
own axis. To avoid chaos, the wheels are tightly coordinated. When
moving straight, for example, all wheels point in the same direction and
move at the same speed. Both differential and synchro drives are
nonholonomic. Some more expensive robots use holonomic drives, which
have three or more wheels that can be oriented and moved independently.

Some mobile robots possess arms. (a) displays a two-armed robot. This
robot’s arms use springs to compensate for gravity, and they provide
minimal resistance to external forces. Such a design minimizes the
physical danger to people who might stumble into such a robot. This is a
key consideration in deploying robots in domestic environments.

[bigdog+robocup-figure]

Legs, unlike wheels, can handle rough terrain. However, legs are
notoriously slow on flat surfaces, and they are mechanically difficult
to build. Robotics researchers have tried designs ranging from one leg
up to dozens of legs. Legged robots have been made to walk, run, and
even hop—as we see with the legged robot in (b). This robot is , meaning
that it can remain upright while hopping around. A robot that can remain
upright without moving its legs is called . A robot is statically stable
if its center of gravity is above the polygon spanned by its
legs.[polygon-stability-condition-page] The quadruped (four-legged)
robot shown in (a) may appear statically stable. However, it walks by
lifting multiple legs at the same time, which renders it dynamically
stable. The robot can walk on snow and ice, and it will not fall over
even if you kick it (as demonstrated in videos available online).
Two-legged robots such as those in (b) are dynamically stable.

Other methods of movement are possible: air vehicles use propellers or
turbines; underwater vehicles use propellers or thrusters, similar to
those used on submarines. Robotic blimps rely on thermal effects to keep
themselves aloft.

Sensors and effectors alone do not make a robot. A complete robot also
needs a source of power to drive its effectors. The is the most popular
mechanism for both manipulator actuation and locomotion, but using
compressed gas and using pressurized fluids also have their application
niches.

Robotic Perception {#robotic-perception-section}
------------------

Perception is the process by which robots map sensor measurements into
internal representations of the environment. Perception is difficult
because sensors are noisy, and the environment is partially observable,
unpredictable, and often dynamic. In other words, robots have all the
problems of (or ) that we discussed in . As a rule of thumb, good
internal representations for robots have three properties: they contain
enough information for the robot to make good decisions, they are
structured so that they can be updated efficiently, and they are natural
in the sense that internal variables correspond to natural state
variables in the physical world.

In , we saw that Kalman filters, HMMs, and dynamic Bayes nets can
represent the transition and sensor models of a partially observable
environment, and we described both exact and approximate algorithms for
updating the —the posterior probability distribution over the
environment state variables. Several dynamic Bayes net models for this
process were shown in . For robotics problems, we include the robot’s
own past actions as observed variables in the model. shows the notation
used in this chapter: $\X_t$ is the state of the environment (including
the robot) at time $t$, $\Z_t$ is the observation received at time $t$,
and $A_t$ is the action taken after the observation is received.

[robotics-ddn-figure]

We would like to compute the new belief state,
$\pv(\X_{t+1}\mid \z_{1:t+1},
a_{1:t})$, from the current belief state $\pv(\X_{t}\mid
\z_{1:t},a_{1:t-1})$ and the new observation $\z_{t+1}$. We did this in
, but here there are two differences: we condition explicitly on the
actions as well as the observations, and we deal with
*continuous* rather than *discrete* variables.
Thus, we modify the recursive filtering equation ([filtering-equation]
on ) to use integration rather than summation:

$$\begin{aligned}
\lefteqn{\pv(\X_{t+1}\mid \z_{1:t+1}, a_{1:t})}\nonumber\\ 
  &=& \alpha \pv(\z_{t+1}\mid \X_{t+1})
   \int \pv(\X_{t+1}\mid \x_{t},a_{t})\;P(\x_{t}\mid \z_{1:t}, a_{1:t-1})\;d\x_{t} \ .
\label{robot-filtering-equation}\end{aligned}$$

This equation states that the posterior over the state variables $\zt
\X$ at time $\zt t+1$ is calculated recursively from the corresponding
estimate one time step earlier. This calculation involves the previous
action $\zt a_{t}$ and the current sensor measurement $\zt
\z_{t+1}$. For example, if our goal is to develop a soccer-playing
robot, $\zt \X_{t+1}$ might be the location of the soccer ball relative
to the robot. The posterior $\zt \pv(\X_{t}\given
\z_{1:t},a_{1:t-1})$ is a probability distribution over all states that
captures what we know from past sensor measurements and controls. tells
us how to recursively estimate this location, by incrementally folding
in sensor measurements (e.g., camera images) and robot motion commands.
The probability $\zt
\pv(\X_{t+1}\mid \x_{t},a_{t})$ is called the or , and
$\zt \pv(\z_{t+1}\mid
\X_{t+1})$ is the .

### Localization and mapping

is the problem of finding out where things are—including the robot
itself. Knowledge about where things are is at the core of any
successful physical interaction with the environment. For example, robot
manipulators must know the location of objects they seek to manipulate;
navigating robots must know where they are to find their way around.

[motion+sensor-figure]

To keep things simple, let us consider a mobile robot that moves slowly
in a flat 2D world. Let us also assume the robot is given an exact map
of the environment. (An example of such a map appears in .) The pose of
such a mobile robot is defined by its two Cartesian coordinates with
values $\zt x$ and $\zt y$ and its heading with value $\zt \theta$, as
illustrated in (a). If we arrange those three values in a vector, then
any particular state is given by
$\X_{t} \eq (x_t,y_t,\theta_t)\transpose$. So far so good.

In the kinematic approximation, each action consists of the
“instantaneous” specification of two velocities—a translational velocity
$\zt v_t$ and a rotational velocity $\zt \omega_t$. For small time
intervals $\zt \Delta t$, a crude deterministic model of the motion of
such robots is given by
$$\hat{\X}_{t+1} = f(\X_{t}, \underbrace{v_{t},\omega_{t}}_{a_{t}})
= \X_{t} + \left(\begin{array}{ccc}
                 v_{t}\Delta t \cos\theta_{t} \\
                 v_{t}\Delta t \sin\theta_{t} \\
                 \omega_{t}\Delta t 
           \end{array}\right)\ .$$ The notation $\zt \hat{\X}$ refers to
a deterministic state prediction. Of course, physical robots are
somewhat unpredictable. This is commonly modeled by a Gaussian
distribution with mean $f(\X_{t}, v_{t},\omega_{t})$ and covariance
$\zt \kftv$. (See for a mathematical definition.)

$$\begin{aligned}
\pv(\X_{t+1} \mid \X_{t}, v_t, \omega_t) &=&
N(\hat{\X}_{t+1}, \kftv)\ .\end{aligned}$$

This probability distribution is the robot’s motion model. It models the
effects of the motion $a_t$ on the location of the robot.

Next, we need a sensor model. We will consider two kinds of sensor
model. The first assumes that the sensors detect *stable*,
*recognizable* features of the environment called . For
each landmark, the range and bearing are reported. Suppose the robot’s
state is $\zt \x_t \eq (x_t, y_t,
\theta_t)\transpose$ and it senses a landmark whose location is known to
be $\zt
(x_i,y_i)\transpose$. Without noise, the range and bearing can be
calculated by simple geometry. (See (a).) The exact prediction of the
observed range and bearing would be $$\hat{\z}_t = h(\x_{t}) = 
\left(\begin{array}{c}
\sqrt{(x_t-x_i)^2+(y_t-y_i)^2}\\
\arctan \frac{y_i-y_t}{x_i-x_t}-\theta_t
\end{array}\right) \ .$$ Again, noise distorts our measurements. To keep
things simple, one might assume Gaussian noise with covariance
$\zt \covariance_z$, giving us the sensor model
$$P(\z_t\mid \x_t) = N(\hat{\z}_t,\covariance_z)\ .$$ A somewhat
different sensor model is used for an array of range sensors, each of
which has a fixed bearing relative to the robot. Such sensors produce a
vector of range values $\zt \z_t = (z_1,\ldots,z_M)\transpose$. Given a
pose $\zt \x_t$, let $\hat{z}_j$ be the exact range along the $\zt j$th
beam direction from $\zt \x_t$ to the nearest obstacle. As before, this
will be corrupted by Gaussian noise. Typically, we assume that the
errors for the different beam directions are independent and identically
distributed, so we have
$$P(\z_t\mid \x_t) = \alpha \prod_{j\eq 1}^M e^{-(z_j - \hat{z}_j)/2\sigma^2} .$$
(b) shows an example of a four-beam range scan and two possible robot
poses, one of which is reasonably likely to have produced the observed
scan and one of which is not. Comparing the range-scan model to the
landmark model, we see that the range-scan model has the advantage that
there is no need to *identify* a landmark before the range
scan can be interpreted; indeed, in (b), the robot faces a featureless
wall. On the other hand, if there *are* visible,
identifiable landmarks, they may provide instant localization.

described the Kalman filter, which represents the belief state as a
single multivariate Gaussian, and the particle filter, which represents
the belief state by a collection of particles that correspond to states.
Most modern localization algorithms use one of two representations of
the robot’s belief $\zt
\pv(\X_{t}\mid \z_{1:t},a_{1:t-1})$.

[mcl-algorithm]

Localization using particle filtering is called , or MCL. The MCL
alfgorithm is an instance of the particle-filtering algorithm of (). All
we need to do is supply the appropriate motion model and sensor model.
shows one version using the range-scan model. The operation of the
algorithm is illustrated in as the robot finds out where it is inside an
office building. In the first image, the particles are uniformly
distributed based on the prior, indicating global uncertainty about the
robot’s position. In the second image, the first set of measurements
arrives and the particles form clusters in the areas of high posterior
belief. In the third, enough measurements are available to push all the
particles to a single location.

[mcl-progress-figure]

[linearization-figure]

The is the other major way to localize. A Kalman filter represents the
posterior $\zt \pv(\X_{t}\mid \z_{1:t}, a_{1:t-1})$ by a Gaussian. The
mean of this Gaussian will be denoted $\zt \mean_{t}$ and its covariance
$\zt \covariance_t$. The main problem with Gaussian beliefs is that they
are only closed under linear motion models $f$ and linear measurement
models $h$. For nonlinear $f$ or $h$, the result of updating a filter is
in general not Gaussian. Thus, localization algorithms using the Kalman
filter the motion and sensor models. Linearization is a local
approximation of a nonlinear function by a linear function. illustrates
the concept of linearization for a (one-dimensional) robot motion model.
On the left, it depicts a nonlinear motion model $\zt f(\x_{t}, a_{t})$
(the control $\zt a_{t}$ is omitted in this graph since it plays no role
in the linearization). On the right, this function is approximated by a
linear function $\zt \tilde{f}(\x_{t}, a_{t})$. This linear function is
tangent to $\zt f$ at the point $\zt \mean_t$, the mean of our state
estimate at time $\zt t$. Such a linearization is called (first degree)
. A Kalman filter that linearizes $f$ and $h$ via Taylor expansion is
called an (or EKF).

[kalman-localization-figure]

shows a sequence of estimates of a robot running an extended Kalman
filter localization algorithm. As the robot moves, the uncertainty in
its location estimate increases, as shown by the error ellipses. Its
error decreases as it senses the range and bearing to a landmark with
known location and increases again as the robot loses sight of the
landmark. EKF algorithms work well if landmarks are easily identified.
Otherwise, the posterior distribution may be multimodal, as in (b). The
problem of needing to know the identity of landmarks is an instance of
the problem discussed in .

In some situations, no map of the environment is available. Then the
robot will have to acquire a map. This is a bit of a chicken-and-egg
problem: the navigating robot will have to determine its location
relative to a map it doesn’t quite know, at the same time building this
map while it doesn’t quite know its actual location. This problem is
important for many robot applications, and it has been studied
extensively under the name , abbreviated as **SLAM**.

SLAM problems are solved using many different probabilistic techniques,
including the extended Kalman filter discussed above. Using the EKF is
straightforward: just augment the state vector to include the locations
of the landmarks in the environment. Luckily, the EKF update scales
quadratically, so for small maps (e.g., a few hundred landmarks) the
computation is quite feasible. Richer maps are often obtained using
graph relaxation methods, similar to the Bayesian network inference
techniques discussed in . Expectation–maximization is also used for
SLAM.

### Other types of perception

Not all of robot perception is about localization or mapping. Robots
also perceive the temperature, odors, acoustic signals, and so on. Many
of these quantities can be estimated using variants of dynamic Bayes
networks. All that is required for such estimators are conditional
probability distributions that characterize the evolution of state
variables over time, and sensor models that describe the relation of
measurements to state variables.

It is also possible to program a robot as a reactive agent, without
explicitly reasoning about probability distributions over states. We
cover that approach in .

The trend in robotics is clearly towards representations with
well-defined semantics. Probabilistic techniques outperform other
approaches in many hard perceptual problems such as localization and
mapping. However, statistical techniques are sometimes too cumbersome,
and simpler solutions may be just as effective in practice. To help
decide which approach to take, experience working with real physical
robots is your best teacher.

### Machine learning in robot perception

Machine learning plays an important role in robot perception. This is
particularly the case when the best internal representation is not
known. One common approach is to map high-dimensional sensor streams
into lower-dimensional spaces using unsupervised machine learning
methods (see ). Such an approach is called . Machine learning makes it
possible to learn sensor and motion models from data, while
simultaneously discovering a suitable internal representations.

Another machine learning technique enables robots to continuously adapt
to broad changes in sensor measurements. Picture yourself walking from a
sun-lit space into a dark neon-lit room. Clearly things are darker
inside. But the change of light source also affects all the colors: Neon
light has a stronger component of green light than sunlight. Yet somehow
we seem not to notice the change. If we walk together with people into a
neon-lit room, we don’t think that suddenly their faces turned green.
Our perception quickly adapts to the new lighting conditions, and our
brain ignores the differences.

[fig-adaptive-vision]

Adaptive perception techniques enable robots to adjust to such changes.
One example is shown in , taken from the autonomous driving domain. Here
an unmanned ground vehicle adapts its classifier of the concept
“drivable surface.” How does this work? The robot uses a laser to
provide classification for a small area right in front of the robot.
When this area is found to be flat in the laser range scan, it is used
as a positive training example for the concept “drivable surface.” A
mixture-of-Gaussians technique similar to the EM algorithm discussed in
is then trained to recognize the specific color and texture coefficients
of the small sample patch. The images in are the result of applying this
classifier to the full image.

Methods that make robots collect their own training data (with labels!)
are called . In this instance, the robot uses machine learning to
leverage a short-range sensor that works well for terrain classification
into a sensor that can see much farther. That allows the robot to drive
faster, slowing down only when the sensor model says there is a change
in the terrain that needs to be examined more carefully by the
short-range sensors.

Planning to Move {#navigation-section}
----------------

All of a robot’s deliberations ultimately come down to deciding how to
move effectors. The problem is to deliver the robot or its end effector
to a designated target location. A greater challenge is the problem, in
which a robot moves while being in physical contact with an obstacle. An
example of compliant motion is a robot manipulator that screws in a
light bulb, or a robot that pushes a box across a table top.

We begin by finding a suitable representation in which motion-planning
problems can be described and solved. It turns out that the —the space
of robot states defined by location, orientation, and joint angles—is a
better place to work than the original 3D space. The problem is to find
a path from one configuration to another in configuration space. We have
already encountered various versions of the path-planning problem
throughout this book; the complication added by robotics is that path
planning involves *continuous* spaces. There are two main
approaches: and . Each reduces the continuous path-planning problem to a
discrete graph-search problem. In this section, we assume that motion is
deterministic and that localization of the robot is exact. Subsequent
sections will relax these assumptions.

### Configuration space

We will start with a simple representation for a simple robot motion
problem. Consider the robot arm shown in (a). It has two joints that
move independently. Moving the joints alters the $\zt (x, y)$
coordinates of the elbow and the gripper. (The arm cannot move in the
$\zt z$ direction.) This suggests that the robot’s configuration can be
described by a four-dimensional coordinate: $\zt (x_e, y_e)$ for the
location of the elbow relative to the environment and $\zt (x_g, y_g)$
for the location of the gripper. Clearly, these four coordinates
characterize the full state of the robot. They constitute what is known
as , since the coordinates of the robot are specified in the same
coordinate system as the objects it seeks to manipulate (or to avoid).
Workspace representations are well-suited for collision checking,
especially if the robot and all objects are represented by simple
polygonal models.

The problem with the workspace representation is that not all workspace
coordinates are actually attainable, even in the absence of obstacles.
This is because of the on the space of attainable workspace coordinates.
For example, the elbow position $\zt (x_e, y_e)$ and the gripper
position $\zt (x_g, y_g)$ are always a fixed distance apart, because
they are joined by a rigid forearm. A robot motion planner defined over
workspace coordinates faces the challenge of generating paths that
adhere to these constraints. This is particularly tricky because the
state space is continuous and the constraints are nonlinear. It turns
out to be easier to plan with a representation. Instead of representing
the state of the robot by the Cartesian coordinates of its elements, we
represent the state by a configuration of the robot’s joints. Our
example robot possesses two joints. Hence, we can represent its state
with the two angles $\zt \varphi_s$ and $\zt \varphi_e$ for the shoulder
joint and elbow joint, respectively. In the absence of any obstacles, a
robot could freely take on any value in configuration space. In
particular, when planning a path one could simply connect the present
configuration and the target configuration by a straight line. In
following this path, the robot would then move its joints at a constant
velocity, until a target location is reached.

[FigArm1]

Unfortunately, configuration spaces have their own problems. The task of
a robot is usually expressed in workspace coordinates, not in
configuration space coordinates. This raises the question of how to map
between workspace coordinates and configuration space. Transforming
configuration space coordinates into workspace coordinates is simple: it
involves a series of straightforward coordinate transformations. These
transformations are linear for prismatic joints and trigonometric for
revolute joints. This chain of coordinate transformation is known as .

The inverse problem of calculating the configuration of a robot whose
effector location is specified in workspace coordinates is known as .
Calculating the inverse kinematics is hard, especially for robots with
many DOFs. In particular, the solution is seldom
unique.[inverse-kinematics-not-unique] (a) shows one of two possible
configurations that put the gripper in the same location. (The other
configuration would has the elbow below the shoulder.)

In general, this two-link robot arm has between zero and two inverse
kinematic solutions for any set of workspace coordinates. Most
industrial robots have sufficient degrees of freedom to find infinitely
many solutions to motion problems. To see how this is possible, simply
imagine that we added a third revolute joint to our example robot, one
whose rotational axis is parallel to the ones of the existing joints. In
such a case, we can keep the location (but not the orientation!) of the
gripper fixed and still freely rotate its internal joints, for most
configurations of the robot. With a few more joints (how many?) we can
achieve the same effect while keeping the orientation of the gripper
constant as well. We have already seen an example of this in the
“experiment” of placing your hand on the desk and moving your elbow. The
kinematic constraint of your hand position is insufficient to determine
the configuration of your elbow. In other words, the inverse kinematics
of your shoulder–arm assembly possesses an infinite number of solutions.

[FigArm2]

The second problem with configuration space representations arises from
the obstacles that may exist in the robot’s workspace. Our example in
(a) shows several such obstacles, including a free-hanging obstacle that
protrudes into the center of the robot’s workspace. In workspace, such
obstacles take on simple geometric forms—especially in most robotics
textbooks, which tend to focus on polygonal obstacles. But how do they
look in configuration space?

​(b) shows the configuration space for our example robot, under the
specific obstacle configuration shown in (a). The configuration space
can be decomposed into two subspaces: the space of all configurations
that a robot may attain, commonly called , and the space of unattainable
configurations, called . The white area in (b) corresponds to the free
space. All other regions correspond to occupied space. The different
shadings of the occupied space corresponds to the different objects in
the robot’s workspace; the black region surrounding the entire free
space corresponds to configurations in which the robot collides with
itself. It is easy to see that extreme values of the shoulder or elbow
angles cause such a violation. The two oval-shaped regions on both sides
of the robot correspond to the table on which the robot is mounted. The
third oval region corresponds to the left wall. Finally, the most
interesting object in configuration space is the vertical obstacle that
hangs from the ceiling and impedes the robot’s motions. This object has
a funny shape in configuration space: it is highly nonlinear and at
places even concave. With a little bit of imagination the reader will
recognize the shape of the gripper at the upper left end. We encourage
the reader to pause for a moment and study this diagram. The shape of
this obstacle is not at all obvious! The dot inside (b) marks the
configuration of the robot, as shown in (a). depicts three additional
configurations, both in workspace and in configuration space. In
configuration conf-1, the gripper encloses the vertical obstacle.

Even if the robot’s workspace is represented by flat polygons, the shape
of the free space can be very complicated. In practice, therefore, one
usually *probes* a configuration space instead of
constructing it explicitly. A planner may generate a configuration and
then test to see if it is in free space by applying the robot kinematics
and then checking for collisions in workspace coordinates.

### Cell decomposition methods

The first approach to path planning uses —that is, it decomposes the
free space into a finite number of contiguous regions, called cells.
These regions have the important property that the path-planning problem
within a single region can be solved by simple means (e.g., moving along
a straight line). The path-planning problem then becomes a discrete
graph-search problem, very much like the search problems introduced in .

[FigArm3]

The simplest cell decomposition consists of a regularly spaced grid. (a)
shows a square grid decomposition of the space and a solution path that
is optimal for this grid size. Grayscale shading indicates the
*value* of each free-space grid cell—i.e., the cost of the
shortest path from that cell to the goal. (These values can be computed
by a deterministic form of the algorithm given in on .) (b) shows the
corresponding workspace trajectory for the arm. Of course, we can also
use the A algorithm to find a shortest path.

Such a decomposition has the advantage that it is extremely simple to
implement, but it also suffers from three limitations. First, it is
workable only for low-dimensional configuration spaces, because the
number of grid cells increases exponentially with $\zt d$, the number of
dimensions. Sounds familiar? This is the . Second, there is the problem
of what to do with cells that are “mixed”—that is, neither entirely
within free space nor entirely within occupied space. A solution path
that includes such a cell may not be a real solution, because there may
be no way to cross the cell in the desired direction in a straight line.
This would make the path planner *unsound*. On the other
hand, if we insist that only completely free cells may be used, the
planner will be *incomplete*, because it might be the case
that the only paths to the goal go through mixed cells—especially if the
cell size is comparable to that of the passageways and clearances in the
space. And third, any path through a discretized state space will not be
smooth. It is generally difficult to guarantee that a smooth solution
exists near the discrete path. So a robot may not be able to execute the
solution found through this decomposition.

Cell decomposition methods can be improved in a number of ways, to
alleviate some of these problems. The first approach allows
*further subdivision* of the mixed cells—perhaps using
cells of half the original size. This can be continued recursively until
a path is found that lies entirely within free cells. (Of course, the
method only works if there is a way to decide if a given cell is a mixed
cell, which is easy only if the configuration space boundaries have
relatively simple mathematical descriptions.) This method is complete
provided there is a bound on the smallest passageway through which a
solution must pass. Although it focuses most of the computational effort
on the tricky areas within the configuration space, it still fails to
scale well to high-dimensional problems because each recursive splitting
of a cell creates $\zt 2^d$ smaller cells. A second way to obtain a
complete algorithm is to insist on an of the free space. This method
must allow cells to be irregularly shaped where they meet the boundaries
of free space, but the shapes must still be “simple” in the sense that
it should be easy to compute a traversal of any free cell. This
technique requires some quite advanced geometric ideas, so we shall not
pursue it further here.

Examining the solution path shown in (a), we can see an additional
difficulty that will have to be resolved. The path contains arbitrarily
sharp corners; a robot moving at any finite speed could not execute such
a path. This problem is solved by storing certain continuous values for
each grid cell. Consider an algorithm which stores, for each grid cell,
the exact, continuous state that was attained with the cell was first
expanded in the search. Assume further, that when propagating
information to nearby grid cells, we use this continuous state as a
basis, and apply the continuous robot motion model for jumping to nearby
cells. In doing so, we can now guarantee that the resulting trajectory
is smooth and can indeed be executed by the robot. One algorithm that
implements this is .

### Modified cost functions

Notice that in , the path goes very close to the obstacle. Anyone who
has driven a car knows that a parking space with one millimeter of
clearance on either side is not really a parking space at all; for the
same reason, we would prefer solution paths that are robust with respect
to small motion errors.

This problem can be solved by introducing a . A potential field is a
function defined over state space, whose value grows with the distance
to the closest obstacle. (a) shows such a potential field—the darker a
configuration state, the closer it is to an obstacle.

The potential field can be used as an additional cost term in the
shortest-path calculation. This induces an interesting tradeoff. On the
one hand, the robot seeks to minimize path length to the goal. On the
other hand, it tries to stay away from obstacles by virtue of minimizing
the potential function. With the appropriate weight balancing the two
objectives, a resulting path may look like the one shown in (b). This
figure also displays the value function derived from the combined cost
function, again calculated by value iteration. Clearly, the resulting
path is longer, but it is also safer.

There exist many other ways to modify the cost function. For example, it
may be desirable to *smooth* the control parameters over
time. For example, when driving a car, a smooth path is better than a
jerky one. In general, such higher-order constraints are not easy to
accommodate in the planning process, unless we make the most recent
steering command a part of the state. However, it is often easy to
smooth the resulting trajectory after planning, using conjugate gradient
methods. Such post-planning smoothing is essential in many real-world
applications.

[FigArm4]

[FigArm5]

### Skeletonization methods

The second major family of path-planning algorithms is based on the idea
of . These algorithms reduce the robot’s free space to a one-dimensional
representation, for which the planning problem is easier. This
lower-dimensional representation is called a of the configuration space.

shows an example skeletonization: it is a of the free space—the set of
all points that are equidistant to two or more obstacles. To do path
planning with a Voronoi graph, the robot first changes its present
configuration to a point on the Voronoi graph. It is easy to show that
this can always be achieved by a straight-line motion in configuration
space. Second, the robot follows the Voronoi graph until it reaches the
point nearest to the target configuration. Finally, the robot leaves the
Voronoi graph and moves to the target. Again, this final step involves
straight-line motion in configuration space.

In this way, the original path-planning problem is reduced to finding a
path on the Voronoi graph, which is generally one-dimensional (except in
certain nongeneric cases) and has finitely many points where three or
more one-dimensional curves intersect. Thus, finding the shortest path
along the Voronoi graph is a discrete graph-search problem of the kind
discussed in Chapters [search-chapter] and [advanced-search-chapter].
Following the Voronoi graph may not give us the shortest path, but the
resulting paths tend to maximize clearance. Disadvantages of Voronoi
graph techniques are that they are difficult to apply to
higher-dimensional configuration spaces, and that they tend to induce
unnecessarily large detours when the configuration space is wide open.
Furthermore, computing the Voronoi graph can be difficult, especially in
configuration space, where the shapes of obstacles can be complex.

An alternative to the Voronoi graphs is the , a skeletonization approach
that offers more possible routes, and thus deals better with wide-open
spaces. (b) shows an example of a probabilistic roadmap. The graph is
created by randomly generating a large number of configurations, and
discarding those that do not fall into free space. Two nodes are joined
by an arc if it is “easy” to reach one node from the other–for example,
by a straight line in free space. The result of all this is a randomized
graph in the robot’s free space. If we add the robot’s start and goal
configurations to this graph, path planning amounts to a discrete graph
search. Theoretically, this approach is incomplete, because a bad choice
of random points may leave us without any paths from start to goal. It
is possible to bound the probability of failure in terms of the number
of points generated and certain geometric properties of the
configuration space. It is also possible to direct the generation of
sample points towards the areas where a partial search suggests that a
good path may be found, working bidirectionally from both the start and
the goal positions. With these improvements, probabilistic roadmap
planning tends to scale better to high-dimensional configuration spaces
than most alternative path-planning techniques.

Planning Uncertain Movements {#uncertain-motion-section}
----------------------------

None of the robot motion-planning algorithms discussed thus far
addresses a key characteristic of robotics problems:
*uncertainty*. In robotics, uncertainty arises from partial
observability of the environment and from the stochastic (or unmodeled)
effects of the robot’s actions. Errors can also arise from the use of
approximation algorithms such as particle filtering, which does not
provide the robot with an exact belief state even if the stochastic
nature of the environment is modeled perfectly.

Most of today’s robots use deterministic algorithms for decision making,
such as the path-planning algorithms of the previous section. To do so,
it is common practice to extract the from the probability distribution
produced by the state estimation algorithm. The advantage of this
approach is purely computational. Planning paths through configuration
space is already a challenging problem; it would be worse if we had to
work with a full probability distribution over states. Ignoring
uncertainty in this way works when the uncertainty is small. In fact,
when the environment model changes over time as the result of
incorporating sensor measurements, many robots plan paths online during
plan execution. This is the technique of .

Unfortunately, ignoring the uncertainty does not always work. In some
problems the robot’s uncertainty is simply too massive: How can we use a
deterministic path planner to control a mobile robot that has no clue
where it is? In general, if the robot’s true state is not the one
identified by the maximum likelihood rule, the resulting control will be
suboptimal. Depending on the magnitude of the error this can lead to all
sorts of unwanted effects, such as collisions with obstacles.

The field of robotics has adopted a range of techniques for
accommodating uncertainty. Some are derived from the algorithms given in
for decision making under uncertainty. If the robot faces uncertainty
only in its state transition, but its state is fully observable, the
problem is best modeled as a Markov decision process (MDP). The solution
of an MDP is an optimal , which tells the robot what to do in every
possible state. In this way, it can handle all sorts of motion errors,
whereas a single-path solution from a deterministic planner would be
much less robust. In robotics, policies are called . The value function
shown in (a) can be converted into such a navigation function simply by
following the gradient.

Just as in , partial observability makes the problem much harder. The
resulting robot control problem is a partially observable MDP, or POMDP.
In such situations, the robot maintains an internal belief state, like
the ones discussed in . The solution to a POMDP is a policy defined over
the robot’s belief state. Put differently, the input to the policy is an
entire probability distribution. This enables the robot to base its
decision not only on what it knows, but also on what it does not know.
For example, if it is uncertain about a critical state variable, it can
rationally invoke an . This is impossible in the MDP framework, since
MDPs assume full observability. Unfortunately, techniques that solve
POMDPs exactly are inapplicable to robotics—there are no known
techniques for high-dimensional continuous spaces. Discretization
produces POMDPs that are far too large to handle. One remedy is to make
the minimization of uncertainty a control objective. For example, the
heuristic requires the robot to stay near known landmarks to decrease
its uncertainty. Another approach applies variants of the probabilistic
roadmap planning method to the belief space representation. Such methods
tend to scale better to large discrete POMDPs.

### Robust methods

Uncertainty can also be handled using so-called methods (see ) rather
than probabilistic methods. A robust method is one that assumes a
*bounded* amount of uncertainty in each aspect of a
problem, but does not assign probabilities to values within the allowed
interval. A robust solution is one that works no matter what actual
values occur, provided they are within the assumed interval. An extreme
form of robust method is the approach given in —it produces plans that
work with no state information at all.

Here, we look at a robust method that is used for (or FMP) in robotic
assembly tasks. Fine-motion planning involves moving a robot arm in very
close proximity to a static environment object. The main difficulty with
fine-motion planning is that the required motions and the relevant
features of the environment are very small. At such small scales, the
robot is unable to measure or control its position accurately and may
also be uncertain of the shape of the environment itself; we will assume
that these uncertainties are all bounded. The solutions to FMP problems
will typically be conditional plans or policies that make use of sensor
feedback during execution and are guaranteed to work in all situations
consistent with the assumed uncertainty bounds.

[peg-in-hole-figure]

[peg-in-hole-step1-figure]

[peg-in-hole-step2-figure]

A fine-motion plan consists of a series of . Each guarded motion
consists of (1) a motion command and (2) a termination condition, which
is a predicate on the robot’s sensor values, and returns true to
indicate the end of the guarded move. The motion commands are typically
that allow the effector to slide if the motion command would cause
collision with an obstacle. As an example, shows a two-dimensional
configuration space with a narrow vertical hole. It could be the
configuration space for insertion of a rectangular peg into a hole or a
car key into the ignition. The motion commands are constant velocities.
The termination conditions are contact with a surface. To model
uncertainty in control, we assume that instead of moving in the
commanded direction, the robot’s actual motion lies in the cone $C_v$
about it. The figure shows what would happen if we commanded a velocity
straight down from the initial configuration. Because of the uncertainty
in velocity, the robot could move anywhere in the conical envelope,
possibly going into the hole, but more likely landing to one side of it.
Because the robot would not then know which side of the hole it was on,
it would not know which way to move.

A more sensible strategy is shown in Figures [peg-in-hole-step1-figure]
and [peg-in-hole-step2-figure]. In , the robot deliberately moves to one
side of the hole. The motion command is shown in the figure, and the
termination test is contact with any surface. In , a motion command is
given that causes the robot to slide along the surface and into the
hole. Because all possible velocities in the motion envelope are to the
right, the robot will slide to the right whenever it is in contact with
a horizontal surface. It will slide down the right-hand vertical edge of
the hole when it touches it, because all possible velocities are down
relative to a vertical surface. It will keep moving until it reaches the
bottom of the hole, because that is its termination condition. In spite
of the control uncertainty, all possible trajectories of the robot
terminate in contact with the bottom of the hole—that is, unless surface
irregularities cause the robot to stick in one place.

As one might imagine, the problem of *constructing*
fine-motion plans is not trivial; in fact, it is a good deal harder than
planning with exact motions. One can either choose a fixed number of
discrete values for each motion or use the environment geometry to
choose directions that give qualitatively different behavior. A
fine-motion planner takes as input the configuration-space description,
the angle of the velocity uncertainty cone, and a specification of what
sensing is possible for termination (surface contact in this case). It
should produce a multistep conditional plan or policy that is guaranteed
to succeed, if such a plan exists.

Our example assumes that the planner has an exact model of the
environment, but it is possible to allow for bounded error in this model
as follows. If the error can be described in terms of parameters, those
parameters can be added as degrees of freedom to the configuration
space. In the last example, if the depth and width of the hole were
uncertain, we could add them as two degrees of freedom to the
configuration space. It is impossible to move the robot in these
directions in the configuration space or to sense its position directly.
But both those restrictions can be incorporated when describing this
problem as an FMP problem by appropriately specifying control and sensor
uncertainties. This gives a complex, four-dimensional planning problem,
but exactly the same planning techniques can be applied. Notice that
unlike the decision-theoretic methods in , this kind of robust approach
results in plans designed for the worst-case outcome, rather than
maximizing the expected quality of the plan. Worst-case plans are
optimal in the decision-theoretic sense only if failure during execution
is much worse than any of the other costs involved in execution.

Moving {#control-section}
------

So far, we have talked about how to *plan* motions, but not
about how to *move*. Our plans—particularly those produced
by deterministic path planners—assume that the robot can simply follow
any path that the algorithm produces. In the real world, of course, this
is not the case. Robots have inertia and cannot execute arbitrary paths
except at arbitrarily slow speeds. In most cases, the robot gets to
exert forces rather than specify positions. This section discusses
methods for calculating these forces.

### Dynamics and control

introduced the notion of , which extends the kinematic state of a robot
by its velocity. For example, in addition to the angle of a robot joint,
the dynamic state also captures the rate of change of the angle, and
possibly even its momentary acceleration. The transition model for a
dynamic state representation includes the effect of forces on this rate
of change. Such models are typically expressed via , which are equations
that relate a quantity (e.g., a kinematic state) to the change of the
quantity over time (e.g., velocity). In principle, we could have chosen
to plan robot motion using dynamic models, instead of our kinematic
models. Such a methodology would lead to superior robot performance, if
we could generate the plans. However, the dynamic state has higher
dimension than the kinematic space, and the curse of dimensionality
would render many motion planning algorithms inapplicable for all but
the most simple robots. For this reason, practical robot system often
rely on simpler kinematic path planners.

A common technique to compensate for the limitations of kinematic plans
is to use a separate mechanism, a , for keeping the robot on track.
Controllers are techniques for generating robot controls in real time
using feedback from the environment, so as to achieve a control
objective. If the objective is to keep the robot on a preplanned path,
it is often referred to as a and the path is called a . Controllers that
optimize a global cost function are known as . Optimal policies for
continuous MDPs are, in effect, optimal controllers.

[FigArm6]

On the surface, the problem of keeping a robot on a prespecified path
appears to be relatively straightforward. In practice, however, even
this seemingly simple problem has its pitfalls. (a) illustrates what can
go wrong; it shows the path of a robot that attempts to follow a
kinematic path. Whenever a deviation occurs—whether due to noise or to
constraints on the forces the robot can apply—the robot provides an
opposing force whose magnitude is proportional to this deviation.
Intuitively, this might appear plausible, since deviations should be
compensated by a counterforce to keep the robot on track. However, as
(a) illustrates, our controller causes the robot to vibrate rather
violently. The vibration is the result of a natural inertia of the robot
arm: once driven back to its reference position the robot then
overshoots, which induces a symmetric error with opposite sign. Such
overshooting may continue along an entire trajectory, and the resulting
robot motion is far from desirable.

Before we can define a better controller, let us formally describe what
went wrong. Controllers that provide force in negative proportion to the
observed error are known as . The letter ‘P’ stands for
*proportional*, indicating that the actual control is
proportional to the error of the robot manipulator. More formally, let
$\zt y(t)$ be the reference path, parameterized by time index $\zt t$.
The control $\zt a_t$ generated by a P controller has the form:
$$a_t = K_P  (y(t) - x_t) \ .$$ Here $\zt x_t$ is the state of the robot
at time $\zt t$ and $\zt K_P$ is a constant known as the of the
controller and its value is called the gain factor); $K_p$ regulates how
strongly the controller corrects for deviations between the actual state
$\zt x_t$ and the desired one $\zt y(t)$. In our example, $\zt
K_P=1$. At first glance, one might think that choosing a smaller value
for $\zt K_P$ would remedy the problem. Unfortunately, this is not the
case. (b) shows a trajectory for $\zt K_P=.1$, still exhibiting
oscillatory behavior. Lower values of the gain parameter may simply slow
down the oscillation, but do not solve the problem. In fact, in the
absence of friction, the P controller is essentially a spring law; so it
will oscillate indefinitely around a fixed target location.

Traditionally, problems of this type fall into the realm of , a field of
increasing importance to researchers in AI. Decades of research in this
field have led to a large number of controllers that are superior to the
simple control law given above. In particular, a reference controller is
said to be if small perturbations lead to a bounded error between the
robot and the reference signal. It is said to be if it is able to return
to and then stay on its reference path upon such perturbations. Our P
controller appears to be stable but not strictly stable, since it fails
to stay anywhere near its reference trajectory.

The simplest controller that achieves strict stability in our domain is
a . The letter ‘P’ stands again for *proportional*, and ‘D’
stands for *derivative*. PD controllers are described by
the following equation:

$$\begin{aligned}
a_t &=& K_P (y(t) - x_t) + K_D \frac{\partial (y(t) - x_t)}{\partial t} \ .\end{aligned}$$

As this equation suggests, PD controllers extend P controllers by a
differential component, which adds to the value of $\zt a_t$ a term that
is proportional to the first derivative of the error $\zt
y(t)-x_t$ over time. What is the effect of such a term? In general, a
derivative term dampens the system that is being controlled. To see
this, consider a situation where the error $\zt (y(t) - x_t)$ is
changing rapidly over time, as is the case for our P controller above.
The derivative of this error will then counteract the proportional term,
which will reduce the overall response to the perturbation. However, if
the same error persists and does not change, the derivative will vanish
and the proportional term dominates the choice of control.

​(c) shows the result of applying this PD controller to our robot arm,
using as gain parameters $\zt K_P=.3$ and $\zt
K_D=.8$. Clearly, the resulting path is much smoother, and does not
exhibit any obvious oscillations.

PD controllers do have failure modes, however. In particular, PD
controllers may fail to regulate an error down to zero, even in the
absence of external perturbations. Often such a situation is the result
of a systematic external force that is not part of the model. An
autonomous car driving on a banked surface, for example, may find itself
systematically pulled to one side. Wear and tear in robot arms cause
similar systematic errors. In such situations, an over-proportional
feedback is required to drive the error closer to zero. The solution to
this problem lies in adding a third term to the control law, based on
the integrated error over time:

$$\begin{aligned}
a_t &=& K_P (y(t) - x_t) + K_I \int (y(t) - x_t) dt + K_D \frac{\partial (y(t) - x_t)}{\partial t} \ .\end{aligned}$$

Here $\zt K_I$ is yet another gain parameter. The term
$\zt \int (y(t) - x_t)
dt$ calculates the integral of the error over time. The effect of this
term is that long-lasting deviations between the reference signal and
the actual state are corrected. If, for example, $\zt x_t$ is smaller
than $\zt y(t)$ for a long period of time, this integral will grow until
the resulting control $\zt a_t$ forces this error to shrink. Integral
terms, then, ensure that a controller does not exhibit systematic error,
at the expense of increased danger of oscillatory behavior. A controller
with all three terms is called a (for proportional integral derivative).
PID controllers are widely used in industry, for a variety of control
problems.

### Potential-field control

[FigArm55]

We introduced potential fields as an additional cost function in robot
motion planning, but they can also be used for generating robot motion
directly, dispensing with the path planning phase altogether. To achieve
this, we have to define an attractive force that pulls the robot towards
its goal configuration and a repellent potential field that pushes the
robot away from obstacles. Such a potential field is shown in . Its
single global minimum is the goal configuration, and the value is the
sum of the distance to this goal configuration and the proximity to
obstacles. No planning was involved in generating the potential field
shown in the figure. Because of this, potential fields are well suited
to real-time control. (a) shows a trajectory of a robot that performs
hill climbing in the potential field. In many applications, the
potential field can be calculated efficiently for any given
configuration. Moreover, optimizing the potential amounts to calculating
the gradient of the potential for the present robot configuration. These
calculations can be extremely efficient, especially when compared to
path-planning algorithms, all of which are exponential in the
dimensionality of the configuration space (the DOFs) in the worst case.

The fact that the potential field approach manages to find a path to the
goal in such an efficient manner, even over long distances in
configuration space, raises the question as to whether there is a need
for planning in robotics at all. Are potential field techniques
sufficient, or were we just lucky in our example? The answer is that we
were indeed lucky. Potential fields have many local minima that can trap
the robot. In (b), the robot approaches the obstacle by simply rotating
its shoulder joint, until it gets stuck on the wrong side of the
obstacle. The potential field is not rich enough to make the robot bend
its elbow so that the arm fits under the obstacle. In other words,
potential field control is great for local robot motion but sometimes we
still need global planning. Another important drawback with potential
fields is that the forces they generate depend only on the obstacle and
robot positions, not on the robot’s velocity. Thus, potential field
control is really a kinematic method and may fail if the robot is moving
quickly.

### Reactive control {#reactive-control-robotics-section}

So far we have considered control decisions that require some model of
the environment for constructing either a reference path or a potential
field. There are some difficulties with this approach. First, models
that are sufficiently accurate are often difficult to obtain, especially
in complex or remote environments, such as the surface of Mars, or for
robots that have few sensors. Second, even in cases where we can devise
a model with sufficient accuracy, computational difficulties and
localization error might render these techniques impractical. In some
cases, a reflex agent architecture using is more appropriate.

[Fig5]

For example, picture a legged robot that attempts to lift a leg over an
obstacle. We could give this robot a rule that says lift the leg a small
height $h$ and move it forward, and if the leg encounters an obstacle,
move it back and start again at a higher height. You could say that $h$
is modeling an aspect of the world, but we can also think of $h$ as an
auxiliary variable of the robot controller, devoid of direct physical
meaning.

One such example is the six-legged (hexapod) robot, shown in (a),
designed for walking through rough terrain. The robot’s sensors are
inadequate to obtain models of the terrain for path planning. Moreover,
even if we added sufficiently accurate sensors, the twelve degrees of
freedom (two for each leg) would render the resulting path planning
problem computationally intractable.

It is possible, nonetheless, to specify a controller directly without an
explicit environmental model. (We have already seen this with the PD
controller, which was able to keep a complex robot arm on target
*without* an explicit model of the robot dynamics; it did,
however, require a reference path generated from a kinematic model.) For
the hexapod robot we first choose a , or pattern of movement of the
limbs. One statically stable gait is to first move the right front,
right rear, and left center legs forward (keeping the other three
fixed), and then move the other three. This gait works well on flat
terrain. On rugged terrain, obstacles may prevent a leg from swinging
forward. This problem can be overcome by a remarkably simple control
rule: *when a leg’s forward motion is blocked, simply retract it,
lift it higher, and try again.* The resulting controller is shown
in (b) as a finite state machine; it constitutes a reflex agent with
state, where the internal state is represented by the index of the
current machine state ($s_1$ through $s_4$).

Variants of this simple feedback-driven controller have been found to
generate remarkably robust walking patterns, capable of maneuvering the
robot over rugged terrain. Clearly, such a controller is model-free, and
it does not deliberate or use search for generating controls.
Environmental feedback plays a crucial role in the controller’s
execution. The software alone does not specify what will actually happen
when the robot is placed in an environment. Behavior that emerges
through the interplay of a (simple) controller and a (complex)
environment is often referred to as . Strictly speaking, all robots
discussed in this chapter exhibit emergent behavior, due to the fact
that no model is perfect. Historically, however, the term has been
reserved for control techniques that do not utilize explicit
environmental models. Emergent behavior is also characteristic of
biological organisms.

### Reinforcement learning control

[heli]

One particularly exciting form of control is based on the form of
reinforcement learning (see ). This work has been enormously influential
in recent years, at is has solved challenging robotics problems for
which previously no solution existed. An example is acrobatic autonomous
helicopter flight. shows an autonomous flip of a small RC
(radio-controlled) helicopter. This maneuver is challenging due to the
highly nonlinear nature of the aerodynamics involved. Only the most
experienced of human pilots are able to perform it. Yet a policy search
method (as described in ), using only a few minutes of computation,
learned a policy that can safely execute a flip every time.

Policy search needs an accurate model of the domain before it can find a
policy. The input to this model is the state of the helicopter at time
$t$, the controls at time $t$, and the resulting state at time
$t+\Delta t$. The state of a helicopter can be described by the 3D
coordinates of the vehicle, its yaw, pitch, and roll angles, and the
rate of change of these six variables. The controls are the manual
controls of of the helicopter: throttle, pitch, elevator, aileron, and
rudder. All that remains is the resulting state—how are we going to
define a model that accurately says how the helicopter responds to each
control? The answer is simple: Let an expert human pilot fly the
helicopter, and record the controls that the expert transmits over the
radio and the state variables of the helicopter. About four minutes of
human-controlled flight suffices to build a predictive model that is
sufficiently accurate to simulate the vehicle.

What is remarkable about this example is the ease with which this
learning approach solves a challenging robotics problem. This is one of
the many successes of machine learning in scientific fields previously
dominated by careful mathematical analysis and modeling.

Robotic Software Architectures
------------------------------

A methodology for structuring algorithms is called a . An architecture
includes languages and tools for writing programs, as well as an overall
philosophy for how programs can be brought together.

Modern-day software architectures for robotics must decide how to
combine reactive control and model-based deliberative planning. In many
ways, reactive and deliberate techniques have orthogonal strengths and
weaknesses. Reactive control is sensor-driven and appropriate for making
low-level decisions in real time. However, it rarely yields a plausible
solution at the global level, because global control decisions depend on
information that cannot be sensed at the time of decision making. For
such problems, deliberate planning is a more appropriate choice.

Consequently, most robot architectures use reactive techniques at the
lower levels of control and deliberative techniques at the higher
levels. We encountered such a combination in our discussion of PD
controllers, where we combined a (reactive) PD controller with a
(deliberate) path planner. Architectures that combine reactive and
deliberate techniques are called .

### Subsumption architecture

The  @Brooks:1986 is a framework for assembling reactive controllers out
of finite state machines. Nodes in these machines may contain tests for
certain sensor variables, in which case the execution trace of a finite
state machine is conditioned on the outcome of such a test. Arcs can be
tagged with messages that will be generated when traversing them, and
that are sent to the robot’s motors or to other finite state machines.
Additionally, finite state machines possess internal timers (clocks)
that control the time it takes to traverse an arc. The resulting
machines are refereed to as , or AFSMs, where the augmentation refers to
the use of clocks.

An example of a simple AFSM is the four-state machine shown in (b),
which generates cyclic leg motion for a hexapod walker. This AFSM
implements a cyclic controller, whose execution mostly does not rely on
environmental feedback. The forward swing phase, however, does rely on
sensor feedback. If the leg is stuck, meaning that it has failed to
execute the forward swing, the robot retracts the leg, lifts it up a
little higher, and attempts to execute the forward swing once again.
Thus, the controller is able to *react* to contingencies
arising from the interplay of the robot and its environment.

The subsumption architecture offers additional primitives for
synchronizing AFSMs, and for combining output values of multiple,
possibly conflicting AFSMs. In this way, it enables the programmer to
compose increasingly complex controllers in a bottom-up fashion. In our
example, we might begin with AFSMs for individual legs, followed by an
AFSM for coordinating multiple legs. On top of this, we might implement
higher-level behaviors such as collision avoidance, which might involve
backing up and turning.

The idea of composing robot controllers from AFSMs is quite intriguing.
Imagine how difficult it would be to generate the same behavior with any
of the configuration-space path-planning algorithms described in the
previous section. First, we would need an accurate model of the terrain.
The configuration space of a robot with six legs, each of which is
driven by two independent motors, totals eighteen dimensions (twelve
dimensions for the configuration of the legs, and six for the location
and orientation of the robot relative to its environment). Even if our
computers were fast enough to find paths in such high-dimensional
spaces, we would have to worry about nasty effects such as the robot
sliding down a slope. Because of such stochastic effects, a single path
through configuration space would almost certainly be too brittle, and
even a PID controller might not be able to cope with such contingencies.
In other words, generating motion behavior deliberately is simply too
complex a problem for present-day robot motion planning algorithms.

Unfortunately, the subsumption architecture has its own problems. First,
the AFSMs are driven by raw sensor input, an arrangement that works if
the sensor data is reliable and contains all necessary information for
decision making, but fails if sensor data has to be integrated in
nontrivial ways over time. Subsumption-style controllers have therefore
mostly been applied to simple tasks, such as following a wall or moving
towards visible light sources. Second, the lack of deliberation makes it
difficult to change the task of the robot. A subsumption-style robot
usually does just one task, and it has no notion of how to modify its
controls to accommodate different goals (just like the on ). Finally,
subsumption-style controllers tend to be difficult to understand. In
practice, the intricate interplay between dozens of interacting AFSMs
(and the environment) is beyond what most human programmers can
comprehend. For all these reasons, the subsumption architecture is
rarely used in robotics, despite its great historical importance.
However, it has had an influence on other architectures, and on
individual components of some architectures.

### Three-layer architecture

Hybrid architectures combine reaction with deliberation. The most
popular hybrid architecture is the , which consists of a reactive layer,
an executive layer, and a deliberative layer.

The provides low-level control to the robot. It is characterized by a
tight sensor–action loop. Its decision cycle is often on the order of
milliseconds.

The (or sequencing layer) serves as the glue between the reactive layer
and the deliberative layer. It accepts directives by the deliberative
layer, and sequences them for the reactive layer. For example, the
executive layer might handle a set of via-points generated by a
deliberative path planner, and make decisions as to which reactive
behavior to invoke. Decision cycles at the executive layer are usually
in the order of a second. The executive layer is also responsible for
integrating sensor information into an internal state representation.
For example, it may host the robot’s localization and online mapping
routines.

The generates global solutions to complex tasks using planning. Because
of the computational complexity involved in generating such solutions,
its decision cycle is often in the order of minutes. The deliberative
layer (or planning layer) uses models for decision making. Those models
might be either learned from data or supplied and may utilize state
information gathered at the executive layer.

Variants of the three-layer architecture can be found in most modern-day
robot software systems. The decomposition into three layers is not very
strict. Some robot software systems possess additional layers, such as
user interface layers that control the interaction with people, or a
multiagent level for coordinating a robot’s actions with that of other
robots operating in the same environment.

### Pipeline architecture

[pipeline]

Another architecture for robots is known as the . Just like the
subsumption architecture, the pipeline architecture executes multiple
process in parallel. However, the specific modules in this architecture
resemble those in the three-layer architecture.

shows an example pipeline architecture, which is used to control an
autonomous car. Data enters this pipeline at the . The then updates the
robot’s internal models of the environment based on this data. Next,
these models are handed to the , which adjusts the robot’s internal
plans turns them into actual controls for the robot. Those are then
communicated back to the vehicle through the .

The key to the pipeline architecture is that this all happens in
parallel. While the perception layer processes the most recent sensor
data, the control layer bases its choices on slightly older data. In
this way, the pipeline architecture is similar to the human brain. We
don’t switch off our motion controllers when we digest new sensor data.
Instead, we perceive, plan, and act all at the same time. Processes in
the pipeline architecture run asynchronously, and all computation is
data-driven. The resulting system is robust, and it is fast.

The architecture in also contains other, cross-cutting modules,
responsible for establishing communication between the different
elements of the pipeline.

Application Domains
-------------------

[Fig7]

Here are some of the prime application domains for robotic technology.

**Industry and Agriculture.** Traditionally, robots have
been fielded in areas that require difficult human labor, yet are
structured enough to be amenable to robotic automation. The best example
is the assembly line, where manipulators routinely perform tasks such as
assembly, part placement, material handling, welding, and painting. In
many of these tasks, robots have become more cost-effective than human
workers. Outdoors, many of the heavy machines that we use to harvest,
mine, or excavate earth have been turned into robots. For example, a
project at Carnegie Mellon University has demonstrated that robots can
strip paint off large ships about 50 times faster than people can, and
with a much reduced environmental impact. Prototypes of autonomous
mining robots have been found to be faster and more precise than people
in transporting ore in underground mines. Robots have been used to
generate high-precision maps of abandoned mines and sewer systems. While
many of these systems are still in their prototype stages, it is only a
matter of time until robots will take over much of the semimechanical
work that is presently performed by people.

**Transportation.** Robotic transportation has many facets:
from autonomous helicopters that deliver payloads to hard-to-reach
locations, to automatic wheelchairs that transport people who are unable
to control wheelchairs by themselves, to autonomous straddle carriers
that outperform skilled human drivers when transporting containers from
ships to trucks on loading docks. A prime example of indoor
transportation robots, or gofers, is the Helpmate robot shown in (a).
This robot has been deployed in dozens of hospitals to transport food
and other items. In factory settings, autonomous vehicles are now
routinely deployed to transport goods in warehouses and between
production lines. The Kiva system, shown in (b), helps workers at
fulfillment centers package goods into shipping containers.

Many of these robots require environmental modifications for their
operation. The most common modifications are localization aids such as
inductive loops in the floor, active beacons, or barcode tags. An open
challenge in robotics is the design of robots that can use natural cues,
instead of artificial devices, to navigate, particularly in environments
such as the deep ocean where GPS is unavailable.

[boss-surgery-figure]

**Robotic cars.** Most of use cars every day. Many of us
make cell phone calls while driving. Some of us even text. The sad
result: more than a million people die every year in traffic accidents.
Robotic cars like and offer hope: Not only will they make driving much
safer, but they will also free us from the need to pay attention to the
road during our daily commute.

Progress in robotic cars was stimulated by the , a race over 100 miles
of unrehearsed desert terrain, which represented a much more challenging
task than had ever been accomplished before. Stanford’s vehicle
completed the course in less than seven hours in 2005, winning a \$2
million prize and a place in the National Museum of American History.
(a) depicts , which in 2007 won the DARPA Urban Challenge, a complicated
road race on city streets where robots faced other robots and had to
obey traffic rules.

**Health care.** Robots are increasingly used to assist
surgeons with instrument placement when operating on organs as intricate
as brains, eyes, and hearts. (b) shows such a system. Robots have become
indispensable tools in a range of surgical procedures, such as hip
replacements, thanks to their high precision. In pilot studies, robotic
devices have been found to reduce the danger of lesions when performing
colonoscopy. Outside the operating room, researchers have begun to
develop robotic aides for elderly and handicapped people, such as
intelligent robotic walkers and intelligent toys that provide reminders
to take medication and provide comfort. Researchers are also working on
robotic devices for rehabilitation that aid people in performing certain
exercises.

**Hazardous environments.** Robots have assisted people in
cleaning up nuclear waste, most notably in Chernobyl and Three Mile
Island. Robots were present after the collapse of the World Trade
Center, where they entered structures deemed too dangerous for human
search and rescue crews.

Some countries have used robots to transport ammunition and to defuse
bombs—a notoriously dangerous task. A number of research projects are
presently developing prototype robots for clearing minefields, on land
and at sea. Most existing robots for these tasks are teleoperated—a
human operates them by remote control. Providing such robots with
autonomy is an important next step.

[FigMine]

**Exploration.** Robots have gone where no one has gone
before, including the surface of Mars (see (b) and the cover). Robotic
arms assist astronauts in deploying and retrieving satellites and in
building the International Space Station. Robots also help explore under
the sea. They are routinely used to acquire maps of sunken ships. shows
a robot mapping an abandoned coal mine, along with a 3D model of the
mine acquired using range sensors. In 1996, a team of researches
released a legged robot into the crater of an active volcano to acquire
data for climate research. Unmanned air vehicles known as are used in
military operations. Robots are becoming very effective tools for
gathering information in domains that are difficult (or dangerous) for
people to access.

[roomba]

**Personal Services.** Service is an up-and-coming
application domain of robotics. Service robots assist individuals in
performing daily tasks. Commercially available domestic service robots
include autonomous vacuum cleaners, lawn mowers, and golf caddies. The
world’s most popular mobile robot is a personal service robot: the
robotic vacuum cleaner , shown in (a). More than three million Roombas
have been sold. Roomba can navigate autonomously and perform its tasks
without human help.

Other service robots operate in public places, such as robotic
information kiosks that have been deployed in shopping malls and trade
fairs, or in museums as tour guides. Service tasks require human
interaction, and the ability to cope robustly with unpredictable and
dynamic environments.

**Entertainment.** Robots have begun to conquer the
entertainment and toy industry. In (b) we see , a competitive game very
much like human soccer, but played with autonomous mobile robots. Robot
soccer provides great opportunities for research in AI, since it raises
a range of problems relevant to many other, more serious robot
applications. Annual robotic soccer competitions have attracted large
numbers of AI researchers and added a lot of excitement to the field of
robotics.

**Human augmentation.** A final application domain of
robotic technology is that of human augmentation. Researchers have
developed legged walking machines that can carry people around, very
much like a wheelchair. Several research efforts presently focus on the
development of devices that make it easier for people to walk or move
their arms by providing additional forces through extraskeletal
attachments. If such devices are attached permanently, they can be
thought of as artificial robotic limbs. (b) shows a robotic hand that
may serve as a prosthetic device in the future.

Robotic teleoperation, or telepresence, is another form of human
augmentation. Teleoperation involves carrying out tasks over long
distances with the aid of robotic devices. A popular configuration for
robotic teleoperation is the master–slave configuration, where a robot
manipulator emulates the motion of a remote human operator, measured
through a haptic interface. Underwater vehicles are often teleoperated;
the vehicles can go to a depth that would be dangerous for humans but
can still be guided by the human operator. All these systems augment
people’s ability to interact with their environments. Some projects go
as far as replicating humans, at least at a very superficial level.
Humanoid robots are now available commercially through several companies
in Japan.

Robotics concerns itself with intelligent agents that manipulate the
physical world. In this chapter, we have learned the following basics of
robot hardware and software.

-   Robots are equipped with for perceiving their environment and
    effectors with which they can assert physical forces on their
    environment. Most robots are either manipulators anchored at fixed
    locations or mobile robots that can move.

-   Robotic perception concerns itself with estimating decision-relevant
    quantities from sensor data. To do so, we need an internal
    representation and a method for updating this internal
    representation over time. Common examples of hard perceptual
    problems include .

-   such as Kalman filters and particle filters are useful for robot
    perception. These techniques maintain the belief state, a posterior
    distribution over state variables.

-   The planning of robot motion is usually done in , where each point
    specifies the location and orientation of the robot and its joint
    angles.

-   Configuration space search algorithms include techniques, which
    decompose the space of all configurations into finitely many cells,
    and techniques, which project configuration spaces onto
    lower-dimensional manifolds. The motion planning problem is then
    solved using search in these simpler structures.

-   A path found by a search algorithm can be executed by using the path
    as the reference trajectory for a . Controllers are necessary in
    robotics to accommodate small perturbations; path planning alone is
    usually insufficient.

-   techniques navigate robots by potential functions, defined over the
    distance to obstacles and the goal location. Potential field
    techniques may get stuck in local minima, but they can generate
    motion directly without the need for path planning.

-   Sometimes it is easier to specify a robot controller directly,
    rather than deriving a path from an explicit model of the
    environment. Such controllers can often be written as simple .

-   There exist different architectures for software design. The enables
    programmers to compose robot controllers from interconnected finite
    state machines. are common frameworks for developing robot software
    that integrate deliberation, sequencing of subgoals, and control.
    The related processes data in parallel through a sequence of
    modules, corresponding to perception, modeling, planning, control,
    and robot interfaces.

The word was popularized by Czech playwright Karel Capek in his 1921
play *R.U.R.* (Rossum’s Universal Robots). The robots,
which were grown chemically rather than constructed mechanically, end up
resenting their masters and decide to take over. It appears @Glanc:1978
it was Capek’s brother, Josef, who first combined the Czech words
“robota” (obligatory work) and “robotnik” (serf) to yield “robot” in his
1917 short story *Opilec*.

The term *robotics* was first used by . Robotics (under
other names) has a much longer history, however. In ancient Greek
mythology, a mechanical man named Talos was supposedly designed and
built by Hephaistos, the Greek god of metallurgy. Wonderful automata
were built in the 18th century—Jacques Vaucanson’s mechanical duck from
1738 being one early example—but the complex behaviors they exhibited
were entirely fixed in advance. Possibly the earliest example of a
programmable robot-like device was the Jacquard loom (1805), described
on .

The first commercial robot was a robot arm called , short for
*universal automation*, developed by Joseph Engelberger and
George Devol. In 1961, the first Unimate robot was sold to General
Motors, where it was used for manufacturing TV picture tubes. 1961 was
also the year when Devol obtained the first U.S. patent on a robot.
Eleven years later, in 1972, Nissan Corp. was among the first to
automate an entire assembly line with robots, developed by Kawasaki with
robots supplied by Engelberger and Devol’s company Unimation. This
development initiated a major revolution that took place mostly in Japan
and the U.S., and that is still ongoing. Unimation followed up in 1978
with the development of the robot, short for Programmable Universal
Machine for Assembly. The PUMA robot, initially developed for General
Motors, was the *de facto* standard for robotic
manipulation for the two decades that followed. At present, the number
of operating robots is estimated at one million worldwide, more than
half of which are installed in Japan.

The literature on robotics research can be divided roughly into two
parts: mobile robots and stationary manipulators. Grey Walter’s
“turtle,” built in 1948, could be considered the first autonomous mobile
robot, although its control system was not programmable. The “Hopkins
Beast,” built in the early 1960s at Johns Hopkins University, was much
more sophisticated; it had pattern-recognition hardware and could
recognize the cover plate of a standard AC power outlet. It was capable
of searching for outlets, plugging itself in, and then recharging its
batteries! Still, the Beast had a limited repertoire of skills. The
first general-purpose mobile robot was “Shakey,” developed at what was
then the Stanford Research Institute (now SRI) in the late
1960s @Fikes+Nilsson:1971 [@Nilsson:1984]. Shakey was the first robot to
integrate perception, planning, and execution, and much subsequent
research in AI was influenced by this remarkable achievement. Shakey
appears on the cover of this book with project leader Charlie Rosen
(1917–2002). Other influential projects include the Stanford Cart and
the CMU Rover @Moravec83a. describes classic work on autonomous
vehicles.

The field of robotic mapping has evolved from two distinct origins. The
first thread began with work by Smith and Cheeseman [-@Smith86a], who
applied Kalman filters to the simultaneous localization and mapping
problem. This algorithm was first implemented by Moutarlier and
Chatila [-@Moutarlier89b], and later extended by ; see for an overview
of early Kalman filter variations. The second thread began with the
development of the representation for probabilistic mapping, which
specifies the probability that each $(x,y)$ location is occupied by an
obstacle @Moravec+Elfes:1985. were among the first to propose
topological rather than metric mapping, motivated by models of human
spatial cognition. A seminal paper by recognized the sparseness of the
simultaneous localization and mapping problem, which gave rise to the
development of nonlinear optimization techniques by and , as well as
hierarchical methods by . and introduced the EM algorithm into the field
of robotic mapping for data association. An overview of probabilistic
mapping methods can be found in @Thrun05a.

Early mobile robot localization techniques are surveyed by . Although
Kalman filtering was well known as a localization method in control
theory for decades, the general probabilistic formulation of the
localization problem did not appear in the AI literature until much
later, through the work of Tom Dean and colleagues @Dean+al:1990
[@Dean+al:1990b] and of . The latter work introduced the term . The
first real-world application of this technique was by , through a series
of robots that were deployed in museums. Monte Carlo localization based
on particle filters was developed by and is now widely used. The
combines particle filtering for robot localization with exact filtering
for map building @Murphy+Russell:2001 [@Montemerlo+al:2002].

The study of manipulator robots, originally called , has evolved along
quite different lines. The first major effort at creating a hand–eye
machine was Heinrich Ernst’s MH-1, described in his MIT Ph.D. thesis
@Ernst:1961. The Machine Intelligence project at Edinburgh also
demonstrated an impressive early system for vision-based assembly called
 @Michie:1972. After these pioneering efforts, a great deal of work
focused on geometric algorithms for deterministic and fully observable
motion planning problems. The PSPACE-hardness of robot motion planning
was shown in a seminal paper by Reif [-@Reif:1979]. The configuration
space representation is due to . A series of papers by Schwartz and
Sharir on what they called problems @Schwartz87a was highly influential.

Recursive cell decomposition for configuration space planning was
originated by and improved significantly by . The earliest
skeletonization algorithms were based on Voronoi diagrams @Rowat79a and
 @Wesley79a. developed efficient techniques for calculating Voronoi
diagrams incrementally, and generalized Voronoi diagrams to broader
motion-planning problems. John Canny [-@Canny:1988] established the
first singly exponential algorithm for motion planning. The seminal text
by Latombe [-@Latombe:1991] covers a variety of approaches to
motion-planning, as do the texts by and . developed probabilistic
roadmaps, which are currently one of the most effective methods.
Fine-motion planning with limited sensing was investigated by and .
Landmark-based navigation @Lazanas+Latombe:1992 uses many of the same
ideas in the mobile robot arena. Key work applying POMDP methods () to
motion planning under uncertainty in robotics is due to and .

The control of robots as dynamical systems—whether for manipulation or
navigation—has generated a huge literature that is barely touched on by
this chapter. Important works include a trilogy on impedance control by
Hogan [-@Hogan85a] and a general study of robot dynamics by . were among
the first to try to tie together control theory and AI planning systems.
Three classic textbooks on the mathematics of robot manipulation are due
to Paul [-@Paul:1981], Craig [-@Craig89a], and
Yoshikawa [-@Yoshikawa:1990]. The area of is also important in
robotics—the problem of determining a stable grasp is quite
difficult @Mason85a. Competent grasping requires touch sensing, or , to
determine contact forces and detect slip @Fearing+Hollerbach:1985.

Potential-field control, which attempts to solve the motion planning and
control problems simultaneously, was introduced into the robotics
literature by . In mobile robotics, this idea was viewed as a practical
solution to the collision avoidance problem, and was later extended into
an algorithm called by Borenstein [-@Borenstein91a]. Navigation
functions, the robotics version of a control policy for deterministic
MDPs, were introduced by . Reinforcement learning in robotics took off
with the seminal work by and , who developed the paradigm in the context
of autonomous helicopter control.

The topic of software architectures for robots engenders much religious
debate. The good old-fashioned AI candidate—the three-layer
architecture—dates back to the design of Shakey and is reviewed by
Gat [-@Gat96b]. The subsumption architecture is due to
Brooks [-@Brooks:1986], although similar ideas were developed
independently by , whose book, *Vehicles*, describes a
series of simple robots based on the behavioral approach. The success of
Brooks’s six-legged walking robot was followed by many other projects.
Connell, in his Ph.D. thesis [-@Connell89a], developed a mobile robot
capable of retrieving objects that was entirely reactive. Extensions of
the behavior-based paradigm to multirobot systems can be found
in @Mataric:1997 and @Parker96a. @Horswill00a and @Konolige97a abstract
the ideas of concurrent behavior-based robotics into general robot
control languages. surveys some of the most popular approaches in this
field.

Research on mobile robotics has been stimulated over the last decade by
several important competitions. The earliest competition, AAAI’s annual
mobile robot competition, began in 1992. The first competition winner
was  @Congdon92a. Progress has been steady and impressive: in more
recent competitions robots entered the conference complex, found their
way to the registration desk, registered for the conference, and even
gave a short talk. The competition, launched in 1995 by Kitano and
colleagues [-@Kitano97a], aims to “develop a team of fully autonomous
humanoid robots that can win against the human world champion team in
soccer” by 2050. Play occurs in leagues for simulated robots, wheeled
robots of different sizes, and humanoid robots. In 2009 teams from 43
countries participated and the event was broadcast to millions of
viewers. track the improvements that have been made in perception, team
coordination, and low-level skills over the past decade.

The , organized by DARPA in 2004 and 2005, required autonomous robots to
travel more than 100 miles through unrehearsed desert terrain in less
than 10 hours @Buehler06a. In the original event in 2004, no robot
traveled more than 8 miles, leading many to believe the prize would
never be claimed. In 2005, Stanford’s robot won the competition in just
under 7 hours of travel @Thrun+al:2006. DARPA then organized the , a
competition in which robots had to navigate 60 miles in an urban
environment with other traffic. Carnegie Mellon University’s robot took
first place and claimed the \$2 million prize @Urmson08a. Early pioneers
in the development of robotic cars included and .

Two early textbooks, by and Murphy [-@Murphy00a], cover robotics
generally. A more recent overview is due to . An excellent book on robot
manipulation addresses advanced topics such as compliant
motion @Mason01a. Robot motion planning is covered in and . provide an
introduction into probabilistic robotics. The premiere conference for
robotics is Robotics: Science and Systems Conference,
followed by the IEEE International Conference on Robotics and
Automation. Leading robotics journals include *IEEE
Robotics and Automation*, the *International Journal of
Robotics Research*, and *Robotics and Autonomous
Systems*.

[mcl-biasdness-exercise]Monte Carlo localization is
*biased* for any finite sample size—i.e., the expected
value of the location computed by the algorithm differs from the true
expected value—because of the way particle filtering works. In this
question, you are asked to quantify this bias.

To simplify, consider a world with four possible robot locations:
$X=\{x_\N{1},x_\N{2},x_\N{3},x_\N{4}\}$. Initially, we draw
$N\geq \N{1}$ samples uniformly from among those locations. As usual, it
is perfectly acceptable if more than one sample is generated for any of
the locations $X$. Let $Z$ be a Boolean sensor variable characterized by
the following conditional probabilities:

$$\begin{aligned}
P(z\mid x_\N{1}) &=& \N{{0.8}} \qquad\qquad P(\lnot z\mid x_\N{1})\;\;=\;\;\N{{0.2}} \\
P(z\mid x_\N{2}) &=& \N{{0.4}} \qquad\qquad P(\lnot z\mid x_\N{2})\;\;=\;\;\N{{0.6}} \\
P(z\mid x_\N{3}) &=& \N{{0.1}} \qquad\qquad P(\lnot z\mid x_\N{3})\;\;=\;\;\N{{0.9}} \\
P(z\mid x_\N{4}) &=& \N{{0.1}} \qquad\qquad P(\lnot z\mid x_\N{4})\;\;=\;\;\N{{0.9}}\ .\end{aligned}$$

MCL uses these probabilities to generate particle weights, which are
subsequently normalized and used in the resampling process. For
simplicity, let us assume we generate only one new sample in the
resampling process, regardless of $N$. This sample might correspond to
any of the four locations in $X$. Thus, the sampling process defines a
probability distribution over $X$.

1.  What is the resulting probability distribution over $X$ for this new
    sample? Answer this question separately for
    $N=\N{1},\ldots,\N{{10}}$, and for $N=\infty$.

2.  The difference between two probability distributions $P$ and $Q$ can
    be measured by the KL divergence, which is defined as
    $${KL}(P,Q) = \sum_i P(x_i)\log\frac{P(x_i)}{Q(x_i)}\ .$$ What are
    the KL divergences between the distributions in (a) and the true
    posterior?

3.  What modification of the problem formulation (not the algorithm!)
    would guarantee that the specific estimator above is unbiased even
    for finite values of $N$? Provide at least two such modifications
    (each of which should be sufficient).

[mcl-implement-exercise]Monte Carlo localization for a simulated robot
with range sensors. A grid map and range data are available from the
code repository at [aima.cs.berkeley.edu](aima.cs.berkeley.edu). You
should demonstrate successful global localization of the robot.

[figRobot2]

[AB-manipulator-ex] Consider a robot with two simple manipulators, as
shown in figure [figRobot2]. Manipulator A is a square block of side 2
which can slide back and on a rod that runs along the x-axis from
x=$-$10 to x=10. Manipulator B is a square block of side 2 which can
slide back and on a rod that runs along the y-axis from y=$-$10 to y=10.
The rods lie outside the plane of manipulation, so the rods do not
interfere with the movement of the blocks. A configuration is then a
pair $\la x,y\ra$ where $x$ is the x-coordinate of the center of
manipulator A and where $y$ is the y-coordinate of the center of
manipulator B. Draw the configuration space for this robot, indicating
the permitted and excluded zones.

Suppose that you are working with the robot in and you are given the
problem of finding a path from the starting configuration of
figure [figRobot2] to the ending configuration. Consider a potential
function $$D(A, {Goal})^2 + D(B, {Goal})^2 + \frac{1}{D(A, B)^2}$$
where $D(A,B)$ is the distance between the closest points of A and B.

1.  Show that hill climbing in this potential field will get stuck in a
    local minimum.

2.  Describe a potential field where hill climbing will solve this
    particular problem. You need not work out the exact numerical
    coefficients needed, just the general form of the solution. (Hint:
    Add a term that \`\`rewards" the hill climber for moving A out of
    B’s way, even in a case like this where this does not reduce the
    distance from A to B in the above sense.)

[inverse-kinematics-exercise]Consider the robot arm shown in . Assume
that the robot’s base element is 60cm long and that its upper arm and
forearm are each 40cm long. As argued on , the inverse kinematics of a
robot is often not unique. State an explicit closed-form solution of the
inverse kinematics for this arm. Under what exact conditions is the
solution unique?

[inverse-kinematics-exercise]Consider the robot arm shown in . Assume
that the robot’s base element is 70cm long and that its upper arm and
forearm are each 50cm long. As argued on , the inverse kinematics of a
robot is often not unique. State an explicit closed-form solution of the
inverse kinematics for this arm. Under what exact conditions is the
solution unique?

[voronoi-exercise]an algorithm for calculating the Voronoi diagram of an
arbitrary 2D environment, described by an $n\stimes n$ Boolean array.
Illustrate your algorithm by plotting the Voronoi diagram for 10
interesting maps. What is the complexity of your algorithm?

[confspace-exercise]This exercise explores the relationship between
workspace and configuration space using the examples shown in .

1.  Consider the robot configurations shown in (a) through (c), ignoring
    the obstacle shown in each of the diagrams. Draw the corresponding
    arm configurations in configuration space. (*Hint:*
    Each arm configuration maps to a single point in configuration
    space, as illustrated in (b).)

2.  Draw the configuration space for each of the workspace diagrams in
    (a)–(c). (*Hint:* The configuration spaces share with
    the one shown in (a) the region that corresponds to self-collision,
    but differences arise from the lack of enclosing obstacles and the
    different locations of the obstacles in these individual figures.)

3.  For each of the black dots in (e)–(f), draw the corresponding
    configurations of the robot arm in workspace. Please ignore the
    shaded regions in this exercise.

4.  The configuration spaces shown in (e)–(f) have all been generated by
    a single workspace obstacle (dark shading), plus the constraints
    arising from the self-collision constraint (light shading). Draw,
    for each diagram, the workspace obstacle that corresponds to the
    darkly shaded area.

5.  ​(d) illustrates that a single planar obstacle can decompose the
    workspace into two disconnected regions. What is the maximum number
    of disconnected regions that can be created by inserting a planar
    obstacle into an obstacle-free, connected workspace, for a 2DOF
    robot? Give an example, and argue why no larger number of
    disconnected regions can be created. How about a non-planar
    obstacle?

[FigEx2]

Consider a mobile robot moving on a horizontal surface. Suppose that the
robot can execute two kinds of motions:

-   Rolling forward a specified distance.

-   Rotating in place through a specified angle.

The state of such a robot can be characterized in terms of three
parameters $\la x,y,\phi$, the x-coordinate and y-coordinate of the
robot (more precisely, of its center of rotation) and the robot’s
orientation expressed as the angle from the positive x direction. The
action “$Roll(D)$” has the effect of changing state $\la x,y,\phi$ to
$\la x+D \cos(\phi), y+D \sin(\phi), \phi \ra$, and the action
$Rotate(\theta)$ has the effect of changing state $\la x,y,\phi \ra$ to
$\la x,y, \phi + \theta \ra$.

1.  Suppose that the robot is initially at $\la 0,0,0 \ra$ and then
    executes the actions $Rotate(60^{\circ})$, $Roll(1)$,
    $Rotate(25^{\circ})$, $Roll(2)$. What is the final state of the
    robot?

2.  Now suppose that the robot has imperfect control of its own
    rotation, and that, if it attempts to rotate by $\theta$, it may
    actually rotate by any angle between $\theta-10^{\circ}$ and
    $\theta+10^{\circ}$. In that case, if the robot attempts to carry
    out the sequence of actions in (A), there is a range of possible
    ending states. What are the minimal and maximal values of the
    x-coordinate, the y-coordinate and the orientation in the final
    state?

3.  Let us modify the model in (B) to a probabilistic model in which,
    when the robot attempts to rotate by $\theta$, its actual angle of
    rotation follows a Gaussian distribution with mean $\theta$ and
    standard deviation $10^{\circ}$. Suppose that the robot executes the
    actions $Rotate(90^{\circ})$, $Roll(1)$. Give a simple argument that
    (a) the expected value of the location at the end is not equal to
    the result of rotating exactly $90^{\circ}$ and then rolling forward
    1 unit, and (b) that the distribution of locations at the end does
    not follow a Gaussian. (Do not attempt to calculate the true mean or
    the true distribution.)

    The point of this exercise is that rotational uncertainty quickly
    gives rise to a lot of positional uncertainty and that dealing with
    rotational uncertainty is painful, whether uncertainty is treated in
    terms of hard intervals or probabilistically, due to the fact that
    the relation between orientation and position is both non-linear and
    non-monotonic.

[FigEx3]

[robot-exploration-exercise]Consider the simplified robot shown in .
Suppose the robot’s Cartesian coordinates are known at all times, as are
those of its goal location. However, the locations of the obstacles are
unknown. The robot can sense obstacles in its immediate proximity, as
illustrated in this figure. For simplicity, let us assume the robot’s
motion is noise-free, and the state space is discrete. is only one
example; in this exercise you are required to address all possible grid
worlds with a valid path from the start to the goal location.

1.  Design a deliberate controller that guarantees that the robot always
    reaches its goal location if at all possible. The deliberate
    controller can memorize measurements in the form of a map that is
    being acquired as the robot moves. Between individual moves, it may
    spend arbitrary time deliberating.

2.  Now design a *reactive* controller for the same task.
    This controller may not memorize past sensor measurements. (It may
    not build a map!) Instead, it has to make all decisions based on the
    current measurement, which includes knowledge of its own location
    and that of the goal. The time to make a decision must be
    independent of the environment size or the number of past time
    steps. What is the maximum number of steps that it may take for your
    robot to arrive at the goal?

3.  How will your controllers from (a) and (b) perform if any of the
    following six conditions apply: continuous state space, noise in
    perception, noise in motion, noise in both perception and motion,
    unknown location of the goal (the goal can be detected only when
    within sensor range), or moving obstacles. For each condition and
    each controller, give an example of a situation where the robot
    fails (or explain why it cannot fail).

[subsumption-exercise]In (b) on , we encountered an augmented finite
state machine for the control of a single leg of a hexapod robot. In
this exercise, the aim is to design an AFSM that, when combined with six
copies of the individual leg controllers, results in efficient, stable
locomotion. For this purpose, you have to augment the individual leg
controller to pass messages to your new AFSM and to wait until other
messages arrive. Argue why your controller is efficient, in that it does
not unnecessarily waste energy (e.g., by sliding legs), and in that it
propels the robot at reasonably high speeds. Prove that your controller
satisfies the dynamic stability condition given on .

[human-robot-exercise](This exercise was first devised by Michael
Genesereth and Nils Nilsson. It works for first graders through graduate
students.) Humans are so adept at basic household tasks that they often
forget how complex these tasks are. In this exercise you will discover
the complexity and recapitulate the last 30 years of developments in
robotics. Consider the task of building an arch out of three blocks.
Simulate a robot with four humans as follows:

**Brain.** The Brain direct the hands in the execution of a
plan to achieve the goal. The Brain receives input from the Eyes, but
*cannot see the scene directly*. The brain is the only one
who knows what the goal is.

**Eyes.** The Eyes report a brief description of the scene
to the Brain: “There is a red box standing on top of a green box, which
is on its side” Eyes can also answer questions from the Brain such as,
“Is there a gap between the Left Hand and the red box?” If you have a
video camera, point it at the scene and allow the eyes to look at the
viewfinder of the video camera, but not directly at the scene.

**Left hand** and **right hand.** One person
plays each Hand. The two Hands stand next to each other, each wearing an
oven mitt on one hand, Hands execute only simple commands from the
Brain—for example, “Left Hand, move two inches forward.” They cannot
execute commands other than motions; for example, they cannot be
commanded to “Pick up the box.” The Hands must be
*blindfolded*. The only sensory capability they have is the
ability to tell when their path is blocked by an immovable obstacle such
as a table or the other Hand. In such cases, they can beep to inform the
Brain of the difficulty.

[^1]: In we talked about **actuators**, not effectors. Here
    we distinguish the effector (the physical device) from the actuator
    (the control line that communicates a command to the effector).

[^2]: “Kinematic” is from the Greek word for *motion*, as
    is “cinema.”
[search-part]

Solving Problems by Searching {#search-chapter}
=============================

The simplest agents discussed in were the reflex agents, which base
their actions on a direct mapping from states to actions. Such agents
cannot operate well in environments for which this mapping would be too
large to store and would take too long to learn. Goal-based agents, on
the other hand, consider future actions and the desirability of their
outcomes.

This chapter describes one kind of goal-based agent called a .
Problem-solving agents use representations, as described in —that is,
states of the world are considered as wholes, with no internal structure
visible to the problem-solving algorithms. Goal-based agents that use
more advanced or representations are usually called and are discussed in
Chapters [knowledge+logic-chapter] and [planning-chapter].

Our discussion of problem solving begins with precise definitions of and
their and give several examples to illustrate these definitions. We then
describe several general-purpose search algorithms that can be used to
solve these problems. We will see several search algorithms—algorithms
that are given no information about the problem other than its
definition. Although some of these algorithms can solve any solvable
problem, none of them can do so efficiently. search algorithms, on the
other hand, can do quite well given some guidance on where to look for
solutions.

In this chapter, we limit ourselves to the simplest kind of task
environment, for which the solution to a problem is always a
*fixed sequence* of actions. The more general case—where
the agent’s future actions may vary depending on future percepts—is
handled in .

This chapter uses the concepts of asymptotic complexity (that is, $O()$
notation) and NP-completeness. Readers unfamiliar with these concepts
should consult .

Problem-Solving Agents
----------------------

[problem-solving-agent-section]

Intelligent agents are supposed to maximize their performance measure.
As we mentioned in , achieving this is sometimes simplified if the agent
can adopt a and aim at satisfying it. Let us first look at why and how
an agent might do this.

Imagine an agent in the city of Arad, Romania, enjoying a touring
holiday. The agent’s performance measure contains many factors: it wants
to improve its suntan, improve its Romanian, take in the sights, enjoy
the nightlife (such as it is), avoid hangovers, and so on. The decision
problem is a complex one involving many tradeoffs and careful reading of
guidebooks. Now, suppose the agent has a nonrefundable ticket to fly out
of Bucharest the following day. In that case, it makes sense for the
agent to adopt the of getting to Bucharest. Courses of action that don’t
reach Bucharest on time can be rejected without further consideration
and the agent’s decision problem is greatly simplified. Goals help
organize behavior by limiting the objectives that the agent is trying to
achieve and hence the actions it needs to consider. , based on the
current situation and the agent’s performance measure, is the first step
in problem solving.

We will consider a goal to be a set of world states—exactly those states
in which the goal is satisfied. The agent’s task is to find out how to
act, now and in the future, so that it reaches a goal state. Before it
can do this, it needs to decide (or we need to decide on its behalf)
what sorts of actions and states it should consider. If it were to
consider actions at the level of “move the left foot forward an inch” or
“turn the steering wheel one degree left,” the agent would probably
never find its way out of the parking lot, let alone to Bucharest,
because at that level of detail there is too much uncertainty in the
world and there would be too many steps in a solution. is the process of
deciding what actions and states to consider, given a goal. We discuss
this process in more detail later. For now, let us assume that the agent
will consider actions at the level of driving from one major town to
another. Each state therefore corresponds to being in a particular town.

Our agent has now adopted the goal of driving to Bucharest and is
considering where to go from Arad. Three roads lead out of Arad, one
toward Sibiu, one to Timisoara, and one to Zerind. None of these
achieves the goal, so unless the agent is familiar with the geography of
Romania, it will not know which road to follow.[^1] In other words, the
agent will not know which of its possible actions is best, because it
does not yet know enough about the state that results from taking each
action. If the agent has no additional information—i.e., if the
environment is in the sense defined in —then it is has no choice but to
try one of the actions at random. This sad situation is discussed in .

But suppose the agent has a of Romania. The point of a map is to provide
the agent with information about the states it might get itself into and
the actions it can take. The agent can use this information to consider
*subsequent* stages of a hypothetical journey via each of
the three towns, trying to find a journey that eventually gets to
Bucharest. Once it has found a path on the map from Arad to Bucharest,
it can achieve its goal by carrying out the driving actions that
correspond to the legs of the journey. In general,

an agent with several immediate options of unknown value can decide what
to do by first examining *future* actions that eventually
lead to states of known value.

To be more specific about what we mean by “examining future actions,” we
have to be more specific about properties of the environment, as defined
in . For now, we assume that the environment is , so the agent always
knows the current state. For the agent driving in Romania, it’s
reasonable to suppose that each city on the map has a sign indicating
its presence to arriving drivers. We also assume the environment is , so
at any given state there are only finitely many actions to choose from.
This is true for navigating in Romania because each city is connected to
a small number of other cities. We will assume the environment is , so
the agent knows which states are reached by each action. (Having an
accurate map suffices to meet this condition for navigation problems.)
Finally, we assume that the environment is , so each action has exactly
one outcome. Under ideal conditions, this is true for the agent in
Romania—it means that if it chooses to drive from Arad to Sibiu, it does
end up in Sibiu. Of course, conditions are not always ideal, as we show
in .

Under these assumptions, the solution to any problem is a fixed sequence
of actions.

“Of course!” one might say, “What else could it be?” Well, in general it
could be a branching strategy that recommends different actions in the
future depending on what percepts arrive. For example, under less than
ideal conditions, the agent might plan to drive from Arad to Sibiu and
then to Rimnicu Vilcea but may also need to have a contingency plan in
case it arrives by accident in Zerind instead of Sibiu. Fortunately, if
the agent knows the initial state and the environment is known and
deterministic, it knows exactly where it will be after the first action
and what it will perceive. Since only one percept is possible after the
first action, the solution can specify only one possible second action,
and so on.

The process of looking for a sequence of actions that reaches the goal
is called . A search algorithm takes a problem as input and returns a in
the form of an action sequence. Once a solution is found, the actions it
recommends can be carried out. This is called the phase. Thus, we have a
simple “formulate, search, execute” design for the agent, as shown in .
After formulating a goal and a problem to solve, the agent calls a
search procedure to solve it. It then uses the solution to guide its
actions, doing whatever the solution recommends as the next thing to
do—typically, the first action of the sequence—and then removing that
step from the sequence. Once the solution has been executed, the agent
will formulate a new goal.

Notice that while the agent is executing the solution sequence it
*ignores its percepts* when choosing an action because it
knows in advance what they will be. An agent that carries out its plans
with its eyes closed, so to speak, must be quite certain of what is
going on. Control theorists call this an system, because ignoring the
percepts breaks the loop between agent and environment.

[simple-PS-agent-algorithm]

We first describe the process of problem formulation, and then devote
the bulk of the chapter to various algorithms for the function. We do
not discuss the workings of the and functions further in this chapter.

### Well-defined problems and solutions

A can be defined formally by five components:

-   The that the agent starts in. For example, the initial state for our
    agent in Romania might be described as ${In}({Arad})$.

-   A description of the possible available to the agent. Given a
    particular state $s$, $(s)$ returns the set of actions that can be
    executed in $s$. We say that each of these actions is in $s$. For
    example, from the state ${In}({Arad})$, the applicable actions
    are
    $\{{Go}({Sibiu}), {Go}({Timisoara}), {Go}({Zerind})\}$.

-   A description of what each action does; the formal name for this is
    the , specified by a function that returns the state that results
    from doing action $a$ in state $s$. We also use the term to refer to
    any state reachable from a given state by a single action.[^2] For
    example, we have
    $$\result{{In}({Arad})}{{Go}({Zerind})} = {In}({Zerind})\ .$$
    Together, the initial state, actions, and transition model
    implicitly define the of the problem—the set of all states reachable
    from the initial state by any sequence of actions. The state space
    forms a directed network or in which the nodes are states and the
    links between nodes are actions. (The map of Romania shown in can be
    interpreted as a state-space graph if we view each road as standing
    for two driving actions, one in each direction.) A in the state
    space is a sequence of states connected by a sequence of actions.

-   The , which determines whether a given state is a goal state.
    Sometimes there is an explicit set of possible goal states, and the
    test simply checks whether the given state is one of them. The
    agent’s goal in Romania is the singleton set
    $\{\v{In}(\v{Bucharest})\}$. Sometimes the goal is specified by an
    abstract property rather than an explicitly enumerated set of
    states. For example, in chess, the goal is to reach a state called
    “checkmate,” where the opponent’s king is under attack and can’t
    escape.

-   A function that assigns a numeric cost to each path. The
    problem-solving agent chooses a cost function that reflects its own
    performance measure. For the agent trying to get to Bucharest, time
    is of the essence, so the cost of a path might be its length in
    kilometers. In this chapter, we assume that the cost of a path can
    be described as the *sum* of the costs of the
    individual actions along the path.[^3] The of taking action $a$ in
    state $s$ to reach state $s'$ is denoted by $c(s,a,s')$. The step
    costs for Romania are shown in as route distances. We assume that
    step costs are nonnegative.[^4][non-negative-g]

The preceding elements define a problem and can be gathered into a
single data structure that is given as input to a problem-solving
algorithm. A to a problem is an action sequence that leads from the
initial state to a goal state. Solution quality is measured by the path
cost function, and an has the lowest path cost among all
solutions.[optimal-soln-page]

[romania-distances-figure]

### Formulating problems

In the preceding section we proposed a formulation of the problem of
getting to Bucharest in terms of the initial state, actions, transition
model, goal test, and path cost. This formulation seems reasonable, but
it is still a *model*—an abstract mathematical
description—and not the real thing. Compare the simple state description
we have chosen, *In(Arad)*, to an actual cross-country
trip, where the state of the world includes so many things: the
traveling companions, the current radio program, the scenery out of the
window, the proximity of law enforcement officers, the distance to the
next rest stop, the condition of the road, the weather, and so on. All
these considerations are left out of our state descriptions because they
are irrelevant to the problem of finding a route to Bucharest. The
process of removing detail from a representation is called .

In addition to abstracting the state description, we must abstract the
actions themselves. A driving action has many effects. Besides changing
the location of the vehicle and its occupants, it takes up time,
consumes fuel, generates pollution, and changes the agent (as they say,
travel is broadening). Our formulation takes into account only the
change in location. Also, there are many actions that we omit
altogether: turning on the radio, looking out of the window, slowing
down for law enforcement officers, and so on. And of course, we don’t
specify actions at the level of “turn steering wheel to the left by one
degree.”

Can we be more precise about defining the appropriate level of
abstraction? Think of the abstract states and actions we have chosen as
corresponding to large sets of detailed world states and detailed action
sequences. Now consider a solution to the abstract problem: for example,
the path from Arad to Sibiu to Rimnicu Vilcea to Pitesti to Bucharest.
This abstract solution corresponds to a large number of more detailed
paths. For example, we could drive with the radio on between Sibiu and
Rimnicu Vilcea, and then switch it off for the rest of the trip. The
abstraction is *valid* if we can expand any abstract
solution into a solution in the more detailed world; a sufficient
condition is that for every detailed state that is “in Arad,” there is a
detailed path to some state that is “in Sibiu,” and so on.[^5] The
abstraction is *useful* if carrying out each of the actions
in the solution is easier than the original problem; in this case they
are easy enough that they can be carried out without further search or
planning by an average driving agent. The choice of a good abstraction
thus involves removing as much detail as possible while retaining
validity and ensuring that the abstract actions are easy to carry out.
Were it not for the ability to construct useful abstractions,
intelligent agents would be completely swamped by the real world.

Example Problems {#example-problem-section}
----------------

The problem-solving approach has been applied to a vast array of task
environments. We list some of the best known here, distinguishing
between *toy* and *real-world* problems. A is
intended to illustrate or exercise various problem-solving methods. It
can be given a concise, exact description and hence is usable by
different researchers to compare the performance of algorithms. A is one
whose solutions people actually care about. Such problems tend not to
have a single agreed-upon description, but we can give the general
flavor of their formulations.

### Toy problems {#toy-problem-section}

The first example we examine is the first introduced in . (See .) This
can be formulated as a problem as follows:

The state is determined by both the agent location and the dirt
locations. The agent is in one of two locations, each of which might or
might not contain dirt. Thus, there are $2 \times 2^2 = 8$ possible
world states. A larger environment with $n$ locations has $n\cdot 2^n$
states.

Any state can be designated as the initial state.

In this simple environment, each state has just three actions: , , and .
Larger environments might also include and .

The actions have their expected effects, except that moving in the
leftmost square, moving in the rightmost square, and ing in a clean
square have no effect. The complete state space is shown in .

This checks whether all the squares are clean.

Each step costs 1, so the path cost is the number of steps in the path.

Compared with the real world, this toy problem has discrete locations,
discrete dirt, reliable cleaning, and it never gets any dirtier. relaxes
some of these assumptions.

[vacuum2-state-space-figure]

The , an instance of which is shown in , consists of a 3$\times$3 board
with eight numbered tiles and a blank space. A tile adjacent to the
blank space can slide into the space. The object is to reach a specified
goal state, such as the one shown on the right of the figure. The
standard formulation is as follows:

A state description specifies the location of each of the eight tiles
and the blank in one of the nine squares.

Any state can be designated as the initial state. Note that any given
goal can be reached from exactly half of the possible initial states ().

The simplest formulation defines the actions as movements of the blank
space , , , or . Different subsets of these are possible depending on
where the blank is.

Given a state and action, this returns the resulting state; for example,
if we apply to the start state in , the resulting state has the 5 and
the blank switched.

This checks whether the state matches the goal configuration shown in .
(Other goal configurations are possible.)

Each step costs 1, so the path cost is the number of steps in the path.

[8puzzle-figure]

What abstractions have we included here? The actions are abstracted to
their beginning and final states, ignoring the intermediate locations
where the block is sliding. We have abstracted away actions such as
shaking the board when pieces get stuck and ruled out extracting the
pieces with a knife and putting them back again. We are left with a
description of the rules of the puzzle, avoiding all the details of
physical manipulations.

The 8-puzzle belongs to the family of , which are often used as test
problems for new search algorithms in AI. This family is known to be ,
so one does not expect to find methods significantly better in the worst
case than the search algorithms described in this chapter and the next.
The 8-puzzle has $9!/2 \eq
{181},{440}$ reachable states and is easily solved. The 15-puzzle (on a
$4 \times 4$ board) has around 1.3 trillion states, and random instances
can be solved optimally in a few milliseconds by the best search
algorithms. The 24-puzzle (on a $5 \times 5$ board) has around
${10}^{{25}}$ states, and random instances take several hours to solve
optimally.

The goal of the is to place eight queens on a chessboard such that no
queen attacks any other. (A queen attacks any piece in the same row,
column or diagonal.) shows an attempted solution that fails: the queen
in the rightmost column is attacked by the queen at the top
left[8queens-page].

[8queens-figure]

Although efficient special-purpose algorithms exist for this problem and
for the whole family, it remains a useful test problem for search
algorithms. There are two main kinds of formulation. An involves
operators that *augment* the state description, starting
with an empty state; for the 8-queens problem, this means that each
action adds a queen to the state. A starts with all 8 queens on the
board and moves them around. In either case, the path cost is of no
interest because only the final state counts. The first incremental
formulation one might try is the following:

Any arrangement of 0 to 8 queens on the board is a state.

No queens on the board.

Add a queen to any empty square.

Returns the board with a queen added to the specified square.

8 queens are on the board, none attacked.

In this formulation, we have
${64}\cdot {63}\cdots {57}\approx 1.8\stimes
{10}^{{14}}$ possible sequences to investigate. A better formulation
would prohibit placing a queen in any square that is already attacked:

All possible arrangements of $n$ queens $(0 \le n \le 8)$, one per
column in the leftmost $n$ columns, with no queen attacking another.

Add a queen to any square in the leftmost empty column such that it is
not attacked by any other queen.

[nqueens-page] This formulation reduces the 8-queens state space from
$1.8\stimes
{10}^{{14}}$ to just 2,057, and solutions are easy to find. On the other
hand, for 100 queens the reduction is from roughly ${10}^{{400}}$ states
to about ${10}^{{52}}$ states ()—a big improvement, but not enough to
make the problem tractable. describes the complete-state formulation,
and gives a simple algorithm that solves even the million-queens problem
with ease.

Our final toy problem was devised by Donald  and illustrates how
infinite state spaces can arise. Knuth conjectured that, starting with
the number 4, a sequence of factorial, square root, and floor operations
will reach any desired positive integer. For example, we can reach 5
from 4 as follows:
$$\Bigl\lfloor \sqrt{\sqrt{\sqrt{\sqrt{\sqrt{(4!)!}}}}} \Bigr\rfloor = 5\ .$$
The problem definition is very simple:

Positive numbers.

4.

Apply factorial, square root, or floor operation (factorial for integers
only).

As given by the mathematical definitions of the operations.

State is the desired positive integer.

To our knowledge there is no bound on how large a number might be
constructed in the process of reaching a given target—for example, the
number 620,448,401,733,239,439,360,000 is generated in the expression
for 5—so the state space for this problem is infinite. Such state spaces
arise frequently in tasks involving the generation of mathematical
expressions, circuits, proofs, programs, and other recursively defined
objects.

### Real-world problems

We have already seen how the is defined in terms of specified locations
and transitions along links between them. Route-finding algorithms are
used in a variety of applications. Some, such as Web sites and in-car
systems that provide driving directions, are relatively straightforward
extensions of the Romania example. Others, such as routing video streams
in computer networks, military operations planning, and airline
travel-planning systems, involve much more complex specifications.
Consider the airline travel problems that must be solved by a
travel-planning Web site:

Each state obviously includes a location (e.g., an airport) and the
current time. Furthermore, because the cost of an action (a flight
segment) may depend on previous segments, their fare bases, and their
status as domestic or international, the state must record extra
information about these “historical” aspects.

This is specified by the user’s query.

Take any flight from the current location, in any seat class, leaving
after the current time, leaving enough time for within-airport transfer
if needed. The state resulting from taking a flight will have the
flight’s destination as the current location and the flight’s arrival
time as the current time.

Are we at the final destination specified by the user?

This depends on monetary cost, waiting time, flight time, customs and
immigration procedures, seat quality, time of day, type of airplane,
frequent-flyer mileage awards, and so on.

Commercial travel advice systems use a problem formulation of this kind,
with many additional complications to handle the byzantine fare
structures that airlines impose. Any seasoned traveler knows, however,
that not all air travel goes according to plan. A really good system
should include contingency plans—such as backup reservations on
alternate flights—to the extent that these are justified by the cost and
likelihood of failure of the original plan.

are closely related to route-finding problems, but with an important
difference. Consider, for example, the problem “Visit every city in at
least once, starting and ending in Bucharest.” As with route finding,
the actions correspond to trips between adjacent cities. The state
space, however, is quite different. Each state must include not just the
current location but also the *set of cities the agent has
visited*. So the initial state would be
${In}({Bucharest}), {Visited}(\{{Bucharest}\})$, a typical
intermediate state would be
${In}({Vaslui}), {Visited}(\{{Bucharest},{Urziceni},{Vaslui}\})$,
and the goal test would check whether the agent is in Bucharest and all
20 cities have been visited.

The (TSP) is a touring problem in which each city must be visited
exactly once[TSP-page]. The aim is to find the *shortest*
tour. The problem is known to be NP-hard, but an enormous amount of
effort has been expended to improve the capabilities of TSP algorithms.
In addition to planning trips for traveling salespersons, these
algorithms have been used for tasks such as planning movements of
automatic circuit-board drills and of stocking machines on shop floors.

A problem requires positioning millions of components and connections on
a chip to minimize area, minimize circuit delays, minimize stray
capacitances, and maximize manufacturing yield. The layout problem comes
after the logical design phase and is usually split into two parts: and
. In cell layout, the primitive components of the circuit are grouped
into cells, each of which performs some recognized function. Each cell
has a fixed footprint (size and shape) and requires a certain number of
connections to each of the other cells. The aim is to place the cells on
the chip so that they do not overlap and so that there is room for the
connecting wires to be placed between the cells. Channel routing finds a
specific route for each wire through the gaps between the cells. These
search problems are extremely complex, but definitely worth solving.
Later in this chapter, we present some algorithms capable of solving
them.

is a generalization of the route-finding problem described earlier.
Rather than following a discrete set of routes, a robot can move in a
continuous space with (in principle) an infinite set of possible actions
and states. For a circular robot moving on a flat surface, the space is
essentially two-dimensional. When the robot has arms and legs or wheels
that must also be controlled, the search space becomes many-dimensional.
Advanced techniques are required just to make the search space finite.
We examine some of these methods in . In addition to the complexity of
the problem, real robots must also deal with errors in their sensor
readings and motor controls.

of complex objects by a robot was first demonstrated by  @Michie:1972.
Progress since then has been slow but sure, to the point where the
assembly of intricate objects such as electric motors is economically
feasible. In assembly problems, the aim is to find an order in which to
assemble the parts of some object. If the wrong order is chosen, there
will be no way to add some part later in the sequence without undoing
some of the work already done. Checking a step in the sequence for
feasibility is a difficult geometrical search problem closely related to
robot navigation. Thus, the generation of legal actions is the expensive
part of assembly sequencing. Any practical algorithm must avoid
exploring all but a tiny fraction of the state space. Another important
assembly problem is , in which the goal is to find a sequence of amino
acids that will fold into a three-dimensional protein with the right
properties to cure some disease.

Searching for Solutions {#basic-search-section}
-----------------------

Having formulated some problems, we now need to solve them. A solution
is an action sequence, so search algorithms work by considering various
possible action sequences. The possible action sequences starting at the
initial state form a with the initial state at the root; the branches
are actions and the correspond to states in the state space of the
problem. shows the first few steps in growing the search tree for
finding a route from Arad to Bucharest. The root node of the tree
corresponds to the initial state, *In(Arad)*. The first
step is to test whether this is a goal state. (Clearly it is not, but it
is important to check so that we can solve trick problems like “starting
in Arad, get to Arad.”) Then we need to consider taking various actions.
We do this by the current state; that is, applying each legal action to
the current state, thereby a new set of states. In this case, we add
three branches from the *In(Arad)* leading to three new :
*In(Sibiu), In(Timisoara),* and *In(Zerind)*.
Now we must choose which of these three possibilities to consider
further.

This is the essence of search—following up one option now and putting
the others aside for later, in case the first choice does not lead to a
solution. Suppose we choose Sibiu first. We check to see whether it is a
goal state (it is not) and then expand it to get
*In(Arad)*, *In(Fagaras)*,
*In(Oradea)*, and *In(RimnicuVilcea)*. We can
then choose any of these four or go back and choose Timisoara or Zerind.
Each of these six nodes is a , that is, a node with no children in the
tree. The set of all leaf nodes available for expansion at any given
point is called the . (Many authors call it the , which is both
geographically less evocative and less accurate, because other data
structures are better suited than a list.) In , the frontier of each
tree consists of those nodes with bold outlines.

The process of expanding nodes on the frontier continues until either a
solution is found or there are no more states to expand. The general
algorithm is shown informally in . Search algorithms all share this
basic structure; they vary primarily according to how they choose which
state to expand next—the so-called .

[search-map-figure]

[informal-tree+graph-search-algorithm]

The eagle-eyed reader will notice one peculiar thing about the search
tree shown in : it includes the path from Arad to Sibiu and back to Arad
again! We say that *In(Arad)* is a in the search tree,
generated in this case by a . Considering such loopy paths means that
the complete search tree for Romania is *infinite* because
there is no limit to how often one can traverse a loop. On the other
hand, the state space—the map shown in —has only 20 states. As we
discuss in , loops can cause certain algorithms to fail, making
otherwise solvable problems unsolvable. Fortunately, there is no need to
consider loopy paths. We can rely on more than intuition for this:
because path costs are additive and step costs are nonnegative, a loopy
path to any given state is never better than the same path with the loop
removed.

Loopy paths are a special case of the more general concept of , which
exist whenever there is more than one way to get from one state to
another. Consider the paths Arad–Sibiu (140 km long) and
Arad–Zerind–Oradea–Sibiu (297 km long). Obviously, the second path is
redundant—it’s just a worse way to get to the same state. If you are
concerned about reaching the goal, there’s never any reason to keep more
than one path to any given state, because any goal state that is
reachable by extending one path is also reachable by extending the
other.

In some cases, it is possible to define the problem itself so as to
eliminate redundant paths. For example, if we formulate the 8-queens
problem () so that a queen can be placed in any column, then each state
with $n$ queens can be reached by $n!$ different paths; but if we
reformulate the problem so that each new queen is placed in the leftmost
empty column, then each state can be reached only through one path.

In other cases, redundant paths are unavoidable. This includes all
problems where the actions are reversible, such as route-finding
problems and sliding-block puzzles. Route-finding on a (like the one
used later for ) is a particularly important example in computer games.
In such a grid, each state has four successors, so a search tree of
depth $d$ that includes repeated states has $4^d$ leaves; but there are
only about $2d^2$ distinct states within $d$ steps of any given state.
For $d={20}$, this means about a trillion nodes but only about 800
distinct states. Thus, following redundant paths can cause a tractable
problem to become intractable. This is true even for algorithms that
know how to avoid infinite loops.

As the saying goes,

algorithms that forget their history are doomed to repeat it.

The way to avoid exploring redundant paths is to remember where one has
been. To do this, we augment the algorithm with a data structure called
the (also known as the ), which remembers every expanded node. Newly
generated nodes that match previously generated nodes—ones in the
explored set or the frontier—can be discarded instead of being added to
the frontier. The new algorithm, called , is shown informally in . The
specific algorithms in this chapter draw on this general design.

Clearly, the search tree constructed by the algorithm contains at most
one copy of each state, so we can think of it as growing a tree directly
on the state-space graph, as shown in . The algorithm has another nice
property: the frontier the state-space graph into the explored region
and the unexplored region, so that every path from the initial state to
an unexplored state has to pass through a state in the frontier. (If
this seems completely obvious, try now.) This property is illustrated in
. As every step moves a state from the frontier into the explored region
while moving some states from the unexplored region into the frontier,
we see that the algorithm is *systematically* examining the
states in the state space, one by one, until it finds a solution.

[romania-graph-search-figure]

[graph-separation-property-figure]

### Infrastructure for search algorithms

Search algorithms require a data structure to keep track of the search
tree that is being constructed. For each node $n$ of the tree, we have a
structure that contains four components:

-   $n$.: the state in the state space to which the node corresponds;

-   $n$.: the node in the search tree that generated this node;

-   $n$.: the action that was applied to the parent to generate the
    node;

-   $n$.: the cost, traditionally denoted by $g(n)$, of the path from
    the initial state to the node, as indicated by the parent pointers.

Given the components for a parent node, it is easy to see how to compute
the necessary components for a child node. The function takes a parent
node and an action and returns the resulting child node:

The node data structure is depicted in . Notice how the pointers string
the nodes together into a tree structure. These pointers also allow the
solution path to be extracted when a goal node is found; we use the
function to return the sequence of actions obtained by following parent
pointers back to the root.

Up to now, we have not been very careful to distinguish between nodes
and states, but in writing detailed algorithms it’s important to make
that distinction. A node is a bookkeeping data structure used to
represent the search tree. A state corresponds to a configuration of the
world. Thus, nodes are on particular paths, as defined by pointers,
whereas states are not. Furthermore, two different nodes can contain the
same world state if that state is generated via two different search
paths.

[state-vs-node-figure]

Now that we have nodes, we need somewhere to put them. The frontier
needs to be stored in such a way that the search algorithm can easily
choose the next node to expand according to its preferred strategy. The
appropriate data structure for this is a . The operations on a queue are
as follows:

-   () returns true only if there are no more elements in the queue.

-   () removes the first element of the queue and returns it.

-   (, ) inserts an element and returns the resulting queue.

Queues are characterized by the *order* in which they store
the inserted nodes. Three common variants are the first-in, first-out or
, which pops the *oldest* element of the queue; the
last-in, first-out or (also known as a ), which pops the
*newest* element of the queue; and the , which pops the
element of the queue with the highest priority according to some
ordering function.

The explored set can be implemented with a hash table to allow efficient
checking for repeated states. With a good implementation, insertion and
lookup can be done in roughly constant time no matter how many states
are stored. One must take care to implement the hash table with the
right notion of equality between states. For example, in the traveling
salesperson problem (), the hash table needs to know that the set of
visited cities $\{$Bucharest,Urziceni,Vaslui$\}$ is the same as
$\{$Urziceni,Vaslui,Bucharest$\}$. Sometimes this can be achieved most
easily by insisting that the data structures for states be in some ;
that is, logically equivalent states should map to the same data
structure. In the case of states described by sets, for example, a
bit-vector representation or a sorted list without repetition would be
canonical, whereas an unsorted list would not.

### Measuring problem-solving performance {#search-criteria-section}

Before we get into the design of specific search algorithms, we need to
consider the criteria that might be used to choose among them. We can
evaluate an algorithm’s performance in four ways:

Is the algorithm guaranteed to find a solution when there is one? Does
the strategy find the optimal solution, as defined on ? How long does it
take to find a solution? How much memory is needed to perform the
search?

Time and space complexity are always considered with respect to some
measure of the problem difficulty. In theoretical computer science, the
typical measure is the size of the state space graph, $|V|+|E|$, where
$V$ is the set of vertices (nodes) of the graph and $E$ is the set of
edges (links). This is appropriate when the graph is an explicit data
structure that is input to the search program. (The map of Romania is an
example of this.) In AI, the graph is often represented
*implicitly* by the initial state, actions, and transition
model and is frequently infinite. For these reasons, complexity is
expressed in terms of three quantities: $b$, the or maximum number of
successors of any node; $d$, the of the shallowest goal node (i.e., the
number of steps along the path from the root); and $m$, the maximum
length of any path in the state space. Time is often measured in terms
of the number of nodes generated during the search, and space in terms
of the maximum number of nodes stored in memory. For the most part, we
describe time and space complexity for search on a tree; for a graph,
the answer depends on how “redundant” the paths in the state space are.

To assess the effectiveness of a search algorithm, we can consider just
the —which typically depends on the time complexity but can also include
a term for memory usage—or we can use the , which combines the search
cost and the path cost of the solution found. For the problem of finding
a route from Arad to Bucharest, the search cost is the amount of time
taken by the search and the solution cost is the total length of the
path in kilometers. Thus, to compute the total cost, we have to add
milliseconds and kilometers. There is no “official exchange rate”
between the two, but it might be reasonable in this case to convert
kilometers into milliseconds by using an estimate of the car’s average
speed (because time is what the agent cares about). This enables the
agent to find an optimal tradeoff point at which further computation to
find a shorter path becomes counterproductive. The more general problem
of tradeoffs between different goods is taken up in .

Uninformed Search Strategies {#search-strategy-section}
----------------------------

This section covers several search strategies that come under the
heading of (also called ). The term means that the strategies have no
additional information about states beyond that provided in the problem
definition. All they can do is generate successors and distinguish a
goal state from a non-goal state. All search strategies are
distinguished by the *order* in which nodes are expanded.
Strategies that know whether one non-goal state is “more promising” than
another are called or strategies; they are covered in .

### Breadth-first search

is a simple strategy in which the root node is expanded first, then all
the successors of the root node are expanded next, then
*their* successors, and so on. In general, all the nodes
are expanded at a given depth in the search tree before any nodes at the
next level are expanded.

Breadth-first search is an instance of the general graph-search
algorithm () in which the *shallowest* unexpanded node is
chosen for expansion. This is achieved very simply by using a FIFO queue
for the frontier. Thus, new nodes (which are always deeper than their
parents) go to the back of the queue, and old nodes, which are shallower
than the new nodes, get expanded first. There is one slight tweak on the
general graph-search algorithm, which is that the goal test is applied
to each node when it is *generated* rather than when it is
selected for expansion. This decision is explained below, where we
discuss time complexity. Note also that the algorithm, following the
general template for graph search, discards any new path to a state
already in the frontier or explored set; it is easy to see that any such
path must be at least as deep as the one already found. Thus,
breadth-first search always has the shallowest path to every node on the
frontier.

Pseudocode is given in . shows the progress of the search on a simple
binary tree.

[breadth-first-search-algorithm]

How does breadth-first search rate according to the four criteria from
the previous section? We can easily see that it is
*complete*—if the shallowest goal node is at some finite
depth $d$, breadth-first search will eventually find it after generating
all shallower nodes (provided the branching factor $b$ is finite). Note
that as soon as a goal node is generated, we know it is the shallowest
goal node because all shallower nodes must have been generated already
and failed the goal test. Now, the *shallowest* goal node
is not necessarily the *optimal* one; technically,
breadth-first search is optimal if the path cost is a nondecreasing
function of the depth of the node. The most common such scenario is that
all actions have the same cost.

[bfs-progress-figure]

So far, the news about breadth-first search has been good. The news
about time and space is not so good. Imagine searching a uniform tree
where every state has $b$ successors. The root of the search tree
generates $b$ nodes at the first level, each of which generates $b$ more
nodes, for a total of $b^2$ at the second level. Each of
*these* generates $b$ more nodes, yielding $b^3$ nodes at
the third level, and so on. Now suppose that the solution is at depth
$d$. In the worst case, it is the last node generated at that level.
Then the total number of nodes generated is
$$b + b^2 + b^3 + \cdots + b^d = O(b^d)\ .$$ (If the algorithm were to
apply the goal test to nodes when selected for expansion, rather than
when generated, the whole layer of nodes at depth $d$ would be expanded
before the goal was detected and the time complexity would be
$O(b^{d+1})$.)

As for space complexity: for any kind of graph search, which stores
every expanded node in the ěxplored set, the space complexity is always
within a factor of $b$ of the time complexity. For breadth-first graph
search in particular, every node generated remains in memory. There will
be $O(b^{d-1})$ nodes in the ěxplored set and $O(b^{d})$ nodes in the
frontier, so the space complexity is $O(b^{d})$, i.e., it is dominated
by the size of the frontier. Switching to a tree search would not save
much space, and in a state space with many redundant paths, switching
could cost a great deal of time.

An exponential complexity bound such as $O(b^d)$ is scary. shows why. It
lists, for various values of the solution depth $d$, the time and memory
required for a breadth-first search with branching factor $b = {10}$.
The table assumes that 1 million nodes can be generated per second and
that a node requires 1000 bytes of storage. Many search problems fit
roughly within these assumptions (give or take a factor of 100) when run
on a modern personal computer.

[htbp] [bfs-table]

Two lessons can be learned from . First,

the memory requirements are a bigger problem for breadth-first search
than is the execution time.

One might wait 13 days for the solution to an important problem with
search depth 12, but no personal computer has the petabyte of memory it
would take. Fortunately, other strategies require less memory.

The second lesson is that time is still a major factor. If your problem
has a solution at depth 16, then (given our assumptions) it will take
about 350 years for breadth-first search (or indeed any uninformed
search) to find it. In general,

exponential-complexity search problems cannot be solved by uninformed
methods for any but the smallest instances.

### Uniform-cost search

When all step costs are equal, breadth-first search is optimal because
it always expands the *shallowest* unexpanded node. By a
simple extension, we can find an algorithm that is optimal with any
step-cost function. Instead of expanding the shallowest node, expands
the node $n$ with the *lowest path cost* $g(n)$. This is
done by storing the frontier as a priority queue ordered by $g$. The
algorithm is shown in .

In addition to the ordering of the queue by path cost, there are two
other significant differences from breadth-first search. The first is
that the goal test is applied to a node when it is *selected for
expansion* (as in the generic graph-search algorithm shown in )
rather than when it is first generated. The reason is that the first
goal node that is *generated* may be on a suboptimal path.
The second difference is that a test is added in case a better path is
found to a node currently on the frontier.

[uniform-cost-search-algorithm]

[romania-subgraph-figure]

Both of these modifications come into play in the example shown in ,
where the problem is to get from Sibiu to Bucharest. The successors of
Sibiu are Rimnicu Vilcea and Fagaras, with costs 80 and 99,
respectively. The least-cost node, Rimnicu Vilcea, is expanded next,
adding Pitesti with cost $80+97 \eq 177$. The least-cost node is now
Fagaras, so it is expanded, adding Bucharest with cost $99+211 \eq 310$.
Now a goal node has been generated, but uniform-cost search keeps going,
choosing Pitesti for expansion and adding a second path to Bucharest
with cost $80+97+101 \eq 278$. Now the algorithm checks to see if this
new path is better than the old one; it is, so the old one is discarded.
Bucharest, now with $g$-cost 278, is selected for expansion and the
solution is returned.

It is easy to see that uniform-cost search is optimal in general. First,
we observe that whenever uniform-cost search selects a node $n$ for
expansion, the optimal path to that node has been found. (Were this not
the case, there would have to be another frontier node $n'$ on the
optimal path from the start node to $n$, by the graph separation
property of ; by definition, $n'$ would have lower $g$-cost than $n$ and
would have been selected first.) Then, because step costs are
nonnegative, paths never get shorter as nodes are added. These two facts
together imply that

uniform-cost search expands nodes in order of their optimal path cost.

Hence, the first goal node selected for expansion must be the optimal
solution.

Uniform-cost search does not care about the *number* of
steps a path has, but only about their total cost. Therefore, it will
get stuck in an infinite loop if there is a path with an infinite
sequence of zero-cost actions—for example, a sequence of ${NoOp}$
actions.[^6] Completeness is guaranteed provided the cost of every step
exceeds some small positive constant $\epsilon$.

Uniform-cost search is guided by path costs rather than depths, so its
complexity is not easily characterized in terms of $b$ and $d$. Instead,
let $C^*$ be the cost of the optimal solution,[^7] and assume that every
action costs at least $\epsilon$. Then the algorithm’s worst-case time
and space complexity is $O(b^{1+\floor{C^*/\epsilon}})$, which can be
much greater than $b^d$. This is because uniform-cost search can explore
large trees of small steps before exploring paths involving large and
perhaps useful steps. When all step costs are equal,
$b^{1+\floor{C^*/\epsilon}}$ is just $b^{d+1}$. When all step costs are
the same, uniform-cost search is similar to breadth-first search, except
that the latter stops as soon as it generates a goal, whereas
uniform-cost search examines all the nodes at the goal’s depth to see if
one has a lower cost; thus uniform-cost search does strictly more work
by expanding nodes at depth $d$ unnecessarily.

### Depth-first search

always expands the *deepest* node in the current frontier
of the search tree. The progress of the search is illustrated in . The
search proceeds immediately to the deepest level of the search tree,
where the nodes have no successors. As those nodes are expanded, they
are dropped from the frontier, so then the search “backs up” to the next
deepest node that still has unexplored successors.

The depth-first search algorithm is an instance of the graph-search
algorithm in ; whereas breadth-first-search uses a FIFO queue,
depth-first search uses a LIFO queue. A LIFO queue means that the most
recently generated node is chosen for expansion. This must be the
deepest unexpanded node because it is one deeper than its parent—which,
in turn, was the deepest unexpanded node when it was selected.

As an alternative to the -style implementation, it is common to
implement depth-first search with a recursive function that calls itself
on each of its children in turn. (A recursive depth-first algorithm
incorporating a depth limit is shown in .)

[dfs-progress-figure]

The properties of depth-first search depend strongly on whether the
graph-search or tree-search version is used. The graph-search version,
which avoids repeated states and redundant paths, is complete in finite
state spaces because it will eventually expand every node. The
tree-search version, on the other hand, is *not*
complete—for example, in the algorithm will follow the
Arad–Sibiu–Arad–Sibiu loop forever. Depth-first tree search can be
modified at no extra memory cost so that it checks new states against
those on the path from the root to the current node; this avoids
infinite loops in finite state spaces but does not avoid the
proliferation of redundant paths. In infinite state spaces, both
versions fail if an infinite non-goal path is encountered. For example,
in Knuth’s 4 problem, depth-first search would keep applying the
factorial operator forever.

For similar reasons, both versions are nonoptimal. For example, in ,
depth-first search will explore the entire left subtree even if node $C$
is a goal node. If node $J$ were also a goal node, then depth-first
search would return it as a solution instead of $C$, which would be a
better solution; hence, depth-first search is not optimal.

The time complexity of depth-first graph search is bounded by the size
of the state space (which may be infinite, of course). A depth-first
tree search, on the other hand, may generate all of the $O(b^m)$ nodes
in the search tree, where $m$ is the maximum depth of any node; this can
be much greater than the size of the state space. Note that $m$ itself
can be much larger than $d$ (the depth of the shallowest solution) and
is infinite if the tree is unbounded.

So far, depth-first search seems to have no clear advantage over
breadth-first search, so why do we include it? The reason is the space
complexity. For a graph search, there is no advantage, but a depth-first
tree search needs to store only a single path from the root to a leaf
node, along with the remaining unexpanded sibling nodes for each node on
the path. Once a node has been expanded, it can be removed from memory
as soon as all its descendants have been fully explored. (See .) For a
state space with branching factor $b$ and maximum depth $m$, depth-first
search requires storage of only $O(bm)$ nodes. Using the same
assumptions as for and assuming that nodes at the same depth as the goal
node have no successors, we find that depth-first search would require
156 kilobytes instead of 10 exabytes at depth $d={16}$, a factor of 7
trillion times less space. This has led to the adoption of depth-first
tree search as the basic workhorse of many areas of AI, including
constraint satisfaction (), propositional satisfiability (), and logic
programming (). For the remainder of this section, we focus primarily on
the tree-search version of depth-first search.

A variant of depth-first search called [backtracking-search-page] uses
still less memory. (See for more details.) In backtracking, only one
successor is generated at a time rather than all successors; each
partially expanded node remembers which successor to generate next. In
this way, only $O(m)$ memory is needed rather than $O(bm)$. Backtracking
search facilitates yet another memory-saving (and time-saving) trick:
the idea of generating a successor by *modifying* the
current state description directly rather than copying it first. This
reduces the memory requirements to just one state description and $O(m)$
actions. For this to work, we must be able to undo each modification
when we go back to generate the next successor. For problems with large
state descriptions, such as robotic assembly, these techniques are
critical to success.

### Depth-limited search

The embarrassing failure of depth-first search in infinite state spaces
can be alleviated by supplying depth-first search with a predetermined
depth limit $\ell$. That is, nodes at depth $\ell$ are treated as if
they have no successors. This approach is called . The depth limit
solves the infinite-path problem. Unfortunately, it also introduces an
additional source of incompleteness if we choose $\ell<d$, that is, the
shallowest goal is beyond the depth limit. (This is likely when $d$ is
unknown.) Depth-limited search will also be nonoptimal if we choose
$\ell>d$. Its time complexity is $O(b^\ell)$ and its space complexity is
$O(b\ell)$. Depth-first search can be viewed as a special case of
depth-limited search with $\ell\eq\infty$.

Sometimes, depth limits can be based on knowledge of the problem. For
example, on the map of Romania there are 20 cities. Therefore, we know
that if there is a solution, it must be of length 19 at the longest, so
$\ell={19}$ is a possible choice. But in fact if we studied the map
carefully, we would discover that any city can be reached from any other
city in at most 9 steps. This number, known as the [diameter-page] of
the state space, gives us a better depth limit, which leads to a more
efficient depth-limited search. For most problems, however, we will not
know a good depth limit until we have solved the problem.

[recursive-dls-algorithm]

Depth-limited search can be implemented as a simple modification to the
general tree- or graph-search algorithm. Alternatively, it can be
implemented as a simple recursive algorithm as shown in . Notice that
depth-limited search can terminate with two kinds of failure: the
standard value indicates no solution; the value indicates no solution
within the depth limit.

### Iterative deepening depth-first search

[iterative-deepening-section]

(or iterative deepening depth-first search) is a general strategy, often
used in combination with depth-first tree search, that finds the best
depth limit. It does this by gradually increasing the limit—first 0,
then 1, then 2, and so on—until a goal is found. This will occur when
the depth limit reaches $d$, the depth of the shallowest goal node. The
algorithm is shown in . Iterative deepening combines the benefits of
depth-first and breadth-first search. Like depth-first search, its
memory requirements are modest: $O(bd)$ to be precise. Like
breadth-first search, it is complete when the branching factor is finite
and optimal when the path cost is a nondecreasing function of the depth
of the node. shows four iterations of on a binary search tree, where the
solution is found on the fourth iteration.

[ids-algorithm]

[ids-progress-figure]

Iterative deepening search may seem wasteful because states are
generated multiple times. It turns out this is not too costly. The
reason is that in a search tree with the same (or nearly the same)
branching factor at each level, most of the nodes are in the bottom
level, so it does not matter much that the upper levels are generated
multiple times. In an iterative deepening search, the nodes on the
bottom level (depth $d$) are generated once, those on the next-to-bottom
level are generated twice, and so on, up to the children of the root,
which are generated $d$ times. So the total number of nodes generated in
the worst case is
$$N(\mbox{IDS}) = (d)b + (d-1)b^2 + \cdots + (1)b^{d}\ ,$$ which gives a
time complexity of $O(b^d)$—asymptotically the same as breadth-first
search. There is some extra cost for generating the upper levels
multiple times, but it is not large. For example, if $b={10}$ and $d=5$,
the numbers are

$$\begin{aligned}
N(\mbox{IDS}) &=& {50} + {400} + 3,{000} + {20},{000} + {100},{000} = {123},{450} \\
N(\mbox{BFS}) &=& {10} + {100} + 1,{000} + {10},{000} + {100},{000} = {111},{110}\ .\end{aligned}$$

If you are really concerned about repeating the repetition, you can use
a hybrid approach that runs breadth-first search until almost all the
available memory is consumed, and then runs iterative deepening from all
the nodes in the frontier.

In general, iterative deepening is the preferred uninformed search
method when the search space is large and the depth of the solution is
not known.

Iterative deepening search is analogous to breadth-first search in that
it explores a complete layer of new nodes at each iteration before going
on to the next layer. It would seem worthwhile to develop an iterative
analog to uniform-cost search, inheriting the latter algorithm’s
optimality guarantees while avoiding its memory requirements. The idea
is to use increasing path-cost limits instead of increasing depth
limits. The resulting algorithm, called , is explored in . It turns out,
unfortunately, that iterative lengthening incurs substantial overhead
compared to uniform-cost search.[iterative-lengthening-page]

### Bidirectional search

[bidirectional-search]

The idea behind bidirectional search is to run two simultaneous
searches—one forward from the initial state and the other backward from
the goal—hoping that the two searches meet in the middle (). The
motivation is that $b^{d/2} + b^{d/2}$ is much less than $b^{d}$, or in
the figure, the area of the two small circles is less than the area of
one big circle centered on the start and reaching to the goal.

Bidirectional search is implemented by replacing the goal test with a
check to see whether the frontiers of the two searches intersect; if
they do, a solution has been found. (It is important to realize that the
first such solution found may not be optimal, even if the two searches
are both breadth-first; some additional search is required to make sure
there isn’t another short-cut across the gap.) The check can be done
when each node is generated or selected for expansion and, with a hash
table, will take constant time. For example, if a problem has solution
depth $d\eq 6$, and each direction runs breadth-first search one node at
a time, then in the worst case the two searches meet when they have
generated all of the nodes at depth 3. For $b\eq {10}$, this means a
total of 2,220 node generations, compared with 1,111,110 for a standard
breadth-first search. Thus, the time complexity of bidirectional search
using breadth-first searches in both directions is $O(b^{d/2})$. The
space complexity is also $O(b^{d/2})$. We can reduce this by roughly
half if one of the two searches is done by iterative deepening, but at
least one of the frontiers must be kept in memory so that the
intersection check can be done. This space requirement is the most
significant weakness of bidirectional search.

[bidirectional-figure]

The reduction in time complexity makes bidirectional search attractive,
but how do we search backward? This is not as easy as it sounds. Let the
of a state $x$ be all those states that have $x$ as a successor.
Bidirectional search requires a method for computing predecessors. When
all the actions in the state space are reversible, the predecessors of
$x$ are just its successors. Other cases may require substantial
ingenuity.

Consider the question of what we mean by “the goal” in searching
“backward from the goal.” For the 8-puzzle and for finding a route in
Romania, there is just one goal state, so the backward search is very
much like the forward search. If there are several *explicitly
listed* goal states—for example, the two dirt-free goal states in
—then we can construct a new dummy goal state whose immediate
predecessors are all the actual goal states. But if the goal is an
abstract description, such as the goal that “no queen attacks another
queen” in the $n$-queens problem, then bidirectional search is difficult
to use.

### Comparing uninformed search strategies

compares search strategies in terms of the four evaluation criteria set
forth in . This comparison is for tree-search versions. For graph
searches, the main differences are that depth-first search is complete
for finite state spaces and that the space and time complexities are
bounded by the size of the state space.

[htbp] [search-summary-table]

Informed (Heuristic) Search Strategies {#heuristic-search-section}
--------------------------------------

This section shows how an strategy—one that uses problem-specific
knowledge beyond the definition of the problem itself—can find solutions
more efficiently than can an uninformed strategy.

The general approach we consider is called . Best-first search is an
instance of the general or algorithm in which a node is selected for
expansion based on an , $f(n)$. The evaluation function is construed as
a cost estimate, so the node with the *lowest* evaluation
is expanded first. The implementation of best-first graph search is
identical to that for uniform-cost search (), except for the use of $f$
instead of $g$ to order the priority queue.

The choice of $f$ determines the search strategy. (For example, as
shows, best-first tree search includes depth-first search as a special
case.) Most best-first algorithms include as a component of $f$ a ,
denoted $h(n)$:
$$h(n) = \mbox{ estimated cost of the cheapest path from the state at node {\it n} to a goal state.}$$
(Notice that $h(n)$ takes a *node* as input, but, unlike
$g(n)$, it depends only on the *state* at that node.) For
example, in Romania, one might estimate the cost of the cheapest path
from Arad to Bucharest via the straight-line distance from Arad to
Bucharest.

Heuristic functions are the most common form in which additional
knowledge of the problem is imparted to the search algorithm. We study
heuristics in more depth in . For now, we consider them to be arbitrary,
nonnegative, problem-specific functions, with one constraint: if $n$ is
a goal node, then $h(n)\eq 0$. The remainder of this section covers two
ways to use heuristic information to guide search.

### Greedy best-first search

[^8] tries to expand the node that is closest to the goal, on the
grounds that this is likely to lead to a solution quickly. Thus, it
evaluates nodes by using just the heuristic function; that is,
$f(n) = h(n)$.

Let us see how this works for route-finding problems in Romania; we use
the heuristic, which we will call $h_{{SLD}}$. If the goal is
Bucharest, we need to know the straight-line distances to Bucharest,
which are shown in . For example,
$h_{{SLD}}({In}({Arad})) \eq {366}$. Notice that the values of
$h_{{SLD}}$ cannot be computed from the problem description itself.
Moreover, it takes a certain amount of experience to know that
$h_{{SLD}}$ is correlated with actual road distances and is,
therefore, a useful heuristic.

[romania-sld-figure]

shows the progress of a greedy best-first search using $h_{{{SLD}}}$
to find a path from Arad to Bucharest. The first node to be expanded
from Arad will be Sibiu because it is closer to Bucharest than either
Zerind or Timisoara. The next node to be expanded will be Fagaras
because it is closest. Fagaras in turn generates Bucharest, which is the
goal. For this particular problem, greedy best-first search using
$h_{{{SLD}}}$ finds a solution without ever expanding a node that is
not on the solution path; hence, its search cost is minimal. It is not
optimal, however: the path via Sibiu and Fagaras to Bucharest is 32
kilometers longer than the path through Rimnicu Vilcea and Pitesti. This
shows why the algorithm is called “greedy”—at each step it tries to get
as close to the goal as it can.

[greedy-progress-figure]

Greedy best-first tree search is also incomplete even in a finite state
space, much like depth-first search. Consider the problem of getting
from Iasi to Fagaras. [I-to-F] The heuristic suggests that Neamt be
expanded first because it is closest to Fagaras, but it is a dead end.
The solution is to go first to Vaslui—a step that is actually farther
from the goal according to the heuristic—and then to continue to
Urziceni, Bucharest, and Fagaras. The algorithm will never find this
solution, however, because expanding Neamt puts Iasi back into the
frontier, Iasi is closer to Fagaras than Vaslui is, and so Iasi will be
expanded again, leading to an infinite loop. (The graph search version
*is* complete in finite spaces, but not in infinite ones.)
The worst-case time and space complexity for the tree version is
$O(b^m)$, where $m$ is the maximum depth of the search space. With a
good heuristic function, however, the complexity can be reduced
substantially. The amount of the reduction depends on the particular
problem and on the quality of the heuristic.

### A\* search: Minimizing the total estimated solution cost

The most widely known form of best-first search is called (pronounced
“A-star search”). It evaluates nodes by combining $g(n)$, the cost to
reach the node, and $h(n)$, the cost to get from the node to the goal:
$$f(n) = g(n) + h(n)\ .$$ Since $g(n)$ gives the path cost from the
start node to node $n$, and $h(n)$ is the estimated cost of the cheapest
path from $n$ to the goal, we have
$$f(n) = \mbox{ estimated cost of the cheapest solution through } n\ .$$
Thus, if we are trying to find the cheapest solution, a reasonable thing
to try first is the node with the lowest value of $g(n) + h(n)$. It
turns out that this strategy is more than just reasonable: provided that
the heuristic function $h(n)$ satisfies certain conditions, A search is
both complete and optimal. The algorithm is identical to except that A
uses $g+h$ instead of $g$.

#### Conditions for optimality: Admissibility and consistency

The first condition we require for optimality is that $h(n)$ be an . An
admissible heuristic is one that *never overestimates* the
cost to reach the goal. Because $g(n)$ is the actual cost to reach $n$
along the current path, and $f(n) \eq g(n) + h(n)$, we have as an
immediate consequence that $f(n)$ never overestimates the true cost of a
solution along the current path through $n$.

Admissible heuristics are by nature optimistic because they think the
cost of solving the problem is less than it actually is. An obvious
example of an admissible heuristic is the straight-line distance
$h_{{SLD}}$ that we used in getting to Bucharest. Straight-line
distance is admissible because the shortest path between any two points
is a straight line, so the straight line cannot be an overestimate. In ,
we show the progress of an A tree search for Bucharest. The values of
$g$ are computed from the step costs in , and the values of
$h_{{SLD}}$ are given in . Notice in particular that Bucharest first
appears on the frontier at step (e), but it is not selected for
expansion because its $f$-cost (450) is higher than that of Pitesti
(417). Another way to say this is that there *might* be a
solution through Pitesti whose cost is as low as 417, so the algorithm
will not settle for a solution that costs 450.

[astar-progress-figure]

A second, slightly stronger condition called (or sometimes ) is required
only for applications of A to graph search.[^9] A heuristic $h(n)$ is
consistent if, for every node $n$ and every successor $n'$ of $n$
generated by any action $a$,[A\*-consistency] the estimated cost of
reaching the goal from $n$ is no greater than the step cost of getting
to $n'$ plus the estimated cost of reaching the goal from $n'$:
$$h(n) \le c(n,a,n') + h(n') \ .$$ This is a form of the general , which
stipulates that each side of a triangle cannot be longer than the sum of
the other two sides. Here, the triangle is formed by $n$, $n'$, and the
goal $G_n$ closest to $n$. For an admissible heuristic, the inequality
makes perfect sense: if there were a route from $n$ to $G_n$ via $n'$
that was cheaper than $h(n)$, that would violate the property that
$h(n)$ is a lower bound on the cost to reach $G_n$.

It is fairly easy to show () that every consistent heuristic is also
admissible. Consistency is therefore a stricter requirement than
admissibility, but one has to work quite hard to concoct heuristics that
are admissible but not consistent. All the admissible heuristics we
discuss in this chapter are also consistent. Consider, for example,
$h_{{SLD}}$. We know that the general triangle inequality is satisfied
when each side is measured by the straight-line distance and that the
straight-line distance between $n$ and $n'$ is no greater than
$c(n,a,n')$. Hence, $h_{{SLD}}$ is a consistent heuristic.

#### Optimality of A\*

As we mentioned earlier, A has the following properties:

the tree-search version of $A^*$ is optimal if $h(n)$ is admissible,
while the graph-search version is optimal if $h(n)$ is consistent.

We show the second of these two claims since it is more useful. The
argument essentially mirrors the argument for the optimality of
uniform-cost search, with $g$ replaced by $f$—just as in the A algorithm
itself.

The first step is to establish the following:

if $h(n)$ is consistent, then the values of $f(n)$ along any path are
nondecreasing.

The proof follows directly from the definition of consistency. Suppose
$n'$ is a successor of $n$; then $g(n')\eq g(n) + c(n,a,n')$ for some
action $a$, and we have
$$f(n') = g(n')+h(n') = g(n)+c(n,a,n')+h(n') \geq g(n) + h(n) = f(n)\ .$$
The next step is to prove that

whenever $A^*$ selects a node $n$ for expansion, the optimal path to
that node has been found.

Were this not the case, there would have to be another frontier node
$n'$ on the optimal path from the start node to $n$, by the graph
separation property of ; because $f$ is nondecreasing along any path,
$n'$ would have lower $f$-cost than $n$ and would have been selected
first.

From the two preceding observations, it follows that the sequence of
nodes expanded by A using is in nondecreasing order of $f(n)$. Hence,
the first goal node selected for expansion must be an optimal solution
because $f$ is the true cost for goal nodes (which have $h\eq 0$) and
all later goal nodes will be at least as expensive.

The fact that $f$-costs are nondecreasing along any path also means that
we can draw in the state space, just like the contours in a topographic
map. shows an example. Inside the contour labeled ${400}$, all nodes
have $f(n)$ less than or equal to 400, and so on. Then, because A
expands the frontier node of lowest $f$-cost, we can see that an A
search fans out from the start node, adding nodes in concentric bands of
increasing $f$-cost.

[f-circles-figure]

With uniform-cost search (A search using $h(n)=0$), the bands will be
“circular” around the start state. With more accurate heuristics, the
bands will stretch toward the goal state and become more narrowly
focused around the optimal path. If $C^*$ is the cost of the optimal
solution path, then we can say the following:[full-expansion-page]

-   A expands all nodes with $f(n)<C^*$.

-   A might then expand some of the nodes right on the “goal contour”
    (where $f(n)=C^*$) before selecting a goal node.

Completeness requires that there be only finitely many nodes with cost
less than or equal to $C^*$, a condition that is true if all step costs
exceed some finite $\epsilon$ and if $b$ is finite.

Notice that A expands no nodes with $f(n)>C^*$—for example, Timisoara is
not expanded in even though it is a child of the root. We say that the
subtree below Timisoara is ; because $h_{{SLD}}$ is admissible, the
algorithm can safely ignore this subtree while still guaranteeing
optimality. The concept of pruning—eliminating possibilities from
consideration without having to examine them—is important for many areas
of AI.

One final observation is that among optimal algorithms of this
type—algorithms that extend search paths from the root and use the same
heuristic information—A is for any given consistent heuristic. That is,
no other optimal algorithm is guaranteed to expand fewer nodes than A
(except possibly through tie-breaking among nodes with $f(n)\eq C^*$).
This is because any algorithm that *does not* expand all
nodes with $f(n)< C^*$ runs the risk of missing the optimal solution.

That A search is complete, optimal, and optimally efficient among all
such algorithms is rather satisfying. Unfortunately, it does not mean
that A is the answer to all our searching needs. The catch is that, for
most problems, the number of states within the goal contour search space
is still exponential in the length of the solution. The details of the
analysis are beyond the scope of this book, but the basic results are as
follows. For problems with constant step costs, the growth in run time
as a function of the optimal solution depth $d$ is analyzed in terms of
the the or the of the heuristic. The absolute error is defined as
$\Delta \equiv h^* - h$, where $h^*$ is the actual cost of getting from
the root to the goal, and the relative error is defined as
$\epsilon\equiv (h^* - h)/h^*$.

The complexity results depend very strongly on the assumptions made
about the state space. The simplest model studied is a state space that
has a single goal and is essentially a tree with reversible actions.
(The 8-puzzle satisfies the first and third of these assumptions.) In
this case, the time complexity of A is exponential in the maximum
absolute error, that is, $O(b^\Delta)$. For constant step costs, we can
write this as $O(b^{\epsilon d})$, where $d$ is the solution depth. For
almost all heuristics in practical use, the absolute error is at least
proportional to the path cost $h^*$, so $\epsilon$ is constant or
growing and the time complexity is exponential in $d$. We can also see
the effect of a more accurate heuristic:
$O(b^{\epsilon d}) \eq O((b^{\epsilon})^d)$, so the effective branching
factor (defined more formally in the next section) is $b^{\epsilon}$.

When the state space has many goal states—particularly
*near-optimal* goal states—the search process can be led
astray from the optimal path and there is an extra cost proportional to
the number of goals whose cost is within a factor $\epsilon$ of the
optimal cost. Finally, in the general case of a graph, the situation is
even worse. There can be exponentially many states with $f(n)< C^*$ even
if the absolute error is bounded by a constant. For example, consider a
version of the vacuum world where the agent can clean up any square for
unit cost without even having to visit it: in that case, squares can be
cleaned in any order. With $N$ initially dirty squares, there are $2^N$
states where some subset has been cleaned and all of them are on an
optimal solution path—and hence satisfy $f(n)< C^*$—even if the
heuristic has an error of 1.

The complexity of A often makes it impractical to insist on finding an
optimal solution. One can use variants of A that find suboptimal
solutions quickly, or one can sometimes design heuristics that are more
accurate but not strictly admissible. In any case, the use of a good
heuristic still provides enormous savings compared to the use of an
uninformed search. In , we look at the question of designing good
heuristics.

Computation time is not, however, A’s main drawback. Because it keeps
all generated nodes in memory (as do all algorithms), A usually runs out
of space long before it runs out of time. For this reason, A is not
practical for many large-scale problems. There are, however, algorithms
that overcome the space problem without sacrificing optimality or
completeness, at a small cost in execution time. We discuss these next.

### Memory-bounded heuristic search {#memory-bounded-search-section}

The simplest way to reduce memory requirements for A is to adapt the
idea of iterative deepening to the heuristic search context, resulting
in the (IDA) algorithm. The main difference between IDA and standard
iterative deepening is that the cutoff used is the $f$-cost ($g+h$)
rather than the depth; at each iteration, the cutoff value is the
smallest $f$-cost of any node that exceeded the cutoff on the previous
iteration. IDA is practical for many problems with unit step costs and
avoids the substantial overhead associated with keeping a sorted queue
of nodes. Unfortunately, it suffers from the same difficulties with
real-valued costs as does the iterative version of uniform-cost search
described in . This section briefly examines two other memory-bounded
algorithms, called RBFS and MA.

[rbfs-algorithm]

[rbfs-progress-figure]

(RBFS) is a simple recursive algorithm that attempts to mimic the
operation of standard best-first search, but using only linear space.
The algorithm is shown in . Its structure is similar to that of a
recursive depth-first search, but rather than continuing indefinitely
down the current path, it uses the f\_limit variable to keep track of
the $f$-value of the best *alternative* path available from
any ancestor of the current node. If the current node exceeds this
limit, the recursion unwinds back to the alternative path. As the
recursion unwinds, RBFS replaces the $f$-value of each node along the
path with a —the best $f$-value of its children. In this way, RBFS
remembers the $f$-value of the best leaf in the forgotten subtree and
can therefore decide whether it’s worth reexpanding the subtree at some
later time. shows how RBFS reaches Bucharest.

RBFS is somewhat more efficient than IDA, but still suffers from
excessive node regeneration. In the example in , RBFS follows the path
via Rimnicu Vilcea, then “changes its mind” and tries Fagaras, and then
changes its mind back again. These mind changes occur because every time
the current best path is extended, its $f$-value is likely to
increase—$h$ is usually less optimistic for nodes closer to the goal.
When this happens, the second-best path might become the best path, so
the search has to backtrack to follow it. Each mind change corresponds
to an iteration of IDA and could require many reexpansions of forgotten
nodes to recreate the best path and extend it one more node.

Like A tree search, RBFS is an optimal algorithm if the heuristic
function $h(n)$ is admissible. Its space complexity is linear in the
depth of the deepest optimal solution, but its time complexity is rather
difficult to characterize: it depends both on the accuracy of the
heuristic function and on how often the best path changes as nodes are
expanded.

IDA and RBFS suffer from using *too little* memory. Between
iterations, IDA retains only a single number: the current $f$-cost
limit. RBFS retains more information in memory, but it uses only linear
space: even if more memory were available, RBFS has no way to make use
of it. Because they forget most of what they have done, both algorithms
may end up reexpanding the same states many times over. Furthermore,
they suffer the potentially exponential increase in complexity
associated with redundant paths in graphs (see ).

It seems sensible, therefore, to use all available memory. Two
algorithms that do this are (memory-bounded A) and (simplified MA). SMA
is—well—simpler, so we will describe it. SMA proceeds just like A,
expanding the best leaf until memory is full. At this point, it cannot
add a new node to the search tree without dropping an old one. SMA
always drops the *worst* leaf node—the one with the highest
$f$-value. Like RBFS, SMA then backs up the value of the forgotten node
to its parent. In this way, the ancestor of a forgotten subtree knows
the quality of the best path in that subtree. With this information, SMA
regenerates the subtree only when all other paths have been shown to
look worse than the path it has forgotten. Another way of saying this is
that, if all the descendants of a node $n$ are forgotten, then we will
not know which way to go from $n$, but we will still have an idea of how
worthwhile it is to go anywhere from $n$.

The complete algorithm is too complicated to reproduce here,[^10] but
there is one subtlety worth mentioning. We said that SMA expands the
best leaf and deletes the worst leaf. What if *all* the
leaf nodes have the same $f$-value? To avoid selecting the same node for
deletion and expansion, SMA expands the *newest* best leaf
and deletes the *oldest* worst leaf. These coincide when
there is only one leaf, but in that case, the current search tree must
be a single path from root to leaf that fills all of memory. If the leaf
is not a goal node, then *even if it is on an optimal solution
path*, that solution is not reachable with the available memory.
Therefore, the node can be discarded exactly as if it had no successors.

SMA is complete if there is any reachable solution—that is, if $d$, the
depth of the shallowest goal node, is less than the memory size
(expressed in nodes). It is optimal if any optimal solution is
reachable; otherwise, it returns the best reachable solution. In
practical terms, SMA is a fairly robust choice for finding optimal
solutions, particularly when the state space is a graph, step costs are
not uniform, and node generation is expensive compared to the overhead
of maintaining the frontier and the explored set.

On very hard problems, however, it will often be the case that SMA is
forced to switch back and forth continually among many candidate
solution paths, only a small subset of which can fit in memory. (This
resembles the problem of in disk paging systems.) Then the extra time
required for repeated regeneration of the same nodes means that problems
that would be practically solvable by A, given unlimited memory, become
intractable for SMA. That is to say,

memory limitations can make a problem intractable from the point of view
of computation time.

Although no current theory explains the tradeoff between time and
memory, it seems that this is an inescapable problem. The only way out
is to drop the optimality requirement.

### Learning to search better {#metalevel-search-section}

We have presented several fixed strategies—breadth-first, greedy
best-first, and so on—that have been designed by computer scientists.
Could an agent *learn* how to search better? The answer is
yes, and the method rests on an important concept called the . Each
state in a metalevel state space captures the internal (computational)
state of a program that is searching in an such as Romania. For example,
the internal state of the A algorithm consists of the current search
tree. Each action in the metalevel state space is a computation step
that alters the internal state; for example, each computation step in A
expands a leaf node and adds its successors to the tree. Thus, , which
shows a sequence of larger and larger search trees, can be seen as
depicting a path in the metalevel state space where each state on the
path is an object-level search tree.

Now, the path in has five steps, including one step, the expansion of
Fagaras, that is not especially helpful. For harder problems, there will
be many such missteps, and a algorithm can learn from these experiences
to avoid exploring unpromising subtrees. The techniques used for this
kind of learning are described in . The goal of learning is to minimize
the of problem solving, trading off computational expense and path cost.

Heuristic Functions
-------------------

[heuristic-section]

In this section, we look at heuristics for the 8-puzzle, in order to
shed light on the nature of heuristics in general.

The 8-puzzle was one of the earliest heuristic search problems. As
mentioned in , the object of the puzzle is to slide the tiles
horizontally or vertically into the empty space until the configuration
matches the goal configuration ().

[8puzzle-repeat-figure]

The average solution cost for a randomly generated 8-puzzle instance is
about 22 steps. The branching factor is about 3. (When the empty tile is
in the middle, four moves are possible; when it is in a corner, two; and
when it is along an edge, three.) This means that an exhaustive tree
search to depth 22 would look at about
$3^{{22}} \approx {3.1} \stimes {10}^{{10}}$ states. A graph search
would cut this down by a factor of about 170,000 because only
$9!/2 = {181},{440}$ distinct states are reachable. (See .) This is a
manageable number, but the corresponding number for the 15-puzzle is
roughly ${10}^{{13}}$, so the next order of business is to find a good
heuristic function. If we want to find the shortest solutions by using
A, we need a heuristic function that never overestimates the number of
steps to the goal. There is a long history of such heuristics for the
15-puzzle; here are two commonly used candidates:

-   $h_1 = $ the number of misplaced tiles. For , all of the eight tiles
    are out of position, so the start state would have $h_1 = 8$. $h_1$
    is an admissible heuristic because it is clear that any tile that is
    out of place must be moved at least once.

-   $h_2 = $ the sum of the distances of the tiles from their goal
    positions. Because tiles cannot move along diagonals, the distance
    we will count is the sum of the horizontal and vertical distances.
    This is sometimes called the or . $h_2$ is also admissible because
    all any move can do is move one tile one step closer to the goal.
    Tiles 1 to 8 in the start state give a Manhattan distance of
    $$h_2 = 3+1+2+2+2+3+3+2 = {18} \ .$$

As expected, neither of these overestimates the true solution cost,
which is 26.

### The effect of heuristic accuracy on performance

One way to characterize the quality of a heuristic is the $b^*$. If the
total number of nodes generated by A for a particular problem is $N$ and
the solution depth is $d$, then $b^*$ is the branching factor that a
uniform tree of depth $d$ would have to have in order to contain $N+1$
nodes. Thus, $$N + 1 = 1 + b^* + (b^*)^2 + \cdots + (b^*)^d\ .$$ For
example, if A finds a solution at depth 5 using 52 nodes, then the
effective branching factor is 1.92. The effective branching factor can
vary across problem instances, but usually it is fairly constant for
sufficiently hard problems. (The existence of an effective branching
factor follows from the result, mentioned earlier, that the number of
nodes expanded by A grows exponentially with solution depth.) Therefore,
experimental measurements of $b^*$ on a small set of problems can
provide a good guide to the heuristic’s overall usefulness. A
well-designed heuristic would have a value of $b^*$ close to 1, allowing
fairly large problems to be solved at reasonable computational cost.

To test the heuristic functions $h_1$ and $h_2$, we generated 1200
random problems with solution lengths from 2 to 24 (100 for each even
number) and solved them with iterative deepening search and with A tree
search using both $h_1$ and $h_2$. gives the average number of nodes
generated by each strategy and the effective branching factor. The
results suggest that $h_2$ is better than $h_1$, and is far better than
using iterative deepening search. Even for small problems with
$d\eq 12$, A with $h_{2}$ is 50,000 times more efficient than uninformed
iterative deepening search.

[ht] [heuristic-comparison-table]

One might ask whether $h_2$ is *always* better than $h_1$.
The answer is “Essentially, yes.” It is easy to see from the definitions
of the two heuristics that, for any node $n$, $h_2(n) \ge h_1(n)$. We
thus say that $h_2$ $h_1$. Domination translates directly into
efficiency: A using $h_2$ will never expand more nodes than A using
$h_1$ (except possibly for some nodes with $f(n)\eq C^*$). The argument
is simple. Recall the observation on that every node with $f(n) < C^*$
will surely be expanded. This is the same as saying that every node with
$h(n) < C^*
- g(n)$ will surely be expanded. But because $h_2$ is at least as big as
$h_1$ for all nodes, every node that is surely expanded by A search with
$h_2$ will also surely be expanded with $h_1$, and $h_1$ might cause
other nodes to be expanded as well. Hence, it is generally better to use
a heuristic function with higher values, provided it is consistent and
that the computation time for the heuristic is not too long.

### Generating admissible heuristics from relaxed problems

We have seen that both $h_1$ (misplaced tiles) and $h_2$ (Manhattan
distance) are fairly good heuristics for the 8-puzzle and that $h_2$ is
better. How might one have come up with $h_2$? Is it possible for a
computer to invent such a heuristic mechanically?

$h_1$ and $h_2$ are estimates of the remaining path length for the
8-puzzle, but they are also perfectly accurate path lengths for
*simplified* versions of the puzzle. If the rules of the
puzzle were changed so that a tile could move anywhere instead of just
to the adjacent empty square, then $h_1$ would give the exact number of
steps in the shortest solution. Similarly, if a tile could move one
square in any direction, even onto an occupied square, then $h_2$ would
give the exact number of steps in the shortest solution. A problem with
fewer restrictions on the actions is called a . The state-space graph of
the relaxed problem is a *supergraph* of the original state
space because the removal of restrictions creates added edges in the
graph.

Because the relaxed problem adds edges to the state space, any optimal
solution in the original problem is, by definition, also a solution in
the relaxed problem; but the relaxed problem may have
*better* solutions if the added edges provide short cuts.
Hence,

the cost of an optimal solution to a relaxed problem is an admissible
heuristic for the original problem.

Furthermore, because the derived heuristic is an exact cost for the
relaxed problem, it must obey the triangle inequality and is therefore
(see ).

If a problem definition is written down in a formal language, it is
possible to construct relaxed problems automatically.[^11] For example,
if the 8-puzzle actions are described as

\

we can generate three relaxed problems by removing one or both of the
conditions:

\
\

From (a), we can derive $h_{2}$ (Manhattan distance). The reasoning is
that $h_{2}$ would be the proper score if we moved each tile in turn to
its destination. The heuristic derived from (b) is discussed in
[Gaschnig-h-page]. From (c), we can derive $h_{1}$ (misplaced tiles)
because it would be the proper score if tiles could move to their
intended destination in one step. Notice that it is crucial that the
relaxed problems generated by this technique can be solved essentially
*without search*, because the relaxed rules allow the
problem to be decomposed into eight independent subproblems. If the
relaxed problem is hard to solve, then the values of the corresponding
heuristic will be expensive to obtain.[^12]

A program called can generate heuristics automatically from problem
definitions, using the “relaxed problem” method and various other
techniques @Prieditis:1993. generated a new heuristic for the 8-puzzle
that was better than any preexisting heuristic and found the first
useful heuristic for the famous Rubik’s Cube puzzle.

One problem with generating new heuristic functions is that one often
fails to get a single “clearly best” heuristic. If a collection of
admissible heuristics $h_1 \ldots h_m$ is available for a problem and
none of them dominates any of the others, which should we choose? As it
turns out, we need not make a choice. We can have the best of all
worlds, by defining $$h(n) = \max\{h_1(n),\ldots,h_m(n)\}\ .$$ This
composite heuristic uses whichever function is most accurate on the node
in question. Because the component heuristics are admissible, $h$ is
admissible; it is also easy to prove that $h$ is consistent.
Furthermore, $h$ dominates all of its component heuristics.

### Generating admissible heuristics from subproblems: Pattern databases {#pattern-database-section}

Admissible heuristics can also be derived from the solution cost of a of
a given problem. For example, shows a subproblem of the 8-puzzle
instance in . The subproblem involves getting tiles 1, 2, 3, 4 into
their correct positions. Clearly, the cost of the optimal solution of
this subproblem is a lower bound on the cost of the complete problem. It
turns out to be more accurate than Manhattan distance in some cases.

[8puzzle-pattern-figure]

The idea behind is to store these exact solution costs for every
possible subproblem instance—in our example, every possible
configuration of the four tiles and the blank. (The locations of the
other four tiles are irrelevant for the purposes of solving the
subproblem, but moves of those tiles do count toward the cost.) Then we
compute an admissible heuristic $h_{{DB}}$ for each complete state
encountered during a search simply by looking up the corresponding
subproblem configuration in the database. The database itself is
constructed by searching back[^13] from the goal and recording the cost
of each new pattern encountered; the expense of this search is amortized
over many subsequent problem instances.

The choice of 1-2-3-4 is fairly arbitrary; we could also construct
databases for 5-6-7-8, for 2-4-6-8, and so on. Each database yields an
admissible heuristic, and these heuristics can be combined, as explained
earlier, by taking the maximum value. A combined heuristic of this kind
is much more accurate than the Manhattan distance; the number of nodes
generated when solving random 15-puzzles can be reduced by a factor of
1000.

One might wonder whether the heuristics obtained from the 1-2-3-4
database and the 5-6-7-8 could be *added*, since the two
subproblems seem not to overlap. Would this still give an admissible
heuristic? The answer is no, because the solutions of the 1-2-3-4
subproblem and the 5-6-7-8 subproblem for a given state will almost
certainly share some moves—it is unlikely that 1-2-3-4 can be moved into
place without touching 5-6-7-8, and vice versa. But what if we don’t
count those moves? That is, we record not the total cost of solving the
1-2-3-4 subproblem, but just the number of moves involving 1-2-3-4. Then
it is easy to see that the sum of the two costs is still a lower bound
on the cost of solving the entire problem. This is the idea behind .
With such databases, it is possible to solve random 15-puzzles in a few
milliseconds—the number of nodes generated is reduced by a factor of
10,000 compared with the use of Manhattan distance. For 24-puzzles, a
speedup of roughly a factor of a million can be obtained.

Disjoint pattern databases work for sliding-tile puzzles because the
problem can be divided up in such a way that each move affects only one
subproblem—because only one tile is moved at a time. For a problem such
as Rubik’s Cube, this kind of subdivision is difficult because each move
affects 8 or 9 of the 26 cubies. More general ways of defining additive,
admissible heuristics have been proposed that do apply to Rubik’s
cube @Yang+al:2008, but they have not yielded a heuristic better than
the best nonadditive heuristic for the problem.

### Learning heuristics from experience

A heuristic function $h(n)$ is supposed to estimate the cost of a
solution beginning from the state at node $n$. How could an agent
construct such a function? One solution was given in the preceding
sections—namely, to devise relaxed problems for which an optimal
solution can be found easily. Another solution is to learn from
experience. “Experience” here means solving lots of 8-puzzles, for
instance. Each optimal solution to an 8-puzzle problem provides examples
from which $h(n)$ can be learned. Each example consists of a state from
the solution path and the actual cost of the solution from that point.
From these examples, a learning algorithm can be used to construct a
function $h(n)$ that can (with luck) predict solution costs for other
states that arise during search. Techniques for doing just this using
neural nets, decision trees, and other methods are demonstrated in .
(The reinforcement learning methods described in are also applicable.)

Inductive learning methods work best when supplied with of a state that
are relevant to predicting the state’s value, rather than with just the
raw state description. For example, the feature “number of misplaced
tiles” might be helpful in predicting the actual distance of a state
from the goal. Let’s call this feature $x_1(n)$. We could take 100
randomly generated 8-puzzle configurations and gather statistics on
their actual solution costs. We might find that when $x_1(n)$ is 5, the
average solution cost is around 14, and so on. Given these data, the
value of $x_1$ can be used to predict $h(n)$. Of course, we can use
several features. A second feature $x_2(n)$ might be “number of pairs of
adjacent tiles that are not adjacent in the goal state.” How should
$x_1(n)$ and $x_2(n)$ be combined to predict $h(n)$? A common approach
is to use a linear combination: $$h(n) = c_1 x_1(n) + c_2 x_2(n)\ .$$
The constants $c_1$ and $c_2$ are adjusted to give the best fit to the
actual data on solution costs. One expects both $c_1$ and $c_2$ to be
positive because misplaced tiles and incorrect adjacent pairs make the
problem harder to solve. Notice that this heuristic does satisfy the
condition that $h(n)\eq 0$ for goal states, but it is not necessarily
admissible or consistent.

This chapter has introduced methods that an agent can use to select
actions in environments that are deterministic, observable, static, and
completely known. In such cases, the agent can construct sequences of
actions that achieve its goals; this process is called .

-   Before an agent can start searching for solutions, a must be
    identified and a well-defined must be formulated.

-   A problem consists of five parts: the , a set of , a describing the
    results of those actions, a function, and a function. The
    environment of the problem is represented by a . A through the state
    space from the initial state to a goal state is a .

-   Search algorithms treat states and actions as : they do not consider
    any internal structure they might possess.

-   A general algorithm considers all possible paths to find a solution,
    whereas a algorithm avoids consideration of redundant paths.

-   Search algorithms are judged on the basis of , , , and . Complexity
    depends on $b$, the branching factor in the state space, and $d$,
    the depth of the shallowest solution.

-   methods have access only to the problem definition. The basic
    algorithms are as follows:

    -   expands the shallowest nodes first; it is complete, optimal for
        unit step costs, but has exponential space complexity.

    -   expands the node with lowest path cost, $g(n)$, and is optimal
        for general step costs.

    -   expands the deepest unexpanded node first. It is neither
        complete nor optimal, but has linear space complexity. adds a
        depth bound.

    -   calls depth-first search with increasing depth limits until a
        goal is found. It is complete, optimal for unit step costs, has
        time complexity comparable to breadth-first search, and has
        linear space complexity.

    -   can enormously reduce time complexity, but it is not always
        applicable and may require too much space.

-   methods may have access to a function $h(n)$ that estimates the cost
    of a solution from $n$.

    -   The generic algorithm selects a node for expansion according to
        an .

    -   expands nodes with minimal $h(n)$. It is not optimal but is
        often efficient.

    -   expands nodes with minimal $f(n) = g(n) + h(n)$. A is complete
        and optimal, provided that $h(n)$ is admissible (for ) or
        consistent (for ). The space complexity of A is still
        prohibitive.

    -   (recursive best-first search) and (simplified memory-bounded A)
        are robust, optimal search algorithms that use limited amounts
        of memory; given enough time, they can solve problems that A
        cannot solve because it runs out of memory.

-   The performance of heuristic search algorithms depends on the
    quality of the heuristic function. One can sometimes construct good
    heuristics by relaxing the problem definition, by storing
    precomputed solution costs for subproblems in a pattern database, or
    by learning from experience with the problem class.

The topic of state-space search originated in more or less its current
form in the early years of AI. Newell and Simon’s work on the Logic
Theorist [-@Newell+al:1957] and GPS [-@Newell+Simon:1961] led to the
establishment of search algorithms as the primary weapons in the armory
of 1960s AI researchers and to the establishment of problem solving as
the canonical AI task. Work in operations research by Richard
Bellman [-@Bellman:1957] showed the importance of additive path costs in
simplifying optimization algorithms. The text on *Automated
Problem Solving* by Nils Nilsson [-@Nilsson:1971] established the
area on a solid theoretical footing.

Most of the state-space search problems analyzed in this chapter have a
long history in the literature and are less trivial than they might
seem. The missionaries and cannibals problem used in was analyzed in
detail by Amarel [-@Amarel:1968]. It had been considered earlier—in AI
by Simon and Newell [-@Simon+Newell:1961] and in operations research by
Bellman and Dreyfus [-@Bellman+Dreyfus:1962].

The 8-puzzle is a smaller cousin of the 15-puzzle, whose history is
recounted at length by . It was widely believed to have been invented by
the famous American game designer Sam Loyd, based on his claims to that
effect from 1891 onward @Loyd:1959. Actually it was invented by Noyes
Chapman, a postmaster in Canastota, New York, in the mid-1870s. (Chapman
was unable to patent his invention, as a generic patent covering sliding
blocks with letters, numbers, or pictures was granted to Ernest Kinsey
in 1878.) It quickly attracted the attention of the public and of
mathematicians @Johnson+Story:1879 [@Tait:1880]. The editors of the
*American Journal of Mathematics* stated, “The ‘15’ puzzle
for the last few weeks has been prominently before the American public,
and may safely be said to have engaged the attention of nine out of ten
persons of both sexes and all ages and conditions of the community.”
Ratner and Warmuth [-@Ratner+Warmuth:1986] showed that the general
$n\times n$ version of the 15-puzzle belongs to the class of problems.

The 8-queens problem was first published anonymously in the German chess
magazine *Schach* in 1848; it was later attributed to one
Max Bezzel. It was republished in 1850 and at that time drew the
attention of the eminent mathematician Carl Friedrich Gauss, who
attempted to enumerate all possible solutions; initially he found only
72, but eventually he found the correct answer of 92, although Nauck
published all 92 solutions first, in 1850. Netto [-@Netto:1901]
generalized the problem to $n$ queens, and Abramson and
Yung [-@Abramson+Yung:1989] found an $O(n)$ algorithm.

Each of the real-world search problems listed in the chapter has been
the subject of a good deal of research effort. Methods for selecting
optimal airline flights remain proprietary for the most part, but Carl
de Marcken (personal communication) has shown that airline ticket
pricing and restrictions have become so convoluted that the problem of
selecting an optimal flight is formally *undecidable*. The
traveling-salesperson problem is a standard combinatorial problem in
theoretical computer science @Lawler+al:1992. proved the TSP to be
NP-hard, but effective heuristic approximation methods were
developed @Lin+Kernighan:1973. devised a fully polynomial approximation
scheme for Euclidean TSPs. VLSI layout methods are surveyed by , and
many layout optimization papers appear in VLSI journals. Robotic
navigation and assembly problems are discussed in .

Uninformed search algorithms for problem solving are a central topic of
classical computer science @Horowitz+Sahni:1978 and operations
research @Dreyfus:1969. Breadth-first search was formulated for solving
mazes by . The method of  @Bellman:1957 [@Bellman+Dreyfus:1962], which
systematically records solutions for all subproblems of increasing
lengths, can be seen as a form of breadth-first search on graphs. The
two-point shortest-path algorithm of is the origin of uniform-cost
search. These works also introduced the idea of explored and frontier
sets (closed and open lists).

A version of iterative deepening designed to make efficient use of the
chess clock was first used by Slate and Atkin [-@Slate+Atkin:1977] in
the Chess 4.5 game-playing program. Martelli’s algorithm
B [-@Martelli:1977] includes an iterative deepening aspect and also
dominates A’s worst-case performance with admissible but inconsistent
heuristics. The iterative deepening technique came to the fore in work
by . Bidirectional search, which was introduced by Pohl [-@Pohl:1971],
can also be effective in some cases.

The use of heuristic information in problem solving appears in an early
paper by , but the phrase “heuristic search” and the use of heuristic
functions that estimate the distance to the goal came somewhat
later @Newell+Ernst:1965 [@Lin:1965]. Doran and
Michie [-@Doran+Michie:1966] conducted extensive experimental studies of
heuristic search. Although they analyzed path length and “penetrance”
(the ratio of path length to the total number of nodes examined so far),
they appear to have ignored the information provided by the path cost
$g(n)$. The A algorithm, incorporating the current path cost into
heuristic search, was developed by Hart, Nilsson, and
Raphael [-@Hart+al:1968], with some later corrections @Hart+al:1972.
demonstrated the optimal efficiency of A.

The original A paper introduced the consistency condition on heuristic
functions. The monotone condition was introduced by Pohl [-@Pohl:1977]
as a simpler replacement, but Pearl [-@Pearl:1984] showed that the two
were equivalent.

Pohl [-@Pohl:1977] pioneered the study of the relationship between the
error in heuristic functions and the time complexity of A. Basic results
were obtained for tree search with unit step costs and a single goal
node @Pohl:1977 [@Gaschnig:1979; @Huyn+al:1980; @Pearl:1984] and with
multiple goal nodes @Dinh+al:2007. The “effective branching factor” was
proposed by Nilsson [-@Nilsson:1971] as an empirical measure of the
efficiency; it is equivalent to assuming a time cost of $O((b^*)^d)$.
For tree search applied to a graph, argue that the time cost is better
modeled as $O(b^{d-k})$, where $k$ depends on the heuristic accuracy;
this analysis has elicited some controversy, however. For graph search,
noted that several well-known problems contained exponentially many
nodes on optimal solution paths, implying exponential time complexity
for A even with constant absolute error in $h$.

There are many variations on the A algorithm. Pohl [-@Pohl:1973]
proposed the use of *dynamic weighting*, which uses a
weighted sum $f_w(n)
\eq w_{g}g(n) + w_{h}h(n)$ of the current path length and the heuristic
function as an evaluation function, rather than the simple sum $f(n) \eq
g(n) + h(n)$ used in A. The weights $w_{g}$ and $w_{h}$ are adjusted
dynamically as the search progresses. Pohl’s algorithm can be shown to
be $\epsilon$-admissible—that is, guaranteed to find solutions within a
factor $1+\epsilon$ of the optimal solution, where $\epsilon$ is a
parameter supplied to the algorithm. The same property is exhibited by
the A$^*_\epsilon$ algorithm @Pearl:1984, which can select any node from
the frontier provided its $f$-cost is within a factor $1+\epsilon$ of
the lowest-$f$-cost frontier node. The selection can be done so as to
minimize search cost.

Bidirectional versions of A have been investigated; a combination of
bidirectional A and known landmarks was used to efficiently find driving
routes for Microsoft’s online map service @Goldberg+al:2006. After
caching a set of paths between landmarks, the algorithm can find an
optimal path between any pair of points in a 24 million point graph of
the United States, searching less than 0.1% of the graph. Others
approaches to bidirectional search include a breadth-first search
backward from the goal up to a fixed depth, followed by a forward IDA
search @Dillenberg+Nelson:1994 [@Manzini:1995].

A and other state-space search algorithms are closely related to the
*branch-and-bound* techniques that are widely used in
operations research  @Lawler+Wood:1966. The relationships between
state-space search and branch-and-bound have been investigated in depth
@Kumar+Kanal:1983 [@Nau+al:1984; @Kumar+al:1988]. Martelli and
Montanari [-@Martelli+Montanari:1978] demonstrate a connection between
dynamic programming (see ) and certain types of state-space search.
Kumar and Kanal [-@Kumar+Kanal:1988] attempt a “grand unification” of
heuristic search, dynamic programming, and branch-and-bound techniques
under the name of CDP—the “composite decision process.”

Because computers in the late 1950s and early 1960s had at most a few
thousand words of main memory, memory-bounded heuristic search was an
early research topic. The Graph Traverser @Doran+Michie:1966, one of the
earliest search programs, commits to an operator after searching
best-first up to the memory limit. IDA @Korf:1985b [@Korf:1985a] was the
first widely used optimal, memory-bounded heuristic search algorithm,
and a large number of variants have been developed. An analysis of the
efficiency of IDA and of its difficulties with real-valued heuristics
appears in Patrick *et al.* [-@Patrick+al:1992].

RBFS @Korf:1993 is actually somewhat more complicated than the algorithm
shown in , which is closer to an independently developed algorithm
called  @Russell:1992. RBFS uses a lower bound as well as the upper
bound; the two algorithms behave identically with admissible heuristics,
but RBFS expands nodes in best-first order even with an inadmissible
heuristic. The idea of keeping track of the best alternative path
appeared earlier in Bratko’s [-@Bratko:1986] elegant Prolog
implementation of A and in the DTA algorithm @Russell+Wefald:1991. The
latter work also discusses metalevel state spaces and metalevel
learning.

The MA algorithm appeared in . SMA, or Simplified MA, emerged from an
attempt to implement MA as a comparison algorithm for IE @Russell:1992.
Kaindl and Khorsand [-@Kaindl+Khorsand:1994] have applied SMA to produce
a bidirectional search algorithm that is substantially faster than
previous algorithms. describe a divide-and-conquer approach, and
introduce memory-bounded A graph search and a strategy for switching to
breadth-first search to increase memory-efficiency @Zhou+Hansen:2006.
Korf [-@Korf:1995] surveys memory-bounded search techniques.

The idea that admissible heuristics can be derived by problem relaxation
appears in the seminal paper by , who used the minimum-spanning-tree
heuristic to solve the TSP. (See .)

The automation of the relaxation process was implemented successfully by
Prieditis [-@Prieditis:1993], building on earlier work with
Mostow @Mostow+Prieditis:1989. describe more recent steps towards
automating the process. The use of pattern databases to derive
admissible heuristics is due to and ; disjoint pattern databases are
described by ; a similar method using symbolic patterns is due to . show
how to compress pattern databases to save space. The probabilistic
interpretation of heuristics was investigated in depth by and .

By far the most comprehensive source on heuristics and heuristic search
algorithms is Pearl’s [-@Pearl:1984] *Heuristics* text.
This book provides especially good coverage of the wide variety of
offshoots and variations of A, including rigorous proofs of their formal
properties. Kanal and Kumar [-@Kanal+Kumar:1988] present an anthology of
important articles on heuristic search, and cover approaches from
Operations Research. Papers about new search algorithms—which,
remarkably, continue to be discovered—appear in journals such as
*Artificial Intelligence* and *Journal of the
ACM*.

The topic of algorithms was not covered in the chapter, partly because
it requires a lengthy discussion of parallel computer architectures.
Parallel search became a popular topic in the 1990s in both AI and
theoretical computer science @Mahanti+Daniels:1993
[@Grama+Kumar:1995; @Crauser+al:1998] and is making a comeback in the
era of new multicore and cluster architectures @Ralphs+al:2004
[@Korf+Schultze:2005]. Also of increasing importance are search
algorithms for very large graphs that require disk storage @Korf:2008.

Explain why problem formulation must follow goal formulation.

Give a complete problem formulation for each of the following problems.
Choose a formulation that is precise enough to be implemented.

1.  There are six glass boxes in a row, each with a lock. Each of the
    first five boxes holds a key unlocking the next box in line; the
    last box holds a banana. You have the key to the first box, and you
    want the banana.

2.  You start with the sequence ABABAECCEC, or in general any sequence
    made from A, B, C, and E. You can transform this sequence using the
    following equalities: AC = E, AB = BC, BB = E, and E$x$ = $x$ for
    any $x$. For example, ABBC can be transformed into AEC, and then AC,
    and then E. Your goal is to produce the sequence E.

3.  There is an $n \stimes n$ grid of squares, each square initially
    being either unpainted floor or a bottomless pit. You start standing
    on an unpainted floor square, and can either paint the square under
    you or move onto an adjacent unpainted floor square. You want the
    whole floor painted.

4.  A container ship is in port, loaded high with containers. There 13
    rows of containers, each 13 containers wide and 5 containers tall.
    You control a crane that can move to any location above the ship,
    pick up the container under it, and move it onto the dock. You want
    the ship unloaded.

Your goal is to navigate a robot out of a maze. The robot starts in the
center of the maze facing north. You can turn the robot to face north,
east, south, or west. You can direct the robot to move forward a certain
distance, although it will stop before hitting a wall.

1.  Formulate this problem. How large is the state space?

2.  In navigating a maze, the only place we need to turn is at the
    intersection of two or more corridors. Reformulate this problem
    using this observation. How large is the state space now?

3.  From each point in the maze, we can move in any of the four
    directions until we reach a turning point, and this is the only
    action we need to do. Reformulate the problem using these actions.
    Do we need to keep track of the robot’s orientation now?

4.  In our initial description of the problem we already abstracted from
    the real world, restricting actions and removing details. List three
    such simplifications we made.

You have a $9 \stimes 9$ grid of squares, each of which can be colored
red or blue. The grid is initially colored all blue, but you can change
the color of any square any number of times. Imagining the grid divided
into nine $3 \stimes 3$ sub-squares, you want each sub-square to be all
one color but neighboring sub-squares to be different colors.

1.  Formulate this problem in the straightforward way. Compute the size
    of the state space.

2.  You need color a square only once. Reformulate, and compute the size
    of the state space. Would breadth-first graph search perform faster
    on this problem than on the one in (a)? How about iterative
    deepening tree search?

3.  Given the goal, we need consider only colorings where each
    sub-square is uniformly colored. Reformulate the problem and compute
    the size of the state space.

4.  How many solutions does this problem have?

5.  Parts (b) and (c) successively abstracted the original problem (a).
    Can you give a translation from solutions in problem (c) into
    solutions in problem (b), and from solutions in problem (b) into
    solutions for problem (a)?

[two-friends-exercise]Suppose two friends live in different cities on a
map, such as the Romania map shown in . On every turn, we can
simultaneously move each friend to a neighboring city on the map. The
amount of time needed to move from city $i$ to neighbor $j$ is equal to
the road distance $d(i,j)$ between the cities, but on each turn the
friend that arrives first must wait until the other one arrives (and
calls the first on his/her cell phone) before the next turn can begin.
We want the two friends to meet as quickly as possible.

1.  Write a detailed formulation for this search problem. (You will find
    it helpful to define some formal notation here.)

2.  Let $D(i,j)$ be the straight-line distance between cities $i$ and
    $j$. Which of the following heuristic functions are admissible? (i)
    $D(i,j)$; (ii) $2\cdot D(i,j)$; (iii) $D(i,j)/2$.

3.  Are there completely connected maps for which no solution exists?

4.  Are there maps in which all solutions require one friend to visit
    the same city twice?

[8puzzle-parity-exercise] Show that the 8-puzzle states are divided into
two disjoint sets, such that any state is reachable from any other state
in the same set, while no state is reachable from any state in the other
set. (*Hint:* See .) Devise a procedure to decide which set
a given state is in, and explain why this is useful for generating
random states.

[nqueens-size-exercise] Consider the $n$-queens problem using the
“efficient” incremental formulation given on . Explain why the state
space has at least $\sqrt[3]{n!}$ states and estimate the largest $n$
for which exhaustive exploration is feasible. (*Hint*:
Derive a lower bound on the branching factor by considering the maximum
number of squares that a queen can attack in any column.)

Give a complete problem formulation for each of the following. Choose a
formulation that is precise enough to be implemented.

1.  Using only four colors, you have to color a planar map in such a way
    that no two adjacent regions have the same color.

2.  A 3-foot-tall monkey is in a room where some bananas are suspended
    from the 8-foot ceiling. He would like to get the bananas. The room
    contains two stackable, movable, climbable 3-foot-high crates.

3.  You have a program that outputs the message “illegal input record”
    when fed a certain file of input records. You know that processing
    of each record is independent of the other records. You want to
    discover what record is illegal.

4.  You have three jugs, measuring 12 gallons, 8 gallons, and 3 gallons,
    and a water faucet. You can fill the jugs up or empty them out from
    one to another or onto the ground. You need to measure out exactly
    one gallon.

[path-planning-exercise]Consider the problem of finding the shortest
path between two points on a plane that has convex polygonal obstacles
as shown in . This is an idealization of the problem that a robot has to
solve to navigate in a crowded environment.

1.  Suppose the state space consists of all positions $(x,y)$ in the
    plane. How many states are there? How many paths are there to the
    goal?

2.  Explain briefly why the shortest path from one polygon vertex to any
    other in the scene must consist of straight-line segments joining
    some of the vertices of the polygons. Define a good state space now.
    How large is this state space?

3.  Define the necessary functions to implement the search problem,
    including an function that takes a vertex as input and returns a set
    of vectors, each of which maps the current vertex to one of the
    vertices that can be reached in a straight line. (Do not forget the
    neighbors on the same polygon.) Use the straight-line distance for
    the heuristic function.

4.  Apply one or more of the algorithms in this chapter to solve a range
    of problems in the domain, and comment on their performance.

[geometric-scene-figure]

[negative-g-exercise]On , we said that we would not consider problems
with negative path costs. In this exercise, we explore this decision in
more depth.

1.  Suppose that actions can have arbitrarily large negative costs;
    explain why this possibility would force any optimal algorithm to
    explore the entire state space.

2.  Does it help if we insist that step costs must be greater than or
    equal to some negative constant $c$? Consider both trees and graphs.

3.  Suppose that a set of actions forms a loop in the state space such
    that executing the set in some order results in no net change to the
    state. If all of these actions have negative cost, what does this
    imply about the optimal behavior for an agent in such an
    environment?

4.  One can easily imagine actions with high negative cost, even in
    domains such as route finding. For example, some stretches of road
    might have such beautiful scenery as to far outweigh the normal
    costs in terms of time and fuel. Explain, in precise terms, within
    the context of state-space search, why humans do not drive around
    scenic loops indefinitely, and explain how to define the state space
    and actions for route finding so that artificial agents can also
    avoid looping.

5.  Can you think of a real domain in which step costs are such as to
    cause looping?

[mc-problem] The problem is usually stated as follows. Three
missionaries and three cannibals are on one side of a river, along with
a boat that can hold one or two people. Find a way to get everyone to
the other side without ever leaving a group of missionaries in one place
outnumbered by the cannibals in that place. This problem is famous in AI
because it was the subject of the first paper that approached problem
formulation from an analytical viewpoint @Amarel:1968.

1.  Formulate the problem precisely, making only those distinctions
    necessary to ensure a valid solution. Draw a diagram of the complete
    state space.

2.  Implement and solve the problem optimally using an appropriate
    search algorithm. Is it a good idea to check for repeated states?

3.  Why do you think people have a hard time solving this puzzle, given
    that the state space is so simple?

Define in your own words the following terms: state, state space, search
tree, search node, goal, action, transition model, and branching factor.

What’s the difference between a world state, a state description, and a
search node? Why is this distinction useful?

An action such as really consists of a long sequence of finer-grained
actions: turn on the car, release the brake, accelerate forward, etc.
Having composite actions of this kind reduces the number of steps in a
solution sequence, thereby reducing the search time. Suppose we take
this to the logical extreme, by making super-composite actions out of
every possible sequence of actions. Then every problem instance is
solved by a single super-composite action, such as . Explain how search
would work in this formulation. Is this a practical approach for
speeding up problem solving?

Does a finite state space always lead to a finite search tree? How about
a finite state space that is a tree? Can you be more precise about what
types of state spaces always lead to finite search trees? (Adapted from
, 1996.)

[graph-separation-property-exercise] Prove that satisfies the graph
separation property illustrated in . (*Hint*: Begin by
showing that the property holds at the start, then show that if it holds
before an iteration of the algorithm, it holds afterwards.) Describe a
search algorithm that violates the property.

Which of the following are true and which are false? Explain your
answers.

1.  Depth-first search always expands at least as many nodes as A search
    with an admissible heuristic.

2.  $h(n)=0$ is an admissible heuristic for the 8-puzzle.

3.  A is of no use in robotics because percepts, states, and actions are
    continuous.

4.  Breadth-first search is complete even if zero step costs are
    allowed.

5.  Assume that a rook can move on a chessboard any number of squares in
    a straight line, vertically or horizontally, but cannot jump over
    other pieces. Manhattan distance is an admissible heuristic for the
    problem of moving the rook from square A to square B in the smallest
    number of moves.

Consider a state space where the start state is number 1 and each state
$k$ has two successors: numbers $2k$ and $2k+1$.

1.  Draw the portion of the state space for states 1 to 15.

2.  Suppose the goal state is 11. List the order in which nodes will be
    visited for breadth-first search, depth-limited search with limit 3,
    and iterative deepening search.

3.  How well would bidirectional search work on this problem? What is
    the branching factor in each direction of the bidirectional search?

4.  Does the answer to (c) suggest a reformulation of the problem that
    would allow you to solve the problem of getting from state 1 to a
    given goal state with almost no search?

5.  Call the action going from $k$ to $2k$ Left, and the action going to
    $2k+1$ Right. Can you find an algorithm that outputs the solution to
    this problem without any search at all?

[brio-figure]

[brio-exercise]A basic wooden railway set contains the pieces shown in .
The task is to connect these pieces into a railway that has no
overlapping tracks and no loose ends where a train could run off onto
the floor.

1.  Suppose that the pieces fit together *exactly* with no
    slack. Give a precise formulation of the task as a search problem.

2.  Identify a suitable uninformed search algorithm for this task and
    explain your choice.

3.  Explain why removing any one of the “fork” pieces makes the problem
    unsolvable.

4.  Give an upper bound on the total size of the state space defined by
    your formulation. (*Hint*: think about the maximum
    branching factor for the construction process and the maximum depth,
    ignoring the problem of overlapping pieces and loose ends. Begin by
    pretending that every piece is unique.)

Implement two versions of the function for the 8-puzzle: one that copies
and edits the data structure for the parent node $s$ and one that
modifies the parent state directly (undoing the modifications as
needed). Write versions of iterative deepening depth-first search that
use these functions and compare their performance.

[iterative-lengthening-exercise]On , we mentioned , an iterative analog
of uniform cost search. The idea is to use increasing limits on path
cost. If a node is generated whose path cost exceeds the current limit,
it is immediately discarded. For each new iteration, the limit is set to
the lowest path cost of any node discarded in the previous iteration.

1.  Show that this algorithm is optimal for general path costs.

2.  Consider a uniform tree with branching factor $b$, solution depth
    $d$, and unit step costs. How many iterations will iterative
    lengthening require?

3.  Now consider step costs drawn from the continuous range
    $[\epsilon,1]$, where $0 < \epsilon < 1$. How many iterations are
    required in the worst case?

4.  Implement the algorithm and apply it to instances of the 8-puzzle
    and traveling salesperson problems. Compare the algorithm’s
    performance to that of uniform-cost search, and comment on your
    results.

Describe a state space in which iterative deepening search performs much
worse than depth-first search (for example, $O(n^{2})$ vs. $O(n)$).

Write a program that will take as input two Web page URLs and find a
path of links from one to the other. What is an appropriate search
strategy? Is bidirectional search a good idea? Could a search engine be
used to implement a predecessor function?

[vacuum-search-exercise]Consider the vacuum-world problem defined in .

1.  Which of the algorithms defined in this chapter would be appropriate
    for this problem? Should the algorithm use tree search or graph
    search?

2.  Apply your chosen algorithm to compute an optimal sequence of
    actions for a $3\stimes 3$ world whose initial state has dirt in the
    three top squares and the agent in the center.

3.  Construct a search agent for the vacuum world, and evaluate its
    performance in a set of $3\stimes 3$ worlds with probability 0.2 of
    dirt in each square. Include the search cost as well as path cost in
    the performance measure, using a reasonable exchange rate.

4.  Compare your best search agent with a simple randomized reflex agent
    that sucks if there is dirt and otherwise moves randomly.

5.  Consider what would happen if the world were enlarged to
    $n \times n$. How does the performance of the search agent and of
    the reflex agent vary with $n$?

[search-special-case-exercise] Prove each of the following statements,
or give a counterexample:

1.  Breadth-first search is a special case of uniform-cost search.

2.  Depth-first search is a special case of best-first tree search.

3.  Uniform-cost search is a special case of A search.

Compare the performance of A and RBFS on a set of randomly generated
problems in the 8-puzzle (with Manhattan distance) and TSP (with MST—see
) domains. Discuss your results. What happens to the performance of RBFS
when a small random number is added to the heuristic values in the
8-puzzle domain?

Trace the operation of A search applied to the problem of getting to
Bucharest from Lugoj using the straight-line distance heuristic. That
is, show the sequence of nodes that the algorithm will consider and the
$f$, $g$, and $h$ score for each node.

Sometimes there is no good evaluation function for a problem but there
is a good comparison method: a way to tell whether one node is better
than another without assigning numerical values to either. Show that
this is enough to do a best-first search. Is there an analog of A for
this setting?

[a\*-failure-exercise]Devise a state space in which A using returns a
suboptimal solution with an $h(n)$ function that is admissible but
inconsistent.

Accurate heuristics don’t necessarily reduce search time in the worst
case. Given any depth $d$, define a search problem with a goal node at
depth $d$, and write a heuristic function such that $|h(n) -
 h^*(n)| \le O(\log h^*(n))$ but A$^*$ expands all nodes of depth less
than $d$.

The  @Pohl:1977 is a best-first search in which the evaluation function
is $f(n) =
(2-w)g(n) + wh(n)$. For what values of $w$ is this complete? For what
values is it optimal, assuming that $h$ is admissible? What kind of
search does this perform for $w=0$, $w=1$, and $w=2$?

Consider the unbounded version of the regular 2D grid shown in . The
start state is at the origin, (0,0), and the goal state is at $(x,y)$.

1.  What is the branching factor $b$ in this state space?

2.  How many distinct states are there at depth $k$ (for $k>0$)?

3.  What is the maximum number of nodes expanded by breadth-first tree
    search?

4.  What is the maximum number of nodes expanded by breadth-first graph
    search?

5.  Is $h = |u-x| + |v-y|$ an admissible heuristic for a state at
    $(u,v)$? Explain.

6.  How many nodes are expanded by A graph search using $h$?

7.  Does $h$ remain admissible if some links are removed?

8.  Does $h$ remain admissible if some links are added between
    nonadjacent states?

$n$ vehicles occupy squares $(1,1)$ through $(n,1)$ (i.e., the bottom
row) of an $n\stimes n$ grid. The vehicles must be moved to the top row
but in reverse order; so the vehicle $i$ that starts in $(i,1)$ must end
up in $(n-i+1,n)$. On each time step, every one of the $n$ vehicles can
move one square up, down, left, or right, or stay put; but if a vehicle
stays put, one other adjacent vehicle (but not more than one) can hop
over it. Two vehicles cannot occupy the same square.

1.  Calculate the size of the state space as a function of $n$.

2.  Calculate the branching factor as a function of $n$.

3.  Suppose that vehicle $i$ is at $(x_i,y_i)$; write a nontrivial
    admissible heuristic $h_i$ for the number of moves it will require
    to get to its goal location $(n-i+1,n)$, assuming no other vehicles
    are on the grid.

4.  Which of the following heuristics are admissible for the problem of
    moving all $n$ vehicles to their destinations? Explain.

    1.  $\sum_{i\eq 1}^{n} h_i$.

    2.  $\max\{h_1,\ldots,h_n\}$.

    3.  $\min\{h_1,\ldots,h_n\}$.

Consider the problem of moving $k$ knights from $k$ starting squares
$s_1,\ldots,s_k$ to $k$ goal squares $g_1,\ldots,g_k$, on an unbounded
chessboard, subject to the rule that no two knights can land on the same
square at the same time. Each action consists of moving *up
to* $k$ knights simultaneously. We would like to complete the
maneuver in the smallest number of actions.

1.  What is the maximum branching factor in this state space, expressed
    as a function of $k$?

2.  Suppose $h_i$ is an admissible heuristic for the problem of moving
    knight $i$ to goal $g_i$ by itself. Which of the following
    heuristics are admissible for the $k$-knight problem? Of those,
    which is the best?

    1.  $\min\{h_1,\ldots,h_k\}$.

    2.  $\max\{h_1,\ldots,h_k\}$.

    3.  $\sum_{i\eq 1}^{k} h_i$.

3.  Repeat (b) for the case where you are allowed to move only one
    knight at a time.

We saw on that the straight-line distance heuristic leads greedy
best-first search astray on the problem of going from Iasi to Fagaras.
However, the heuristic is perfect on the opposite problem: going from
Fagaras to Iasi. Are there problems for which the heuristic is
misleading in both directions?

Invent a heuristic function for the 8-puzzle that sometimes
overestimates, and show how it can lead to a suboptimal solution on a
particular problem. (You can use a computer to help if you want.) Prove
that if $h$ never overestimates by more than $c$, A using $h$ returns a
solution whose cost exceeds that of the optimal solution by no more than
$c$.

[consistent-heuristic-exercise]Prove that if a heuristic is consistent,
it must be admissible. Construct an admissible heuristic that is not
consistent.

[tsp-mst-exercise]The traveling salesperson problem (TSP) can be solved
with the minimum-spanning-tree (MST) heuristic, which estimates the cost
of completing a tour, given that a partial tour has already been
constructed. The MST cost of a set of cities is the smallest sum of the
link costs of any tree that connects all the cities.

1.  Show how this heuristic can be derived from a relaxed version of the
    TSP.

2.  Show that the MST heuristic dominates straight-line distance.

3.  Write a problem generator for instances of the TSP where cities are
    represented by random points in the unit square.

4.  Find an efficient algorithm in the literature for constructing the
    MST, and use it with A graph search to solve instances of the TSP.

[Gaschnig-h-exercise]On , we defined the relaxation of the 8-puzzle in
which a tile can move from square A to square B if B is blank. The exact
solution of this problem defines  @Gaschnig:1979. Explain why Gaschnig’s
heuristic is at least as accurate as $h_1$ (misplaced tiles), and show
cases where it is more accurate than both $h_1$ and $h_2$ (Manhattan
distance). Explain how to calculate Gaschnig’s heuristic efficiently.

We gave two simple heuristics for the 8-puzzle: Manhattan distance and
misplaced tiles. Several heuristics in the literature purport to improve
on this—see, for example, , , and . Test these claims by implementing
the heuristics and comparing the performance of the resulting
algorithms.

[^1]: We are assuming that most readers are in the same position and can
    easily imagine themselves to be as clueless as our agent. We
    apologize to Romanian readers who are unable to take advantage of
    this pedagogical device.

[^2]: Many treatments of problem solving, including previous editions of
    this book, use a , which returns the set of all successors, instead
    of separate and functions. The successor function makes it difficult
    to describe an agent that knows what actions it can try but not what
    they achieve. Also, note some author use instead of , and some use
    instead of .

[^3]: This assumption is algorithmically convenient but also
    theoretically justifiable—see in .

[^4]: The implications of negative costs are explored in .

[^5]: See for a more complete set of definitions and algorithms.

[^6]: ${NoOp}$, or “no operation,” is the name of an assembly language
    instruction that does nothing.

[^7]: Here, and throughout the book, the “star” in $C^*$ means an
    optimal value for $C$.

[^8]: Our first edition called this ; other authors have called it . Our
    more general usage of the latter term follows .

[^9]: With an admissible but inconsistent heuristic, A requires some
    extra bookkeeping to ensure optimality.

[^10]: A rough sketch appeared in the first edition of this book.

[^11]: In Chapters [fol-chapter] and [planning-chapter], we describe
    formal languages suitable for this task; with formal descriptions
    that can be manipulated, the construction of relaxed problems can be
    automated. For now, we use English.

[^12]: Note that a perfect heuristic can be obtained simply by allowing
    $h$ to run a full breadth-first search “on the
    sly.”[perfect-heuristic] Thus, there is a tradeoff between accuracy
    and computation time for heuristic functions.

[^13]: By working backward from the goal, the exact solution cost of
    every instance encountered is immediately available. This is an
    example of , which we discuss further in .
